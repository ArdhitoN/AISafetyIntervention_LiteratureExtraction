{"id": "366ca9a6f026d6c77d5702d277a1cf76", "title": "Will Superhuman AI be created?", "url": "https://aiimpacts.org/argument-for-likelihood-of-superhuman-ai/", "source": "blogs", "source_type": "blog", "text": "*Published 6 Aug 2022*\n\n\n*This page may represent little of what is known on the topic. It is incomplete, under active work and may be updated soon.*\n\n\nSuperhuman AI appears to be very likely to be created at some point.\n\n\nDetails\n-------\n\n\nLet ‘superhuman’ AI be a set of AI systems that together achieve human-level performance across virtually all tasks (i.e. [HLMI](https://aiimpacts.org/human-level-ai/#High_Level_Machine_Intelligence_HLMI)) and substantially surpass human-level performance on some tasks.\n\n\n### Arguments\n\n\n#### A. Superhuman AI is very likely to be physically possible\n\n\n##### 1. Human brains prove that it is physically possible to create human-level intelligence\n\n\nA single human brain is not an existence proof of human-level intelligence according to our definition, because no specific human brain is able to perform at the level of any human brain, on all tasks. Given only the observation of the existence of human brains, it could conceivably be impossible to build a machine that performs any task at the level of any chosen human brain at the total cost of a single human brain. For instance, if each brain could only hold the information required to specialize in one career, then a machine that could do any career would need to store much more information than a brain, and thus could be more expensive.\n\n\nThe entire population of human brains together could be said to perform at ‘human-level’, if the cost of doing a single task is considered to be the cost of the single person’s labor used for that task, rather than the cost of maintaining the entire human race. This seems like a reasonable accounting for the present purposes. Thus, the entire collection of human brains demonstrates that it is physically possible to have a system which can do any task as well as the most proficient human, and can do marginal tasks at the cost of human labor (even if the cost of maintaining the entire system would be much higher, were it not spread between many tasks).\n\n\n##### 2. We know of no reason to expect that human brains are near the limits of possible intelligence\n\n\nHuman brains do appear to be near the limits of performance for some specific tasks. For instance, humans can play tic-tac-toe perfectly. Also for many tasks, human performance reaps a lot of the value potentially available, so it is impossible to perform much better in terms of value (e.g. selecting lunch from a menu, making a booking, recording a phone number). \n\n\nHowever, many tasks do not appear to be like this (e.g. winning at Go), and even for the above mentioned tasks, there is room to carry out the task substantially faster or more cheaply than a human does. Thus there appears to be room for substantially better-than-human performance on a wide range of tasks, though we have not seen a careful accounting of this.\n\n\n##### 3. Artificial minds appear to have some intrinsic advantages over human minds\n\n\na) Human brains developed under constraints that would not apply to artificial brains. In particular, energy use was a more important cost, and there were reproductive constraints to human head size. \n\n\nb) Machines appear to have huge potential performance advantages over biological systems on some fronts. Carlsmith summarizes Bostrom[1](https://aiimpacts.org/argument-for-likelihood-of-superhuman-ai/#easy-footnote-bottom-1-3001 \"Carlsmith, Joseph. “Is Power-Seeking AI an Existential Risk? [Draft].” Open Philanthropy Project, April 2021. <a href=\\\"https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit?usp=embed_facebook\\\">https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit?usp=embed_facebook</a>.<br><br>Bostrom, Nick. <em>Superintelligence: Paths, Dangers, Strategies</em>. 1st edition. Oxford: Oxford University Press, 2014.\"):\n\n\n\n> Thus, as Bostrom (2014, Chapter 3) discusses, where neurons can fire a maximum of hundreds of Hz, the clock speed of modern computers can reach some 2 Ghz — ~ten million times faster. Where action potentials travel at some hundreds of m/s, optical communication can take place at 300,000,000 m/s — ~ a million times faster. Where brain size and neuron count are limited by cranial volume, metabolic constraints, and other factors, supercomputers can be the size of warehouses. And artificial systems need not suffer, either, from the brain’s constraints with respect to memory and component reliability, input/output bandwidth, tiring after hours, degrading after decades, storing its own repair mechanisms and blueprints inside itself, and so forth. Artificial systems can also be edited and duplicated much more easily than brains, and information can be more easily transferred between them.\n> \n> \n\n\n##### 4. Superhuman AI is very likely to be physically possible (from 1-3)\n\n\nThe human species exists (1). There seems little reason to think that no system could perform tasks substantially better (2), and multiple moderately strong reasons to think that more capable systems are possible (3). Thus it seems very likely that more capable systems are possible.\n\n\n##### 5. The likely physical possibility of superhuman AI minds strongly suggests the physical feasibility of creating such minds\n\n\nA superhuman mind is a physically possible object (4). However, that a physical configuration is possible does not imply that bringing about such a configuration intentionally can be feasible in practice. For an example of the difference, a waterfall whose water is in exactly the configuration as that of the Niagara Falls for the last ten minutes is physically possible (the Niagara Falls just did it), yet bringing this about again intentionally may remain intractable forever.\n\n\nIn fact we know that human brains specifically are not only physically possible, but feasible for humans to create. However this is in the form of biological reproduction, and does not appear to straightforwardly imply that humans can create arbitrary different systems with at least the intelligence of humans. That is, human creation of human brains does not obviously imply that it is possible for humans to intentionally create human-level intelligence that isn’t a human brain. \n\n\nHowever it seems strongly suggestive. If a physical configuration is possible, natural reasons it might be intractable to bring about are a) that it is computationally difficult to transform the given reference to an actionable description of the physical state required (e.g. ‘Niagara Falls ten minutes ago’ points at a particular configuration of water, but not in a way that is easily convertible to a detailed specification of the locations of that water[2](https://aiimpacts.org/argument-for-likelihood-of-superhuman-ai/#easy-footnote-bottom-2-3001 \"Another example of evident physical possibility diverging from tractability that is being given the output of a hash function and wanting to create an input that produces that output\")), and b) the actionable description is hard to bring about. For instance, it might require minute manipulation of particles beyond what is feasible today, either to get the required degree of specificity, or to avoid chaotic dynamics taking the system away from the desired state even at a macroscopic level (e.g. even if your description of the starting state of the waterfall is fairly detailed, it will quickly diverge farther from the real waterfall.)\n\n\nThese issues don’t appear to apply to creating superhuman intelligence, so in the absence of other evident defeaters, its physical possibility seems to strongly suggest its physical feasibility.\n\n\n##### 6. Contemporary AI systems exhibit qualitatively similar capabilities to human minds, suggesting modified versions of similar processes would give rise to capabilities matching human minds.\n\n\nThat is, given that current AI techniques create systems that do a lot of human-like tasks at some level of performance, e.g. recognize images and write human-like language, it would be somewhat surprising if getting to human level performance on these tasks required such starkly different methods as to be impossible.\n\n\n##### 7. Superhuman AI systems will very likely be physically feasible to create (from 4-6)\n\n\nSuperhuman AI systems are probably physically possible (4), and this suggests that they are feasible to create (5). Separately, presently feasible AI systems exhibit qualitatively similar behavior to human minds (6), weakly suggesting that systems exhibiting similar behavior at a higher level of performance will also be feasible to create.\n\n\n#### B. If feasible, superhuman AI will very likely be created\n\n\n##### 8. Superhuman AI would appear to be very economically valuable, given its potential to do most human work better or more cheaply\n\n\nWhenever such minds become feasible, by stipulation they will be superior in ways to existing sources of cognitive labor, or those sources would have already constituted superhuman AI. Unless the gap is quite small between human-level AI and the best feasible superhuman AI (which seems unlikely given the large potential room for improvement over human minds), the economic value from superhuman AI should be at least at the scale of the global labor market. \n\n\n##### 9. It appears there will be large incentives to create such systems (from 8)\n\n\nThat a situation would make large amounts of economic value available does not imply that any individual has an incentive to make that situation happen, because the value may not accrue to the possible decision maker. In this case however, substantial parts of the economic value created by superhuman AI systems appear likely to be captured by their creators, by analogy to other commercial software. \n\n\nThese economic incentives may not be the only substantial incentives in play. Creating such systems could incur social or legal consequences which could negate the positive incentives. Thus this step is relatively uncertain.\n\n\n##### 10. Superhuman AI seems likely to be created\n\n\nGiven that creation of a type of machine is physically feasible (7) and strongly incentivized (9), it seems likely that it will be created.\n\n\nNotes\n-----", "date_published": "2022-08-08T09:07:00Z", "authors": ["Katja Grace"], "summaries": [], "initial_source": "aiimpacts"}
{"id": "a49aa94834fea068aba10189aa5576d8", "title": "List of sources arguing against existential risk from AI", "url": "https://aiimpacts.org/list-of-sources-arguing-against-existential-risk-from-ai/", "source": "blogs", "source_type": "blog", "text": "*Published 6 Aug 2022*\n\n\n*This page is incomplete, under active work and may be updated soon.*\n\n\nThis is a bibliography of pieces arguing against the idea that AI poses an existential risk.\n\n\nList\n----\n\n\n**Cegłowski, Maciej. “Superintelligence: The Idea That Eats Smart People.”** *Idle Words* (blog). Accessed December 9, 2021. <https://idlewords.com/talks/superintelligence.htm>.\n\n\n**Garfinkel, Ben, and Lempel, Howie. “How Sure Are We about This AI Stuff?”** 80,000 Hours. Accessed September 16, 2020. <https://80000hours.org/podcast/episodes/ben-garfinkel-classic-ai-risk-arguments/>.\n\n\n**Garfinkel, Ben. *How Sure Are We about This AI Stuff?*** ***(talk)** | EA Global: London 2018*, 2019. <https://www.youtube.com/watch?v=E8PGcoLDjVk>. Also in blog form: <https://ea.greaterwrong.com/posts/9sBAW3qKppnoG3QPq/ben-garfinkel-how-sure-are-we-about-this-ai-stuff>\n\n\n**LeCun, Yann, and Anthony Zador. “Don’t Fear the Terminator.”** Scientific American Blog Network. Accessed December 9, 2021. <https://blogs.scientificamerican.com/observations/dont-fear-the-terminator/>.\n\n\n**Yudkowsky, Eliezer, and Robin Hanson. “The Hanson-Yudkowsky AI-Foom Debate – LessWrong.”** Accessed August 6, 2022. <https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate>.\n\n\n\nSee also\n--------\n\n\n* [List of sources arguing for existential risk from AI](https://aiimpacts.org/list-of-sources-arguing-for-existential-risk-from-ai/)\n* [Is AI an existential threat to humanity?](https://aiimpacts.org/does-ai-pose-an-existential-risk/)\n\n\n*Primary author: Katja Grace*\n\n\nNotes\n-----", "date_published": "2022-08-07T00:05:00Z", "authors": ["Katja Grace"], "summaries": [], "initial_source": "aiimpacts"}
{"id": "daf1ef05a42a68301bbcf322425e6cbd", "title": "List of sources arguing for existential risk from AI", "url": "https://aiimpacts.org/list-of-sources-arguing-for-existential-risk-from-ai/", "source": "blogs", "source_type": "blog", "text": "*Published 6 Aug 2022*\n\n\n*This page is incomplete, under active work and may be updated soon.*\n\n\nThis is a bibliography of pieces arguing that AI poses an existential risk.\n\n\nList\n----\n\n\n**Adamczewski, Tom. “A Shift in Arguments for AI Risk.”** Fragile Credences. Accessed October 20, 2020. <https://fragile-credences.github.io/prioritising-ai/>.\n\n\n**Amodei, Dario, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. “Concrete Problems in AI Safety.”** *ArXiv:1606.06565 [Cs]*, July 25, 2016. <http://arxiv.org/abs/1606.06565>.\n\n\n**Bensinger, Rob, Eliezer Yudkowsky, Richard Ngo, So8res, Holden Karnofsky, Ajeya Cotra, Carl Shulman, and Rohin Shah. “2021 MIRI Conversations – LessWrong.”** Accessed August 6, 2022. <https://www.lesswrong.com/s/n945eovrA3oDueqtq>.\n\n\n**Bostrom, N., *Superintelligence***, Oxford University Press, 2014.\n\n\n**Carlsmith, Joseph. “Is Power-Seeking AI an Existential Risk? [Draft].”** Open Philanthropy Project, April 2021. <https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit?usp=embed_facebook>.\n\n\n**Christian, Brian. *The Alignment Problem: Machine Learning and Human Values*.** W. W. Norton & Company, 2021.\n\n\n**Christiano, Paul. “What Failure Looks Like.”** *AI Alignment Forum* (blog), March 17, 2019. <https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like>.\n\n\n**Dai, Wei. “Comment on Disentangling Arguments for the Importance of AI Safety – LessWrong.”** Accessed December 9, 2021. <https://www.lesswrong.com/posts/JbcWQCxKWn3y49bNB/disentangling-arguments-for-the-importance-of-ai-safety>.\n\n\n**Garfinkel, Ben, Miles Brundage, Daniel Filan, Carrick Flynn, Jelena Luketina, Michael Page, Anders Sandberg, Andrew Snyder-Beattie, and Max Tegmark. “On the Impossibility of Supersized Machines.”** *ArXiv:1703.10987 [Physics]*, March 31, 2017. <http://arxiv.org/abs/1703.10987>.\n\n\n**Hubinger, Evan, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. “Risks from Learned Optimization in Advanced Machine Learning Systems,”** June 5, 2019. <https://arxiv.org/abs/1906.01820v3>.\n\n\n**Ngo, Richard. “Thinking Complete: Disentangling Arguments for the Importance of AI Safety.”** *Thinking Complete* (blog), January 21, 2019. <http://thinkingcomplete.blogspot.com/2019/01/disentangling-arguments-for-importance.html>. (Also [LessWrong](https://www.lesswrong.com/posts/JbcWQCxKWn3y49bNB/disentangling-arguments-for-the-importance-of-ai-safety) and the [Alignment Forum](https://www.alignmentforum.org/posts/JbcWQCxKWn3y49bNB/disentangling-arguments-for-the-importance-of-ai-safety), with relevant comment threads.)\n\n\n**Ngo, Richard. “AGI Safety from First Principles,”** September 28, 2020. <https://www.lesswrong.com/s/mzgtmmTKKn5MuCzFJ>.\n\n\n**Ord, Toby. *The Precipice: Existential Risk and the Future of Humanity*.** Illustrated Edition. New York: Hachette Books, 2020.\n\n\n**Piper, Kelsey. “The Case for Taking AI Seriously as a Threat to Humanity.”** Vox, December 21, 2018. <https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment>.\n\n\n**Russell, Stuart. *Human Compatible: Artificial Intelligence and the Problem of Control*.** Viking, 2019.\n\n\n**Turner, Alexander Matt, Logan Smith, Rohin Shah, Andrew Critch, and Prasad Tadepalli. “Optimal Policies Tend to Seek Power.”** *ArXiv:1912.01683 [Cs]*, December 3, 2021. <http://arxiv.org/abs/1912.01683>.\n\n\n**Yudkowsky, Eliezer. “Artificial Intelligence as a Positive and Negative Factor in Global Risk.”** In *Global Catastrophic Risks*, edited by Nick Bostrom and Milan M. Ćirković, 46. New York, n.d. <https://intelligence.org/files/AIPosNegFactor.pdf>.\n\n\n**Yudkowsky, Eliezer, Rob Bensinger, and So8res. “2022 MIRI Alignment Discussion – LessWrong.”** Accessed August 6, 2022. <https://www.lesswrong.com/s/v55BhXbpJuaExkpcD>.\n\n\n**Yudkowsky, Eliezer, and Robin Hanson. “The Hanson-Yudkowsky AI-Foom Debate – LessWrong.”** Accessed August 6, 2022. <https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate>.\n\n\n\nSee also\n--------\n\n\n* [List of sources arguing against existential risk from AI](https://aiimpacts.org/list-of-sources-arguing-against-existential-risk-from-ai/)\n* [Is AI an existential threat to humanity?](https://aiimpacts.org/does-ai-pose-an-existential-risk/)\n\n\n*Primary author: Katja Grace*\n\n\nNotes\n-----", "date_published": "2022-08-06T23:45:00Z", "authors": ["Katja Grace"], "summaries": [], "initial_source": "aiimpacts"}
