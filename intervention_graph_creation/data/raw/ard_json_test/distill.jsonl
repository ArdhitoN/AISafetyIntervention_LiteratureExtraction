{"id": "06edf116f01d5ef4bb68d6fa102c6127", "title": "Understanding Convolutions on Graphs", "url": "https://distill.pub/2021/understanding-gnns", "source": "distill", "source_type": "blog", "text": "### Contents\n\n\n[Introduction](#introduction)\n[The Challenges of Computation on Graphs](#challenges)\n* [Lack of Consistent Structure](#lack-of-consistent-structure)\n* [Node-Order Equivariance](#node-order)\n* [Scalability](#scalability)\n\n\n[Problem Setting and Notation](#problem-and-notation)\n[Extending Convolutions to Graphs](#extending)\n[Polynomial Filters on Graphs](#polynomial-filters)\n[Modern Graph Neural Networks](#modern-gnns)\n[Interactive Graph Neural Networks](#interactive)\n[From Local to Global Convolutions](#from-local-to-global)\n* [Spectral Convolutions](#spectral)\n* [Global Propagation via Graph Embeddings](#graph-embeddings)\n\n\n[Learning GNN Parameters](#learning)\n\n[Conclusions and Further Reading](#further-reading)\n* [GNNs in Practice](#practical-techniques)\n* [Different Kinds of Graphs](#different-kinds-of-graphs)\n* [Pooling](#pooling)\n\n\n[Supplementary Material](#supplementary)\n* [Reproducing Experiments](#experiments-notebooks)\n* [Recreating Visualizations](#visualizations-notebooks)\n\n\n\n\n\n*This article is one of two Distill publications about graph neural networks.\n Take a look at\n [A Gentle Introduction to Graph Neural Networks](https://distill.pub/2021/gnn-intro/)\n\n for a companion view on many things graph and neural network related.* \n\n\n\n Many systems and interactions - social networks, molecules, organizations, citations, physical models, transactions - can be represented quite naturally as graphs.\n How can we reason about and make predictions within these systems?\n \n\n\n\n One idea is to look at tools that have worked well in other domains: neural networks have shown immense predictive power in a variety of learning tasks.\n However, neural networks have been traditionally used to operate on fixed-size and/or regular-structured inputs (such as sentences, images and video).\n This makes them unable to elegantly process graph-structured data.\n \n\n\n\n![Neural networks generally operate on fixed-size input vectors. How do we input a graph to a neural network?](images/standard-neural-networks.svg \"How do we input a graph to a neural network?\")\n\n\n\n Graph neural networks (GNNs) are a family of neural networks that can operate naturally on graph-structured data. \n By extracting and utilizing features from the underlying graph,\n GNNs can make more informed predictions about entities in these interactions,\n as compared to models that consider individual entities in isolation.\n \n\n\n\n GNNs are not the only tools available to model graph-structured data:\n graph kernels \n and random-walk methods \n were some of the most popular ones.\n Today, however, GNNs have largely replaced these techniques\n because of their inherent flexibility to model the underlying systems\n better.\n \n\n\n\n In this article, we will illustrate\n the challenges of computing over graphs, \n describe the origin and design of graph neural networks,\n and explore the most popular GNN variants in recent times.\n Particularly, we will see that many of these variants\n are composed of similar building blocks.\n \n\n\n\n First, let’s discuss some of the complications that graphs come with.\n \n\n\n\n The Challenges of Computation on Graphs\n-----------------------------------------\n\n\n### \n Lack of Consistent Structure\n\n\n\n Graphs are extremely flexible mathematical models; but this means they lack consistent structure across instances.\n Consider the task of predicting whether a given chemical molecule is toxic  :\n \n\n\n\n![The molecular structure of non-toxic 1,2,6-trigalloyl-glucose.](images/1,2,6-trigalloyl-glucose-molecule.svg)\n![The molecular structure of toxic caramboxin.](images/caramboxin-molecule.svg)\n\n\n\n**Left:** A non-toxic 1,2,6-trigalloyl-glucose molecule.\n\n\n**Right:** A toxic caramboxin molecule.\n\n\n\n Looking at a few examples, the following issues quickly become apparent:\n \n\n\n* Molecules may have different numbers of atoms.\n* The atoms in a molecule may be of different types.\n* Each of these atoms may have different number of connections.\n* These connections can have different strengths.\n\n\n\n Representing graphs in a format that can be computed over is non-trivial,\n and the final representation chosen often depends significantly on the actual problem.\n \n\n\n### \n Node-Order Equivariance\n\n\n\n Extending the point above: graphs often have no inherent ordering present amongst the nodes.\n Compare this to images, where every pixel is uniquely determined by its absolute position within the image!\n \n\n\n\n![Representing the graph as one vector requires us to fix an order on the nodes. But what do we do when the nodes have no inherent order?](images/node-order-alternatives.svg)\n\n Representing the graph as one vector requires us to fix an order on the nodes.\n But what do we do when the nodes have no inherent order?\n **Above:** \n The same graph labelled in two different ways. The alphabets indicate the ordering of the nodes.\n \n\n\n As a result, we would like our algorithms to be node-order equivariant:\n they should not depend on the ordering of the nodes of the graph.\n If we permute the nodes in some way, the resulting representations of \n the nodes as computed by our algorithms should also be permuted in the same way.\n \n\n\n### \n Scalability\n\n\n\n Graphs can be really large! Think about social networks like Facebook and Twitter, which have over a billion users. \n Operating on data this large is not easy.\n \n\n\n\n Luckily, most naturally occuring graphs are ‘sparse’:\n they tend to have their number of edges linear in their number of vertices.\n We will see that this allows the use of clever methods\n to efficiently compute representations of nodes within the graph.\n Further, the methods that we look at here will have significantly fewer parameters\n in comparison to the size of the graphs they operate on.\n \n\n\n\n Problem Setting and Notation\n------------------------------\n\n\n\n There are many useful problems that can be formulated over graphs:\n \n\n\n* **Node Classification:** Classifying individual nodes.\n* **Graph Classification:** Classifying entire graphs.\n* **Node Clustering:** Grouping together similar nodes based on connectivity.\n* **Link Prediction:** Predicting missing links.\n* **Influence Maximization:** Identifying influential nodes.\n\n\n\n![Examples of problems that can be defined over graphs.](images/graph-tasks.svg)\n\n Examples of problems that can be defined over graphs.\n This list is not exhaustive!\n \n\n\n A common precursor in solving many of these problems is **node representation learning**:\n learning to map individual nodes to fixed-size real-valued vectors (called ‘representations’ or ‘embeddings’).\n \n\n\n\n In [Learning GNN Parameters](#learning), we will see how the learnt embeddings can be used for these tasks.\n \n\n\n\n Different GNN variants are distinguished by the way these representations are computed.\n Generally, however, GNNs compute node representations in an iterative process.\n We will use the notation hv(k)h\\_v^{(k)}hv(k)​ to indicate the representation of node vvv after the kthk^{\\text{th}}kth iteration.\n Each iteration can be thought of as the equivalent of a ‘layer’ in standard neural networks.\n \n\n\n\n We will define a graph GGG as a set of nodes, VVV, with a set of edges EEE connecting them.\n Nodes can have individual features as part of the input: we will denote by xvx\\_vxv​ the individual feature for node v∈Vv \\in Vv∈V.\n For example, the ‘node features’ for a pixel in a color image\n would be the red, green and blue channel (RGB) values at that pixel.\n \n\n\n\n For ease of exposition, we will assume GGG is undirected, and all nodes are of the same type.\n These kinds of graphs are called ‘homogeneous’.\n Many of the same ideas we will see here \n apply to other kinds of graphs:\n we will discuss this later in [Different Kinds of Graphs](#different-kinds-of-graphs).\n \n\n\n\n Sometimes we will need to denote a graph property by a matrix MMM,\n where each row MvM\\_vMv​ represents a property corresponding to a particular vertex vvv.\n \n\n\n\n Extending Convolutions to Graphs\n----------------------------------\n\n\n\n Convolutional Neural Networks have been seen to be quite powerful in extracting features from images.\n However, images themselves can be seen as graphs with a very regular grid-like structure,\n where the individual pixels are nodes, and the RGB channel values at each pixel as the node features.\n \n\n\n\n A natural idea, then, is to consider generalizing convolutions to arbitrary graphs. Recall, however, the challenges\n listed out in the [previous section](#challenges): in particular, ordinary convolutions are not node-order invariant, because\n they depend on the absolute positions of pixels.\n It is initially unclear as how to generalize convolutions over grids to convolutions over general graphs,\n where the neighbourhood structure differs from node to node.\n \n The curious reader may wonder if performing some sort of padding and ordering\n could be done to ensure the consistency of neighbourhood structure across nodes.\n This has been attempted with some success ,\n but the techniques we will look at here are more general and powerful.\n \n\n\n\n\n\n\n\n Convolutions in CNNs are inherently localized.\n Neighbours participating in the convolution at the center pixel are highlighted in gray.\n \n\n\n\n\n GNNs can perform localized convolutions mimicking CNNs.\n Hover over a node to see its immediate neighbourhood highlighted on the left.\n The structure of this neighbourhood changes from node to node.\n \n\n\n import {Runtime, Inspector} from \"./observablehq-base/runtime.js\";\n import define from \"./notebooks/neighbourhoods-for-cnns-and-gnns.js\";\n setTimeout(() => {\n new Runtime().module(define, name => {\n if (name === \"cnn\\_svg\") return new Inspector(document.querySelector(\"#observablehq-cnn\\_svg-35509536\"));\n if (name === \"svg\") return new Inspector(document.querySelector(\"#observablehq-svg-35509536\"));\n return [\"adjust\\_dimensions\",\"reset\\_nodes\",\"highlight\\_nodes\",\"get\\_node\\_position\",\"remove\\_old\\_arrows\",\"draw\\_arrows\\_to\\_updated\\_node\",\"add\\_interactivity\",\"on\\_selected\\_node\\_change\",\"updated\\_node\\_position\"].includes(name);\n });\n }, 200);\n \n\n\n We begin by introducing the idea of constructing polynomial filters over node neighbourhoods,\n much like how CNNs compute localized filters over neighbouring pixels.\n Then, we will see how more recent approaches extend on this idea with more powerful mechanisms.\n Finally, we will discuss alternative methods\n that can use ‘global’ graph-level information for computing node representations.\n \n\n\n\n Polynomial Filters on Graphs\n------------------------------\n\n\n### \n The Graph Laplacian\n\n\n\n Given a graph GGG, let us fix an arbitrary ordering of the nnn nodes of GGG.\n We denote the 0−10-10−1 adjacency matrix of GGG by AAA, we can construct the diagonal degree matrix DDD of GGG as: \n \n\n\n\nDv=∑uAvu.\n D\\_v = \\sum\\_u A\\_{vu}.\n Dv​=u∑​Avu​.\n\n\n The degree of node vvv is the number of edges incident at vvv.\n \n\n\n\n where AvuA\\_{vu}Avu​ denotes the entry in the row corresponding to vvv and the column corresponding to uuu\n in the matrix AAA. We will use this notation throughout this section.\n \n\n\n\n Then, the graph Laplacian LLL is the square n×nn \\times nn×n matrix defined as:\n L=D−A.\n L = D - A.\n L=D−A.\n\n\n\n\n![](images/laplacian.svg)\n\n The Laplacian LLL for an undirected graph GGG, with the row corresponding to node C\\textsf{C}C highlighted.\n Zeros in LLL are not displayed above.\n The Laplacian LLL depends only on the structure of the graph GGG, not on any node features.\n \n\n\n The graph Laplacian gets its name from being the discrete analog of the\n [Laplacian operator](https://mathworld.wolfram.com/Laplacian.html)\n from calculus.\n \n\n\n\n Although it encodes precisely the same information as the adjacency matrix AAA\n\n In the sense that given either of the matrices AAA or LLL, you can construct the other.\n ,\n the graph Laplacian has many interesting properties of its own.\n \n The graph Laplacian shows up in many mathematical problems involving graphs:\n [random walks](https://people.math.sc.edu/lu/talks/nankai_2014/spec_nankai_2.pdf),\n [spectral clustering](https://arxiv.org/abs/0711.0189),\n and\n [diffusion](https://www.math.fsu.edu/~bertram/lectures/Diffusion.pdf), to name a few.\n \n We will see some of these properties\n in [a later section](#spectral),\n but will instead point readers to\n [this tutorial](https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf)\n for greater insight into the graph Laplacian.\n \n\n\n### \n Polynomials of the Laplacian\n\n\n\n Now that we have understood what the graph Laplacian is,\n we can build polynomials  of the form:\n pw(L)=w0In+w1L+w2L2+…+wdLd=∑i=0dwiLi.\n p\\_w(L) = w\\_0 I\\_n + w\\_1 L + w\\_2 L^2 + \\ldots + w\\_d L^d = \\sum\\_{i = 0}^d w\\_i L^i.\n pw​(L)=w0​In​+w1​L+w2​L2+…+wd​Ld=i=0∑d​wi​Li.\n Each polynomial of this form can alternately be represented by\n its vector of coefficients w=[w0,…,wd]w = [w\\_0, \\ldots, w\\_d]w=[w0​,…,wd​].\n Note that for every www, pw(L)p\\_w(L)pw​(L) is an n×nn \\times nn×n matrix, just like LLL.\n \n\n\n\n These polynomials can be thought of as the equivalent of ‘filters’ in CNNs,\n and the coefficients www as the weights of the ‘filters’.\n \n\n\n\n For ease of exposition, we will focus on the case where nodes have one-dimensional features:\n each of the xvx\\_vxv​ for v∈Vv \\in Vv∈V is just a real number. \n The same ideas hold when each of the xvx\\_vxv​ are higher-dimensional vectors, as well.\n \n\n\n\n Using the previously chosen ordering of the nodes,\n we can stack all of the node features xvx\\_vxv​\n to get a vector x∈Rnx \\in \\mathbb{R}^nx∈Rn.\n \n\n\n\n![Fixing a node order and collecting all node features into a single vector.](images/node-order-vector.svg)\n\n Fixing a node order (indicated by the alphabets) and collecting all node features into a single vector xxx.\n \n\n\n Once we have constructed the feature vector xxx,\n we can define its convolution with a polynomial filter pwp\\_wpw​ as:\n x′=pw(L) x\n x’ = p\\_w(L) \\ x\n x′=pw​(L) x\n To understand how the coefficients www affect the convolution,\n let us begin by considering the ‘simplest’ polynomial:\n when w0=1w\\_0 = 1w0​=1 and all of the other coefficients are 000.\n In this case, x′x’x′ is just xxx:\n x′=pw(L) x=∑i=0dwiLix=w0Inx=x.\n x’ = p\\_w(L) \\ x = \\sum\\_{i = 0}^d w\\_i L^ix = w\\_0 I\\_n x = x.\n x′=pw​(L) x=i=0∑d​wi​Lix=w0​In​x=x.\n Now, if we increase the degree, and consider the case where\n instead w1=1w\\_1 = 1w1​=1 and and all of the other coefficients are 000.\n Then, x′x’x′ is just LxLxLx, and so:\n xv′=(Lx)v=Lvx=∑u∈GLvuxu=∑u∈G(Dvu−Avu)xu=Dv xv−∑u∈N(v)xu\n \\begin{aligned}\n x’\\_v = (Lx)\\_v &= L\\_v x \\\\ \n &= \\sum\\_{u \\in G} L\\_{vu} x\\_u \\\\ \n &= \\sum\\_{u \\in G} (D\\_{vu} - A\\_{vu}) x\\_u \\\\ \n &= D\\_v \\ x\\_v - \\sum\\_{u \\in \\mathcal{N}(v)} x\\_u\n \\end{aligned}\n xv′​=(Lx)v​​=Lv​x=u∈G∑​Lvu​xu​=u∈G∑​(Dvu​−Avu​)xu​=Dv​ xv​−u∈N(v)∑​xu​​\n We see that the features at each node vvv are combined\n with the features of its immediate neighbours u∈N(v)u \\in \\mathcal{N}(v)u∈N(v).\n \n For readers familiar with\n [Laplacian filtering of images](https://docs.opencv.org/3.4/d5/db5/tutorial_laplace_operator.html),\n this is the exact same idea. When xxx is an image, \n x′=Lxx’ = Lxx′=Lx is exactly the result of applying a ‘Laplacian filter’ to xxx.\n \n\n\n\n\n At this point, a natural question to ask is:\n How does the degree ddd of the polynomial influence the behaviour of the convolution?\n Indeed, it is not too hard to show that:\n This is Lemma 5.2 from .\ndistG(v,u)>i⟹Lvui=0.\n \\text{dist}\\_G(v, u) > i \\quad \\Longrightarrow \\quad L\\_{vu}^i = 0.\n distG​(v,u)>i⟹Lvui​=0.\n \n This implies, when we convolve xxx with pw(L)p\\_w(L)pw​(L) of degree ddd to get x′x’x′:\n xv′=(pw(L)x)v=(pw(L))vx=∑i=0dwiLvix=∑i=0dwi∑u∈GLvuixu=∑i=0dwi∑u∈GdistG(v,u)≤iLvuixu.\n \\begin{aligned}\n x’\\_v = (p\\_w(L)x)\\_v &= (p\\_w(L))\\_v x \\\\\n &= \\sum\\_{i = 0}^d w\\_i L\\_v^i x \\\\\n &= \\sum\\_{i = 0}^d w\\_i \\sum\\_{u \\in G} L\\_{vu}^i x\\_u \\\\\n &= \\sum\\_{i = 0}^d w\\_i \\sum\\_{u \\in G \\atop \\text{dist}\\_G(v, u) \\leq i} L\\_{vu}^i x\\_u.\n \\end{aligned}\n xv′​=(pw​(L)x)v​​=(pw​(L))v​x=i=0∑d​wi​Lvi​x=i=0∑d​wi​u∈G∑​Lvui​xu​=i=0∑d​wi​distG​(v,u)≤iu∈G​∑​Lvui​xu​.​\n\n\n\n Effectively, the convolution at node vvv occurs only with nodes uuu which are not more than ddd hops away.\n Thus, these polynomial filters are localized. The degree of the localization is governed completely by ddd.\n \n\n\n\n To help you understand these ‘polynomial-based’ convolutions better, we have created the visualization below.\n Vary the polynomial coefficients and the input grid xxx to see how the result x′x’x′ of the convolution changes.\n The grid under the arrow shows the equivalent convolutional kernel applied at the highlighted pixel in xxx to get\n the resulting pixel in x′x’x′.\n The kernel corresponds to the row of pw(L)p\\_w(L)pw​(L) for the highlighted pixel.\n Note that even after adjusting for position,\n this kernel is different for different pixels, depending on their position within the grid.\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n import {Runtime, Inspector} from \"./observablehq-base/runtime.js\";\n import define from \"./notebooks/cleaner-interactive-graph-polynomial-convolutions.js\";\n setTimeout(() => {\n new Runtime().module(define, name => {\n if (name === \"grid\\_buttons\\_display\") return new Inspector(document.querySelector(\"#observablehq-grid\\_buttons\\_display-05850d43\"));\n if (name === \"poly\\_color\\_scale\") return new Inspector(document.querySelector(\"#observablehq-poly\\_color\\_scale-05850d43\"));\n if (name === \"poly\\_figcaptions\") return new Inspector(document.querySelector(\"#observablehq-poly\\_figcaptions-05850d43\"));\n if (name === \"poly\\_conv\\_main\\_div\") return new Inspector(document.querySelector(\"#observablehq-poly\\_conv\\_main\\_div-05850d43\"));\n if (name === \"viewof laplacian\\_type\") return new Inspector(document.querySelector(\"#observablehq-viewof-laplacian\\_type-05850d43\"));\n if (name === \"polynomial\\_display\") return new Inspector(document.querySelector(\"#observablehq-polynomial\\_display-05850d43\"));\n if (name === \"poly\\_conv\\_sliders\") return new Inspector(document.querySelector(\"#observablehq-poly\\_conv\\_sliders-05850d43\"));\n if (name === \"highlight\\_selected\\_cell\") return new Inspector(document.querySelector(\"#observablehq-highlight\\_selected\\_cell-05850d43\"));\n if (name === \"reset\\_coeffs\\_button\\_display\") return new Inspector(document.querySelector(\"#observablehq-reset\\_coeffs\\_button\\_display-05850d43\"));\n if (name === \"poly\\_input\\_slider\\_watch\") return new Inspector(document.querySelector(\"#observablehq-poly\\_input\\_slider\\_watch-05850d43\"));\n return [\"svg\",\"draw\\_bottom\\_line\",\"draw\\_arrow\",\"draw\\_original\\_img\",\"draw\\_convolutional\\_kernel\",\"draw\\_updated\\_img\",\"draw\\_static\\_graph\\_orig\",\"draw\\_static\\_graph\\_upd\",\"draw\\_dyn\\_graph\\_orig\",\"draw\\_dyn\\_graph\\_upd\"].includes(name);\n });\n }, 200);\n \n\n\n\n\n Hover over a pixel in the input grid (left, representing xxx)\n to highlight it and see the equivalent convolutional kernel\n for that pixel under the arrow.\n The result x′x’x′ of the convolution is shown on the right:\n note that different convolutional kernels are applied at different pixels,\n depending on their location.\n \n\n\n\n Click on the input grid to toggle pixel values between 000 (white) and 111 (blue).\n To randomize the input grid, press ‘Randomize Grid’. To reset all pixels to 000, press ‘Reset Grid’.\n Use the sliders at the bottom to change the coefficients www.\n To reset all coefficients www to 000, press ‘Reset Coefficients.’\n \n\n\n\n\n### \n ChebNet\n\n\n\n ChebNet  refines this idea of polynomial filters by looking at polynomial filters of the form:\n \npw(L)=∑i=1dwiTi(L~)\n p\\_w(L) = \\sum\\_{i = 1}^d w\\_i T\\_i(\\tilde{L})\n pw​(L)=i=1∑d​wi​Ti​(L~)\n\n where TiT\\_iTi​ is the degree-iii\n[Chebyshev polynomial of the first kind](https://en.wikipedia.org/wiki/Chebyshev_polynomials) and\n L~\\tilde{L}L~ is the normalized Laplacian defined using the largest eigenvalue of LLL:\n \n We discuss the eigenvalues of the Laplacian LLL in more detail in [a later section](#spectral).\n \n\nL~=2Lλmax(L)−In.\n \\tilde{L} = \\frac{2L}{\\lambda\\_{\\max}(L)} - I\\_n.\n L~=λmax​(L)2L​−In​.\n\n What is the motivation behind these choices?\n \n\n\n* LLL is actually positive semi-definite: all of the eigenvalues of LLL are not lesser than 000.\n If λmax(L)>1\\lambda\\_{\\max}(L) > 1λmax​(L)>1, the entries in the powers of LLL rapidly increase in size.\n L~\\tilde{L}L~ is effectively a scaled-down version of LLL, with eigenvalues guaranteed to be in the range [−1,1][-1, 1][−1,1].\n This prevents the entries of powers of L~\\tilde{L}L~ from blowing up.\n Indeed, in the [visualization above](#polynomial-convolutions): we restrict the higher-order coefficients\n when the unnormalized Laplacian LLL is selected, but allow larger values when the normalized Laplacian L~\\tilde{L}L~ is selected,\n in order to show the result x′x’x′ on the same color scale.\n* The Chebyshev polynomials have certain interesting properties that make interpolation more numerically stable.\n We won’t talk about this in more depth here,\n but will advise interested readers to take a look at  as a definitive resource.\n\n\n### \n Polynomial Filters are Node-Order Equivariant\n\n\n\n The polynomial filters we considered here are actually independent of the ordering of the nodes.\n This is particularly easy to see when the degree of the polynomial pwp\\_wpw​ is 111:\n where each node’s feature is aggregated with the sum of its neighbour’s features.\n Clearly, this sum does not depend on the order of the neighbours.\n A similar proof follows for higher degree polynomials:\n the entries in the powers of LLL are equivariant to the ordering of the nodes.\n \n\n\n\n**Details for the Interested Reader**\n\n As above, let’s assume an arbitrary node-order over the nnn nodes of our graph.\n Any other node-order can be thought of as a permutation of this original node-order.\n We can represent any permutation by a\n [permutation matrix](https://en.wikipedia.org/wiki/Permutation_matrix) PPP.\n PPP will always be an orthogonal 0−10-10−1 matrix:\n PPT=PTP=In.\n PP^T = P^TP = I\\_n.\n PPT=PTP=In​.\n Then, we call a function fff node-order equivariant iff for all permutations PPP:\n f(Px)=Pf(x).\n f(Px) = P f(x).\n f(Px)=Pf(x).\n\n When switching to the new node-order using the permutation PPP,\n the quantities below transform in the following way:\n x→PxL→PLPTLi→PLiPT\n \\begin{aligned}\n x &\\to Px \\\\\n L &\\to PLP^T \\\\\n L^i &\\to PL^iP^T\n \\end{aligned}\n xLLi​→Px→PLPT→PLiPT​\n and so, for the case of polynomial filters where f(x)=pw(L) xf(x) = p\\_w(L) \\ xf(x)=pw​(L) x, we can see that:\n f(Px)=∑i=0dwi(PLiPT)(Px)=P∑i=0dwiLix=Pf(x).\n \\begin{aligned}\n f(Px) & = \\sum\\_{i = 0}^d w\\_i (PL^iP^T) (Px) \\\\\n & = P \\sum\\_{i = 0}^d w\\_i L^i x \\\\\n & = P f(x).\n \\end{aligned}\n f(Px)​=i=0∑d​wi​(PLiPT)(Px)=Pi=0∑d​wi​Lix=Pf(x).​ \n as claimed.\n \n\n\n\n### \n Embedding Computation\n\n\n\n We now describe how we can build a graph neural network\n by stacking ChebNet (or any polynomial filter) layers\n one after the other with non-linearities,\n much like a standard CNN.\n In particular, if we have KKK different polynomial filter layers,\n the kthk^{\\text{th}}kth of which has its own learnable weights w(k)w^{(k)}w(k),\n we would perform the following computation:\n \n\n\n\n\n\n\n import {Runtime, Inspector} from \"./observablehq-base/runtime.js\";\n import define from \"./notebooks/updated-chebnet-equations.js\";\n setTimeout(() => {\n new Runtime().module(define, name => {\n if (name === \"cheb\\_figure\") return new Inspector(document.querySelector(\"#observablehq-cheb\\_figure-fa1f970f\"));\n if (name === \"style\") return new Inspector(document.querySelector(\"#observablehq-style-fa1f970f\"));\n });\n }, 200);\n \n\n Note that these networks\n reuse the same filter weights across different nodes,\n exactly mimicking weight-sharing in Convolutional Neural Networks (CNNs)\n which reuse weights for convolutional filters across a grid.\n \n\n\n\n Modern Graph Neural Networks\n------------------------------\n\n\n\n ChebNet was a breakthrough in learning localized filters over graphs,\n and it motivated many to think of graph convolutions from a different perspective.\n \n\n\n\n We return back to the result of convolving xxx by the polynomial kernel pw(L)=Lp\\_w(L) = Lpw​(L)=L,\n focussing on a particular vertex vvv:\n \n(Lx)v=Lvx=∑u∈GLvuxu=∑u∈G(Dvu−Avu)xu=Dv xv−∑u∈N(v)xu\n \\begin{aligned}\n (Lx)\\_v &= L\\_v x \\\\ \n &= \\sum\\_{u \\in G} L\\_{vu} x\\_u \\\\ \n &= \\sum\\_{u \\in G} (D\\_{vu} - A\\_{vu}) x\\_u \\\\ \n &= D\\_v \\ x\\_v - \\sum\\_{u \\in \\mathcal{N}(v)} x\\_u\n \\end{aligned}\n (Lx)v​​=Lv​x=u∈G∑​Lvu​xu​=u∈G∑​(Dvu​−Avu​)xu​=Dv​ xv​−u∈N(v)∑​xu​​\n \n As we noted before, this is a 111-hop localized convolution.\n But more importantly, we can think of this convolution as arising of two steps:\n \n\n\n* Aggregating over immediate neighbour features xux\\_uxu​.\n* Combining with the node’s own feature xvx\\_vxv​.\n\n\n\n**Key Idea:**\n What if we consider different kinds of ‘aggregation’ and ‘combination’ steps,\n beyond what are possible using polynomial filters?\n \n\n\n\n By ensuring that the aggregation is node-order equivariant,\n the overall convolution becomes node-order equivariant.\n \n\n\n\n These convolutions can be thought of as ‘message-passing’ between adjacent nodes:\n after each step, every node receives some ‘information’ from its neighbours.\n \n\n\n\n By iteratively repeating the 111-hop localized convolutions KKK times (i.e., repeatedly ‘passing messages’),\n the receptive field of the convolution effectively includes all nodes upto KKK hops away.\n \n\n\n### \n Embedding Computation\n\n\n\n Message-passing forms the backbone of many GNN architectures today.\n We describe the most popular ones in depth below:\n \n\n\n* Graph Convolutional Networks (GCN)\n* Graph Attention Networks (GAT)\n* Graph Sample and Aggregate (GraphSAGE)\n* Graph Isomorphism Network (GIN)\n\n\n\n\n\n\n\n\n import {Runtime, Inspector} from \"./observablehq-base/runtime.js\"; \n import define from \"./notebooks/interactive-gnn-equations.js\";\n setTimeout(() => {\n new Runtime().module(define, name => {\n if (name === \"fig\\_div\") return Inspector.into(\".interactive-gnn-equations-fig\\_div\")();\n if (name === \"text\\_div\") return Inspector.into(\".interactive-gnn-equations-text\\_div\")();\n if (name === \"interactive\\_list\") return Inspector.into(\".interactive-gnn-equations-interactive\\_list\")();\n if (name === \"style\") return Inspector.into(\".interactive-gnn-equations-style\")();\n });\n }, 200);\n \n\n### \n Thoughts\n\n\n\n An interesting point is to assess different aggregation functions: are some better and others worse?\n  demonstrates that aggregation functions indeed can be compared on how well\n they can uniquely preserve node neighbourhood features;\n we recommend the interested reader take a look at the detailed theoretical analysis there.\n \n\n\n\n Here, we’ve talk about GNNs where the computation only occurs at the nodes.\n More recent GNN models\n such as Message-Passing Neural Networks \n and Graph Networks \n perform computation over the edges as well;\n they compute edge embeddings together with node embeddings.\n This is an even more general framework -\n but the same ‘message passing’ ideas from this section apply.\n \n\n\n\n Interactive Graph Neural Networks\n-----------------------------------\n\n\n\n Below is an interactive visualization of these GNN models on small graphs.\n For clarity, the node features are just real numbers here, shown inside the squares next to each node,\n but the same equations hold when the node features are vectors.\n \n\n\n\n\n\n\n\n\n\n\n\n import {Runtime, Inspector} from \"./observablehq-base/runtime.js\";\n import define from \"./notebooks/interactive-gnn-visualizations.js\";\n setTimeout(() => {\n new Runtime().module(define, name => {\n if (name === \"viz\\_list\") return Inspector.into(\".interactive-gnn-visualizations-viz\\_list\")();\n if (name === \"buttons\") return Inspector.into(\".interactive-gnn-visualizations-buttons\")();\n if (name === \"fig\") return Inspector.into(\".interactive-gnn-visualizations-fig\")();\n if (name === \"eqn\") return Inspector.into(\".interactive-gnn-visualizations-eqn\")();\n if (name === \"network\\_display\\_hack\") return Inspector.into(\".interactive-gnn-visualizations-network\\_display\\_hack\")();\n if (name === \"style\") return Inspector.into(\".interactive-gnn-visualizations-style\")();\n return [\"interactive\\_list\",\"select\\_fig\",\"handle\\_click\"].includes(name) || null;\n });\n }, 200);\n \n\n\n Choose a GNN model using the tabs at the top. Click on a node to see the update equation at that node for the next iteration.\n Use the sliders on the left to change the weights for the current iteration, and watch how the update equation changes. \n \n\n\n In practice, each iteration above is generally thought of as a single ‘neural network layer’.\n This ideology is followed by many popular Graph Neural Network libraries,\n \n For example: [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html)\n and [StellarGraph](https://stellargraph.readthedocs.io/en/stable/api.html#module-stellargraph.layer).\n \n allowing one to compose different types of graph convolutions in the same model.\n \n\n\n\n From Local to Global Convolutions\n-----------------------------------\n\n\n\n The methods we’ve seen so far perform ‘local’ convolutions:\n every node’s feature is updated using a function of its local neighbours’ features.\n \n\n\n\n While performing enough steps of message-passing will eventually ensure that\n information from all nodes in the graph is passed,\n one may wonder if there are more direct ways to perform ‘global’ convolutions.\n \n\n\n\n The answer is yes; we will now describe an approach that was actually first put forward\n in the context of neural networks by ,\n much before any of the GNN models we looked at above.\n \n\n\n### \n Spectral Convolutions\n\n\n\n As before, we will focus on the case where nodes have one-dimensional features.\n After choosing an arbitrary node-order, we can stack all of the node features to get a\n ‘feature vector’ x∈Rnx \\in \\mathbb{R}^nx∈Rn.\n \n\n\n\n**Key Idea:**\n Given a feature vector xxx, \n the Laplacian LLL allows us to quantify how smooth xxx is, with respect to GGG.\n \n\n\n\n How?\n \n\n\n\n After normalizing xxx such that ∑i=1nxi2=1\\sum\\_{i = 1}^n x\\_i^2 = 1∑i=1n​xi2​=1,\n if we look at the following quantity involving LLL:\n \nRLR\\_LRL​ is formally called the [Rayleigh quotient](https://en.wikipedia.org/wiki/Rayleigh_quotient).\n \nRL(x)=xTLxxTx=∑(i,j)∈E(xi−xj)2∑ixi2=∑(i,j)∈E(xi−xj)2.\n R\\_L(x) = \\frac{x^T L x}{x^T x} = \\frac{\\sum\\_{(i, j) \\in E} (x\\_i - x\\_j)^2}{\\sum\\_i x\\_i^2} = \\sum\\_{(i, j) \\in E} (x\\_i - x\\_j)^2.\n RL​(x)=xTxxTLx​=∑i​xi2​∑(i,j)∈E​(xi​−xj​)2​=(i,j)∈E∑​(xi​−xj​)2.\n we immediately see that feature vectors xxx that assign similar values to \n adjacent nodes in GGG (hence, are smooth) would have smaller values of RL(x)R\\_L(x)RL​(x).\n \n\n\n\nLLL is a real, symmetric matrix, which means it has all real eigenvalues λ1≤…≤λn\\lambda\\_1 \\leq \\ldots \\leq \\lambda\\_{n}λ1​≤…≤λn​.\n \n An eigenvalue λ\\lambdaλ of a matrix AAA is a value\n satisfying the equation Au=λuAu = \\lambda uAu=λu for a certain vector uuu, called an eigenvector.\n For a friendly introduction to eigenvectors,\n please see [this tutorial](http://www.sosmath.com/matrix/eigen0/eigen0.html).\n \n Further, the corresponding eigenvectors u1,…,unu\\_1, \\ldots, u\\_{n}u1​,…,un​ can be taken to be orthonormal:\n uk1Tuk2={1 if k1=k2.0 if k1≠k2.\n u\\_{k\\_1}^T u\\_{k\\_2} =\n \\begin{cases}\n 1 \\quad \\text{ if } {k\\_1} = {k\\_2}. \\\\\n 0 \\quad \\text{ if } {k\\_1} \\neq {k\\_2}.\n \\end{cases}\n uk1​T​uk2​​={1 if k1​=k2​.0 if k1​≠k2​.​\n It turns out that these eigenvectors of LLL are successively less smooth, as RLR\\_LRL​ indicates:\n This is the [min-max theorem for eigenvalues.](https://en.wikipedia.org/wiki/Min-max_theorem)\nargminx, x⊥{u1,…,ui−1}RL(x)=ui.minx, x⊥{u1,…,ui−1}RL(x)=λi.\n \\underset{x, \\ x \\perp \\{u\\_1, \\ldots, u\\_{i - 1}\\}}{\\text{argmin}} R\\_L(x) = u\\_i.\n \\qquad\n \\qquad\n \\qquad\n \\min\\_{x, \\ x \\perp \\{u\\_1, \\ldots, u\\_{i - 1}\\}} R\\_L(x) = \\lambda\\_i.\n x, x⊥{u1​,…,ui−1​}argmin​RL​(x)=ui​.x, x⊥{u1​,…,ui−1​}min​RL​(x)=λi​.\n The set of eigenvalues of LLL are called its ‘spectrum’, hence the name!\n We denote the ‘spectral’ decomposition of LLL as:\n L=UΛUT.\n L = U \\Lambda U^T.\n L=UΛUT.\n where Λ\\LambdaΛ is the diagonal matrix of sorted eigenvalues,\n and UUU denotes the matrix of the eigenvectors (sorted corresponding to increasing eigenvalues):\n Λ=[λ1⋱λn]U=[u1 ⋯ un].\n \\Lambda = \\begin{bmatrix}\n \\lambda\\_{1} & & \\\\\n & \\ddots & \\\\\n & & \\lambda\\_{n}\n \\end{bmatrix}\n \\qquad\n \\qquad\n \\qquad\n \\qquad\n U = \\begin{bmatrix} \\\\ u\\_1 \\ \\cdots \\ u\\_n \\\\ \\end{bmatrix}.\n Λ=⎣⎡​λ1​​⋱​λn​​⎦⎤​U=⎣⎡​u1​ ⋯ un​​⎦⎤​.\n The orthonormality condition between eigenvectors gives us that UTU=IU^T U = IUTU=I, the identity matrix.\n As these nnn eigenvectors form a basis for Rn\\mathbb{R}^nRn,\n any feature vector xxx can be represented as a linear combination of these eigenvectors:\n x=∑i=1nxi^ui=Ux^.\n x = \\sum\\_{i = 1}^n \\hat{x\\_i} u\\_i = U \\hat{x}.\n x=i=1∑n​xi​^​ui​=Ux^.\n where x^\\hat{x}x^ is the vector of coefficients [x0,…xn][x\\_0, \\ldots x\\_n][x0​,…xn​].\n We call x^\\hat{x}x^ as the spectral representation of the feature vector xxx.\n The orthonormality condition allows us to state:\n x=Ux^⟺UTx=x^.\n x = U \\hat{x} \\quad \\Longleftrightarrow \\quad U^T x = \\hat{x}.\n x=Ux^⟺UTx=x^.\n This pair of equations allows us to interconvert\n between the ‘natural’ representation xxx and the ‘spectral’ representation x^\\hat{x}x^\n for any vector x∈Rnx \\in \\mathbb{R}^nx∈Rn.\n \n\n\n### \n Spectral Representations of Natural Images\n\n\n\n As discussed before, we can consider any image as a grid graph, where each pixel is a node,\n connected by edges to adjacent pixels.\n Thus, a pixel can have either 3,5,3, 5,3,5, or 888 neighbours, depending on its location within the image grid.\n Each pixel gets a value as part of the image. If the image is grayscale, each value will be a single \n real number indicating how dark the pixel is. If the image is colored, each value will be a 333-dimensional\n vector, indicating the values for the red, green and blue (RGB) channels.\n We use the alpha channel as well in the visualization below, so this is actually RGBA.\n\n\n\n\n This construction allows us to compute the graph Laplacian and the eigenvector matrix UUU.\n Given an image, we can then investigate what its spectral representation looks like.\n \n\n\n\n To shed some light on what the spectral representation actually encodes,\n we perform the following experiment over each channel of the image independently: \n \n\n\n* We first collect all pixel values across a channel into a feature vector xxx.\n* Then, we obtain its spectral representation x^\\hat{x}x^.\n x^=UTx\n \\hat{x} = U^T x\n x^=UTx\n* We truncate this to the first mmm components to get x^m\\hat{x}\\_mx^m​.\n By truncation, we mean zeroing out all of the remaining n−mn - mn−m components of x^\\hat{x}x^.\n This truncation is equivalent to using only the first mmm eigenvectors to compute the spectral representation.\n x^m=Truncatem(x^)\n \\hat{x}\\_m = \\text{Truncate}\\_m(\\hat{x})\n x^m​=Truncatem​(x^)\n* Then, we convert this truncated representation x^m\\hat{x}\\_mx^m​ back to the natural basis to get xmx\\_mxm​.\n xm=Ux^m\n x\\_m = U \\hat{x}\\_m\n xm​=Ux^m​\n\n\n\n Finally, we stack the resulting channels back together to get back an image.\n We can now see how the resulting image changes with choices of mmm.\n Note that when m=nm = nm=n, the resulting image is identical to the original image,\n as we can reconstruct each channel exactly.\n \n\n\n\n\n\n\n\n\n\n\n\n import {Runtime, Inspector} from \"./observablehq-base/runtime.js\"; \n import define from \"./notebooks/spectral-decompositions-of-natural-images.js\";\n setTimeout(() => {\n new Runtime().module(define, name => {\n if (name === \"spectralDecompositionsAll\") return new Inspector(document.querySelector(\"#observablehq-spectralDecompositionsAll-59114e0b\"));\n if (name === \"updateArrowCaption\") return new Inspector(document.querySelector(\"#observablehq-updateArrowCaption-59114e0b\"));\n if (name === \"drawArrow\") return new Inspector(document.querySelector(\"#observablehq-drawArrow-59114e0b\"));\n if (name === \"drawCurrBaseImg\") return new Inspector(document.querySelector(\"#observablehq-drawCurrBaseImg-59114e0b\"));\n if (name === \"drawCurrUpdImg\") return new Inspector(document.querySelector(\"#observablehq-drawCurrUpdImg-59114e0b\"));\n if (name === \"style\") return new Inspector(document.querySelector(\"#observablehq-style-59114e0b\"));\n });\n }, 200);\n \n\n\n\n Use the radio buttons at the top to chose one of the four sample images.\n Each of these images has been taken from the ImageNet \n dataset and downsampled to 505050 pixels wide and 404040 pixels tall.\n As there are n=50×40=2000n = 50 \\times 40 = 2000n=50×40=2000 pixels in each image, there are 200020002000 Laplacian eigenvectors.\n Use the slider at the bottom to change the number of spectral components to keep, noting how\n images get progressively blurrier as the number of components decrease.\n \n\n\n As mmm decreases, we see that the output image xmx\\_mxm​ gets blurrier.\n If we decrease mmm to 111, the output image xmx\\_mxm​ is entirely the same color throughout.\n We see that we do not need to keep all nnn components;\n we can retain a lot of the information in the image with significantly fewer components.\n\n We can relate this to the Fourier decomposition of images:\n the more eigenvectors we use, the higher frequencies we can represent on the grid.\n \n\n\n\n To complement the visualization above,\n we additionally visualize the first few eigenvectors on a smaller 8×88 \\times 88×8 grid below.\n We change the coefficients of the first 101010 out of 646464 eigenvectors\n in the spectral representation\n and see how the resulting image changes:\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n import {Runtime, Inspector} from \"./observablehq-base/runtime.js\"; \n import define from \"./notebooks/interactive-spectral-conversions.js\";\n setTimeout(() => {\n new Runtime().module(define, name => {\n if (name === \"figcaptions\") return new Inspector(document.querySelector(\"#observablehq-figcaptions-6ac8e785\"));\n if (name === \"spec\\_conv\\_main\\_div\") return new Inspector(document.querySelector(\"#observablehq-spec\\_conv\\_main\\_div-6ac8e785\"));\n if (name === \"spec\\_color\\_scale\") return new Inspector(document.querySelector(\"#observablehq-spec\\_color\\_scale-6ac8e785\"));\n if (name === \"subgrid\\_main\\_div\") return new Inspector(document.querySelector(\"#observablehq-subgrid\\_main\\_div-6ac8e785\"));\n if (name === \"spec\\_conv\\_sliders\") return new Inspector(document.querySelector(\"#observablehq-spec\\_conv\\_sliders-6ac8e785\"));\n if (name === \"spec\\_conv\\_buttons\\_display\") return new Inspector(document.querySelector(\"#observablehq-spec\\_conv\\_buttons\\_display-6ac8e785\"));\n if (name === \"spec\\_input\\_slider\\_watch\") return new Inspector(document.querySelector(\"#observablehq-spec\\_input\\_slider\\_watch-6ac8e785\"));\n return [\"svg\",\"draw\\_img\",\"draw\\_static\\_graph\",\"draw\\_dyn\\_graph\",\"draw\\_eigenvectors\"].includes(name);\n });\n }, 200);\n \n\n\n Move the sliders to change the spectral representation x^\\hat{x}x^ (right),\n and see how xxx itself changes on the image (left).\n Note how the first eigenvectors are much ‘smoother’ than the later ones,\n and the many patterns we can make with only 101010 eigenvectors.\n \n\n\n These visualizations should convince you that the first eigenvectors are indeed smooth,\n and the smoothness correspondingly decreases as we consider later eigenvectors.\n \n\n\n\n For any image xxx, we can think of\n the initial entries of the spectral representation x^\\hat{x}x^\n as capturing ‘global’ image-wide trends, which are the low-frequency components,\n while the later entries as capturing ‘local’ details, which are the high-frequency components.\n \n\n\n### Embedding Computation\n\n\n\n We now have the background to understand spectral convolutions\n and how they can be used to compute embeddings/feature representations of nodes.\n \n\n\n\n As before, the model we describe below has KKK layers:\n each layer kkk has learnable parameters w^(k)\\hat{w}^{(k)}w^(k),\n called the ‘filter weights’.\n These weights will be convolved with the spectral representations of the node features.\n As a result, the number of weights needed in each layer is equal to mmm, the number of \n eigenvectors used to compute the spectral representations.\n We had shown in the previous section that we can take m≪nm \\ll nm≪n\n and still not lose out on significant amounts of information.\n \n\n\n\n Thus, convolution in the spectral domain enables the use of significantly fewer parameters\n than just direct convolution in the natural domain.\n Further, by virtue of the smoothness of the Laplacian eigenvectors across the graph,\n using spectral representations automatically enforces an inductive bias for\n neighbouring nodes to get similar representations.\n \n\n\n\n Assuming one-dimensional node features for now,\n the output of each layer is a vector of node representations h(k)h^{(k)}h(k),\n where each node’s representation corresponds to a row\n of the vector.\n \n\n\n\n\n We fix an ordering of the nodes in GGG. This gives us the adjacency matrix AAA and the graph Laplacian LLL,\n allowing us to compute UmU\\_mUm​.\n Finally, we can describe the computation that the layers perform, one after the other:\n \n\n\n\n\n\n import {Runtime, Inspector} from \"./observablehq-base/runtime.js\"; \n import define from \"./notebooks/spectral-convolutions-equation.js\";\n setTimeout(() => {\n new Runtime().module(define, name => {\n if (name === \"spec\\_figure\\_init\") return Inspector.into(\".spec\\_figure\\_init\")();\n if (name === \"spec\\_figure\") return Inspector.into(\".spec\\_figure\")();\n if (name === \"style\") return Inspector.into(\".spec\\_figure\\_style\")();\n });\n }, 200);\n \n\n The method above generalizes easily to the case where each h(k)∈Rdkh^{(k)} \\in \\mathbb{R}^{d\\_k}h(k)∈Rdk​, as well:\n see  for details.\n \n\n\n\n With the insights from the previous section, we see that convolution in the spectral-domain of graphs\n can be thought of as the generalization of convolution in the frequency-domain of images.\n \n\n\n### \n Spectral Convolutions are Node-Order Equivariant\n\n\n\n We can show spectral convolutions are node-order equivariant using a similar approach\n as for Laplacian polynomial filters. \n \n\n\n\n**Details for the Interested Reader** \n\n As in [our proof before](#poly-filters-equivariance),\n let’s fix an arbitrary node-order.\n Then, any other node-order can be represented by a\n permutation of this original node-order.\n We can associate this permutation with its permutation matrix PPP.\n\n Under this new node-order,\n the quantities below transform in the following way:\n x→PxA→PAPTL→PLPTUm→PUm\n \\begin{aligned}\n x &\\to Px \\\\\n A &\\to PAP^T \\\\\n L &\\to PLP^T \\\\\n U\\_m &\\to PU\\_m\n \\end{aligned}\n xALUm​​→Px→PAPT→PLPT→PUm​​\n which implies that, in the embedding computation:\n x^→(PUm)T(Px)=UmTx=x^w^→(PUm)T(Pw)=UmTw=w^g^→g^g→(PUm)g^=P(Umg^)=Pg\n \\begin{aligned}\n \\hat{x} &\\to \\left(PU\\_m\\right)^T (Px) = U\\_m^T x = \\hat{x} \\\\\n \\hat{w} &\\to \\left(PU\\_m\\right)^T (Pw) = U\\_m^T w = \\hat{w} \\\\\n \\hat{g} &\\to \\hat{g} \\\\\n g &\\to (PU\\_m)\\hat{g} = P(U\\_m\\hat{g}) = Pg\n \\end{aligned}\n x^w^g^​g​→(PUm​)T(Px)=UmT​x=x^→(PUm​)T(Pw)=UmT​w=w^→g^​→(PUm​)g^​=P(Um​g^​)=Pg​\n Hence, as σ\\sigmaσ is applied elementwise:\n f(Px)=σ(Pg)=Pσ(g)=Pf(x)\n f(Px) = \\sigma(Pg) = P \\sigma(g) = P f(x)\n f(Px)=σ(Pg)=Pσ(g)=Pf(x)\n as required.\n Further, we see that the spectral quantities x^,w^\\hat{x}, \\hat{w}x^,w^ and g^\\hat{g}g^​\n are unchanged by permutations of the nodes.\n \n Formally, they are what we would call node-order invariant.\n \n\n\n\n\n\n The theory of spectral convolutions is mathematically well-grounded;\n however, there are some key disadvantages that we must talk about:\n \n\n\n* We need to compute the eigenvector matrix UmU\\_mUm​ from LLL. For large graphs, this becomes quite infeasible.\n* Even if we can compute UmU\\_mUm​, global convolutions themselves are inefficient to compute,\n because of the repeated\n multiplications with UmU\\_mUm​ and UmTU\\_m^TUmT​.\n* The learned filters are specific to the input graphs,\n as they are represented in terms\n of the spectral decomposition of input graph Laplacian LLL.\n This means they do not transfer well to new graphs\n which have significantly different structure (and hence, significantly\n different eigenvalues) .\n\n\n\n While spectral convolutions have largely been superseded by\n ‘local’ convolutions for the reasons discussed above,\n there is still much merit to understanding the ideas behind them.\n Indeed, a recently proposed GNN model called Directional Graph Networks\n \n actually uses the Laplacian eigenvectors\n and their mathematical properties\n extensively.\n \n\n\n### \n Global Propagation via Graph Embeddings\n\n\n\n A simpler way to incorporate graph-level information\n is to compute embeddings of the entire graph by pooling node\n (and possibly edge) embeddings,\n and then using the graph embedding to update node embeddings,\n following an iterative scheme similar to what we have looked at here.\n This is an approach used by Graph Networks\n .\n We will briefly discuss how graph-level embeddings\n can be constructed in [Pooling](#pooling).\n However, such approaches tend to ignore the underlying\n topology of the graph that spectral convolutions can capture.\n \n\n\n\n Learning GNN Parameters\n-------------------------\n\n\n\n All of the embedding computations we’ve described here, whether spectral or spatial, are completely differentiable.\n This allows GNNs to be trained in an end-to-end fashion, just like a standard neural network,\n once a suitable loss function L\\mathcal{L}L is defined:\n \n\n\n* **Node Classification**: By minimizing any of the standard losses for classification tasks,\n such as categorical cross-entropy when multiple classes are present:\n L(yv,yv^)=−∑cyvclogyvc^.\n \\mathcal{L}(y\\_v, \\hat{y\\_v}) = -\\sum\\_{c} y\\_{vc} \\log{\\hat{y\\_{vc}}}.\n L(yv​,yv​^​)=−c∑​yvc​logyvc​^​.\n where yvc^\\hat{y\\_{vc}}yvc​^​ is the predicted probability that node vvv is in class ccc.\n GNNs adapt well to the semi-supervised setting, which is when only some nodes in the graph are labelled.\n In this setting, one way to define a loss LG\\mathcal{L}\\_{G}LG​ over an input graph GGG is:\n LG=∑v∈Lab(G)L(yv,yv^)∣Lab(G)∣\n \\mathcal{L}\\_{G} = \\frac{\\sum\\limits\\_{v \\in \\text{Lab}(G)} \\mathcal{L}(y\\_v, \\hat{y\\_v})}{| \\text{Lab}(G) |}\n LG​=∣Lab(G)∣v∈Lab(G)∑​L(yv​,yv​^​)​\n where, we only compute losses over labelled nodes Lab(G)\\text{Lab}(G)Lab(G).\n* **Graph Classification**: By aggregating node representations,\n one can construct a vector representation of the entire graph.\n This graph representation can be used for any graph-level task, even beyond classification.\n See [Pooling](#pooling) for how representations of graphs can be constructed.\n* **Link Prediction**: By sampling pairs of adjacent and non-adjacent nodes,\n and use these vector pairs as inputs to predict the presence/absence of an edge.\n For a concrete example, by minimizing the following ‘logistic regression’-like loss:\n L(yv,yu,evu)=−evulog(pvu)−(1−evu)log(1−pvu)pvu=σ(yvTyu)\n \\begin{aligned}\n \\mathcal{L}(y\\_v, y\\_u, e\\_{vu}) &= -e\\_{vu} \\log(p\\_{vu}) - (1 - e\\_{vu}) \\log(1 - p\\_{vu}) \\\\\n p\\_{vu} &= \\sigma(y\\_v^Ty\\_u)\n \\end{aligned}\n L(yv​,yu​,evu​)pvu​​=−evu​log(pvu​)−(1−evu​)log(1−pvu​)=σ(yvT​yu​)​\n where σ\\sigmaσ is the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function),\n and evu=1e\\_{vu} = 1evu​=1 iff there is an edge between nodes vvv and uuu, being 000 otherwise.\n* **Node Clustering**: By simply clustering the learned node representations.\n\n\n\n The broad success of pre-training for natural language processing models\n such as ELMo  and BERT \n has sparked interest in similar techniques for GNNs\n .\n The key idea in each of these papers is to train GNNs to predict\n local (eg. node degrees, clustering coefficient, masked node attributes)\n and/or global graph properties (eg. pairwise distances, masked global attributes).\n \n\n\n\n Another self-supervised technique is to enforce that neighbouring nodes get similar embeddings,\n mimicking random-walk approaches such as node2vec  and DeepWalk :\n \n\n\nLG=∑v∑u∈NR(v)logexpzvTzu∑u′expzu′Tzu.\n L\\_{G} = \\sum\\_{v} \\sum\\_{u \\in N\\_R(v)} \\log\\frac{\\exp{z\\_v^T z\\_u}}{\\sum\\limits\\_{u’} \\exp{z\\_{u’}^T z\\_u}}.\n LG​=v∑​u∈NR​(v)∑​logu′∑​expzu′T​zu​expzvT​zu​​.\n\n where NR(v)N\\_R(v)NR​(v) is a multi-set of nodes visited when random walks are started from vvv.\n For large graphs, where computing the sum over all nodes may be computationally expensive,\n techniques such as Noise Contrastive Estimation  are especially useful.\n \n\n\n\n\n Conclusion and Further Reading\n--------------------------------\n\n\n\n While we have looked at many techniques and ideas in this article,\n the field of Graph Neural Networks is extremely vast.\n We have been forced to restrict our discussion to a small subset of the entire literature,\n while still communicating the key ideas and design principles behind GNNs.\n We recommend the interested reader take a look at\n  for a more comprehensive survey.\n \n\n\n\n We end with pointers and references for additional concepts readers might be interested in:\n \n\n\n### \n GNNs in Practice\n\n\n\n It turns out that accomodating the different structures of graphs is often hard to do efficiently,\n but we can still represent many GNN update equations using\n as sparse matrix-vector products (since generally, the adjacency matrix is sparse for most real-world graph datasets.)\n For example, the GCN variant discussed here can be represented as:\n h(k)=D−1A⋅h(k−1)W(k)T+h(k−1)B(k)T.\n h^{(k)} = D^{-1} A \\cdot h^{(k - 1)} {W^{(k)}}^T + h^{(k - 1)} {B^{(k)}}^T.\n h(k)=D−1A⋅h(k−1)W(k)T+h(k−1)B(k)T.\n Restructuring the update equations in this way allows for efficient vectorized implementations of GNNs on accelerators\n such as GPUs.\n \n\n\n\n Regularization techniques for standard neural networks,\n such as Dropout ,\n can be applied in a straightforward manner to the parameters\n (for example, zero out entire rows of W(k)W^{(k)}W(k) above).\n However, there are graph-specific techniques such as DropEdge \n that removes entire edges at random from the graph,\n that also boost the performance of many GNN models.\n \n\n\n### \n Different Kinds of Graphs\n\n\n\n Here, we have focused on undirected graphs, to avoid going into too many unnecessary details.\n However, there are some simple variants of spatial convolutions for:\n \n\n\n* Directed graphs: Aggregate across in-neighbourhood and/or out-neighbourhood features.\n* Temporal graphs: Aggregate across previous and/or future node features.\n* Heterogeneous graphs: Learn different aggregation functions for each node/edge type.\n\n\n\n There do exist more sophisticated techniques that can take advantage of the different structures of these graphs:\n see  for more discussion.\n \n\n\n### \n Pooling\n\n\n\n This article discusses how GNNs compute useful representations of nodes.\n But what if we wanted to compute representations of graphs for graph-level tasks (for example, predicting the toxicity of a molecule)?\n \n\n\n\n A simple solution is to just aggregate the final node embeddings and pass them through another neural network PREDICTG\\text{PREDICT}\\_GPREDICTG​:\n hG=PREDICTG(AGGv∈G({hv}))\n h\\_G = \\text{PREDICT}\\_G \\Big( \\text{AGG}\\_{v \\in G}\\left(\\{ h\\_v \\} \\right) \\Big)\n hG​=PREDICTG​(AGGv∈G​({hv​}))\n However, there do exist more powerful techniques for ‘pooling’ together node representations:\n \n\n\n* SortPool: Sort vertices of the graph to get a fixed-size node-order invariant representation of the graph, and then apply any standard neural network architecture.\n* DiffPool: Learn to cluster vertices, build a coarser graph over clusters instead of nodes, then apply a GNN over the coarser graph. Repeat until only one cluster is left.\n* SAGPool: Apply a GNN to learn node scores, then keep only the nodes with the top scores, throwing away the rest. Repeat until only one node is left.\n\n\n\n Supplementary Material\n------------------------\n\n\n### \n Reproducing Experiments\n\n\n\n The experiments from\n [Spectral Representations of Natural Images](#spectral-decompositions-of-natural-images)\n can be reproduced using the following\n Colab ![Google Colaboratory](images/colab.svg) notebook:\n [Spectral Representations of Natural Images](https://colab.research.google.com/github/google-research/google-research/blob/master/understanding_convolutions_on_graphs/SpectralRepresentations.ipynb).\n \n\n\n\n### \n Recreating Visualizations\n\n\n\n To aid in the creation of future interactive articles,\n we have created ObservableHQ\n ![ObservableHQ](images/observable.svg)\n notebooks for each of the interactive visualizations here:\n \n\n\n* [Neighbourhood Definitions for CNNs and GNNs](https://observablehq.com/@ameyasd/neighbourhoods-for-cnns-and-gnns)\n* [Graph Polynomial Convolutions on a Grid](https://observablehq.com/@ameyasd/cleaner-interactive-graph-polynomial-convolutions)\n* [Graph Polynomial Convolutions: Equations](https://observablehq.com/@ameyasd/updated-chebnet-equations)\n* [Modern Graph Neural Networks: Equations](https://observablehq.com/@ameyasd/interactive-gnn-equations)\n* [Modern Graph Neural Networks: Interactive Models](https://observablehq.com/@ameyasd/interactive-gnn-visualizations)\n which pulls together the following standalone notebooks:\n\t+ [Graph Convolutional Networks](https://observablehq.com/@ameyasd/graph-convolutional-networks)\n\t+ [Graph Attention Networks](https://observablehq.com/@ameyasd/graph-attention-networks)\n\t+ [GraphSAGE](https://observablehq.com/@ameyasd/graph-sample-and-aggregate-graphsage)\n\t+ [Graph Isomorphism Networks](https://observablehq.com/@ameyasd/graph-isomorphism-networks)\n* [Laplacian Eigenvectors for Grids](https://observablehq.com/@ameyasd/interactive-spectral-conversions)\n* [Spectral Decomposition of Natural Images](https://observablehq.com/@ameyasd/spectral-decompositions-of-natural-images)\n* [Spectral Convolutions: Equations](https://observablehq.com/@ameyasd/spectral-convolutions-equation)", "date_published": "2021-09-02T20:00:00Z", "authors": ["Ameya Daigavane", "Balaraman Ravindran", "Gaurav Aggarwal"], "summaries": ["Understanding the building blocks and design choices of graph neural networks."], "doi": "10.23915/distill.00032", "journal_ref": "distill-pub", "bibliography": [{"link": "https://doi.org/10.23915/distill.00033", "title": "A Gentle Introduction to Graph Neural Networks"}, {"link": "http://jmlr.org/papers/v11/vishwanathan10a.html", "title": "Graph Kernels"}, {"link": "https://doi.org/10.1145/2939672.2939754", "title": "Node2vec: Scalable Feature Learning for Networks"}, {"link": "https://doi.org/10.1145/2623330.2623732", "title": "DeepWalk: Online Learning of Social Representations"}, {"link": "https://proceedings.neurips.cc/paper/2015/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf", "title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints"}, {"link": "http://proceedings.mlr.press/v70/gilmer17a.html", "title": "Neural Message Passing for Quantum Chemistry"}, {"link": "http://arxiv.org/pdf/0711.0189.pdf", "title": "A Tutorial on Spectral Clustering"}, {"link": "https://proceedings.neurips.cc/paper/2016/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf", "title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering"}, {"link": "http://www.sciencedirect.com/science/article/pii/S1063520310000552", "title": "Wavelets on Graphs via Spectral Graph Theory"}, {"link": "https://books.google.co.in/books?id=8FHf0P3to0UC", "title": "Chebyshev Polynomials"}, {"link": "https://openreview.net/forum?id=SJU4ayYgl", "title": "Semi-Supervised Classification with Graph Convolutional Networks"}, {"link": "https://openreview.net/forum?id=rJXMpikCZ", "title": "Graph Attention Networks"}, {"link": "https://proceedings.neurips.cc/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf", "title": "Inductive Representation Learning on Large Graphs"}, {"link": "https://openreview.net/forum?id=ryGs6iA5Km", "title": "How Powerful are Graph Neural Networks?"}, {"link": "http://arxiv.org/pdf/1806.01261.pdf", "title": "Relational inductive biases, deep learning, and graph networks"}, {"link": "http://arxiv.org/pdf/1312.6203.pdf", "title": "Spectral Networks and Locally Connected Networks on Graphs"}, {"link": "https://doi.org/10.1109/SampTA45681.2019.9030932", "title": "On the Transferability of Spectral Graph Filters"}, {"link": "https://www.aclweb.org/anthology/N19-1423", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"link": "https://openreview.net/forum?id=HJlWWJSFDH", "title": "Strategies for Pre-training Graph Neural Networks"}, {"link": "https://aaai.org/ojs/index.php/AAAI/article/view/6048", "title": "Multi-Stage Self-Supervised Learning for Graph Convolutional Networks on Graphs with Few Labeled Nodes"}, {"link": "http://arxiv.org/pdf/2006.09136.pdf", "title": "When Does Self-Supervision Help Graph Convolutional Networks?"}, {"link": "http://arxiv.org/pdf/2006.10141.pdf", "title": "Self-supervised Learning on Graphs: Deep Insights and New Direction"}, {"link": "http://jmlr.org/papers/v13/gutmann12a.html", "title": "Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics"}, {"link": "https://proceedings.neurips.cc/paper/2013/file/db2b4182156b2f1f817860ac9f409ad7-Paper.pdf", "title": "Learning word embeddings efficiently with noise-contrastive estimation"}, {"link": "https://ieeexplore.ieee.org/document/9046288", "title": "A Comprehensive Survey on Graph Neural Networks"}, {"link": "http://arxiv.org/pdf/1812.08434.pdf", "title": "Graph Neural Networks: A Review of Methods and Applications"}, {"link": "http://jmlr.org/papers/v15/srivastava14a.html", "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"}, {"link": "https://openreview.net/forum?id=Hkx1qkrKPr", "title": "DropEdge: Towards Deep Graph Convolutional Networks on Node Classification"}, {"link": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17146", "title": "An End-to-End Deep Learning Architecture for Graph Classification"}, {"link": "https://proceedings.neurips.cc/paper/2018/file/e77dbaf6759253c7c6d0efc5690369c7-Paper.pdf", "title": "Hierarchical Graph Representation Learning with Differentiable Pooling"}, {"link": "http://proceedings.mlr.press/v97/lee19c.html", "title": "Self-Attention Graph Pooling"}]}
{"id": "dd6a446845d57fed72059ee8bdb37857", "title": "A Gentle Introduction to Graph Neural Networks", "url": "https://distill.pub/2021/gnn-intro", "source": "distill", "source_type": "blog", "text": "*This article is one of two Distill publications about graph neural networks. Take a look at [Understanding Convolutions on Graphs](https://distill.pub/2021/understanding-gnns/) to understand how convolutions over images generalize naturally to convolutions over graphs.*\n\n\nGraphs are all around us; real world objects are often defined in terms of their connections to other things. A set of objects, and the connections between them, are naturally expressed as a *graph*. Researchers have developed neural networks that operate on graph data (called graph neural networks, or GNNs) for over a decade. Recent developments have increased their capabilities and expressive power. We are starting to see practical applications in areas such as antibacterial discovery , physics simulations , fake news detection , traffic prediction  and recommendation systems .\n\n\nThis article explores and explains modern graph neural networks. We divide this work into four parts. First, we look at what kind of data is most naturally phrased as a graph, and some common examples. Second, we explore what makes graphs different from other types of data, and some of the specialized choices we have to make when using graphs. Third, we build a modern GNN, walking through each of the parts of the model, starting with historic modeling innovations in the field. We move gradually from a bare-bones implementation to a state-of-the-art GNN model. Fourth and finally, we provide a GNN playground where you can play around with a real-word task and dataset to build a stronger intuition of how each component of a GNN model contributes to the predictions it makes.\n\n\nTo start, let’s establish what a graph is. A graph represents the relations (*edges*) between a collection of entities (*nodes*). \n\n\n\n\nThree types of attributes we might find in a graph, hover over to highlight each attribute. Other types of graphs and attributes are explored in the [Other types of graphs](#other-types-of-graphs-multigraphs-hypergraphs-hypernodes) section.\n\nTo further describe each node, edge or the entire graph, we can store information in each of these pieces of the graph. \n\n\n\n\nInformation in the form of scalars or embeddings can be stored at each graph node (left) or edge (right).\n\nWe can additionally specialize graphs by associating directionality to edges (*directed, undirected*). \n\n\n![](directed_undirected.e4b1689d.png)\n\nThe edges can be directed, where an edge $e$ has a source node, $v\\_{src}$, and a destination node $v\\_{dst}$. In this case, information flows from $v\\_{src}$ to $v\\_{dst}$. They can also be undirected, where there is no notion of source or destination nodes, and information flows both directions. Note that having a single undirected edge is equivalent to having one directed edge from $v\\_{src}$ to $v\\_{dst}$, and another directed edge from $v\\_{dst}$ to $v\\_{src}$.\n\nGraphs are very flexible data structures, and if this seems abstract now, we will make it concrete with examples in the next section. \n\n\nGraphs and where to find them\n-----------------------------\n\n\nYou’re probably already familiar with some types of graph data, such as social networks. However, graphs are an extremely powerful and general representation of data, we will show two types of data that you might not think could be modeled as graphs: images and text. Although counterintuitive, one can learn more about the symmetries and structure of images and text by viewing them as graphs,, and build an intuition that will help understand other less grid-like graph data, which we will discuss later.\n\n\n### Images as graphs\n\n\nWe typically think of images as rectangular grids with image channels, representing them as arrays (e.g., 244x244x3 floats). Another way to think of images is as graphs with regular structure, where each pixel represents a node and is connected via an edge to adjacent pixels. Each non-border pixel has exactly 8 neighbors, and the information stored at each node is a 3-dimensional vector representing the RGB value of the pixel.\n\n\nA way of visualizing the connectivity of a graph is through its *adjacency matrix*. We order the nodes, in this case each of 25 pixels in a simple 5x5 image of a smiley face, and fill a matrix of $n\\_{nodes} \\times n\\_{nodes}$ with an entry if two nodes share an edge. Note that each of these three representations below are different views of the same piece of data. \n\n\n\n\n\nClick on an image pixel to toggle its value, and see how the graph representation changes.\n\n\n### Text as graphs\n\n\nWe can digitize text by associating indices to each character, word, or token, and representing text as a sequence of these indices. This creates a simple directed graph, where each character or index is a node and is connected via an edge to the node that follows it.\n\n\n\n\n\nEdit the text above to see how the graph representation changes.\n\n\nOf course, in practice, this is not usually how text and images are encoded: these graph representations are redundant since all images and all text will have very regular structures. For instance, images have a banded structure in their adjacency matrix because all nodes (pixels) are connected in a grid. The adjacency matrix for text is just a diagonal line, because each word only connects to the prior word, and to the next one. \n\n\n\nThis representation (a sequence of character tokens) refers to the way text is often represented in RNNs; other models, such as Transformers, can be considered to view text as a fully connected graph where we learn the relationship between tokens. See more in [Graph Attention Networks](#graph-attention-networks).\n\n### Graph-valued data in the wild\n\n\nGraphs are a useful tool to describe data you might already be familiar with. Let’s move on to data which is more heterogeneously structured. In these examples, the number of neighbors to each node is variable (as opposed to the fixed neighborhood size of images and text). This data is hard to phrase in any other way besides a graph.\n\n\n**Molecules as graphs.** Molecules are the building blocks of matter, and are built of atoms and electrons in 3D space. All particles are interacting, but when a pair of atoms are stuck in a stable distance from each other, we say they share a covalent bond. Different pairs of atoms and bonds have different distances (e.g. single-bonds, double-bonds). It’s a very convenient and common abstraction to describe this 3D object as a graph, where nodes are atoms and edges are covalent bonds.  Here are two common molecules, and their associated graphs.\n\n\n\n\n(Left) 3d representation of the Citronellal molecule (Center) Adjacency matrix of the bonds in the molecule (Right) Graph representation of the molecule.\n\n\n\n\n(Left) 3d representation of the Caffeine molecule (Center) Adjacency matrix of the bonds in the molecule (Right) Graph representation of the molecule.\n\n\n**Social networks as graphs.** Social networks are tools to study patterns in collective behaviour of people, institutions and organizations. We can build a graph representing groups of people by modelling individuals as nodes, and their relationships as edges. \n\n\n\n\n(Left) Image of a scene from the play “Othello”. (Center) Adjacency matrix of the interaction between characters in the play. (Right) Graph representation of these interactions.\n\n\nUnlike image and text data, social networks do not have identical adjacency matrices. \n\n\n\n\n(Left) Image of karate tournament. (Center) Adjacency matrix of the interaction between people in a karate club. (Right) Graph representation of these interactions.\n\n\n**Citation networks as graphs.** Scientists routinely cite other scientists’ work when publishing papers. We can visualize these networks of citations as a graph, where each paper is a node, and each *directed* edge is a citation between one paper and another. Additionally, we can add information about each paper into each node, such as a word embedding of the abstract. (see ,  , ). \n\n\n**Other examples.** In computer vision, we sometimes want to tag objects in visual scenes. We can then build graphs by treating these objects as nodes, and their relationships as edges. [Machine learning models](https://www.tensorflow.org/tensorboard/graphs), [programming code](https://openreview.net/pdf?id=BJOFETxR-)  and [math equations](https://openreview.net/forum?id=S1eZYeHFDS) can also be phrased as graphs, where the variables are nodes, and edges are operations that have these variables as input and output. You might see the term “dataflow graph” used in some of these contexts.\n\n\nThe structure of real-world graphs can vary greatly between different types of data — some graphs have many nodes with few connections between them, or vice versa. Graph datasets can vary widely (both within a given dataset, and between datasets) in terms of the number of nodes, edges, and the connectivity of nodes.\n\n\n\n\n\nSummary statistics on graphs found in the real world. Numbers are dependent on featurization decisions. More useful statistics and graphs can be found in KONECT\n\n\n\nWhat types of problems have graph structured data?\n--------------------------------------------------\n\n\nWe have described some examples of graphs in the wild, but what tasks do we want to perform on this data? There are three general types of prediction tasks on graphs: graph-level, node-level, and edge-level. \n\n\nIn a graph-level task, we predict a single property for a whole graph. For a node-level task, we predict some property for each node in a graph. For an edge-level task, we want to predict the property or presence of edges in a graph.\n\n\nFor the three levels of prediction problems described above (graph-level, node-level, and edge-level), we will show that all of the following problems can be solved with a single model class, the GNN. But first, let’s take a tour through the three classes of graph prediction problems in more detail, and provide concrete examples of each.\n\n\n\nThere are other related tasks that are areas of active research. For instance, we might want to [generate graphs](#generative-modelling), or [explain predictions on a graph](#graph-explanations-and-attributions). More topics can be found in the [Into the weeds section](#into-the-weeds) .\n\n### Graph-level task\n\n\nIn a graph-level task, our goal is to predict the property of an entire graph. For example, for a molecule represented as a graph, we might want to predict what the molecule smells like, or whether it will bind to a receptor implicated in a disease.\n\n\n\n\n\nThis is analogous to image classification problems with MNIST and CIFAR, where we want to associate a label to an entire image. With text, a similar problem is sentiment analysis where we want to identify the mood or emotion of an entire sentence at once.\n\n\n### Node-level task\n\n\nNode-level tasks are concerned with predicting the identity or role of each node within a graph.\n\n\nA classic example of a node-level prediction problem is Zach’s karate club. The dataset is a single social network graph made up of individuals that have sworn allegiance to one of two karate clubs after a political rift. As the story goes, a feud between Mr. Hi (Instructor) and John H (Administrator) creates a schism in the karate club. The nodes represent individual karate practitioners, and the edges represent interactions between these members outside of karate. The prediction problem is to classify whether a given member becomes loyal to either Mr. Hi or John H, after the feud. In this case, distance between a node to either the Instructor or Administrator is highly correlated to this label.\n\n\n\n\n\nOn the left we have the initial conditions of the problem, on the right we have a possible solution, where each node has been classified based on the alliance. The dataset can be used in other graph problems like unsupervised learning. \n\nFollowing the image analogy, node-level prediction problems are analogous to *image segmentation*, where we are trying to label the role of each pixel in an image. With text, a similar task would be predicting the parts-of-speech of each word in a sentence (e.g. noun, verb, adverb, etc).\n\n\n### Edge-level task\n\n\nThe remaining prediction problem in graphs is *edge prediction*. \n\n\nOne example of edge-level inference is in image scene understanding. Beyond identifying objects in an image, deep learning models can be used to predict the relationship between them. We can phrase this as an edge-level classification: given nodes that represent the objects in the image, we wish to predict which of these nodes share an edge or what the value of that edge is. If we wish to discover connections between entities, we could consider the graph fully connected and based on their predicted value prune edges to arrive at a sparse graph.\n\n\n\n![](merged.0084f617.png)\n\nIn (b), above, the original image (a) has been segmented into five entities: each of the fighters, the referee, the audience and the mat. (C) shows the relationships between these entities. \n\n\n![](edges_level_diagram.c40677db.png)\n\nOn the left we have an initial graph built from the previous visual scene. On the right is a possible edge-labeling of this graph when some connections were pruned based on the model’s output.\n\nThe challenges of using graphs in machine learning\n--------------------------------------------------\n\n\nSo, how do we go about solving these different graph tasks with neural networks? The first step is to think about how we will represent graphs to be compatible with neural networks.\n\n\nMachine learning models typically take rectangular or grid-like arrays as input. So, it’s not immediately intuitive how to represent them in a format that is compatible with deep learning. Graphs have up to four types of information that we will potentially want to use to make predictions: nodes, edges, global-context and connectivity. The first three are relatively straightforward: for example, with nodes we can form a node feature matrix $N$ by assigning each node an index $i$ and storing the feature for $node\\_i$ in $N$. While these matrices have a variable number of examples, they can be processed without any special techniques.\n\n\nHowever, representing a graph’s connectivity is more complicated. Perhaps the most obvious choice would be to use an adjacency matrix, since this is easily tensorisable. However, this representation has a few drawbacks. From the [example dataset table](#table), we see the number of nodes in a graph can be on the order of millions, and the number of edges per node can be highly variable. Often, this leads to very sparse adjacency matrices, which are space-inefficient.\n\n\nAnother problem is that there are many adjacency matrices that can encode the same connectivity, and there is no guarantee that these different matrices would produce the same result in a deep neural network (that is to say, they are not permutation invariant).\n\n\n\nLearning permutation invariant operations is an area of recent research.\n\nFor example, the  [Othello graph](mols-as-graph-othello)  from before can be described equivalently with these two adjacency matrices. It can also be described with every other possible permutation of the nodes.\n\n\n\n\n![](othello1.246371ea.png)\n![](othello2.6897c848.png)\n\n\nTwo adjacency matrices representing the same graph.\n\n\nThe example below shows every adjacency matrix that can describe this small graph of 4 nodes. This is already a significant number of adjacency matrices–for larger examples like Othello, the number is untenable.\n\n\n\n\nAll of these adjacency matrices represent the same graph. Click on an edge to remove it on a “virtual edge” to add it and the matrices will update accordingly.\n\n\nOne elegant and memory-efficient way of representing sparse matrices is as adjacency lists. These describe the connectivity of edge $e\\_k$ between nodes $n\\_i$ and $n\\_j$ as a tuple (i,j) in the k-th entry of an adjacency list. Since we expect the number of edges to be much lower than the number of entries for an adjacency matrix ($n\\_{nodes}^2$), we avoid computation and storage on the disconnected parts of the graph. \n\n\n\nAnother way of stating this is with Big-O notation, it is preferable to have $O(n\\_{edges})$, rather than $O(n\\_{nodes}^2)$.\n\nTo make this notion concrete, we can see how information in different graphs might be represented under this specification:\n\n\n\n\n\nHover and click on the edges, nodes, and global graph marker to view and change attribute representations. On one side we have a small graph and on the other the information of the graph in a tensor representation.\n\nIt should be noted that the figure uses scalar values per node/edge/global, but most practical tensor representations have vectors per graph attribute. Instead of a node tensor of size $[n\\_{nodes}]$ we will be dealing with node tensors of size $[n\\_{nodes}, node\\_{dim}]$. Same for the other graph attributes.\n\n\nGraph Neural Networks\n---------------------\n\n\nNow that the graph’s description is in a matrix format that is permutation invariant, we will describe using graph neural networks (GNNs) to solve graph prediction tasks. **A GNN is an optimizable transformation on all attributes of the graph (nodes, edges, global-context) that preserves graph symmetries (permutation invariances).** We’re going to build GNNs using the “message passing neural network” framework proposed by Gilmer et al. using the Graph Nets architecture schematics introduced by Battaglia et al. GNNs adopt a “graph-in, graph-out” architecture meaning that these model types accept a graph as input, with information loaded into its nodes, edges and global-context, and progressively transform these embeddings, without changing the connectivity of the input graph. \n\n\n### The simplest GNN\n\n\nWith the numerical representation of graphs that [we’ve constructed above](#graph-to-tensor) (with vectors instead of scalars), we are now ready to build a GNN. We will start with the simplest GNN architecture, one where we learn new embeddings for all graph attributes (nodes, edges, global), but where we do not yet use the connectivity of the graph.\n\n\n\nFor simplicity, the previous diagrams used scalars to represent graph attributes; in practice feature vectors, or embeddings, are much more useful. \n\nThis GNN uses a separate multilayer perceptron (MLP) (or your favorite differentiable model) on each component of a graph; we call this a GNN layer. For each node vector, we apply the MLP and get back a learned node-vector. We do the same for each edge, learning a per-edge embedding, and also for the global-context vector, learning a single embedding for the entire graph.\n\n\n\nYou could also call it a GNN block. Because it contains multiple operations/layers (like a ResNet block).\n\n\n![](arch_independent.0efb8ae7.png)\n\nA single layer of a simple GNN. A graph is the input, and each component (V,E,U) gets updated by a MLP to produce a new graph. Each function subscript indicates a separate function for a different graph attribute at the n-th layer of a GNN model.\n\nAs is common with neural networks modules or layers, we can stack these GNN layers together. \n\n\nBecause a GNN does not update the connectivity of the input graph, we can describe the output graph of a GNN with the same adjacency list and the same number of feature vectors as the input graph. But, the output graph has updated embeddings, since the GNN has updated each of the node, edge and global-context representations.\n\n\n### GNN Predictions by Pooling Information\n\n\nWe have built a simple GNN, but how do we make predictions in any of the tasks we described above?\n\n\nWe will consider the case of binary classification, but this framework can easily be extended to the multi-class or regression case. If the task is to make binary predictions on nodes, and the graph already contains node information, the approach is straightforward — for each node embedding, apply a linear classifier.\n\n\n![](prediction_nodes_nodes.c2c8b4d0.png)\n\nWe could imagine a social network, where we wish to anonymize user data (nodes) by not using them, and only using relational data (edges). One instance of such a scenario is the node task we specified in the  [Node-level task](#node-level-task) subsection. In the Karate club example, this would be just using the number of meetings between people to determine the alliance to Mr. Hi or John H.\n\nHowever, it is not always so simple. For instance, you might have information in the graph stored in edges, but no information in nodes, but still need to make predictions on nodes. We need a way to collect information from edges and give them to nodes for prediction. We can do this by *pooling*. Pooling proceeds in two steps:\n\n\n1. For each item to be pooled, *gather* each of their embeddings and concatenate them into a matrix.\n2. The gathered embeddings are then *aggregated*, usually via a sum operation.\n\n\n\nFor a more in-depth discussion on aggregation operations go to the  [Comparing aggregation operations](#comparing-aggregation-operations)  section.\n\nWe represent the *pooling* operation by the letter $\\rho$, and denote that we are gathering information from edges to nodes as $p\\_{E\\_n \\to V\\_{n}}$. \n\n\n\n\nHover over a node (black node) to visualize which edges are gathered and aggregated to produce an embedding for that target node.\n\nSo If we only have edge-level features, and are trying to predict binary node information, we can use pooling to route (or pass) information to where it needs to go. The model looks like this. \n\n\n![](prediction_edges_nodes.e6796b8e.png)\n\n\n\nIf we only have node-level features, and are trying to predict binary edge-level information, the model looks like this.\n\n\n![](prediction_nodes_edges.26fadbcc.png)\n\nOne example of such a scenario is the edge task we specified in  [Edge level task](#edge-level-task) sub section. Nodes can be recognized as image entities, and we are trying to predict if the entities share a relationship (binary edges).\n\nIf we only have node-level features, and need to predict a binary global property, we need to gather all available node information together and aggregate them. This is similar to *Global Average Pooling* layers in CNNs. The same can be done for edges.\n\n\n![](prediction_nodes_edges_global.7a535eb8.png)\n\nThis is a common scenario for predicting molecular properties. For example, we have atomic information, connectivity and we would like to know the toxicity of a molecule (toxic/not toxic), or if it has a particular odor (rose/not rose).\n\nIn our examples, the classification model *$c$* can easily be replaced with any differentiable model, or adapted to multi-class classification using a generalized linear model.\n\n\n![](Overall.e3af58ab.png)\n\nAn end-to-end prediction task with a GNN model.\n\n\nNow we’ve demonstrated that we can build a simple GNN model, and make binary predictions by routing information between different parts of the graph. This pooling technique will serve as a building block for constructing more sophisticated GNN models. If we have new graph attributes, we just have to define how to pass information from one attribute to another. \n\n\nNote that in this simplest GNN formulation, we’re not using the connectivity of the graph at all inside the GNN layer. Each node is processed independently, as is each edge, as well as the global context. We only use connectivity when pooling information for prediction. \n\n\n### Passing messages between parts of the graph\n\n\nWe could make more sophisticated predictions by using pooling within the GNN layer, in order to make our learned embeddings aware of graph connectivity. We can do this using *message passing*, where neighboring nodes or edges exchange information and influence each other’s updated embeddings.\n\n\nMessage passing works in three steps: \n\n\n1. For each node in the graph, *gather* all the neighboring node embeddings (or messages), which is the $g$ function described above.\n2. Aggregate all messages via an aggregate function (like sum).\n3. All pooled messages are passed through an *update function*, usually a learned neural network.\n\n\n\nYou could also 1) gather messages, 3) update them and 2) aggregate them and still have a permutation invariant operation.\n\nJust as pooling can be applied to either nodes or edges, message passing can occur between either nodes or edges.\n\n\nThese steps are key for leveraging the connectivity of graphs. We will build more elaborate variants of message passing in GNN layers that yield GNN models of increasing expressiveness and power. \n\n\n\n\nHover over a node, to highlight adjacent nodes and visualize the adjacent embedding that would be pooled, updated and stored.\n\n\nThis sequence of operations, when applied once, is the simplest type of message-passing GNN layer.\n\n\nThis is reminiscent of standard convolution: in essence, message passing and convolution are operations to aggregate and process the information of an element’s neighbors in order to update the element’s value. In graphs, the element is a node, and in images, the element is a pixel. However, the number of neighboring nodes in a graph can be variable, unlike in an image where each pixel has a set number of neighboring elements.\n\n\nBy stacking message passing GNN layers together, a node can eventually incorporate information from across the entire graph: after three layers, a node has information about the nodes three steps away from it.\n\n\nWe can update our architecture diagram to include this new source of information for nodes:\n\n\n![](arch_gcn.40871750.png)\n\nSchematic for a GCN architecture, which updates node representations of a graph by pooling neighboring nodes at a distance of one degree.\n\n### Learning edge representations\n\n\nOur dataset does not always contain all types of information (node, edge, and global context). \nWhen we want to make a prediction on nodes, but our dataset only has edge information, we showed above how to use pooling to route information from edges to nodes, but only at the final prediction step of the model. We can share information between nodes and edges within the GNN layer using message passing.\n\n\nWe can incorporate the information from neighboring edges in the same way we used neighboring node information earlier, by first pooling the edge information, transforming it with an update function, and storing it.\n\n\nHowever, the node and edge information stored in a graph are not necessarily the same size or shape, so it is not immediately clear how to combine them. One way is to learn a linear mapping from the space of edges to the space of nodes, and vice versa. Alternatively, one may concatenate them together before the update function.\n\n\n![](arch_mpnn.a13c2294.png)\n\nArchitecture schematic for Message Passing layer. The first step “prepares” a message composed of information from an edge and it’s connected nodes and then “passes” the message to the node.\n\nWhich graph attributes we update and in which order we update them is one design decision when constructing GNNs. We could choose whether to update node embeddings before edge embeddings, or the other way around. This is an open area of research with a variety of solutions– for example we could update in a ‘weave’ fashion where we have four updated representations that get combined into new node and edge representations: node to node (linear), edge to edge (linear), node to edge (edge layer), edge to node (node layer).\n\n\n![](arch_weave.352befc0.png)\n\nSome of the different ways we might combine edge and node representation in a GNN layer.\n\n### Adding global representations\n\n\nThere is one flaw with the networks we have described so far: nodes that are far away from each other in the graph may never be able to efficiently transfer information to one another, even if we apply message passing several times. For one node, If we have k-layers, information will propagate at most k-steps away. This can be a problem for situations where the prediction task depends on nodes, or groups of nodes, that are far apart. One solution would be to have all nodes be able to pass information to each other. \nUnfortunately for large graphs, this quickly becomes computationally expensive (although this approach, called ‘virtual edges’, has been used for small graphs such as molecules).\n\n\nOne solution to this problem is by using the global representation of a graph (U) which is sometimes called a **master node**  or context vector. This global context vector is connected to all other nodes and edges in the network, and can act as a bridge between them to pass information, building up a representation for the graph as a whole. This creates a richer and more complex representation of the graph than could have otherwise been learned. \n\n\n![](arch_graphnet.b229be6d.png)\nSchematic of a Graph Nets architecture leveraging global representations.\n\nIn this view all graph attributes have learned representations, so we can leverage them during pooling by conditioning the information of our attribute of interest with respect to the rest. For example, for one node we can consider information from neighboring nodes, connected edges and the global information. To condition the new node embedding on all these possible sources of information, we can simply concatenate them. Additionally we may also map them to the same space via a linear map and add them or apply a feature-wise modulation layer, which can be considered a type of featurize-wise attention mechanism.\n\n\n![](graph_conditioning.3017e214.png)\nSchematic for conditioning the information of one node based on three other embeddings (adjacent nodes, adjacent edges, global). This step corresponds to the node operations in the Graph Nets Layer. \n\nGNN playground\n--------------\n\n\nWe’ve described a wide range of GNN components here, but how do they actually differ in practice? This GNN playground allows you to see how these different components and architectures contribute to a GNN’s ability to learn a real task. \n\n\nOur playground shows a graph-level prediction task with small molecular graphs. We use the the Leffingwell Odor Dataset, which is composed of molecules with associated odor percepts (labels). Predicting the relation of a molecular structure (graph) to its smell is a 100 year-old problem straddling chemistry, physics, neuroscience, and machine learning.\n\n\nTo simplify the problem, we consider only a single binary label per molecule, classifying if a molecular graph smells “pungent” or not, as labeled by a professional perfumer. We say a molecule has a “pungent” scent if it has a strong, striking smell. For example, garlic and mustard, which might contain the molecule *allyl alcohol* have this quality. The molecule *piperitone*, often used for peppermint-flavored candy, is also described as having a pungent smell.\n\n\nWe represent each molecule as a graph, where atoms are nodes containing a one-hot encoding for its atomic identity (Carbon, Nitrogen, Oxygen, Fluorine) and bonds are edges containing a one-hot encoding its bond type (single, double, triple or aromatic). \n\n\nOur general modeling template for this problem will be built up using sequential GNN layers, followed by a linear model with a sigmoid activation for classification. The design space for our GNN has many levers that can customize the model:\n\n\n1. The number of GNN layers, also called the *depth*.\n2. The dimensionality of each attribute when updated. The update function is a 1-layer MLP with a relu activation function and a layer norm for normalization of activations.\n3. The aggregation function used in pooling: max, mean or sum.\n4. The graph attributes that get updated, or styles of message passing: nodes, edges and global representation. We control these via boolean toggles (on or off). A baseline model would be a graph-independent GNN (all message-passing off) which aggregates all data at the end into a single global attribute. Toggling on all message-passing functions yields a GraphNets architecture.\n\n\nTo better understand how a GNN is learning a task-optimized representation of a graph, we also look at the penultimate layer activations of the GNN. These ‘graph embeddings’ are the outputs of the GNN model right before prediction. Since we are using a generalized linear model for prediction, a linear mapping is enough to allow us to see how we are learning representations around the decision boundary. \n\n\nSince these are high dimensional vectors, we reduce them to 2D via principal component analysis (PCA). \nA perfect model would visibility separate labeled data, but since we are reducing dimensionality and also have imperfect models, this boundary might be harder to see.\n\n\nPlay around with different model architectures to build your intuition. For example, see if you can edit the molecule on the left to make the model prediction increase. Do the same edits have the same effects for different model architectures?\n\n\nThis playground is running live on the browser in [tfjs](https://www.tensorflow.org/js/).\n\nEdit the molecule to see how the prediction changes, or change the model params to load a different model. Select a different molecule in the scatter plot.\n### Some empirical GNN design lessons\n\n\nWhen exploring the architecture choices above, you might have found some models have better performance than others. Are there some clear GNN design choices that will give us better performance? For example, do deeper GNN models perform better than shallower ones? or is there a clear choice between aggregation functions? The answers are going to depend on the data,  , and even different ways of featurizing and constructing graphs can give different answers.\n\n\nWith the following interactive figure, we explore the space of GNN architectures and the performance of this task across a few major design choices: Style of message passing, the dimensionality of embeddings, number of layers, and aggregation operation type.\n\n\nEach point in the scatter plot represents a model: the x axis is the number of trainable variables, and the y axis is the performance. Hover over a point to see the GNN architecture parameters.\n\n\n\n\nvar spec = \"BasicArchitectures.json\";\nvegaEmbed('#BasicArchitectures', spec).then(function (result) {// Access the Vega view instance (https://vega.github.io/vega/docs/api/view/) as result.view\n}).catch(console.error);\nScatterplot of each model’s performance vs its number of trainable variables. Hover over a point to see the GNN architecture parameters.\n\nThe first thing to notice is that, surprisingly, a higher number of parameters does correlate with higher performance. GNNs are a very parameter-efficient model type: for even a small number of parameters (3k) we can already find models with high performance. \n\n\nNext, we can look at the distributions of performance aggregated based on the dimensionality of the learned representations for different graph attributes.\n\n\n\n\nvar spec = \"ArchitectureNDim.json\";\nvegaEmbed('#ArchitectureNDim', spec).then(function (result) {// Access the Vega view instance (https://vega.github.io/vega/docs/api/view/) as result.view\n}).catch(console.error);\nAggregate performance of models across varying node, edge, and global dimensions.\n\nWe can notice that models with higher dimensionality tend to have better mean and lower bound performance but the same trend is not found for the maximum. Some of the top-performing models can be found for smaller dimensions. Since higher dimensionality is going to also involve a higher number of parameters, these observations go in hand with the previous figure.\n\n\nNext we can see the breakdown of performance based on the number of GNN layers.\n\n\n\n\nvar spec = \"ArchitectureNLayers.json\";\nvegaEmbed('#ArchitectureNLayers', spec).then(function (result) {// Access the Vega view instance (https://vega.github.io/vega/docs/api/view/) as result.view\n}).catch(console.error);\n Chart of number of layers vs model performance, and scatterplot of model performance vs number of parameters. Each point is colored by the number of layers. Hover over a point to see the GNN architecture parameters.\n\nThe box plot shows a similar trend, while the mean performance tends to increase with the number of layers, the best performing models do not have three or four layers, but two. Furthermore, the lower bound for performance decreases with four layers. This effect has been observed before, GNN with a higher number of layers will broadcast information at a higher distance and can risk having their node representations ‘diluted’ from many successive iterations .\n\n\nDoes our dataset have a preferred aggregation operation? Our following figure breaks down performance in terms of aggregation type.\n\n\n\n\nvar spec = \"ArchitectureAggregation.json\";\nvegaEmbed('#ArchitectureAggregation', spec).then(function (result) {// Access the Vega view instance (https://vega.github.io/vega/docs/api/view/) as result.view\n}).catch(console.error);\nChart of aggregation type vs model performance, and scatterplot of model performance vs number of parameters. Each point is colored by aggregation type. Hover over a point to see the GNN architecture parameters.\n\nOverall it appears that sum has a very slight improvement on the mean performance, but max or mean can give equally good models. This is useful to contextualize when looking at the  [discriminatory/expressive capabilities](#comparing-aggregation-operations) of aggregation operations .\n\n\nThe previous explorations have given mixed messages. We can find mean trends where more complexity gives better performance but we can find clear counterexamples where models with fewer parameters, number of layers, or dimensionality perform better. One trend that is much clearer is about the number of attributes that are passing information to each other.\n\n\nHere we break down performance based on the style of message passing. On both extremes, we consider models that do not communicate between graph entities (“none”) and models that have messaging passed between nodes, edges, and globals.\n\n\n\n\nvar spec = \"ArchitectureMessagePassing.json\";\nvegaEmbed('#ArchitectureMessagePassing', spec).then(function (result) {// Access the Vega view instance (https://vega.github.io/vega/docs/api/view/) as result.view\n}).catch(console.error);\nChart of message passing vs model performance, and scatterplot of model performance vs number of parameters. Each point is colored by message passing. Hover over a point to see the GNN architecture parameters\n\nOverall we see that the more graph attributes are communicating, the better the performance of the average model. Our task is centered on global representations, so explicitly learning this attribute also tends to improve performance. Our node representations also seem to be more useful than edge representations, which makes sense since more information is loaded in these attributes.\n\n\nThere are many directions you could go from here to get better performance. We wish two highlight two general directions, one related to more sophisticated graph algorithms and another towards the graph itself.\n\n\nUp until now, our GNN is based on a neighborhood-based pooling operation. There are some graph concepts that are harder to express in this way, for example a linear graph path (a connected chain of nodes). Designing new mechanisms in which graph information can be extracted, executed and propagated in a GNN is one current research area , , , .\n\n\nOne of the frontiers of GNN research is not making new models and architectures, but “how to construct graphs”, to be more precise, imbuing graphs with additional structure or relations that can be leveraged. As we loosely saw, the more graph attributes are communicating the more we tend to have better models. In this particular case, we could consider making molecular graphs more feature rich, by adding additional spatial relationships between nodes, adding edges that are not bonds, or explicit learnable relationships between subgraphs.\n\n\nSee more in [Other types of graphs](#Other-types-of-graphs ).\nInto the Weeds\n--------------\n\n\nNext, we have a few sections on a myriad of graph-related topics that are relevant for GNNs.\n\n\n### Other types of graphs (multigraphs, hypergraphs, hypernodes, hierarchical graphs)\n\n\nWhile we only described graphs with vectorized information for each attribute, graph structures are more flexible and can accommodate other types of information. Fortunately, the message passing framework is flexible enough that often adapting GNNs to more complex graph structures is about defining how information is passed and updated by new graph attributes. \n\n\nFor example, we can consider multi-edge graphs or *multigraphs*, where a pair of nodes can share multiple types of edges, this happens when we want to model the interactions between nodes differently based on their type. For example with a social network, we can specify edge types based on the type of relationships (acquaintance, friend, family). A GNN can be adapted by having different types of message passing steps for each edge type. \nWe can also consider nested graphs, where for example a node represents a graph, also called a hypernode graph. Nested graphs are useful for representing hierarchical information. For example, we can consider a network of molecules, where a node represents a molecule and an edge is shared between two molecules if we have a way (reaction) of transforming one to the other  .\nIn this case, we can learn on a nested graph by having a GNN that learns representations at the molecule level and another at the reaction network level, and alternate between them during training.\n\n\nAnother type of graph is a hypergraph, where an edge can be connected to multiple nodes instead of just two. For a given graph, we can build a hypergraph by identifying communities of nodes and assigning a hyper-edge that is connected to all nodes in a community.\n\n\n![](multigraphs.1bb84306.png)\nSchematic of more complex graphs. On the left we have an example of a multigraph with three edge types, including a directed edge. On the right we have a three-level hierarchical graph, the intermediate level nodes are hypernodes.\n\nHow to train and design GNNs that have multiple types of graph attributes is a current area of research , .\n\n\n### Sampling Graphs and Batching in GNNs\n\n\nA common practice for training neural networks is to update network parameters with gradients calculated on randomized constant size (batch size) subsets of the training data (mini-batches). This practice presents a challenge for graphs due to the variability in the number of nodes and edges adjacent to each other, meaning that we cannot have a constant batch size. The main idea for batching with graphs is to create subgraphs that preserve essential properties of the larger graph. This graph sampling operation is highly dependent on context and involves sub-selecting nodes and edges from a graph. These operations might make sense in some contexts (citation networks) and in others, these might be too strong of an operation (molecules, where a subgraph simply represents a new, smaller molecule). How to sample a graph is an open research question. \nIf we care about preserving structure at a neighborhood level, one way would be to randomly sample a uniform number of nodes, our *node-set*. Then add neighboring nodes of distance k adjacent to the node-set, including their edges. Each neighborhood can be considered an individual graph and a GNN can be trained on batches of these subgraphs. The loss can be masked to only consider the node-set since all neighboring nodes would have incomplete neighborhoods.\nA more efficient strategy might be to first randomly sample a single node, expand its neighborhood to distance k, and then pick the other node within the expanded set. These operations can be terminated once a certain amount of nodes, edges, or subgraphs are constructed.\nIf the context allows, we can build constant size neighborhoods by picking an initial node-set and then sub-sampling a constant number of nodes (e.g randomly, or via a random walk or Metropolis algorithm).\n\n\n![](sampling.968003b3.png)\nFour different ways of sampling the same graph. Choice of sampling strategy depends highly on context since they will generate different distributions of graph statistics (# nodes, #edges, etc.). For highly connected graphs, edges can be also subsampled. \n\nSampling a graph is particularly relevant when a graph is large enough that it cannot be fit in memory. Inspiring new architectures and training strategies such as Cluster-GCN  and GraphSaint . We expect graph datasets to continue growing in size in the future.\n\n\n### Inductive biases\n\n\nWhen building a model to solve a problem on a specific kind of data, we want to specialize our models to leverage the characteristics of that data. When this is done successfully, we often see better predictive performance, lower training time, fewer parameters and better generalization. \n\n\nWhen labeling on images, for example, we want to take advantage of the fact that a dog is still a dog whether it is in the top-left or bottom-right corner of an image. Thus, most image models use convolutions, which are translation invariant. For text, the order of the tokens is highly important, so recurrent neural networks process data sequentially. Further, the presence of one token (e.g. the word ‘not’) can affect the meaning of the rest of a sentence, and so we need components that can ‘attend’ to other parts of the text, which transformer models like BERT and GPT-3 can do. These are some examples of inductive biases, where we are identifying symmetries or regularities in the data and adding modelling components that take advantage of these properties.\n\n\nIn the case of graphs, we care about how each graph component (edge, node, global) is related to each other so we seek models that have a relational inductive bias. A model should preserve explicit relationships between entities (adjacency matrix) and preserve graph symmetries (permutation invariance). We expect problems where the interaction between entities is important will benefit from a graph structure. Concretely, this means designing transformation on sets: the order of operation on nodes or edges should not matter and the operation should work on a variable number of inputs. \n\n\n### Comparing aggregation operations\n\n\nPooling information from neighboring nodes and edges is a critical step in any reasonably powerful GNN architecture. Because each node has a variable number of neighbors, and because we want a differentiable method of aggregating this information, we want to use a smooth aggregation operation that is invariant to node ordering and the number of nodes provided.\n\n\nSelecting and designing optimal aggregation operations is an open research topic. A desirable property of an aggregation operation is that similar inputs provide similar aggregated outputs, and vice-versa. Some very simple candidate permutation-invariant operations are sum, mean, and max. Summary statistics like variance also work. All of these take a variable number of inputs, and provide an output that is the same, no matter the input ordering. Let’s explore the difference between these operations.\n\n\n \n\nNo pooling type can always distinguish between graph pairs such as max pooling on the left and sum / mean pooling on the right. \n\nThere is no operation that is uniformly the best choice. The mean operation can be useful when nodes have a highly-variable number of neighbors or you need a normalized view of the features of a local neighborhood. The max operation can be useful when you want to highlight single salient features in local neighborhoods. Sum provides a balance between these two, by providing a snapshot of the local distribution of features, but because it is not normalized, can also highlight outliers. In practice, sum is commonly used. \n\n\nDesigning aggregation operations is an open research problem that intersects with machine learning on sets. New approaches such as Principal Neighborhood aggregation take into account several aggregation operations by concatenating them and adding a scaling function that depends on the degree of connectivity of the entity to aggregate. Meanwhile, domain specific aggregation operations can also be designed. One example lies with the “Tetrahedral Chirality” aggregation operators .\n\n\n### GCN as subgraph function approximators\n\n\nAnother way to see GCN (and MPNN) of k-layers with a 1-degree neighbor lookup is as a neural network that operates on learned embeddings of subgraphs of size k.\n\n\nWhen focusing on one node, after k-layers, the updated node representation has a limited viewpoint of all neighbors up to k-distance, essentially a subgraph representation. Same is true for edge representations.\n\n\nSo a GCN is collecting all possible subgraphs of size k and learning vector representations from the vantage point of one node or edge. The number of possible subgraphs can grow combinatorially, so enumerating these subgraphs from the beginning vs building them dynamically as in a GCN, might be prohibitive.\n\n\n![](arch_subgraphs.197f9b0e.png)\n\n\n### Edges and the Graph Dual\n\n\nOne thing to note is that edge predictions and node predictions, while seemingly different, often reduce to the same problem: an edge prediction task on a graph $G$ can be phrased as a node-level prediction on $G$’s dual.\n\n\nTo obtain $G$’s dual, we can convert nodes to edges (and edges to nodes). A graph and its dual contain the same information, just expressed in a different way. Sometimes this property makes solving problems easier in one representation than another, like frequencies in Fourier space. In short, to solve an edge classification problem on $G$, we can think about doing graph convolutions on $G$’s dual (which is the same as learning edge representations on $G$), this idea was developed with Dual-Primal Graph Convolutional Networks.\n\n\n\n### Graph convolutions as matrix multiplications, and matrix multiplications as walks on a graph\n\n\nWe’ve talked a lot about graph convolutions and message passing, and of course, this raises the question of how do we implement these operations in practice? For this section, we explore some of the properties of matrix multiplication, message passing, and its connection to traversing a graph. \n\n\nThe first point we want to illustrate is that the matrix multiplication of an adjacent matrix $A$ $n\\_{nodes} \\times n\\_{nodes}$ with a node feature matrix $X$ of size $n\\_{nodes} \\times node\\_{dim}$ implements an simple message passing with a summation aggregation.\nLet the matrix be $B=AX$, we can observe that any entry $B\\_{ij}$ can be expressed as $<A\\_{row\\_i} \\dot X\\_{column\\_j}>= A\\_{i,1}X\\_{1,j}+A\\_{i,2}X\\_{2, j}+…+A\\_{i,n}X\\_{n, j}=\\sum\\_{A\\_{i,k}>0} X\\_{k,j}$. Because $A\\_{i,k}$ are binary entries only when a edge exists between $node\\_i$ and $node\\_k$, the inner product is essentially “gathering” all node features values of dimension $j$” that share an edge with $node\\_i$. It should be noted that this message passing is not updating the representation of the node features, just pooling neighboring node features. But this can be easily adapted by passing $X$ through your favorite differentiable transformation (e.g. MLP) before or after the matrix multiply.\n\n\nFrom this view, we can appreciate the benefit of using adjacency lists. Due to the expected sparsity of $A$ we don’t have to sum all values where $A\\_{i,j}$ is zero. As long as we have an operation to gather values based on an index, we should be able to just retrieve positive entries. Additionally, this matrix multiply-free approach frees us from using summation as an aggregation operation. \n\n\nWe can imagine that applying this operation multiple times allows us to propagate information at greater distances. In this sense, matrix multiplication is a form of traversing over a graph. This relationship is also apparent when we look at powers $A^K$ of the adjacency matrix. If we consider the matrix $A^2$, the term $A^2\\_{ij}$ counts all walks of length 2 from $node\\_{i}$ to $node\\_{j}$ and can be expressed as the inner product $<A\\_{row\\_i}, A\\_{column\\_j}> = A\\_{i,1}A\\_{1, j}+A\\_{i,2}A\\_{2, j}+…+A\\_{i,n}A{n,j}$. The intuition is that the first term $a\\_{i,1}a\\_{1, j}$ is only positive under two conditions, there is edge that connects $node\\_i$ to $node\\_1$ and another edge that connects $node\\_{1}$ to $node\\_{j}$. In other words, both edges form a path of length 2 that goes from $node\\_i$ to $node\\_j$ passing by $node\\_1$. Due to the summation, we are counting over all possible intermediate nodes. This intuition carries over when we consider $A^3=A \\matrix A^2$.. and so on to $A^k$. \n\n\nThere are deeper connections on how we can view matrices as graphs to explore .\n\n\n### Graph Attention Networks\n\n\nAnother way of communicating information between graph attributes is via attention. For example, when we consider the sum-aggregation of a node and its 1-degree neighboring nodes we could also consider using a weighted sum.The challenge then is to associate weights in a permutation invariant fashion. One approach is to consider a scalar scoring function that assigns weights based on pairs of nodes ( $f(node\\_i, node\\_j)$). In this case, the scoring function can be interpreted as a function that measures how relevant a neighboring node is in relation to the center node. Weights can be normalized, for example with a softmax function to focus most of the weight on a neighbor most relevant for a node in relation to a task. This concept is the basis of Graph Attention Networks (GAT)  and Set Transformers. Permutation invariance is preserved, because scoring works on pairs of nodes. A common scoring function is the inner product and nodes are often transformed before scoring into query and key vectors via a linear map to increase the expressivity of the scoring mechanism. Additionally for interpretability, the scoring weights can be used as a measure of the importance of an edge in relation to a task. \n\n\n![](attention.3c55769d.png)\nSchematic of attention over one node with respect to it’s adjacent nodes. For each edge an interaction score is computed, normalized and used to weight node embeddings.\n\nAdditionally, transformers can be viewed as GNNs with an attention mechanism . Under this view, the transformer models several elements (i.g. character tokens) as nodes in a fully connected graph and the attention mechanism is assigning edge embeddings to each node-pair which are used to compute attention weights. The difference lies in the assumed pattern of connectivity between entities, a GNN is assuming a sparse pattern and the Transformer is modelling all connections.\n\n\n### Graph explanations and attributions\n\n\nWhen deploying GNN in the wild we might care about model interpretability for building credibility, debugging or scientific discovery. The graph concepts that we care to explain vary from context to context. For example, with molecules we might care about the presence or absence of particular subgraphs, while in a citation network we might care about the degree of connectedness of an article. Due to the variety of graph concepts, there are many ways to build explanations. GNNExplainer casts this problem as extracting the most relevant subgraph that is important for a task. Attribution techniques assign ranked importance values to parts of a graph that are relevant for a task. Because realistic and challenging graph problems can be generated synthetically, GNNs can serve as a rigorous and repeatable testbed for evaluating attribution techniques .\n\n\n![](graph_xai.bce4532f.png)\nSchematic of some explanability techniques on graphs. Attributions assign ranked values to graph attributes. Rankings can be used as a basis to extract connected subgraphs that might be relevant to a task.\n\n### Generative modelling\n\n\nBesides learning predictive models on graphs, we might also care about learning a generative model for graphs. With a generative model we can generate new graphs by sampling from a learned distribution or by completing a graph given a starting point. A relevant application is in the design of new drugs, where novel molecular graphs with specific properties are desired as candidates to treat a disease.\n\n\nA key challenge with graph generative models lies in modelling the topology of a graph, which can vary dramatically in size and has $N\\_{nodes}^2$ terms. One solution lies in modelling the adjacency matrix directly like an image with an autoencoder framework. The prediction of the presence or absence of an edge is treated as a binary classification task. The $N\\_{nodes}^2$ term can be avoided by only predicting known edges and a subset of the edges that are not present. The graphVAE learns to model positive patterns of connectivity and some patterns of non-connectivity in the adjacency matrix.\n\n\nAnother approach is to build a graph sequentially, by starting with a graph and applying discrete actions such as addition or subtraction of nodes and edges iteratively. To avoid estimating a gradient for discrete actions we can use a policy gradient. This has been done via an auto-regressive model, such a RNN, or in a reinforcement learning scenario. Furthermore, sometimes graphs can be modeled as just sequences with grammar elements.\n\n\n\nFinal thoughts\n--------------\n\n\nGraphs are a powerful and rich structured data type that have strengths and challenges that are very different from those of images and text. In this article, we have outlined some of the milestones that researchers have come up with in building neural network based models that process graphs. We have walked through some of the important design choices that must be made when using these architectures, and hopefully the GNN playground can give an intuition on what the empirical results of these design choices are. The success of GNNs in recent years creates a great opportunity for a wide range of new problems, and we are excited to see what the field will bring.", "date_published": "2021-09-02T20:00:00Z", "authors": ["Adam Pearce"], "summaries": ["What components are needed for building learning algorithms that leverage the structure and properties of graphs?"], "doi": "10.23915/distill.00033", "journal_ref": "distill-pub", "bibliography": [{"link": "https://doi.org/10.23915/distill.00032", "title": "Understanding Convolutions on Graphs"}, {"link": "https://papers.nips.cc/paper/2020/hash/417fbbf2e9d5a28a855a11894b2e795a-Abstract.html", "title": "Evaluating Attribution for Graph Neural Networks"}]}
{"id": "487640e189145951117a720279b68499", "title": "Distill Hiatus", "url": "https://distill.pub/2021/distill-hiatus", "source": "distill", "source_type": "blog", "text": "*Over the past five years, Distill has supported authors in publishing artifacts that push beyond the traditional expectations of scientific papers. From Gabriel Goh’s interactive exposition of momentum, to an [ongoing collaboration exploring self-organizing systems](https://distill.pub/2020/growing-ca/), to a [community discussion of a highly debated paper](https://distill.pub/2019/advex-bugs-discussion/), Distill has been a venue for authors to experiment in scientific communication.*\n\n*But over this time, the editorial team has become less certain whether it makes sense to run Distill as a journal, rather than encourage authors to self-publish. Running Distill as a journal creates a great deal of structural friction, making it hard for us to focus on the aspects of scientific publishing we’re most excited about. Distill is volunteer run and these frictions have caused our team to struggle with burnout.*\n\n*Starting today Distill will be taking a one year hiatus, which may be extended indefinitely. Papers actively under review are not affected by this change, published threads can continue to add to their exploration, and we may publish commentary articles in limited cases. Authors can continue to write Distill-style papers using the [Distill template](https://github.com/distillpub/template), and either self-publish or submit to venues like [VISxAI](https://visxai.io/).*\n\n\n\n---\n\nThe Distill journal was founded as an adapter between traditional and online scientific publishing. We believed that many valuable scientific contributions — such as explanations, interactive articles, and visualizations — were held back by not being seen as “real scientific publications.” Our theory was that if a journal were to publish such artifacts, it would allow authors to benefit from the traditional academic incentive system and enable more of this kind of work.\n\nAfter four years, we no longer believe this theory of impact. First, we don’t think that publishing in a journal like Distill significantly affects how seriously most institutions take non-traditional publications. Instead, it seems that more liberal institutions will take high-quality articles seriously regardless of their venue and style, while more conservative institutions remain unmoved. Secondly, we don’t believe that having a venue is the primary bottleneck to authors producing more Distill-style articles. Instead, we believe the primary bottleneck is the amount of effort it takes to produce these articles and the unusual combination of scientific and design expertise required.\n\nWe’re proud of the authors Distill has been able to support and the articles it has been able to publish. And we do think that Distill has produced a lot of value. But we don’t think this value has been a product of Distill’s status as a journal. Instead, we believe Distill’s impact has been through:\n\n* Providing mentorship to authors and potential authors.\n* Providing the Distill template (which is used by many non-Distill authors)\n* Individuals involved in Distill producing excellent articles.\n* Providing encouragement and community to authors.\n\nOur sense is that Distill’s journal structure may limit, rather than support, these benefits. It creates a great deal of overhead, political concerns, and is in direct tension with some of these goals.\n\nInstead, we think the future for most types of articles is probably self-publication, either on one-off websites or on a hypothetical “Distill Arxiv.” There are a few exceptions where we think centralized journal-like entities probably have an important enduring role, but we think the majority of papers are best served by self-publication.\n\nChanges in How We Think About Distill\n-------------------------------------\n\n### Mentorship is in Tension with Being a Journal\n\nBehind the scenes, the largest function of Distill is providing feedback and mentorship. For some of our early articles, we provided more than 50 hours of help with designing diagrams, improving writing style, and shaping scientific communication. Although we’ve generally dialed this down over time, each article still requires significant work. All of this is done by our editors in a volunteer capacity, on top of their regular work responsibilities.\n\nThe first problem with providing mentorship through an editorial role is that it’s not a very good mechanism for distributing mentorship. Ideally, one wants to provide mentorship early on in projects, to mentees with similar interests, and to a number of mentees that one is capable of providing good mentorship to. Providing mentorship to everyone who submits an article to Distill is overwhelming. Another problem is that our advice is often too late because the article’s foundation is already set. Finally, many authors don’t realize the amount of effort it takes to publish a Distill article.\n\nProviding mentorship also creates a challenging dual relationship for an editor. They have both the role of closely supporting and championing the author while also having to accept or reject them in the end. We’ve found this to be difficult for both the mentor and mentee.\n\nFinally, the kind of deeply-engaged editing and mentorship that we sometimes provide can often amount to an authorship level contribution, with authors offering co-authorship to editors. This is especially true when an editor was a mentor from early on. In many ways, co-authorship would create healthy incentives, rewarding the editor for spending tens of hours improving the article. But it creates a conflict of interest if the editor is to be an independent decision maker, as the journal format suggests they should be. And even if another editor takes over, it’s a political risk: Distill is sometimes criticized for publishing too many articles with editors as authors.\n\n### Editor Articles are in Tension with Being a Journal\n\nAnother important impact of Distill has been articles written by the editors themselves. Distill’s editorial team consists of volunteer researchers who are deeply excited about explanations and interactive articles and have a long history of doing so. Since the set of people with these interests is small, a non-trivial fraction of Distill’s publications have come from editors. In other cases, authors of existing Distill articles were later invited to become an editor.\n\nEditor articles are sometimes cited as a sign of a kind of corruption for Distill, that Distill is a vehicle for promoting editors. We can see how it might seem dubious for a journal to publish articles by people running it, even if editorial decisions are made by an editor who is at arms-length. This has led Distill to avoid publishing several editor articles despite believing that they are of value to readers.\n\nWe believe that editor articles are actually a good thing about Distill. Each one represents an immense amount of effort in trying new things scientific publishing. Given the large volume of readers and the positive informal comments we receive, we suspect that for every critic there are many silent but happy readers.\n\nWhen a structure turns a public good into an appearance of corruption, it suggests it might not be such a good structure. As editors, we want to share our work with the world in a way that is not seen as corrupt.\n\n### Neutral venues can be achieved in other ways\n\nThe vast majority of Distill articles are written by multiple authors, often from multiple institutions. As a result, an important function of Distill is providing somewhere to publish that isn’t someone’s home turf. If a Distill article were published on one person or organization’s blog, it could lead to a perception that it is primarily theirs and make other authors feel less comfortable with collaboration. Arxiv normally fills this role, but it only supports PDFs.\n\nBut it turns out there’s a simpler solution: self publication on one-off websites. David Ha and his collaborators have done a great job demonstrating this, using the Distill template and GitHub pages to self-publish articles (eg. the [world models](https://worldmodels.github.io/) article). In these cases, the articles are standalone rather than being with a particular author or institution.\n\n### Self-Publication Seems Like the Future (in most cases)\n\nIn many areas of physics, self publishing on Arxiv has become the dominant mode of publication. A great deal of machine learning research is also published on Arxiv. We think this type of self-publication is likely the future for a large fraction of publication, possibly along with alternative models of review that are separated from a publisher.\n\nJournal-led peer review provides many benefits. It can protect against scientific misinformation and non-reproducible results. It can save the research community time by filtering out papers that aren’t worth engaging with. It can provide feedback to junior researchers who may not have other sources of feedback. It can push research groups studying similar topics across institutions to engage with each other’s criticism. And double-blind review may support equity and fairness.\n\nBut is traditional journal-led peer review the most effective way to achieve these benefits? And is it worth the enormous costs it imposes on editors, reviewers, authors, and readers?\n\nFor example, avoiding scientific errors, non-reproducible results, and misinformation is certainly important. But for every paper where there’s a compelling public interest in avoiding misinformation (eg. papers about COVID), there are thousands of papers whose audience is a handful of the same researchers we ask to perform review. Additionally, it’s not clear how effective peer review actually is at catching errors. We suspect that a structure which focuses on reviewing controversial and important papers would be more effective at this goal. Our experience from [discussion articles](https://distill.pub/2019/advex-bugs-discussion/) is that reviewers are willing to spend orders of magnitude more energy when they feel like reviewing a paper genuinely matters to the community, rather than being pro-forma, and their work will be seen as a scientific contribution.\n\nSimilarly, we suspect that journal-led review isn’t a very effective way of providing feedback to junior researchers or of promoting equity. These are all very worthy aims, and we’d like to free energy to pursue them in effective ways.\n\nWe also think there’s a lot of upside to self-publication. Self-publication can move very fast. It doesn’t require a paper to fit into the scope of an existing journal. It allows for more innovation in the format of the paper, such as using interactive diagrams as Distill does. And it aligns incentives better.Self-publication may align certain incentives better than traditional publishing. Many papers go through an informal review process before they’re submitted to a journal or self-published, with authors soliciting feedback from colleagues. This informal review process is often smoother, faster, and provides more constructive and more relevant feedback than a traditional review process. Why is that? In a normal review process, the authors have the highest stakes, but little agency in the process. Meanwhile, neither the reviewers nor the editors share the authors’ incentive to move quickly. And the reviewers are often horribly over-subscribed. In contrast, in an informal review process, the authors have a strong incentive to quickly organize the process and reviewers are focused on providing helpful feedback to someone they know, rather than arbitrating a gatekeeping decision.\n\n### A Half-hearted Distill May Cause Harm\n\nDistill isn’t living up to our standards of author experience. Originally, we had a vision of a much more engaged, responsive, and rapid review process with editors deeply involved in helping authors improve their article. But the truth is that, with us being quite burnt out, our review process has become much slower and more similar to a typical journal. It’s unclear to us whether the value added by our present review process is worth the time costs we impose on authors.\n\nDistill also occupies institutional space, potentially discouraging others from starting similar projects. It’s possible that there are others who could execute something like Distill better than us, but aren’t starting their project because Distill exists.\n\nOn the flip side, Distill often comes up in conversations about the future of publishers and journals in machine learning, as a positive example of the role a journal can play. But if we no longer believe in our model, Distill may be unintentionally supporting something we don’t really stand behind. We may also be setting unrealistic aspirations: if Distill’s level of editorial engagement and editing was unsustainable, even with a deeply passionate set of volunteers and a relatively small number of articles, we should at least be clearly communicating how difficult it is.\n\nWhy a Hiatus?\n-------------\n\nWe think that Distill is a really beautiful artifact which illustrates a vision of scientific publishing. But it is not sustainable for us to continue running the journal in its current form. We think preserving it in its present state is more valuable than diluting it with lower quality editing. We also think that it’s a lot healthier for us and frees up our energy to do new projects that provide value to the community.\n\nWe’ve considered trying to find others to hand Distill off to. But a lot of the value of Distill is illustrating a weird and idiosyncratic vision. We think there’s value in preserving Distill’s original flavor. We are open to changes to better structure Distill, but we feel protective of Distill’s vision and quirkiness.\n\nAlthough Distill is going on hiatus, the [Distill template](https://github.com/distillpub/template) is open source, and we’d love to see others run with it!\n\n### Burnout\n\nOver the last few years, Distill has experienced a significant amount of volunteer burnout. The fact that multiple volunteers experienced burnout makes us think it’s partly caused by the issues described in previous sections.\n\nOne of the biggest risk factors in burnout is having conflicting goals, and as the previous sections describe, we’ve had many conflicting goals. We wanted to mentor people, but we also needed to reject them. We wanted to write beautiful articles ourselves, but we also wanted to be an independent venue.\n\nAnother significant risk factor is having unachievable goals. We set extremely high standards for ourselves: with early articles, volunteer editors would often spend 50 or more hours improving articles that were submitted to Distill and bringing them up to the level of quality we aspired to. This invisible effort was comparable to the work of writing a short article of one’s own. It wasn’t sustainable, and this left us with a constant sense that we were falling short. A related issue is that we had trouble setting well-defined boundaries of what we felt we owed to authors who submitted to us.\n\nBy discussing these challenges, we hope that future projects like Distill will be able to learn from our experiences and find ways to balance these competing values.", "date_published": "2021-07-02T20:00:00Z", "authors": ["Editorial Team"], "summaries": ["After five years, Distill will be taking a break."], "doi": "10.23915/distill.00031", "journal_ref": "distill-pub", "bibliography": []}
