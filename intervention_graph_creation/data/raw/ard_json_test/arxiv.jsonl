{"id": "d8c703e1a63ccaac501894b402ab59e0", "title": "Ontological Crises in Artificial Agents' Value Systems", "url": "https://arxiv.org/abs/1105.3821", "source": "arxiv", "source_type": "html", "text": "1. Abstract\n------------\n\n\n\nDecision-theoretic agents predict and evaluate the results of their actions using a model, or ontology, of their environment. An agent’s goal, or utility function, may also be specified in terms of the states of, or entities within, its ontology. If the agent may upgrade or replace its ontology, it faces a crisis: the agent’s original goal may not be well-defined with respect to its new ontology. This crisis must be resolved before the agent can make plans towards achieving its goals.\n\n\n\n\nWe discuss in this paper which sorts of agents will undergo ontological crises and why we may want to create such agents. We present some concrete examples, and argue that a well-defined procedure for resolving ontological crises is needed. We point to some possible approaches to solving this problem, and evaluate these methods on our examples.\n\n2. Introduction: Goals and Utility Functions\n---------------------------------------------\n\n\n\nAn agent is any person or thing that performs actions in order to achieve a goal. These goals may involve anything of which the agent is aware, from its own inputs and outputs to distant physical objects. When creating an artificial agent, it is natural to be interested in which goals we choose to give it. When we create something, we usually do so because we expect it to be useful to us. Thus the goals we give to artificial agents should be things that we want to see accomplished.\n\n\n\n\nProgrammers of artificial agents are then faced with the task of specifying a goal. In our discussion we assume that goals take the form of utility functions defined on the set of possible states within the agent’s ontology. If a programmer is specifying a utility function ”by hand” – that is, by looking at the ontology and directly assigning utilities to different states – then the ontology must be comprehensible to the programmer. This will typically be the case for an ontology that the programmer has designed, but not necessarily so for one that an agent has learned from experience.\n\n\n\n\nAn agent with a fixed ontology is not a very powerful agent, so we would like to discuss an agent that begins with an ontology that its programmers understand and have specified a utility function over, and then upgrades or replaces its ontology. If the agent’s utility function is defined in terms of states of, or objects within, its initial ontology, then it cannot evaluate utilities within its new ontology unless it translates its utility function somehow.\n\n\n\n\n\nConsider, for example, an agent schooled in classical physics. Perhaps this agent has a goal that is easy to specify in terms of the movement of atoms, such as to maintain a particular temperature within a given region of space. If we replace our agent’s ontology with a quantum one, it is no longer obvious how the agent should evaluate the desirability of a given state. If its utility function is determined by temperature, and temperature is determined by the movement of atoms, then the agent’s utility function is determined by the movement of atoms. Yet in a quantum worldview, atoms are not clearly-defined objects. Atoms are not even fundamental to a quantum worldview, so the agent’s ontology may contain no reference to atoms whatsoever. How then, can the agent define its utility function?\n\n\n\n\nOne way to sidestep the problem of ontological crises is to define the agent’s utility function entirely in terms of its percepts, as the set of possible percept-sequences is one aspect of the agent’s ontology that does not change. Marcus Hutter’s universal agent AIXI [[1](#bib.bib1)] uses this approach, and always tries to maximize the values in its reward channel. Humans and other animals partially rely on a similar sort of reinforcement learning, but not entirely so.\n\n\n\n\nWe find the reinforcement learning approach unsatisfactory. As builders of artificial agents, we care about the changes to the environment that the agent will effect; any reward signal that the agent processes is only a proxy for these external changes. We would like to encode this information directly into the agent’s utility function, rather than in an external system that the agent may seek to manipulate.\n\n3. Our Approach\n----------------\n\n\n\nWe will approach this problem from the perspective of concrete, comprehensible ontologies. An AI programmer may specify an ontology by hand, and then specify a utility function for that ontology. We will then try to devise a systematic way to translate this utility function to different ontologies.\n\n\n\n\nWhen using this method in practice, we might expect the agent to have a probability distribution over many ontologies, perhaps specified concisely by the programmer as members of a parametric family. The programmer would specify a utility function on some concrete ontology which would be automatically translated to all other ontologies before the agent is turned on. In this way the agent has a complete utility function.\n\n\n\n\nHowever, for the purposes of this discussion, we may imagine that the agent has only two ontologies, one old and one new, which we may call O0 and O1. The agent’s utility function is defined in terms of states of O0, but it now believes O1 to be a more accurate model of its environment. The agent now faces an ontological crisis – the problem of translating its utility function to the new ontology O1.\n\n\n\n\nIn this paper we will present a method for addressing these problems. Our intention, however, is not to close the book on ontological crises, but rather to open it. Our method is of an ad-hoc character and only defined for a certain class of ontologies. Furthermore it is not computationally tractable for large ontologies. We hope that this discussion will inspire other thinkers to consider the problem of ontological crises and develop new solutions.\n\n4. Finite State Models\n-----------------------\n\n\n\nWe wil nowl consider a specific kind of ontology, which we may call a *finite state model*. These models have some finite set of possible hidden states, which the agent does not directly observe. On each time step, the model inputs some symbol (the agent’s output), enters some hidden state, and outputs some symbol (the agent’s input). The model’s output depends (stochastically) only on its current state, while its state depends (stochastically) on both the input and the previous state.\n\n\n\n\n![](https://media.arxiv-vanity.com/render-output/7815376/x1.png)\n\n\nLet us call the agent’s output symbols *motor symbols* and the agent’s input symbols *sensor symbols*. We will call the sets of symbols the motor alphabet and the sensor alphabet, denoted M and S respectively. We will assume that the alphabets are fixed properties of our agent’s embodiment; we will not consider models with different alphabets.\n\n\n\n\nLet m=|M| and s=|S|. Then a model with n states may be completely specified by m different n×n transition matrices and one s×n output matrix.\n\n\n\n\nFor each x∈M, let us call the state transition matrix Tx. Note that the superscript here is not an exponent. We may call the output probability matrix A. Since we will be speaking of two ontologies, O0 and O1, we will use subscripts to indicate which ontology we are taking these matrices from; for instance, Tx0 is the state transition matrix for action x in the O0 ontology.\n\n5. Maps between Ontologies\n---------------------------\n\n\n\nOur basic approach to translating our utility function from O0 to O1 will be to construct a function from O1 to O0 and compose our utility function with this function. If\n\n\n\n\n\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n| (1) |  | U:O0→R |  |\n\n\n\n\nis a utility function defined on O0, and ϕ:O1→O0, then U∘ϕ is a utility function defined on O1.\n\n\n\n\nThe function ϕ we will seek to define will be a *stochastic function*; its output will not be a single state within O0, but a probability distribution over states. Thus if O0 has n0 states while O1 has n1 states, ϕ will be most naturally expressed as an n0×n1 matrix.\n\n\n\n\nLet us consider some desiderata for ϕ:\n\n\n\n\n1. ϕ should be determined by the *structure* of the models O0 and O1; the way in which the states are labeled is irrelevant.\n\n\n\n\n2. If O0 and O1 are isomorphic to each other, then ϕ should be an isomorphism.\n\n\n\n\nThis may seem irrelevant, for if O1 is isomorphic to O0, then there is no need to change models at all. Nevertheless, few would object to 2 on grounds other than irrelevance, and 2 may be seen as a special case of a more general statement:\n\n\n\n\n3. If O0 and O1 are *nearly* isomorphic to each other, then ϕ should nearly be an isomorphism.\n\n\n\n\nThis criterion is certainly relevant; since O0 and O1 are both models of the same reality, they can be expected to be similar to that reality, and thus similar to each other.\n\n\n\n\nIn accordance with these desiderata, we will try to construct a function that is as much like an isomorphism as possible. To accomplish this, we will define in quantitative terms what we mean by ”like an isomorphism.” First, we observe that isomorphisms are invertible functions; thus, we will define a second function, which we fancifully call ϕ−1:O0→O1, even though it may not be a true inverse of ϕ, and we will optimize both ϕ and ϕ−1 to be ”like isomorphisms”.\n\n\n\n\nOur criterion is a combination of the computer science notion of *bisimulation* with the information-theoretic idea of *Kullback-Leibler divergence*.\n\n\n\n\nBisimulation means that either model may be used to simulate the other, using ϕ and ϕ−1 to translate states between models. Thus, for any action x, we would like ϕ−1∘Tx0∘ϕ to approximate Tx1. By this we mean that we should be able to predict as accurately as possible the result of some action x in O1 by translating our distribution for the initial state in O1 to a distribution over O0 (using the function ϕ), predicting the result of action x within O0, and translating this result back to O1 using ϕ−1. Similarly, we would like to use O1 to predict the behavior of O0.\n\n\n\n\nFurthermore, we want to to optimize ϕ and ϕ−1 so that both models will make similar predictions about sensory data. Thus A0∘ϕ should be close to A1 and A1∘ϕ−1 should be close to A0.\n\n\n\n\nTo measure distance between two matrices, we treat the columns vectors as probability distributions and sum the Kullback-Leibler divergences of the columns. For two matrices P and Q, let DKL(P||Q) be the sum of the Kullback-Leibler divergences of the columns. When calculating Kullback-Leibler divergence, we consider the columns of the A and T matrices to be the ”true” distributions, while those depending on ϕ or ϕ−1 are regarded as the approximations.\n\n\n\n\nSo we choose ϕ and ϕ−1 to minimize the quantity\n\n\n\n\n\n\n|  |  |  |\n| --- | --- | --- |\n|  | (∑x∈MDKL(Tx1||ϕ−1Tx0ϕ))+DKL(A1||ϕ−1A0ϕ) |  |\n|  | +(∑x∈MDKL(Tx0||ϕTx1ϕ−1))+DKL(A0||ϕA1ϕ−1) |  |\n\n\n\n\nUsing a simple hill-climbing algorithm, we have tested our criterion on a simple example.\n\n6. Example: the long corridor\n------------------------------\n\n\n\nThe agent initially believes that it is standing in a corridor consisting of four discrete locations. The agent’s actions are to move left or right. If the agent is already at the end of the corridor and attempts to move further in that direction, it will remain where it is. The agent can see whether it standing at the left end, the right end, or at neither end of the corridor. The agent’s goal is to stand at the right end of the corridor.\n\n\n\n\n![](https://media.arxiv-vanity.com/render-output/7815376/x2.png)\n\n\nNow the agent discovers that this ontology is incorrect; the corridor actually consists of five discrete locations. What, then, should the agent do? Intuitively, it seems most plausible that the agent should stand at the right end of the corridor. Stretching plausibility a bit, perhaps the agent should stand one step away from the right end of the corridor, since the corridor is longer than expected. Any other solution seems counterintuitive.\n\n\n\n\nOur initial, four-state ontology O0 can be represented in matrix form as follows:\n\n\n\n\n\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n| (2) |  | TL0=⎛⎜\n⎜\n⎜⎝1100001000010000⎞⎟\n⎟\n⎟⎠,TR0=⎛⎜\n⎜\n⎜⎝0000100001000011⎞⎟\n⎟\n⎟⎠A0=⎛⎜⎝100001100001⎞⎟⎠ |  |\n\n\n\n\nAnd the five-state ontology O1 can be represented as:\n\n\n\n\n\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n| (3) |  | TL1=⎛⎜\n⎜\n⎜\n⎜\n⎜\n⎜⎝1100000100000100000100000⎞⎟\n⎟\n⎟\n⎟\n⎟\n⎟⎠,TR1=⎛⎜\n⎜\n⎜\n⎜\n⎜\n⎜⎝0000010000010000010000011⎞⎟\n⎟\n⎟\n⎟\n⎟\n⎟⎠A1=⎛⎜⎝100000111000001⎞⎟⎠ |  |\n\n\n\n\nBy hill-climbing from random initial values, our program found several local optima. After 10 runs, our best result, to three significant figures, was:\n\n\n\n\n\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n| (4) |  | ϕ=⎛⎜\n⎜\n⎜⎝10000010.50300000.4961000001⎞⎟\n⎟\n⎟⎠,ϕ−1=⎛⎜\n⎜\n⎜\n⎜\n⎜\n⎜⎝10.0140.001000.7150000.2700.2830000.71500001⎞⎟\n⎟\n⎟\n⎟\n⎟\n⎟⎠ |  |\n\n\n\n\nWe can now make an interesting observation: ϕϕ−1 is close to an identity matrix, as is ϕ−1ϕ. Thus, after mapping from one ontology to the other, we can nearly recover our initial information.\n\n\n\n\n\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n| (5) |  | ϕϕ−1=⎛⎜\n⎜\n⎜⎝10.0140.137000.8510.142000.1340.8560000.0011⎞⎟\n⎟\n⎟⎠,ϕ−1ϕ=⎛⎜\n⎜\n⎜\n⎜\n⎜\n⎜⎝10.0140.0080.001000.7150.3600000.2700.2760.2830000.3550.7150000.00101⎞⎟\n⎟\n⎟\n⎟\n⎟\n⎟⎠ |  |\n\n\n\n\nThe matrix ϕ represents the following function mapping the 5-state ontology to the 4-state ontology:\n\n\n\n\n![](https://media.arxiv-vanity.com/render-output/7815376/x3.png)\n\n\nThe black arrows indicate near-certainty; the gray arrows indicate probabilities of about 12.\n\n\n\n\nIf we compose ϕ with our utility function, we obtain a utility of 1 for the right square and a utility of 0 for the other squares, which agrees with our intuitions.\n\n7. Outlook\n-----------\n\n\n\nThose wishing to extend our algorithm as presented may consider what to do when the agent’s sensors or motors are replaced, how to deal with differently-sized time steps, how to deal with continuous models, and how to efficiently find mappings between larger, structured ontologies.\n\n\n\n\nFurthermore, there remain difficult philosophical problems. We have made a distinction between the agent’s uncertainty about which model is correct and the agent’s uncertainty about which state the world is in within the model. We may wish to eliminate this distinction; we could specify a single model, but only give utilities for some states of the model. We would then like the agent to generalize this utility function to the entire state space of the model.\n\n\n\n\nHuman beings also confront ontological crises. We should find out what cognitive algorithms humans use to solve the same problems described in this paper. If we wish to build agents that maximize human values, this may be aided by knowing how humans re-interpret their values in new ontologies.\n\n\n\n\nWe hope that other thinkers will consider these questions carefully.\n\nAcknowledgements\n----------------\n\n\n\nThanks to Roko Mijic, with whom I discussed these ideas in 2009, and to Steve Rayhawk, who gave extensive criticism on earlier versions of this paper.", "date_published": "2011-05-19T00:00:00Z", "authors": ["Peter de Blanc"], "summaries": [], "doi": null, "abstract": "Decision-theoretic agents predict and evaluate the results of their actions using a model, or ontology, of their environment. An agent's goal, or utility function, may also be specified in terms of the states of, or entities within, its ontology. If the agent may upgrade or replace its ontology, it faces a crisis: the agent's original goal may not be well-defined with respect to its new ontology. This crisis must be resolved before the agent can make plans towards achieving its goals.   We discuss in this paper which sorts of agents will undergo ontological crises and why we may want to create such agents. We present some concrete examples, and argue that a well-defined procedure for resolving ontological crises is needed. We point to some possible approaches to solving this problem, and evaluate these methods on our examples.", "categories": ["cs.AI"], "journal_ref": null, "author_comment": null, "primary_category": "cs.AI", "data_last_modified": "2011-05-19 09:32:46+00:00"}
{"id": "04ffe15a72632287c51aa1aed0a8ae5b", "title": "I Don't Want to Think About it Now:Decision Theory With Costly Computation", "url": "https://arxiv.org/abs/1106.2657", "source": "arxiv", "source_type": "html", "text": "1 Introduction\n---------------\n\n\n\nComputation plays a major role in decision\nmaking. Even if an agent is willing to ascribe a probability to all states\nand a utility to all outcomes, and maximize expected utility—that is,\nto follow the standard prescription of rationality as\nrecommended by Savage \\citeyearSavage, doing so might present serious\ncomputational problems. Computing the relevant probabilities might be\ndifficult, as might computing the relevant utilities. Work on\nBayesian networks [[PearlPearl1988](#bib.bibx16)] and other representations of probability,\nand related work on representing utilities [[Bacchus and GroveBacchus and\nGrove1995](#bib.bibx2), [Boutilier, Brafman, Domshlak, Hoos, and\nPooleBoutilier et al.2004](#bib.bibx3)]\ncan be viewed as attempts to ameliorate these computational problems.\nOur focus is on the complexity of computing the outcome of an act in a\ngiven state. Consider the following simple example, taken from\n[[Halpern and PassHalpern and\nPass2010](#bib.bibx8)].\n\n\n\n\nSuppose that a decision maker (DM) is given an input n, and is asked\nwhether it is prime. The DM gets a payoff of $1,000 if he gives the\ncorrect answer and loses $1,000 if he gives the wrong answer. However,\nhe also has the option of playing safe, and saying ‘‘pass’’, in which\ncase he gets a payoff of $1. Clearly, many DMs would say ‘‘pass’’ on\nall but simple inputs, where the answer is obvious,\nalthough what counts as a ‘‘simple’’ input may depend on the DM.111While primality testing is now known to be in polynomial time\n[[Agrawal, Keyal, and SaxenaAgrawal\net al.2004](#bib.bibx1)], and there are computationally-efficient randomized\nalgorithms that that give the correct answer with extremely high\nprobability [[RabinRabin1980](#bib.bibx19), [Solovay and StrassenSolovay and\nStrassen1977](#bib.bibx24)], we can assume that the DM has no\naccess to a computer.\n\n\n\n\nIn [[Halpern and PassHalpern and\nPass2010](#bib.bibx8)], we introduced a model of game theory\nwith costly computation. Here we apply that framework to decision\ntheory. We assume that the DM can be viewed as choosing an algorithm\n(i.e., a Turing machine); with each Turing machine (TM) M and input, we\nassociate its *complexity*. The complexity can represent, for\nexample, the running time of M on that input, the space used,\nthe complexity of M (e.g., how many states it has), or the difficulty of\nfinding M (some algorithms are more obvious than others). We\ndeliberately keep the complexity function abstract, to allow for the\npossibility of representing a number of different intuitions. The DM’s\nutility can then depend, not just on the payoff, but on the complexity.\n\n\n\n\nThe DM’s goal is to choose the “best” TM; the one that will give him\nthe greatest expected utility, taking both the payoff and complexity\ninto account. To make this choice, the DM must have beliefs about the\nTM’s running time and the “goodness” of the TM’s output. For example,\nif the TM outputs “prime” on some input n, then TM must have beliefs\nabout how likely n is to actually be prime. As this example suggests,\nwe actually need here to deal with what philosophers have called\n“impossible” possible worlds [[HintikkaHintikka1975](#bib.bibx9), [RantalaRantala1982](#bib.bibx20)]. If n is a prime, then\nthis is a mathematical fact; there can be no state where n is not\nprime; nevertheless, since we want to allow for DMs that are\nresource-bounded and cannot compute whether n is prime, we want it to\nbe possible for the DM to believe that n is not prime. Similarly, if\nthe complexity function is supposed to measure running time, then the\nactual running time of a TM M on input t is a fact of mathematics;\nnevertheless, we want to allow the DM to have false beliefs about M’s\nrunning time.\nWe capture such false beliefs by having both the utility function and the\ncomplexity function depend on the state of nature.\n\n\n\n\nAs we show here, using these simple ideas leads to quite a powerful\nframework. For example, many concerns expressed by the emerging\nfield of *behavioral economics* (pioneered by Kahneman and\nTversky [[Kahneman, Slovic, and TverskyKahneman\net al.1982](#bib.bibx12)]) can be accounted for by simple assumptions\nabout players’\ncost of computation. To illustrate this point, we show that\n*first-impression-matters biases* [[RabinRabin1998](#bib.bibx17)], that is, that\npeople tend to put more weight on evidence they hear early on, can be\neasily captured using computational assumptions. We can similarly\nexplain *belief polarization* [[Lord, Ross, and LepperLord\net al.1979](#bib.bibx13)]—that two people,\nhearing the same\ninformation (but with possibly different prior beliefs) can end up with\ndiametrically opposed conclusions.\nFinally, we can also use the framework to formalize one of the\nintuitions for the well-known *status quo* bias\n[[Samuelson and ZeckhauserSamuelson and\nZeckhauser1998](#bib.bibx22)]: people are much more likely to stick with what they\nalready have.\n\n\n\n\nAs a final application, we use the framework to define a new notion:\n*value of computational information*. To explain it, we first recall\n*value of information*, a standard notion in decision analysis.\nValue of information is meant to be a measure of how much a DM\nshould be willing to\npay to receive new information. The idea is that, before receiving the\ninformation, the DM has a probability on a set of relevant events and\nchooses the action that maximizes his expected utility, given that\nprobability. If he receives new information, he can update his\nprobabilities (by conditioning on the information) and again choose the\naction that maximizes expected utility. The difference between the\nexpected utility before and after receiving the information is the value\nof the information.\n\n\n\n\nIn many cases, a DM seems to be receiving valuable information that\nis not about what seem to be the relevant events. This means that we\ncannot do a value of computation calculation, at least not in the\nobvious way. For example, suppose that the DM is interested in\nlearning a secret, which we assume for simplicity is a number between 1\nand 1000. A priori, suppose that the DM takes each number to be\nequally likely, and so has probability .001.\nLearning the secret has utility, say, $1,000,000; not learning it has\nutility 0. The number is locked in\na safe, whose combination is a 40-digit binary numbers.\nWhat is the value to the DM of learning the first 20 digits of the\ncombination? As far as value of information goes, it seems that the\nvalue is 0. The events relevant to the expected utility are the possible\nvalues of the secret;\nlearning the combination does not change the probabilities of\nthe numbers at all. This is true even if we put the possible\ncombinations of the lock into the sample space.\nOn the other hand, it is clear that people may well be willing to pay\nfor learning the first 20 digits. It converts an infeasible\nproblem (trying 240 combinations by brute force) to a feasible\nproblem (trying 220 combinations).\n\n\n\n\nAlthough this example is clearly contrived, there are many far more\nrealistic situations where people are clearly\nwilling to pay for information to improve computation. For example,\ncompanies pay to learn about a manufacturing process that will speed up\nproduction; people buy books on speedreading; and faster algorithms for\nsearch clearly are considered valuable.\nWe show that we can use our computational framework to make the notion\nof *value of computational information* precise, in a way that\nmakes it a special case of value of information.222Our notion of value of computational information is related\nto, but not quite the same as, the notion of *value of computation*\nintroduced by Horvitz \\citeyearHor87,Hor01; see Section [4](#S4 \"4 Discussion and Related Work ‣ I Don’t Want to Think About it Now: Decision Theory With Costly Computation\").\nIn addition, we define a notion of\n*computational value of conversation*, where the DM can communicate\ninteractively with an informed observer before making a decision (as\nopposed to just getting some information).\nInterestingly, the notion of *zero knowledge* [[Goldwasser, Micali, and RackoffGoldwasser\net al.1989](#bib.bibx6)] gets an\nelegant interpretation in this framework. Roughly speaking, a\nzero-knowledge algorithm for\nmembership in a language L is one where there is no added value of\nconversation in running the algorithm beyond what there would be in\nlearning whether an input x is in L, no matter what random variable\nis of interest to the DM.\n\n\n\n\nIn the next section we define our computational framework carefully, and\nshow how it delivers reasonable results in a number of examples. In\nSection [3](#S3 \"3 Value of computational information ‣ I Don’t Want to Think About it Now: Decision Theory With Costly Computation\"), we consider the value of computational\ninformation. We conclude with a discussion of related work in\nSection [4](#S4 \"4 Discussion and Related Work ‣ I Don’t Want to Think About it Now: Decision Theory With Costly Computation\").\n\n2 A computational framework\n----------------------------\n\n\n\nThe framework we use here for adding computation to decision theory is\nessentially a single-agent version of what were called in [[Halpern and PassHalpern and\nPass2010](#bib.bibx8)]\n*Bayesian machine games*. In a standard Bayesian game, each player\nhas a *type* in some set T, and then makes a single move.\nPlayer i’s type can be viewed as describing i’s\ninitial information; some facts that i knows about the world.\nIn the number-in-the-safe example, there is essentially only one\ntype, since the DM gets no information. In the case of the\nmanufacturing process, the type could be the configuration of the\nsystem; manufacturing processes typically apply to a number of\nconfigurations. We assume that an agent’s move consists of choosing a\nTuring machine. As we said in the introduction, associated with each\nTuring machine and type is its complexity. Given as input a type, the\nTuring machine outputs an action. The utility of a player depends on\nthe type *profile* (i.e., the types of all the players), the\naction profile, and the complexity profile. (While typically all that\nmatters to player i is the complexity of his algorithm, it may, for\nexample, matter to him that his algorithm is faster than that of player\nj.)\n\n\n\n\nTurning to decision theory,\nwe take a *standard decision problem with types* to be characterized by a\ntuple (S,T,A,Pr,u), where S is a state space, T is a set of types,\nA is a set of actions, Pr is a probability distribution on S×T (there may be correlation between states and types),\nand u:S×T×A→IR, where u(s,t,a) is the\nDM’s utility if he performs action a in state s and has type t.333In [[Halpern and PassHalpern and\nPass2010](#bib.bibx8)], we did not have a state space S, but we\nassumed that nature had a type. Nature’s type can be identified with\nthe state. (It is not typical to consider a decision maker’s type in\nstandard decision theory, but it does not hurt to add it; it will prove\nuseful once we consider computation.)\nFor each action a,\nwe can consider the\nrandom variable ua defined on S by taking ua(s,t)=u(s,t,a).\nThe expected utility of action a, denoted EPr[ua],\nis just the expected value of the random variable ua with respect to\nthe probability distribution Pr;\nthat is, EPr[ua]=∑(s,t)∈S×TPr(s,t)u(s,t,a).\nWe assume that\nthe DM is an expected utility maximizer, so he chooses an action a\nwith the largest expected utility.\n\n\n\n\nTo combine the ideas of Bayesian machine games and decision problems, we\nconsider *computational decision problems*. In a computational\ndecision problem, just like in a computational Bayesian machine game,\nthe DM chooses a Turing machine.\nWe assume that\nthe action performed by the TM\ndepends on the type.\nWe denote by M(t) the output of the machine on input the type t.\nTo capture the DM’s uncertainty about the TM’s output, we use an output function O:M×S×T→IN, where M denotes the set of Turing Machines;\nO(M,s,t) is used to describe what the DM thinks the output of M(t) is in state s.\nTo simplify the presentation, we abuse notation and use M(s,t) to denote\nO(M,s,t).\n\n\n\n\nThe DM’s utility will depend on the state s, his type t, and the\naction M(s,t), as is standard; in addition, it will depend on the\n“complexity” of M given input t.\nThe complexity of a machine can represent, for example, the\nrunning time or space usage of M, or the complexity of M itself, or\nsome combination of these factors. For\nexample, Rubinstein \\citeyearRub85 considers what can be viewed as\nspecial case of our model, where the DM chooses a finite automaton (and\nhas no type); the complexity of M is the number of states in the\ndescription of the automaton.\nTo capture the cost of computation formally, we use a *complexity\nfunction* C:M×S×T→IN,\nto describe the complexity of a TM given an input type and state.\n(As we shall see, by allowing the state to be included as an argument to\nC, we can capture the DM’s uncertainty about the complexity.)\n\n\n\n\n\nWe define a *computational decision problem* to be a tuple\nD=(S,T,A,Pr,M,C,O,u), where S, T, A, and Pr\nare as in the definition of a standard decision problem,\nM⊆M is a set of TMs (intuitively, the set that\nthe DM can choose among),\nO is an output function,\nC is a complexity measure, and u:S×T×A×IN→IR. The expected utility of a TM M\n\nin the decision problem D is\n∑s∈S,t∈TPr(s,t)u(s,t,O(M,s,t),C(M,s,t)).\nNote that now the utility function gets the complexity of M as an\nargument.\nFor ease of exposition here, we restrict to deterministic TMs for most\nof the paper; we need to consider randomized TMs for our results on zero\nknowledge.\n\n\n\n\n###### Example 2.1\n\n\n\n*Consider the primality-testing problem discussed in\nthe introduction. Formally, suppose that the\nDM’s type is just a natural number <240, and the DM must determine\nwhether the type is prime. The DM can choose either 0 (the number\nis not prime), 1 (the number is prime), or 2 (pass).\nIf M is a TM, then M(s,t) is M’s output in state s on input t.\nThe state s here is used to capture the DM’s uncertainty about the\noutput. So if the DM believes that the DM will output pass with\nprobability 2/3, then the set of states such that M(s,t)=2 has\nprobability 2/3.\nLet\nC(s,t,M) be 0 if M computes the answer within 220\nsteps on input t, and 10 otherwise.\n(Think of 220 steps as representing representing a hard deadline.)\nHere the state s encodes the DM’s\nuncertainty about the running time of M. For example, if the DM does not\nknow the running time of M, but ascribes probability 2/3 to M\nfinishing in less than 220 steps on input t, then the set of\nstates s such that C(s,t,M)=0 has probability 2/3.\nFinally, let utility u(s,t,a,c)=10−c if a is either 0 or 1, and\nthis is the correct answer in state s (that is, t is viewed as prime\nin state s and a=1, or t is not viewed as prime in state s and a=0),\nand u(s,t,2,c)=1−c. Now the\nstate s is used to encode the DM’s uncertainty about the correctness\nof M’s answer.\n(Note that we are allowing “impossible” states, where t is viewed as\nprime in state s even though it is in fact composite; this is needed\nto model the DM’s uncertainty.)\nThus, if the DM is sure that M always gives\nthe correct output, then u(s,t,a,c)=10−c for all states s and a∈{0,1}.*\n\n\n\n\n*We can also consider a variant\nof this problem, where the DM is given a specific input t and is asked if\nt is prime. Although there is obviously a right answer (the number\nis prime or it’s not), the DM might still have uncertainty regarding\nwhether a particular TM M gives the right answer, the running time of\nM, and the output of M.*\n \n\n\n\n\n\n###### Example 2.2\n\n\n\n*Consider the number-in-the-safe example from the\nintroduction. Here there is only a single type, t0; we can think of\nthe state space S as consisting of pairs (s1,s2,s3), where s1 is\nthe number in the safe, s2 is the combination, and s3 encodes the\nDM’s beliefs about the complexity and correctness of TMs.\nAn algorithm in this case is just a sequence of combinations to try and\na stopping rule. Suppose that the agent gets utility 10−C((s1,s2,s3),t0,M) if s2 (the actual combination) is one of\nthe numbers\ngenerated by M before it halts, and 0−C((s1,s2,s3),t0,M)\notherwise,\nwhere C((s1,s2,s3),t0,M)\nis 0 if M halts within 220 steps in state (s1,s2,s3), and 10\notherwise.*\n \n\n\n\n\n\n###### \nExample 2.3 (Biases in information processing)\n\n\n\nPsychologists have observed many systematic biases in the way that\nindividuals update their beliefs as new information is received (see\n[[RabinRabin1998](#bib.bibx17)] for a survey). In particular, a\n“first-impressions-matter” bias has been observed: individuals put too\nmuch weight on initial signals and less weight on later signals. As they\nbecome more convinced that their beliefs are correct, many individuals\neven seem to simply ignore all information once they reach a confidence\nthreshold.\nSeveral papers in behavioral economics have focused on\nidentifying and modeling some of these biases (see, e.g., [[RabinRabin1998](#bib.bibx17)]\nand the references therein, [[MullainathanMullainathan2002](#bib.bibx15)], and [[Rabin and SchragRabin and\nSchrag1999](#bib.bibx18)]). In\nparticular, Mullainathan \\citeyearM98 makes a potential connection\nbetween memory and biased\ninformation processing, using a model that makes several explicit\n(psychology-based) assumptions on the memory process (e.g., that the\nagent’s ability to recall a past event depends on how often he has\nrecalled the event in the past).\nMore recently, Wilson \\citeyearW02 has presented an elegant model of bounded\nrationality, where agents are described by finite automata, which\n(among other things) can explain why agents eventually choose to ignore\nnew information; her analysis, however, is very complex and holds only\nin the limit\n(specifically, in the limit as the probability ν that a given round\nis the last round goes to 0).\n\n\n\n\nAs we now show, the first-impression-matters bias can be easily\nexplained if we assume that there is a small cost for “absorbing” new\ninformation.\nConsider the following simple game (which is very similar to the one\nstudied by Mullainathan \\citeyearM98 and Wilson \\citeyearW02).\nThe state of nature is a bit b that is 1\nwith probability 1/2. An agent receives as his type a sequence of\nindependent samples s1,s2,…,sn where si=b with\nprobability ρ>1/2. The samples corresponds to signals the agents\nreceive about b.\nAn agent is supposed to output a guess b′ for the bit b. If the\nguess is correct, he receives 1−mc as utility, and −mc otherwise,\nwhere m is the number of bits of the type he read, and c is the cost\nof reading a single bit (c should be thought of the cost of\nabsorbing/interpreting information).\nIt seems\nreasonable to assume that c>0; signals usually require some effort to\ndecode (such as reading a newspaper article, or attentively watching a\nmovie).\nIf c>0, it easily follows by the Chernoff bound that after\nreading a certain (fixed) number of signals s1,…,si, the\nagents will have a sufficiently good estimate of ρ that the\nmarginal cost of reading one extra signal si+1 is higher than the\nexpected gain of finding out the value of si+1.\nThat is, after processing a certain number of signals, agents will\neventually disregard all future signals and base their output\nguess only on the initial sequence.\nWe omit the straightforward details.\n\n\n\n\nEssentially the same approach allows us to capture belief polarization.\nSuppose for simplicity that two agents start out with slightly different\nbeliefs regarding the value of some random variable X (think of X as\nrepresenting something like “O.J. Simpson is guilty”), and get the\nsame sequence s1,s2,…,sn of evidence regarding the value\nof X. (Thus, now the type consists of the initial belief, which can\nfor example be modeled as a probability or a sequence of evidence\nreceived earlier, and the new sequence of evidence). Both agents update\ntheir beliefs by conditioning. As before,\nthere is a cost of processing a piece of evidence, so once a DM gets\nsufficient evidence for either X=0 or X=1, he will stop processing\nany further evidence. If the initial evidence supports X=0, but the\nlater evidence supports X=1 even more strongly, the agent\nthat was initially inclined towards X=0 may raise his beliefs to be\nabove threshold, and thus stop\nprocessing, believing that X=0, while the agent initially\ninclined towards X=1 will continue processing and eventually\nbelieve that X=1.\n\n \n\n\n\n\n\n###### \nExample 2.4 (Status quo bias)\n\n\n\nThe status quo bias is well known. To take just one example,\nSamuelson and Zeckhauser \\citeyearSZ88\nobserved that when Harvard University professors were offered the\npossibility of enrolling in some new health-care options, older\nfaculty, who were already enrolled in a plan, enrolled in the new option\nmuch less often than new faculty. Assuming that all faculty evaluate\nthe plans in essentially the same way, this can be viewed as an instance\nof a status quo bias. Samuelson and Zeckhauser suggested a number of\nexplanations for this phenomenon, one of which was computational. As\nthey point out, the choice to undertake a careful analysis of the\noptions is itself a decision. Someone who is already enrolled in a plan\nand is relatively happy with it can rationally decide that it is not\nworth the cost of analysis (and thus just stick with her current plan),\nwhile someone who is not yet enrolled is more likely to decide that the\nanalysis is worthwhile. This explanation can be readily modeled in our\nframework. An agent’s type can be taken to be a description of the\nalternatives. A TM decides how many alternatives to analyze. There is a\ncost to analyzing an alternative, and we require that the decision made\nbe among the alternatives analyzed or the status quo. (We assume that\nthe status quo has already been analyzed, through experience.) If the\nstatus quo already offers an acceptable return, then a rational agent\nmay well decide not to analyze any new alternatives. Interestingly, Samuelson\nand Zeckhauser found that, in some cases, the status quo bias is even\nmore pronounced when there are more alternatives. We can capture this\nphenomenon if we assume that, for example, that there is an initial cost\nto analyzing, and the initial cost itself depends in part on how many\nalternatives there are to analyze (so that it is more expensive to\nanalyze only three alternatives if there are five alternatives\naltogether than if there only three alternatives). This would be\nreasonable if there is some setup cost in order to start the analysis,\nand the setup depends on the number of items to be analyzed.\n\n3 Value of computational information\n-------------------------------------\n\n\n\n### \n3.1 Value of information: a review\n\n\n\nBefore talking about value of computational information, we briefly\nreview value of information. Consider a standard decision problem.\nTo deal with value of information, we consider a partition of the state\nspace S. The question is what it\nwould be worth to the DM to find out which cell in the partition the true\nstate is in. (Think of the cells in the partition as corresponding to the\npossible realizations of a random variable X, and the value of information\nas corresponding to the value of learning the actual realization of\nX.) Of course, the value may depend on the DM’s type t.\nTo compute the value of information, we compute the\nexpected expected utility of the best action given type t conditional\non receiving the\ninformation, and compare it to the expected utility of the best action\nfor type t\nbefore finding out the information. We talk\nabout “expected expected utility” here because we need to take into\naccount how likely the DM is to discover that he is in a particular cell.\n\n\n\n\n###### Example 3.1\n\n\n\nSuppose that an investor can buy either a stock or bond. There are\ntwo states of the world, s1 and s2, and a single type t0. A\npriori, the investor thinks\ns1 has probability 2/3 and s2 has probability 1/3. Buying the\nbond gives him a guaranteed utility\nof 1 (in both s1 and s2). In state s1, buying the stock gives\na utility of 3; in state s2, buying the stock gives a utility of −4.\nClearly, a priori, buying the stock has an expected utility of 2/3, so\nbuying the bond has a higher expected utility. What is the value of\nlearning the true state (which corresponds to the partition {{s1},{s2}})? Clearly if the true state is s1, buying the stock is the\nbest action, and has (expected) utility 3; in state s2, buying the\nbond is the best action, and has expected utility 1. Thus, the expected\nexpected utility of\nthe information is (2/3)3+(1/3)1=7/3 (since with\nprobability 2/3 the DM expects to learn that it is state s1 and\nwith probability 1/3 the DM expects to learn that it is\ns2), and so the value of information is 7/3−1=4/3.\n\n \n\n\n\n\n\nWe leave it to the reader to write the obvious formal definition of\nvalue of information in type t.\n\n\n\n\n\n### \n3.2 Value of computational information\n\n\n\nIn our framework, it is easy to model the value of computational\ninformation: it is just a special case of value of information.\nFormally, given a standard decision problem (S,T,A,Pr,u), we must\nfirst extend it to a computational decision\nproblem (S′,T,A,Pr,M,C,O,u′). M is some\nappropriate set of TMs; each TM in M outputs an action in A given\nan element of S′×T. As discussed\nin Section [2](#S2 \"2 A computational framework ‣ I Don’t Want to Think About it Now: Decision Theory With Costly Computation\"), we need a richer state space to capture\nthe DM’s uncertainty regarding the output of the TM and the running time\nof the TM chosen. We can take S′ to have the form S×S′′, where s′′∈S′′ determines the running time and output of each\nTM M∈M. Similarly, u′((s,s′′),t0,M((s,s′′),t0),C((s,s′′),t0,M))\ndepends on u(s,M((s,s′′),t)) and C((s,s′′),t,M). (For example, we\ncan assume that u′((s,s′′),t0,M′((s,s′′),t0),C((s,s′′),t0,M))=u(s,M(s,t))−C((s,s′′),t,M), but we do not require this.)\n\n\n\n\nIn this setting, value of computational information\nessentially becomes a special case of value of information.\nThe only difference is that since the machine set M might be\ninfinite, there might not exist a machine with maximal expected\nutility. So, instead of comparing the expected utilities of the best\nmachines (before and after receiving the information), we compare the\n*supremum* of the expected utilities of machine M∈M (before\nand after receiving the information).\nMore precisely, given a partition Q of the state of nature, for every\ncell q∈Q, let Prq denote the distribution Pr conditioned\non event that the state of nature is part of the cell q.\nand let the random variable q(s,t) denote the cell of s.\nThe value of computational information (of learning what cell q∈Q the state of nature is in) is\n\n\n\n\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n|  | EPr[supM∈MEPrq[u′M]]−supM∈MEPr[u′M]. |  | (1) |\n\n\nThat is, on the left-hand side, we compute the expected expected utility\nby summing Pr(s,t)supM∈MEPrq(s,t)[u′M] over\nall pairs (s,t)∈S′×T.\nEffectively, this means that the DM chooses the best TM for each cell,\nafter being informed what the cell is. We discuss this issue in more\ndetail in Section [3.3](#S3.SS3 \"3.3 Value of conversation ‣ 3 Value of computational information ‣ I Don’t Want to Think About it Now: Decision Theory With Costly Computation\").\n\n\n\n\nUsing this formalism, we can consider the value of\nlearning that a particular TM M\nis a “good” algorithm for the problem at hand (i.e., either learning\nthat it always gives the correct answer, or always runs quickly), since\nthis is just an event, just like learning the value of some random\nvariable X is an event in a standard decision problem. In a\ncomputational decision problem,\nthe DM has a prior probability on M being good, and\ncan compute the expected increase in utility resulting from\nlearning that M is good.\n\n\n\n\n###### Example 3.2\n\n\n\nConsider the primality-testing problem from\nExample [2.1](#S2.ThmTHEOREM1 \"Example 2.1 ‣ 2 A computational framework ‣ I Don’t Want to Think About it Now: Decision Theory With Costly Computation\"), viewed as a computational decision problem\n(S,T,A,Pr,M,C,O,u′). Given the utility function, for\nsimplicity, we restrict M to to be a finite set of TMs that all halt\nwithin 220 steps. Thus, the DM is certain of the complexity of all\nTMs in M, and it is 0. On the other hand, the DM can still be uncertain\nabout the output of a TM, and of the “goodness” of the output.\nFor example, if M is a TM that halts after one step and outputs 0, the\nDM may be certain that M’s output is 0, but be uncertain as to the\n“goodness” of its output. Of course, such an algorithm might still be\nworth using: if the agent places a high prior probability on the input\nnot being prime (which would be the case if the input was chosen\nuniformly at random among all numbers less than 240), then the\nexpected utility of answering 0 for all inputs is quite high. A yet\nbetter algorithm would be to use some naive test for primality, run it\nfor 220 steps, and return 0 unless the algorithm says that the\nnumber is prime. The DM can then ask what the value is of learning\nwhether a specific TM M is good (i.e., returns the correct answer for\nall inputs). This depends on the DM’s prior probability that M is\ngood; but if it is low, then the value of information is also low.\nFinally, we can ask the value of being told a good algorithm (assume\nthat the DM is certain that there is a good algorithm, which always\nreturns the right answer in less than 220 steps, but doesn’t know\nwhich it is). This amounts to learning the value of a random variable\nX whose range is a subset of M, where X=M only if M is a good TM.\nClearly, after learning this information, the DM’s expected expected\nutility will be 10 (no matter what he learns, his expected utility will\nbe 10). The value of this information depends on the expected utility\nof the DM’s best current algorithm. Note that if the DM believes that\nthe input is chosen uniformly at random, then the expected utility of\neven the simple algorithm that returns 0 no matter what is close to 10.\nOn the other hand, if the DM believes that the input is chosen so that\nprimes and non-primes are equally likely, the best algorithm is unlikely\nto have expected utility much higher than 1 (the best strategy is likely\nto involve testing whether the number is prime, outputting the answer if the\ntests reveal whether the number is prime within 220 steps, and\noutputting 2 otherwise). In this case, the value of this information\nwould be close to 9.\n\n \n\n\n\n\n\n###### Example 3.3\n\n\n\nConsider the number-in-the-safe example, viewed as a\ncomputational\ndecision problem D=(S,T,A,Pr,M,C,O,u′).\nRecall that the state space S has the form (s1,s2,s3),\nwhere s1 is the number in the safe, s2 is the combination of\nthe safe, and s3 models the DM’s uncertainty regarding the output of\nTMs and their running time. There is only a single type, so we can\ntake T={t0}. We have the obvious uniform probability\non the first two components of S. Again, we restrict M to\nalgorithms that halt within 220 steps. If it takes one time unit\nto test a particular combination, and the DM believes that\nthe best approach is to generate some sequence of 220 combinations\nand test them, then it is clear that the DM believes that the expected\nutility of this approach is 2−20(1,000,000). Learning the first 20\ndigits makes the problem feasible, and thus results in an expected\nexpected utility of 1,000,000 (no matter which 20 digits are the right\nones, the expected utility is 1,000,000),\nand so has a high value of information.\n\n \n\n\n\n\n\n\n### \n3.3 Value of conversation\n\n\n\nRecall that, for value of information, we consider how much it\nis worth for a DM to find out which cell (in some partition of the state\nspace S) the true state s is in. In other words, we consider the\nquestion of how much it is worth for the DM to learn the value of f(s)\nof some\nfunction f on input the true state s. A more general setting\nconsiders how much it is worth for a DM to interact with\nanother TM I (for informant) that is running on input the true\nstate s.\n\n\n\n\n###### Example 3.4\n\n\n\nSuppose a number between 1 and 100 is chosen uniformly at random.\nIf the DM guesses the number correctly,\nhe receives a utility of 100; otherwise, he receives a\nutility of 0. Without any further information, the DM clearly cannot get\nmore than 1 in expected utility.\nBut if he can sequentially ask 7 yes/no questions, he can learn the\nnumber by\nusing binary search (i.e., first asking if the number is\ngreater than 50; if so, asking if it is greater than 75; etc.), getting\na utility of 100.\nThus, the\nvalue of a conversation with a machine that answers 7 yes/no questions\nis 99.\n\n \n\n\n\n\n\nThe *value of conversation with (a TM) I*\nfor standard decision problem\ncan be formalized in\nexactly the same way as value of information.\nFormalizing computational value of conversation\nrequires extending the\nnotion of computational decision problems to allow the DM to choose\namong *interactive* Turing machines M (this was already done in\n[[Halpern and PassHalpern and\nPass2010](#bib.bibx8)]).\n\nWe omit the formal definition of an\ninteractive Turing machine (see, for example, [[GoldreichGoldreich2001](#bib.bibx5)]); roughly\nspeaking, the machines use a special tape where the message to be sent\nis placed and another tape where a message to be received is written.\nWe assume that the DM chooses a TM M. M then proceeds in two\nphases. First there is a communication phase, where M converses with the\ninformant I; then, after the communication phase is over, M chooses\nan action for the underlying decision problem.\nNote that what an interactive TM does (that is, the message it sends or\nthe action it takes after the communication phase is over) can depend on its\ninput, the history of messages received, and the random coins it\ntosses (if it randomizes).\n\n\n\n\nWhen considering an interactive TM M, we\nassume that the complexity function C depends not\nonly on the machine M and its type t, but also on the messages that\nthe DM receives, and its random coin tosses.\nMore precisely, we define the *view* of an interactive machine M\nto be a string t;h;r in {0,1}∗;{0,1}∗;{0,1}∗,\nwhere t is the part of the type actually read by M, r is a\nfinite bitstring representing the string of random bits actually used,\nand h is a finite sequence of messages received and read.\nIf v=t;h;r, we take M(v) to be the output of M given the view.\n(Note that M(v) is either a message or an action in the underlying decision\nproblem, if the conversation phase is over.)\nWe now consider output functions\nO:M×S×{0,1}∗→IN, where M\ndenotes a set of (interactive) Turing Machines, and let O(M,s,v)\ndescribe what the DM thinks the output of the machine M is, given the\nview v, if the state of nature is s. Analogously, we now consider\ncomplexity functions C:M×S×{0,1}∗→IN, and let C(M,s,v) describe the complexity of\nthe machine M given the view v if the state of nature is s.\n\n\n\n\nWhen running with M, I gets as input the actual state s (we want\nto allow for the possibility that I has access to some featuers of\nthe world that M does not). That means that the state s is playing a\ndouble role here; it is used both to capture the fact that M is\ninteracting (in part) with nature, and may get some feedback from\nnature, and to model the DM’s uncertainty about the world.\nTo formalize the computational value of conversation with I,\nlet the random variable viewI,M(s,t,rI,rM) denote the\nview of the DM in state s\nat the end of the communication phase\nwhen communicating with I\n(running on input s with random tape rI) if the DM uses\nthe machine M (running on input t with random tape rM).\nWe assume that viewI,M(s,t,rI,rM) is generated by\ncomputing the messages sent by M and I at each step using O;\nthat is, M’s first message is O(M,s,v0), where v0 is M’s\ninitial view t;⟨⟩;r′M, where ⟨⟩ denotes the empty\nhistory, and r′M is a prefix of rM, M’s sequence of random bits\n(however much randomness M used to determine its first message);\nsimilarly, I’s first message is O(I,s,v1), where v1=s;⟨m0⟩;r′I, r′I is a finite\nprefix of rI, and m0 is the\nfirst message sent by M; and so on. This means that M’s beliefs\nabout the sequence of messages sent is determined by his beliefs about\nthe individual messages sent in all circumstances.444We can allow for M’s beliefs about the sequence of messages\nsent to be independent of his beliefs about individual messages, at the\nprice of complicating the framework.\n\n\n\n\n\nLet Pr+ denote the distribution on S×T×({0,1}∞)2 that is the\nproduct of Pr and the uniform distribution on pairs of random\nstrings.\nFor each pair (I,M) of interactive TMs,\nwe consider the random variable u′I,M defined on S×T×({0,1}∞)2) by taking u′I,M(s,t,rI,rM)=u′(s,t,O(M,s,v),C(M,s,v)), where v=viewI,M(s,t,rI,rM).\nThat is, u′I,M(s,t,rI,rM) describes the utility of the\nactions that result when M converses with I in state s given\ninput t and random tape rI for I and rM for M, taking the\ncomplexity of the interaction into account.\nThe expected utility of M when communicating with I is\nEPr+[u′I,M].\n\n\n\n\nThe computational value of conversation with I is now defined as\n\n\n\n\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n|  | supM∈MEPr+[u′I,M]−supM∈MEPr+[u′⊥,M], |  | (2) |\n\n\nwhere ⊥ is the “silent” machine that sends no messages.\nThat is, we compare the expected utility of best machine communicating\nwith I and the expected utility of the best machine that runs in\nisolation (i.e., is communicating with ⊥).\n\n\n\n\nThere is a subtlety in this definition that is worth emphasizing.\nIn general, when defining determining the best choice of TM, we must ask\nwhether it is reasonable to assume that the TM knows it’s input. That\nis, is the choice of TM being made before the DM knows the input, or\nafter?\nFor example, in the primality-testing problem of Example [2.1](#S2.ThmTHEOREM1 \"Example 2.1 ‣ 2 A computational framework ‣ I Don’t Want to Think About it Now: Decision Theory With Costly Computation\"), does\nthe DM choose a TM before knowing what number is or after. The answer\nto this question has no impact if we do not take complexity into\naccount, but it has a major impact if we do consider complexity.\nClearly, if we know what the input n is, we can choose a TM that is\nlikely to give the right answer for M. There is clearly a very\nefficient TM that gives the right answer for a specific input n; it is\nthe constant-time TM that just says “yes” if n is prime, or the\nconstant-time TM that just says “no” if n is not prime. Of course,\nif there is uncertainty as to the quality of the TM, the DM may be\nuncertain as to what utility he gets with each choice. But the\ncomplexity is guaranteed to be low.\nOn the other hand, if the choice of TM must be made before the TM knows\nthe input, even if the DM understands the quality of the TM chosen,\nthere may be no efficient TM that does well for all possible inputs.\n\n\n\n\nWhether\nit is appropriate\nto assume that the TM is chosen before or after the DM knows the\ninput depends on the application. For the most part, in [[Halpern and PassHalpern and\nPass2010](#bib.bibx8)], we\nimplicitly\nassumed that the choice was made before the DM knew the input; this\nseemed reasonable for the applications of that paper. Here, in the\ndefinition of value of computational information, we implicitly assumed\nthat the DM\nchose the best TM *after* learning the cell q (but before\nlearning the input t). We could also\nhave computed the value of computational information under the\nassumption that the TM had to be chosen before discovering q.\nThis would have amounted to putting the sup outside the scope of the\nEPr in Equation ([1](#S3.E1 \"(1) ‣ 3.2 Value of computational information ‣ 3 Value of computational information ‣ I Don’t Want to Think About it Now: Decision Theory With Costly Computation\")); this would have given\n\n\n\n\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n|  | supM∈MEPr[EPrq[u′M]]−supM∈MEPr[u′M]. |  | (3) |\n\n\nHere we are implicitly assuming that the TM M chosen takes the cell\nq(s,t) as an input; moreover, the TM “understands” that the\n“right” thing to do with q(s,t) is to condition (and thus, to\ncompute the expectation using Prq). Again, it is possible to\nallow more generality—the TM does not have to condition; the\ndefinition of computational value of of conversation implicitly allows this.\nWhile ([3](#S3.E3 \"(3) ‣ 3.3 Value of conversation ‣ 3 Value of computational information ‣ I Don’t Want to Think About it Now: Decision Theory With Costly Computation\")) is a perfectly sensible definition, it seems less\nappropriate\nwhen considering value of information, where a DM might be willing and\nable to devote a great deal of computation to a problem after getting\ninformation (although there may well be cases where ([3](#S3.E3 \"(3) ‣ 3.3 Value of conversation ‣ 3 Value of computational information ‣ I Don’t Want to Think About it Now: Decision Theory With Costly Computation\")) is\nindeed more appropriate than ([1](#S3.E1 \"(1) ‣ 3.2 Value of computational information ‣ 3 Value of computational information ‣ I Don’t Want to Think About it Now: Decision Theory With Costly Computation\"))).\n\n\n\n\n\nBy way of contrast, in ([2](#S3.E2 \"(2) ‣ 3.3 Value of conversation ‣ 3 Value of computational information ‣ I Don’t Want to Think About it Now: Decision Theory With Costly Computation\")), we are implicitly assuming that\nthe DM must choose\nthe interactive TM *before* learning the conversation; he does not\nget to choose a different one for each conversation.\nWe are evaluating the value of conversation with I, rather than the\nvalue of a particular conversation with I.\nThis is why we do not consider the expected expected utility of the\nbest algorithm\nafter receiving the information, but rather consider the expected\nutility of “communicating, interpreting, and finally acting”.\nIntuitively, we are assuming that a DM must choose a TM to interpret and\nmake use of the information gleaned from the conversation; we want to\ntake the cost of doing this interpretation into account, by choosing a\nTM that is able to interpret all possible computations.\n\n\n\n\nWe could in principle define a notion of value of\nparticular conversations with I, rather than the value of conversing\nwith I, by\nassuming that the DM chooses one TM that decides how to converse with\nI, and then, after the conversation, chooses the best TM to take\nadvantage of that particular conversation. Thus, at the second step,\nthe TM chosen would depend on the conversation. Formally, this amounts\nto having\nanother sup inside the scope of EPr+, but this seems less\nappropriate here.\n\n\n\n\nIf we do not take the cost of computation into account, whether we learn\nthe conversation before or after making the choice of TM is\nirrelevant. Indeed, the\nvalue of conversation can be\nviewed as a special case of value of information: for each\n“conversation-strategy” σ for the DM, simply consider the value\nof receiving a transcript of the conversation between I(s) and\nσ(t) (where t is the type of the DM). The value of conversation\nwith I is then simply the maximum value of information over all\nconversation strategies σ.\nBy way of contrast, we cannot reduce computational value of conversation\nto value of information. If there is a computational cost associated with\ncomputing the messages to send to I, the value of a conversation is no\nlonger just the maximum value of information.\n\n\n\n\n###### Example 3.5\n\n\n\nConsider the guess-the-number decision problem from\nExample [3.4](#S3.ThmTHEOREM4 \"Example 3.4 ‣ 3.3 Value of conversation ‣ 3 Value of computational information ‣ I Don’t Want to Think About it Now: Decision Theory With Costly Computation\") again. What is the value of a conversation\nwith\nan informant I that\npicks two large primes p and q, and sends the product\nN=pq to the DM? If the DM manages to factor N, I sends the DM the\nnumber chosen;\notherwise I simply aborts. Clearly, the value of information in the\n“best” conversation is 99 (the DM learns the number and gets a\nutility of 100). However, to implement this conversation requires the DM\nto factor large number. If computation is costly and\nfactoring is hard (as is widely believed), it might not be worth it for\nthe DM to attempt to factor the numbers. Thus, the value of\nconversation with I would be 0 (or close to 0).\n\n \n\n\n\n\n\n\n### \n3.4 Value of conversation and zero knowledge\n\n\n\nThe notion of a *zero-knowledge proof* [[Goldwasser, Micali, and RackoffGoldwasser\net al.1989](#bib.bibx6)] is one of the\ncentral notions\nin cryptography. Intuitively, a zero-knowledge proof allows an\nagent (called the *prover*) to convince another agent (called the\n*verifier*) of the validity of some statement x, without revealing\nany\nadditional information. For instance, using a zero-knowledge proof,\na prover can convince a verifier that a number N is the product of 2\nprimes, without actually revealing the primes.\nThe zero-knowledge requirement is formalized using the so-called\n*simulation paradigm*. Roughly speaking, a proof (P,V)\n(consisting of a strategy P for the prover, and a strategy V for the\nverifier) is said to be\n*perfect zero knowledge* if,\nfor every\nverifier strategy ~V, there exists a\nsimulator S that can\nreconstruct the verifier’s view of the interaction with the\nprover\nwith only a polynomial overhead in runtime.555Technically, what is reconstructed is a distribution over\nviews, since\nboth the prover and the verifier may randomize.\nNote that the simulator is running in isolation and, in particular, is\nnot allowed to interact with the prover.\nThus, intuitively, in a zero-knowledge proof, the verifier receives\nonly messages from the prover that it could have efficiently\ngenerated on its own by running the simulator S.\nThe notion of *precise zero-knowledge* [[Micali and PassMicali and Pass2006](#bib.bibx14)]\naims at more precisely quantifying the knowledge gained by the\nverifier. Intuitively, a zero-knowledge proof\nof a statement x\nhas precision p if any\nview that the verifier receives in time t after talking to the prover\ncan be reconstructed by the simulator (i.e., without the help of the\nprover) in time p(|x|,t).\n(There is nothing special about time\nhere; we can also consider precision with respect more general\ncomplexity measures.)\n\n\n\n\n\nAs we now show, there is a tight connection between the value of\nconversation for computational decision problems and zero knowledge.\nTo explain the ideas, we first need to introduce a new notion, which\nshould be of independent interest:\n*value of computational speedup*.\n\n\n\n\nComputers get faster and faster. How much is it\nworth for a DM to get a faster computer? To formalize this, we say that\na complexity function C′ is *at most a p-speedup* of the\ncomplexity function C if, for\nall machines M, types t, and states s, C′(M,s,t)≤C(M,s,t)≤p(C′(M,s,t)).\nIntuitively, if p is a constant, the value of a p-computational\nspeedup for a DM measures\nhow much it is worth for the DM to change to a\nmachine that runs p times faster than his current machine.\nMore precisely,\nthe *value of a p-speedup* in a computational decision\nproblem D=(S,T,A,Pr,M,C,O,u′) is the difference between the\nmaximum expected utility of the DM in D\nand the maximum expected utility in any decision problem D′ that is\nidentical to D except that the complexity function in D′ is\nC′, where C′ is at most a p-speedup of C.\n\n\n\n\nWe now\nsketch\n\nthe connection between zero-knowledge and value of\nconversation.\nGiven a language L, an objective complexity function C:M×T→IN (one that does not depend on the\nstate of nature), and length parameter n,\nlet DCL,n denote the class of computational decision\nproblems D=(S,T,A,Pr,C′,O,M,u), where M is the set of\ninteractive Turing machines, S⊆{0,1}n,\ntypes in T have the form x;t′, where x∈S and t′∈{0,1}∗, and Pr is\nsuch that Pr(s,t)>0 only if s=x, t=x;t′, and x∈L\n(so that the DM knows x and that x∈L).\nWe also require that (1) the DM does not have any uncertainty about the\noutput and the complexity functions: for all M,s,t, O(M,s,t)=M(t)\n(so the DM knows the correct outputs of all machines) and\nC′(M,s,t)=C(M,t) (so the DM knows the complexities of\nall machines); and (2)\nD is *monotone in complexity*:\nfor all types t∈T, actions a∈A, and complexities c≤c′,\nu(t,a,c)≥u(t,a,c′); that is, the DM never prefers to compute more.\n\nIn the full paper we prove the following.\n\n\n\n\n\n\n###### Theorem 3.6\n\n\n\nIf (P,V) is a zero-knowledge proof system for the\nlanguage L with precision p(⋅,⋅) with respect to the\ncomplexity function C, then for all n∈N and all\ncomputational decision problem D∈DCL,n, the value\nof conversation with P in D is no higher than the value of a\np(n,⋅)-computational speedup in D.\n\n\n\n\n\nThus, intuitively,\nif the DM is not uncertain about the complexities and the outputs of\nmachines, the value of participating in a zero-knowledge proof is\nnever higher than the value of (appropriately) upgrading computers.\n\n4 Discussion and Related Work\n------------------------------\n\n\n\nWe have introduced a formal framework for decision making that\nexplicitly takes into account the cost of computation. Doing so\nrequires taking\ninto account the uncertainty that a DM may have about the running time\nof an algorithm, and the quality of its output. The framework allows us\nto provide formal decision-theoretic solutions to well-known\nobservations such as the status-quo bias and belief polarization.\n\n\n\n\nOf course, we are far from the first to recognize that decision making\nrequires computation—computation for knowledge acquisition and for\ninference. Nor are we the first to suggest that the costs for such\ncomputation should be explicitly reflected in the utility function.\nHorvitz \\citeyearHor87 credits Good \\citeyearGood52 for being the\nfirst to explicitly integrate the costs of computation into a framework\nof normative rationality. For example, Good points out that “less good\nmethods may therefore sometimes be preferred” (for computational\nreasons). In a sequence of papers (see, for example, [[HorvitzHorvitz1987](#bib.bibx10), [HorvitzHorvitz2001](#bib.bibx11)]\nand the references therein), Horvitz continues this theme, investigating\nvarious policies that trade off deliberation and action, taking into\naccount computation costs. The framework presented here could be used\nto provide formal underpinnings to Horvitz’s work.\n\n\n\n\n\nIn terms of next steps, we have considered only one-shot decision\nproblems here. It would be very interesting to extend this framework\nto sequential decision problems. Moreover, we have assumed that agents can\ncompute the probability of (or, at least, are willing to assign a\nprobability to) events like “TM M will halt in 10,000 steps” or\n“the output of TM M solves the problem I am interested in on this\ninput”. Of course, calculating such probabilities itself involves\ncomputation. Similarly, calculating utilities may involve computation;\nalthough the utility was easy to compute in the simple examples we gave,\nthis is certainly not the case in general. It would be\nrelatively straightforward to extend our framework so that the TMs\ncomputed probabilities and utilities, as well as actions.\n\nIn this\nsetting, it may make sense to allow for a more general\nrepresentation of uncertainty. That is, an agent may start with a set\nof probabilities rather than a single probability, and may then refine\nthat set (perhaps to a single probability) over time. Similarly, an agent may\nstart with a set of possible utilities, rather than a single utility.\n\n\n\n\nOnce we allow sets of probabilities and utilities, we need to reconsider\nhow to define the notion of “optimal choice”. We could, for example,\nuse the maxmin expected utility approach of Gilboa\nand Schmeidler \\citeyearGS1989, associating with each action the\nworst-case expected utility (over all probability distributions and\nutility functions considered possible) and choose the action with the\nbest worst-case expected utility; other approaches may also be\nreasonable. \nHowever, once we do this, we need to think about what counts as an\n“optimal” decision if the DM does not have a probability and utility,\nor has a probability only on a coarse space.\nAn alternative approach might be to allow the set of TMs that the DM\nconsiders possible to increase (at some computational cost), but assume\nthat DM has all the relevant probabilistic information about the TMs\nthat it can choose among. As this discussion should make clear, there\nis much fascinating research to be done in this area.\n\n\n\n\nConsidering sequential decision-making also allows us to\nexamine *consistency* of decisions. Taking cost of computation\ninto account may make decisions appear consistent that are not\nconsistent without taking cost of computation into account.\nAs this discussion should make clear, there is much\nfascinating research to be done in this area.", "date_published": "2011-06-14T00:00:00Z", "authors": ["Joseph Y. Halpern", "Rafael Pass"], "summaries": [], "doi": null, "abstract": "Computation plays a major role in decision making. Even if an agent is willing to ascribe a probability to all states and a utility to all outcomes, and maximize expected utility, doing so might present serious computational problems. Moreover, computing the outcome of a given act might be difficult. In a companion paper we develop a framework for game theory with costly computation, where the objects of choice are Turing machines. Here we apply that framework to decision theory. We show how well-known phenomena like first-impression-matters biases (i.e., people tend to put more weight on evidence they hear early on), belief polarization (two people with different prior beliefs, hearing the same evidence, can end up with diametrically opposed conclusions), and the status quo bias (people are much more likely to stick with what they already have) can be easily captured in that framework. Finally, we use the framework to define some new notions: value of computational information (a computational variant of value of information) and and computational value of conversation.", "categories": ["cs.GT"], "journal_ref": null, "author_comment": "In Conference on Knowledge Representation and Reasoning (KR '10)", "primary_category": "cs.GT", "data_last_modified": "2011-06-14 09:52:38+00:00"}
{"id": "cb16287a3025fe592980c1a15afb18cc", "title": "Can Intelligence Explode?", "url": "https://arxiv.org/abs/1202.6177", "source": "arxiv", "source_type": "html", "text": "1 Introduction\n---------------\n\n\n\nThe technological singularity is a hypothetical scenario\nin which self-accelerating technological advances cause\ninfinite progress in finite time.\nThe most popular scenarios are an intelligence explosion\n[[Goo65](#bib.bibx13)] or a speed explosion [[Yud96](#bib.bibx38)] or a\ncombination of both [[Cha10](#bib.bibx5)].\nThis quite plausibly is accompanied by a radically changing\nsociety, which will become incomprehensible to us current\nhumans close to and in particular at or beyond the singularity.\nStill some general aspects may be predictable.\n\n\n\n\nAlready the invention of the first four-function mechanical\ncalculator one-and-a-half centuries ago [[Tho47](#bib.bibx33)]\ninspired dreams of self-amplifying technology. With the advent\nof general purpose computers and the field of artificial\nintelligence half-a-century ago, some mathematicians, such as\nStanislaw Ulam [[Ula58](#bib.bibx34)], I.J. Good [[Goo65](#bib.bibx13)], Ray\nSolomonoff [[Sol85](#bib.bibx32)], and Vernor Vinge\n[[Vin93](#bib.bibx35)] engaged in singularity thoughts.\nBut it was only in the last decade that the singularity idea\nachieved wide-spread popularity. Ray Kurzweil popularized the\nidea in two books [[Kur99](#bib.bibx20), [Kur05](#bib.bibx21)], and the\nInternet helped in the formation of an initially small\ncommunity discussing this idea. There are now annual\nSingularity Summits approaching a thousand participants per\nyear, and even a Singularity Institute.\n\n\n\n\nThe singularity euphoria seems in part to have been triggered\nby the belief that intelligent machines that possess general\nintelligence on a human-level or beyond can be built within our\nlife time, but it is hard to tell what is cause and effect. For\ninstance, there is now a new conference series on Artificial\nGeneral Intelligence (AGI) as well as some whole-brain\nemulation projects like Blue Brain [[dGSGR10](#bib.bibx10), [GLA+10](#bib.bibx12)].\n\n\n\n\nA loosely related set of communities which are increasing in\nmomentum are the “Immortalists” whose goal is to extend the\nhuman life-span, ideally indefinitely. Immortality and\nlife-extension organizations are sprouting like mushrooms:\ne.g. the Immortality and the Extropy Institute, the Humanity+\nAssociation, and the Alcor Life Extension, Acceleration\nStudies, Life Extension, Maximum Life, and Methusalem\nFoundations.\n\n\n\n\nThere are many different potential paths toward a singularity.\nMost of them seem to be based on software intelligence on\nincreasingly powerful hardware. Still this leaves many options,\nthe major ones being mind uploading (via brain scan) and subsequent improvement, knowledge-based reasoning and planning software (traditional AI research), artificial agents that learn from experience (the machine learning approach), self-evolving intelligent systems (genetic algorithms and artificial life approach), and the awakening of the Internet (or digital Gaia scenario).\nPhysical and biological limitations likely do not allow\nsingularities based on (non-software) physical brain\nenhancement technologies such as drugs and genetic engineering.\n\n\n\n\nAlthough many considerations in this article should be\nindependent of the realized path, I will assume a virtual\nsoftware society consisting of interacting rational agents\nwhose intelligence is high enough to construct the next\ngeneration of more intelligent rational agents. Indeed, one of\nthe goals of the article is to discuss what (super)intelligence\nand rationality could mean in this setup. For concreteness, the\nreader may want envisage an initial virtual world like Second\nLife that is similar to our current real world and inhabited by\nhuman mind uploads.\n\n\n\n\nMuch has been written about the singularity and David Chalmers’\narticle [[Cha10](#bib.bibx5)] covers quite wide ground. I\nessentially agree with all his statements, analysis, and also\nshare his personal opinions and beliefs. Most of his\nconclusions I will adopt without repeating his arguments. The\nmotivation of my article is to augment Chalmers’ and to discuss\nsome issues not addressed by him,\nin particular what it could mean for intelligence to explode.\nThis is less obvious than it might appear, and requires a more\ncareful treatment of what intelligence actually is.\nChalmers cleverly circumvents a proper discussion or\ndefinition of intelligence by arguing (a) there is something like intelligence,\n(b) there are many cognitive capacities correlated with intelligence, (c) these capacities might explode, therefore (d) intelligence might amplify and explode.\nWhile I mostly agree with this analysis, it does not tell us\nwhat a society of ultra-intelligent beings might look like. For\ninstance, if a hyper-advanced virtual world looks like random\nnoise for humans watching them from the “outside”, what does\nit mean for intelligence to explode for an outside observer?\nConversely, can an explosion actually be felt from the\n“inside” if everything is sped up uniformly? If neither\ninsiders nor outsiders experience an intelligence explosion,\nhas one actually happened?\n\n\n\n\nThe paper is organized as follows:\nSection [2](#S2 \"2 Will there be a Singularity ‣ Can Intelligence Explode?\") briefly recapitulates the most\npopular arguments why to expect a singularity and why “the\nsingularity is near” [[Kur05](#bib.bibx21)], obstacles towards a\nsingularity, and which choices we have.\nSection [3](#S3 \"3 The Singularity from the Outside ‣ Can Intelligence Explode?\") describes how an outside observer\nwho does not participate in the singularity might experience\nthe singularity and the consequences he faces. This will depend\non whether the singularity is directed inwards or outwards.\nSection [4](#S4 \"4 The Singularity from the Inside ‣ Can Intelligence Explode?\") investigates what a participant in\nthe singularity will experience, which is quite different from\nan outsider and depends on details of the virtual society; in\nparticular how resources are distributed.\nSection [5](#S5 \"5 Speed versus Intelligence Explosion ‣ Can Intelligence Explode?\") takes a closer look at what\nactually explodes when computing power is increased without\nlimits in finite real time. While by definition there is a\nspeed explosion, who, if anyone at all, perceives an\nintelligence explosion/singularity depends on what is sped up.\nIn order to determine whether anyone perceives an intelligence\nexplosion, it is necessary to clarify what intelligence\nactually is and what super-intelligences might do, which is\ndone in Section [6](#S6 \"6 What is Intelligence ‣ Can Intelligence Explode?\").\nThe considered formal theory of rational intelligence allows\ninvestigating a wide range of questions about\nsuper-intelligences, in principle rigorously mathematically.\nSection [7](#S7 \"7 Is Intelligence Unlimited or Bounded ‣ Can Intelligence Explode?\") elucidates the possibility that\nintelligence might be upper bounded, and whether this would\nprevent an intelligence singularity.\nSection [8](#S8 \"8 Singularitarian Intelligences ‣ Can Intelligence Explode?\") explains how a society right at the\nedge of an intelligence singularity might be theoretically studied\nwith current scientific tools.\nEven when setting up a virtual society in our image, there are\nlikely some immediate differences, e.g. copying and modifying\nvirtual structures, including virtual life, should be very easy.\nSection [9](#S9 \"9 Diversity Explosion and the Value of a Virtual Life ‣ Can Intelligence Explode?\") shows that this will have immediate\n(i.e. way before the singularity) consequences on the\ndiversity and value of life.\nSection [10](#S10 \"10 Personal Remarks ‣ Can Intelligence Explode?\") contains some personal remarks and\nSection [11](#S11 \"11 Conclusions ‣ Can Intelligence Explode?\") draws some conclusions.\n\n\n\n\nI will use the following terminology throughout this article.\nSome terms are taken over or refined from other authors and\nsome are new:\n\n\n* comp = computational resources\n* singularity = infinite change of an observable quantity in finite time\n* intelligence explosion = rapidly increasing intelligence far beyond human level\n* intelligence singularity = infinite intelligence in finite time\n* speed explosion/singularity = rapid/infinite increase of computational resources\n* outsider = biological = non-accelerated real human watching a singularity\n* insider = virtual = software intelligence participating in a singularity\n* computronium = theoretically best possible computer per unit of matter [[Bre65](#bib.bibx4)]\n* real/true intelligence = what we intuitively would regard as intelligence\n* numerical intelligence = numerical measure of intelligence like IQ score\n* AI = artificial intelligence (used generically in different ways)\n* AGI = artificial general intelligence = general human-level intelligence or beyond.\n* super-intelligence = AI+ = super-human intelligence [[Cha10](#bib.bibx5)]\n* hyper-intelligent = AI++ = incomprehensibly more intelligent than humans\n* vorld = virtual world. A popular oxymoron is ‘virtual reality’\n* virtual = software simulation in a computer.\n\n\nI drop the qualifier ‘virtual’ if this does not cause any\nconfusion, e.g. when talking about a human in a vorld, I mean\nof course a virtual human.\n\n\n\n\nI will assume a strong/physical form of the Church-Turing\nthesis that everything in nature can be calculated by a Turing\nmachine, i.e. our world including the human mind and body and\nour environment are computable [[Deu97](#bib.bibx9), [RH11](#bib.bibx27)].\nSo in the following I will assume without further argument that\nall physical processes we desire to virtualize are indeed\ncomputational and can be simulated by a sufficiently powerful\n(theoretical) computer.\nThis assumption simplifies many of the considerations to\nfollow, but is seldom essential, and could be lifted or\nweakened.\n\n2 Will there be a Singularity\n------------------------------\n\n\n\nThe current generations Y or Z may finally realize the age-old\ndream of creating systems with human-level intelligence or\nbeyond, which revived the interest in this endeavor. This\noptimism is based on the belief that in 20–30 years the raw\ncomputing power of a single computer will reach that of a human\nbrain and that software will not lag far behind.\nThis prediction is based on extrapolating Moore’s law, now\nvalid for 50 years, which implies that comp doubles every 1.5\nyears. As long as there is demand for more comp, Moore’s law\ncould continue to hold for many more decades before\ncomputronium is reached.\nFurther, different estimates of the computational capacity of a\nhuman brain consistently point towards 1015…1016\nflop/s [[Kur05](#bib.bibx21)]: Counting of neurons and synapses,\nextrapolating tiny-brain-part simulations, and comparing the\nspeech recognition capacities of computers to the auditory\ncortex.\n\n\n\n\nThe most compelling argument for the emergence of a singularity\nis based on Solomonoff’s law [[Sol85](#bib.bibx32)] which\nYudkowski [[Yud96](#bib.bibx38)] succinctly describes as follows:\n\n\n\n> \n> “If computing speeds double every two years,\n>   \n> \n> what happens when computer-based AIs are doing the research?\n>   \n> \n> Computing speed doubles every two years.\n>   \n> \n> Computing speed doubles every two years of work.\n>   \n> \n> Computing speed doubles every two subjective years of work.\n> \n>   \n> \n> Two years after Artificial Intelligences reach human\n> equivalence, their speed doubles. One year later, their speed\n> doubles again.\n> \n>   \n> \n> Six months - three months - 1.5 months … Singularity.”\n> \n> \n> \n\n\nInterestingly, if this argument is valid, then Moore’s law in a\nsense predicts its own break-down; not the usually anticipated\nslow-down, but an enormous acceleration of progress when\nmeasured in physical time.\n\n\n\n\nThe above acceleration would indeed not be the first time of an\nenormous acceleration in growth. The economist Robin Hanson\nargues that “Dramatic changes in the rate of economic growth\nhave occurred in the past because of some technological\nadvancement. Based on population growth, the economy doubled\nevery 250’000 years from the Paleolithic era until the\nNeolithic Revolution. This new agricultural economy began to\ndouble every 900 years, a remarkable increase. In the current\nera, beginning with the Industrial Revolution, the world s\neconomic output doubles every fifteen years, sixty times faster\nthan during the agricultural era.” Given the increasing role\nof computers in our economy, computers might soon dominate it,\nlocking the economic growth pattern to computing speed, which\nwould lead to a doubling of the economy every two (or more\nprecisely 1.5) years, another 10 fold increase. If the rise of\nsuperhuman intelligences causes a similar revolution, argues\nHanson [[Han08](#bib.bibx14)], one could expect the virtual economy\nto double on a monthly or possibly on a weekly basis. So the\ntechnological singularity phenomenon would be the next and\npossibly last growth acceleration.\nRay Kurzweil is a master of producing exponential, double\nexponential, and singular plots [[Kur05](#bib.bibx21)], but one has\nto be wary of data selection, as Juergen Schmidhuber has\npointed out.\n\n\n\n\nChalmers [[Cha10](#bib.bibx5)] discusses various potential\nobstacles for a singularity to emerge. He classifies them into\nstructural obstacles (limits in intelligence space, failure to\ntakeoff, diminishing returns, local maxima) and manifestation\nobstacles (disasters, disinclination, active prevention) and\ncorrelation obstacles.\nFor instance, self-destruction or a natural catastrophe\nmight wipe out the human race [[BC08](#bib.bibx2)].\n\n\n\n\nAlso, the laws of physics will likely prevent a singularity in\nthe strict mathematical sense. While some physical theories in\nisolation allow infinite computation in finite time (see Zeno\nmachines [[Wey27](#bib.bibx37)] and hypercomputation [[Cop02](#bib.bibx7)]\nin general), modern physics raises severe barriers\n[[Bre65](#bib.bibx4), [Bek03](#bib.bibx3), [Llo00](#bib.bibx24), [Aar05](#bib.bibx1)].\nBut even if so, today’s computers are so far away from these\nlimits, that converting our planet into computronium would\nstill result in a vastly different vorld, which is considered a\nreasonable approximation to a true singularity. Of course,\nengineering difficulties and many other obstructions may stop\nthe process well before this point, in which case the end\nresult may not account as a singularity but more as a phase\ntransition à la Hanson or even less spectacular.\n\n\n\n\nLike Chalmers, I also believe that disinclination is the most\n(but not very) likely defeater of a singularity. In the\nremainder of this article I will assume absence of any such\ndefeaters, and will only discuss the structural obstacles\nrelated to limits in intelligence space later.\n\n\n\n\nThe appearance of the first super-intelligences is usually\nregarded as the ignition of the detonation cord towards the\nsingularity – the point of no return. But it might well be\nthat a singularity is already now unavoidable. Politically it\nis very difficult (but not impossible) to resist technology or\nmarket forces as e.g. the dragging discussions on climate\nchange vividly demonstrate, so it would be similarly difficult\nto prevent AGI research and even more so to prevent the\ndevelopment of faster computers. Whether we are before, at, or\nbeyond the point of no return is also philosophically intricate\nas it depends on how much free will one attributes to people\nand society;\nlike a spaceship close to the event horizon might in principle\nescape a black hole but is doomed in practice due to limited\npropulsion.\n\n3 The Singularity from the Outside\n-----------------------------------\n\n\n\nLet us first view the singularity from the outside. What will\nobservers who do not participate in it “see”. How will it\naffect them?\n\n\n\n\nFirst, the hardware (computers) for increasing comp must be\nmanufactured somehow. As already today, this will be done by\n(real) machines/robots in factories. Insiders will provide\nblue-prints to produce better computers and better machines\nthat themselves produce better computers and better machines ad\ninfinitum at an accelerated pace. Later I will explain why\ninsiders desire more comp. Non-accelerated real human\n(outsiders) will play a diminishing role in this process due to\ntheir cognitive and speed limitations. Quickly they will only\nbe able to passively observe some massive but incomprehensible\ntransformation of matter going on.\n\n\n\n\nImagine an inward explosion, where a fixed amount of matter is\ntransformed into increasingly efficient computers until it\nbecomes computronium. The virtual society like a\nwell-functioning real society will likely evolve and progress,\nor at least change. Soon the speed of their affairs will make\nthem beyond comprehension for the outsiders. For a while,\noutsiders may be able to make records and analyze them in slow\nmotion with an increasing lag. Ultimately the outsiders’\nrecording technology will not be sufficient anymore, but some\ncoarse statistical or thermodynamical properties could still be\nmonitored, which besides other things may indicate an upcoming\nphysical singularity. I doubt that the outsiders will be able\nto link what is going on with intelligence or a technological\nsingularity anymore.\n\n\n\n\nInsiders may decide to interact with outsiders in slow motion\nand feed them with pieces of information at the maximal\ndigestible rate, but even with direct brain-computer\ninterfaces, the cognitive capacity of a human brain is bounded\nand cannot explode. A technologically augmented brain may\nexplode, but what would explode is the increasingly dominant\nartificial part, rendering the biological brain eventually\nsuperfluous — a gradual way of getting sucked into the inside\nworld. For this reason, also intelligence amplification by\nhuman-computer interfaces are only temporarily viable before\nthey either break down or the extended human becomes\neffectively virtual.\n\n\n\n\nAfter a brief period, intelligent interaction between insiders\nand outsiders becomes impossible. The inside process may from\nthe outside resemble a black hole watched from a safe distance,\nand look like another interesting physical, rather than\nsocietal, phenomenon.\n\n\n\n\nThis non-comprehensibility conclusion can be supported by an\ninformation-theoretic argument: The characterization of our\nsociety as an information society becomes even better, if not\nperfect, for a virtual society. There is lots of motivation to\ncompress information (save memory, extract regularities, and\nothers), but it is well-known [[LV08](#bib.bibx25)] that maximally\ncompressed information is indistinguishable from random noise.\nAlso, if too much information is produced, it may actually\n“collapse”. Here, I am not referring to the formation of\nblack holes [[Bek03](#bib.bibx3)], but to the fact that a\nlibrary that contains all possible books has zero information\ncontent (cf. the Library of Babel). Maybe a society of\nincreasing intelligence will become increasingly\nindistinguishable from noise when viewed from the outside.\n\n\n\n\nLet us now consider outward explosion, where an increasing\namount of matter is transformed into computers of fixed\nefficiency (fixed comp per unit time/space/energy). Outsiders\nwill soon get into resource competition with the expanding\ncomputer world, and being inferior to the virtual\nintelligences, probably only have the option to flee. This\nmight work for a while, but soon the expansion rate of the\nvirtual world should become so large, theoretically only\nbounded by the speed of light, that escape becomes impossible,\nending or converting the outsiders’ existence.\n\n\n\n\nSo while an inward explosion is interesting, an outward\nexplosion will be a threat to outsiders. In both cases,\noutsiders will observe a speedup of cognitive processes and\npossibly an increase of intelligence up to a certain point. In\nneither case will outsiders be able to witness a true\nintelligence singularity.\n\n\n\n\n\nHistorically, mankind was always outward exploring; just in\nrecent times it has become more inward exploring. Now people\nmore and more explore virtual worlds rather than new real\nworlds. There are two reasons for this. First, virtual worlds\ncan be designed as one sees fit and hence are arguably more\ninteresting, and second, outward expansion now means deep sea\nor space, which is an expensive endeavor. Expansion usually\nfollows the way of least resistance.\n\n\n\n\nCurrently the technological explosion is both inward and\noutward (more and faster computers). Their relative speed in\nthe future will depend on external constraints. Inward\nexplosion will stop when computronium is reached. Outward\nexplosion will stop when all accessible convertible matter has\nbeen used up (all on earth, or in our galaxy, or in our\nuniverse).\n\n4 The Singularity from the Inside\n----------------------------------\n\n\n\nLet us now consider the singularity from the inside.\nWhat will a participant experience?\n\n\n\n\nMany things of course will depend on how the virtual world is\norganized. It is plausible that various characteristics of our\ncurrent society will be incorporated, at least initially. Our\nworld consists of a very large number of individuals, who\npossess some autonomy and freedom, and who interact with each\nother and with their environment in cooperation and in\ncompetition over resources and other things.\nLet us assume a similar setup in a virtual world of intelligent\nactors. The vorld might actually be quite close to our real\nworld. Imagine populating already existing virtual worlds like\nSecond Life or World of Warcraft with intelligent agents\nsimulating scans of human brains.\n\n\n\n\nConsider first a vorld based on fixed computational resources.\nAs indicated, initially, the virtual society might be similar\nto its real counter-part, if broadly understood. But some\nthings will be easier, such a duplicating (virtual) objects and\ndirected artificial evolution. Other things will be harder or\nimpossible, such as building faster virtual computers and\nfancier gadgets reliant on them.\nThis will affect how the virtual society will value different\nthings (the value of virtual life and its implications will be\ndiscussed later), but I would classify most of this as a\nchange, not unlike in the real world when discovering or\nrunning out of some natural resource or adapting to new models\nof society and politics.\nOf course, the virtual society, like our real one, will also\ndevelop: there will be new inventions, technologies, fashions,\ninterests, art, etc., all virtual, all software, of course, but\nfor the virtuals it will feel real.\nIf virtuals are isolated from the outside world and have\nknowledge of their underlying computational processes, there\nwould be no quest for a virtual theory of everything\n[[Hut10](#bib.bibx17)], since they would already know it.\nThe evolution of this vorld might include weak singularities in\nthe sense of sudden phase transitions or collapses of the\nsociety, but an intelligence explosion with fixed comp, even\nwith algorithmic improvements seems implausible.\n\n\n\n\nConsider now the case of a vorld with increasing comp. If extra\ncomp is used for speeding up the whole virtual world uniformly,\nvirtuals and their virtual environment alike, the inhabitants\nwould actually not be able to recognize this. If their\nsubjective thought processes will be sped up at the same rate\nas their surroundings, nothing would change for them. The only\ndifference, provided virtuals have a window to the outside real\nworld, would be that the outside world slows down. If comp is\nsped up hyperbolically, the subjectively infinite future of the\nvirtuals would fit into finite real time: For the virtuals, the\nexternal universe would get slower and slower and ultimately\ncome to a halt. Also outsiders would appear slower (but not\ndumber).\n\n\n\n\nThis speed-up/slow-down phenomenon is inverse compared to\nflying into a black hole. An astronaut flying into a black hole\nwill pass the Schwarzschild radius and hit the singularity in\nfinite subjective time. For an outside observer, though, the\nastronaut gets slower and slower and actually takes infinite\ntime to vanish behind the Schwarzschild radius.\n\n\n\n\nIf extra comp is exclusively used to expand the vorld and add\nmore virtuals, there is no individual speedup, and the bounded\nindividual comp forces intelligence to stay bounded, even with\nalgorithmic improvements. But larger societies can also evolve\nfaster (more inventions per real time unit), and if regarded as\na super-organism, there might be an intelligence explosion, but\nnot necessarily so: Ant colonies and bee hives seem more\nintelligent than their individuals in isolation, but it is not\nobvious how this scales to unbounded size. Also, there seems to\nbe no clear positive correlation between the number of\nindividuals involved in a decision process and the intelligence\nof its outcome.\n\n\n\n\nIn any case, the virtuals as individuals will not experience an\nintelligence explosion, even if there was one. The outsiders\nwould observe virtuals speeding up beyond comprehension and\nwould ultimately not recognize any further intelligence\nexplosion.\n\n\n\n\nThe scenarios considered in this and the last section are of\ncourse only caricatures. An actual vorld will more likely\nconsist of a wide diversity of intelligences: faster and slower\nones, higher and lower ones, and a hierarchy of super-organisms\nand sub-vorlds. The analysis becomes more complicated, but the\nfundamental conclusion that an intelligence explosion might be\nunobservable does not change.\n\n5 Speed versus Intelligence Explosion\n--------------------------------------\n\n\n\nThe comparison of the inside and outside view has revealed that\na speed explosion is not necessarily an intelligence explosion.\nIn the extreme case, insiders may not experience anything and\noutsiders may witness only noise.\n\n\n\n\nConsider an agent interacting with an environment. If both are\nsped up at the same rate, their behavioral interaction will not\nchange except for speed. If there is no external clock\nmeasuring absolute time, there is no net effect at all.\n\n\n\n\nIf only the environment is sped up, this has the same effect as\nslowing down the agent. This does not necessarily make the\nagent dumber. He will receive more information per action, and\ncan make more informed decisions, provided he is left with\nenough comp to process the information. Imagine being inhibited\nby very slowly responding colleagues. If you could speed them\nup, this would improve your own throughput, and subjectively\nthis is the same as slowing yourself down. But (how much) can\nthis improve the agent’s intelligence? In the extreme case,\nassume the agent has instant access to all information, not\nmuch unlike we already have by means of the Internet but much\nfaster. Both usually increase the quality of decisions, which\nmight be viewed as an increase in intelligence. But intuitively\nthere should be a limit on how much information a comp-limited\nagent can usefully process or even search through.\n\n\n\n\nConsider now the converse and speed up the agent (or\nequivalently slow down the environment). From the agent’s view,\nhe becomes deprived of information, but has now increased\ncapacity to process and think about his observations. He\nbecomes more reflective and cognitive, a key aspect of\nintelligence, and this should lead to better decisions. But\nalso in this case, although it is much less clear, there might\nbe a limit to how much can be done with a limited amount of\ninformation.\n\n\n\n\nThe speed-up/slow-down effects might be summarized as follows:\n\n\n\n\n* Performance per unit real time:\n* Speed of agent positively correlates with cognition and intelligence of decisions\n* Speed of environment positively correlates with informed decisions\n* Performance per subjective unit of agent time from agent’s perspective:\n* slow down environment = increases cognition and intelligence but decisions become less informed\n* speed up environment = more informed but less reasoned decisions\n* Performance per environment time from environment perspective:\n* speed up agent = more intelligent decisions\n* slow down agent = less intelligent decisions\n\n\n\n\nI have argued that more comp, i.e. speeding up hardware, does\nnot necessarily correspond to more intelligence. But then the\nsame could be said of software speedups, i.e. more efficient\nways of computing the same function. If two agent algorithms\nhave the same I/O behavior, just one is faster than the other,\nis the faster one more intelligent?\n\n\n\n\nAn interesting related question is whether progress in AI has\nbeen mainly due to improved hardware or improved software. If\nwe believe in the former, and we accept that speed is\northogonal to intelligence, and we believe that humans are\n“truly” intelligent (a lot of ifs), then building AGIs may\nstill be far distant.\n\n\n\n\nAs detailed in Section [7](#S7 \"7 Is Intelligence Unlimited or Bounded ‣ Can Intelligence Explode?\"), if intelligence is\nupper-bounded (like playing optimal minimax chess), then past\nthis bound, intelligences can only differ by speed and\navailable information to process. In this case, and if humans\nare not too far below this upper bound (which seems unlikely),\noutsiders could, as long as their technology permits, record\nand play a virtual world in slow motion and be able to grasp\nwhat is going on inside.\n\n\n\n\nIn this sense, a singularity may be more interesting for\noutsiders than for insiders. On the other hand, insiders\nactively “live” potential societal changes, while outsiders\nonly passively observe them.\n\n\n\n\nOf course, more comp only leads to more intelligent decisions\nif the decision algorithm puts it to good use. Many algorithms\nin AI are so-called anytime algorithms that indeed produce\nbetter results if given more comp. In the limit of infinite\ncomp, in simple and well-defined settings (usually search and\nplanning problems), some algorithms can produce optimal\nresults, but for more realistic complex situations (usually\nlearning problems), they saturate and remain sub-optimal\n[[RN10](#bib.bibx28)]. But there is one algorithm, namely AIXI\ndescribed in Section [7](#S7 \"7 Is Intelligence Unlimited or Bounded ‣ Can Intelligence Explode?\"), that is able to make\noptimal decisions in arbitrary situations given infinite comp.\n\n\n\n\nTogether this shows that it is non-trivial to draw a clear\nboundary between speed and intelligence.\n\n6 What is Intelligence\n-----------------------\n\n\n\nThere have been numerous attempts to define intelligence; see\ne.g. [[LH07a](#bib.bibx22)] for a collection of 70+ definitions\nfrom the philosophy, psychology, and AI literature, by\nindividual researchers as well as collective attempts.\n\n\n\n\nIf/since intelligence is not (just) speed, what is it then?\nWhat will super-intelligences actually do?\n\n\n\n\nHistorically-biologically, higher intelligence, via some\ncorrelated practical cognitive capacity, increased the chance\nof survival and number of offspring of an individual and the\nsuccess of a species. At least for primates leading to homo\nsapiens this was the case until recently. Within the human\nrace, intelligence is now positively correlated with power\nand/or economic success [[Gea07](#bib.bibx11)] and actually negatively\nwith number of children [[Kan07](#bib.bibx19)]. Genetic evolution\nhas been largely replaced by memetic evolution\n[[Daw76](#bib.bibx8)], the replication, variation, selection, and\nspreading of ideas causing cultural evolution.\n\n\n\n\nWhat activities could be regarded as or are positively\ncorrelated with intelligence?\nSelf-preservation? Self-replication? Spreading? Creating faster/better/higher intelligences? Learning as much as possible? Understanding the universe? Maximizing power over men and/or organizations? Transformation of matter (into computronium?)? Maximum self-sufficiency?\nThe search for the meaning of life?\n\n\n\n\nHas intelligence more to do with thinking or is thinking only a\ntool for acting smartly? Is intelligence something\nanthropocentric or does it exist objectively?\nWhat are the relations between other predicates of human\n“spirit” like consciousness, emotions, and religious faith to\nintelligence? Are they part of it or separate characteristics\nand how are they interlinked?\n\n\n\n\nOne might equate intelligence with rationality, but what is\nrationality? Reasoning, which requires internal logical\nconsistency, is a good start for a characterization but is\nalone not sufficient as a definition. Indiscriminately\nproducing one true statement after the other without\nprioritization or ever doing anything with them is not too\nintelligent (current automated theorem provers can already do\nthis).\n\n\n\n\nIt seems hard if not impossible to define rationality without\nthe notion of a goal. If rationality is reasoning towards a\ngoal, then there is no intelligence without goals. This idea\ndates back at least to Aristotle, if not further; see\n[[LH07b](#bib.bibx23)] for details. But what are the goals?\nSlightly more flexible notions are that of expected utility\nmaximization and cumulative life-time reward maximization\n[[RN10](#bib.bibx28)]. But who provides the rewards, and how? For\nanimals, one might try to equate the positive and negative\nrewards with pleasure and pain, and indeed one can explain a\nlot of behavior as attempts to maximize rewards/pleasure.\nHumans seem to exhibit astonishing flexibility in choosing\ntheir goals and passions, especially during childhood.\nGoal-oriented behavior often appears to be at odds with\nlong-term pleasure maximization. Still, the evolved biological\ngoals and desires to survive, procreate, parent, spread,\ndominate, etc. are seldom disowned.\n\n\n\n\nBut who sets the goal for super-intelligences and how? When\nbuilding AIs or tinkering with our virtual selves, we could try\nout a lot of different goals, e.g. selected from the list\nabove or others. But ultimately we will lose control, and the\nAGIs themselves will build further AGIs (if they were motivated\nto do so) and this will gain its own dynamic. Some aspects of\nthis might be independent of the initial goal structure and\npredictable. Probably this initial vorld is a society of\ncooperating and competing agents. There will be competition\nover limited (computational) resources, and those virtuals who\nhave the goal to acquire them will naturally be more successful\nin this endeavor compared to those with different goals.\nOf course, improving the efficiency of resource use is\nimportant too, e.g. optimizing own algorithms, but still,\nhaving more resources is advantageous.\nThe successful virtuals will spread (in various ways), the\nothers perish, and soon their society will consist mainly of\nvirtuals whose goal is to compete over resources, where\nhostility will only be limited if this is in the virtuals’ best\ninterest. For instance, current society has replaced war mostly\nby economic competition, since modern weaponry makes most wars\na loss for both sides, while economic competition in most cases\nbenefits the better.\n\n\n\n\nWhatever amount of resources are available, they will (quickly)\nbe used up, and become scarce. So in any world inhabited by\nmultiple individuals, evolutionary and/or economic-like forces\nwill “breed” virtuals with the goal to acquire as much (comp)\nresources as possible. This world will likely neither be heaven\nnor hell for the virtuals. They will “like” to fight over\nresources, and the winners will “enjoy” it, while the losers\nwill “hate” it. In such evolutionary vorlds, the ability to\nsurvive and replicate is a key trait of intelligence. On the\nother hand, this is not a sufficient characterization, since\ne.g. bacteria are quite successful in this endeavor too, but\nnot very intelligent.\n\n\n\n\nFinally, let us consider some alternative (real or virtual)\nworlds.\nIn the human world, local conflicts and global war is\nincreasingly replaced by economic competition, which might\nitself be replaced by even more constructive global\ncollaboration, as long as violaters can quickly and effectively\n(and non-violently?) be eliminated.\nIt is possible that this requires a powerful single (virtual)\nworld government, to give up individual privacy, and to\nseverely limit individual freedom (cf. ant hills or bee hives).\nAn alternative societal setup that can only produce conforming\nindividuals might only be possible by severely limiting\nindividual’s creativity (cf. flock of sheep or school of fish).\n\n\n\n\nSuch well-regulated societies might better be viewed as a\nsingle organism or collective mind. Or maybe the vorld is\ninhabited from the outset by a single individual.\nBoth vorlds could look quite different and more peaceful than\nthe traditional ones created by evolution.\nIntelligence would have to be defined quite differently in such\nvorlds.\nMany science fiction authors have conceived and extensively\nwritten about a plethora of other future, robot, virtual, and\nalien societies in the last century.\n\n\n\n\nIn the following I will only consider vorlds shaped by\nevolutionary pressures as described above.\n\n7 Is Intelligence Unlimited or Bounded\n---------------------------------------\n\n\n\nAnother important aspect of intelligence is how flexible or\nadaptive an individual is. Deep blue might be the best chess\nplayer on Earth, but is unable to do anything else. On the\ncontrary, higher animals and humans have remarkably broad\ncapacities and can perform well in a wide range of\nenvironments.\n\n\n\n\nIn [[LH07b](#bib.bibx23)] intelligence has been defined as the\nability to achieve goals in a wide range of environments. It\nhas been argued that this is a very suitable characterization,\nimplicitly capturing most, if not all traits of rational\nintelligence, such as reasoning, creativity, generalization,\npattern recognition, problem solving, memorization, planning,\nlearning, self-preservation, and many others. Furthermore, this\ndefinition has been rigorously formalized in mathematical\nterms. It is non-anthropocentric, wide-range, general,\nunbiased, fundamental, objective, complete, and universal. It\nis the most comprehensive formal definition of intelligence so\nfar. It assigns a real number Υ between zero and one\nto every agent, namely the to-be-expected performance averaged\nover all environments/problems the agent potentially has to\ndeal with, with an Ockham’s razor inspired prior weight for\neach environment. Furthermore there is a maximally intelligent\nagent, called AIXI, w.r.t. this measure. The precise formal\ndefinitions and details can be found in [[LH07b](#bib.bibx23)],\nbut do not matter for our purpose. This paper also contains a\ncomprehensive justification and defense of this approach.\n\n\n\n\nThe theory suggests that there is a maximally intelligent\nagent, or in other words, that intelligence is upper bounded\n(and is actually lower bounded too). At face value, this would\nmake an intelligence explosion impossible.\n\n\n\n\nTo motivate this possibility, consider some simple examples.\nAssume the vorld consists only of tic-tac-toe games, and the\ngoal is to win or second-best not lose them. The notion of\nintelligence in this simple vorld is beyond dispute. Clearly\nthere is an optimal strategy (actually many) and it is\nimpossible to behave more intelligently than this strategy. It\nis even easy to artificially evolve or learn these strategies\nfrom repeated (self)play [[Hoc03](#bib.bibx15), [VNH+11](#bib.bibx36)].\nSo in this vorld there clearly will be no intelligence\nexplosion or intelligence singularity, even if there were a\nspeed explosion.\n\n\n\n\nWe get a slightly different situation when we replace\ntic-tac-toe by chess. There is also an optimal way of playing\nchess, namely minimax tree search to the end of the game, but\nunlike in tic-tac-toe this strategy is computationally\ninfeasible in our universe. So in theory (i.e. given enough\ncomp) intelligence is upper-bounded in a chess vorld, while in\npractice we can get only ever closer but never reach the bound.\n(Actually there might be enough matter in the universe to build\nan optimal chess player, but likely not an optimal Go player.\nIn any case it is easy to design a game that is beyond the\ncapacity of our accessible universe, even if completely\nconverted into computronium).\n\n\n\n\nStill, this causes two potential obstacles for an intelligence\nexplosion. First, we are only talking about the speed of\nalgorithms, which I explained before not to equate with\nintelligence. Second, intelligence is upper bounded by the\ntheoretical optimal chess strategy, which makes an intelligence\nexplosion difficult but not necessarily impossible: Assume the\noptimal program has intelligence I=1 and at real time t<1\nwe have access to or evolved a chess program with intelligence\nt. This approaches 1 in finite time, but doesn’t “explode”.\nBut if we use the monotone transformation 1/(1−I) to measure\nintelligence, the chess program at time t has transformed\nintelligence 1/(1−t) which tends to infinity for t→1.\nWhile this is a mathematical singularity, it is likely not\naccompanied by a real intelligence explosion. The original\nscale seems more plausible in the sense that t+0.001 is just\na tiny bit more intelligent than t, and 1 is just 1000\ntimes more intelligent than 0.001 but not infinitely more.\nAlthough the vorld of chess is quite rich, the real world is\nvastly and possibly unlimitedly richer. In such a more open\nworld, the intelligence scale may be genuinely unbounded, but\nnot necessarily as we will see.\nIt is not easy though to make these arguments rigorous.\n\n\n\n\nLet us return to the real world and intelligent measure\nΥ upper bounded by Υmax=Υ(AIXI).\nSince AIXI is incomputable, we can never reach intelligence\nΥmax in a computational universe, but similarly to\nthe chess example we can get closer and closer. The numerical\nadvance is bounded, and so is possibly the real intelligence\nincrease, hence no intelligence explosion. But it might also be\nthe case that in a highly sophisticated AIXI-close society, one\nagent beating another by a tiny epsilon on the Υ-scale\nmakes all the difference for survival and/or power and/or other\nmeasurable impact like transforming the universe. In many sport\ncontests split seconds determine a win, and the winner takes it\nall — an admittedly weak analogy.\n\n\n\n\nAn interesting question is where humans range on the\nΥ-scale: is it so low with so much room above that\noutsiders would effectively experience an intelligence\nexplosion (as far as recognizable), even if intelligence is\nultimately upper bounded? Or are we already quite close to the\nupper bound, so that even AGIs with enormous comp (but\ncomparable I/O limitations) would just be more intelligent but\nnot incomprehensibly so. We tend to believe that we are quite\nfar from Υ, but is this really so? For instance, what\nhas once been argued to be irrational (i.e. not very\nintelligent) behavior in the past, can often be regarded as\nrational w.r.t. the appropriate goal. Maybe we are already\nnear-optimal goal achievers. I doubt this, but cannot rule it\nout either.\n\n\n\n\nHumans are not faster but more intelligent than dogs, and dogs\nin turn are more intelligent than worms and not just faster,\neven if we cannot pinpoint exactly why we are more intelligent:\nis it our capacity to produce technology or to transform our\nenvironment on a large scale or consciousness or domination\nover all other species? There are no good arguments why humans\nshould be close to the top of the possible biological\nintelligence scale, and even less so on a vorld scale. By\nextrapolation it is plausible that a vorld of much more\nintelligent trans-humans or machines is possible. They will\nlikely be able to perform better in an even wider range of\nenvironments on an even wider range of problems than humans.\nWhether this results in anything that deserves the name\nintelligence explosion is unclear.\n\n8 Singularitarian Intelligences\n--------------------------------\n\n\n\nConsider a vorld inhabited by competing agents, initialized\nwith human mind-uploads or non-human AGIs, and increasing comp\nper virtual.\nSections [6](#S6 \"6 What is Intelligence ‣ Can Intelligence Explode?\") and [7](#S7 \"7 Is Intelligence Unlimited or Bounded ‣ Can Intelligence Explode?\") then indicate that\nevolutionary pressure increases the individuals’ intelligence\nand the vorld should converge to a society of AIXIs.\nAlternatively, if we postulate an intelligence singularity and\naccept that AIXI is the most intelligent agent, we arrive at\nthe same conclusion. More precisely, the society consists of\nagents that aim at being AIXIs only being constrained by comp.\nIf this is so, the intelligence singularity might be identified\nwith a society of AIXIs, so studying AIXI can tell us something\nabout how a singularity might look like. Since AIXI is\ncompletely and formally defined, properties of this society can\nbe studied rigorously mathematically. Here are some questions\nthat could be asked and answered:\n\n\n\n\nWill a pure reward maximizer such as AIXI listen to and trust a teacher? Likely yes. Will it take drugs (i.e. hack the reward system)? Likely no, since cumulative long-term reward would be small (death). Will AIXI replicate itself or procreate? Likely yes, if AIXI believes that clones or descendants are useful for its own goals. Will AIXI commit suicide? Likely yes (no), if AIXI is raised to believe in going to heaven (hell) i.e. maximal (minimal) reward forever. Will sub-AIXIs self-improve? Likely yes, since this helps to increase reward. Will AIXI manipulate or threaten teachers to give more reward? Likely yes. Are pure reward maximizers like AIXI egoists, psychopaths, and/or killers, or will they be friendly (altruism as extended ego(t)ism)? Curiosity killed the cat and maybe AIXI, or is extra reward for curiosity necessary? Immortality can cause laziness. Will AIXI be lazy? Can self-preservation be learned or need (parts of) it be innate. How will AIXIs interact/socialize in general?\n\n\n\n\nFor some of these questions, partial and informal discussions\nand plausible answers are available, and a couple have been\nrigorously defined, studied and answered, but most of them are open to\ndate\n[[Hut05](#bib.bibx16), [Sch07](#bib.bibx31), [OR11](#bib.bibx26), [RO11](#bib.bibx29), [Hut12](#bib.bibx18)].\nBut the AIXI theory has the potential to arrive at definite answers\nto various questions regarding the social behavior of\nsuper-intelligences close to or at an intelligence singularity.\n\n9 Diversity Explosion and the Value of a Virtual Life\n------------------------------------------------------\n\n\n\nAs indicated, some things will be harder or impossible in a\nvirtual world (e.g. to discover new physics) but many things\nshould be easier. Unless a global copy protection mechanism is\ndeliberately installed (like e.g. in Second Life) or copyright\nlaws prevent it, copying virtual structures should be as cheap\nand effortless as it is for software and data today. The only\ncost is developing the structures in the first place, and the\nmemory to store and the comp to run them. With this comes the\npossibility of cheap manipulation and experimentation.\n\n\n\n\nIt becomes particularly interesting when virtual life itself\ngets copied and/or modified. Many science fiction stories cover\nthis subject, so I will be brief and selective here.\nOne consequence should be a “virtuan” explosion with life\nbecoming much more diverse. Andy Clarke [[Cla09](#bib.bibx6)] writes\n(without particularly referring to virtuals) that “The humans\nof the next century will be vastly more heterogenous, more\nvaried along physical and cognitive dimensions, than those of\nthe past as we deliberately engineer a new Cambrian explosion\nof body and mind.” In addition, virtual lives could be\nsimulated in different speeds, with speeders experiencing\nslower societal progress than laggards. Designed intelligences\nwill fill economic niches. Our current society already relies\non specialists with many years of training, so it is natural to\ngo the next step to ease this process with “designer babies”.\n\n\n\n\nAnother consequence should be that life becomes less valuable.\nOur society values life, since life is a valuable commodity and\nexpensive/laborious to replace/produce/raise. We value our own\nlife, since evolution selects only organisms that value their\nlife. Our human moral code mainly mimics this, with cultural\ndifferences and some excesses (e.g. suicide attacks on the one\nside and banning stem cell research on the other).\n\n\n\n\nIf life becomes ‘cheap’, motivation to value it will decline.\nAnalogies are abundant: Cheap machines decreased the value of\nphysical labor. Some expert knowledge was replaced by\nhand-written documents, then printed books, and finally\nelectronic files, where each transition reduced the value of\nthe same information. Digital computers made human computers\nobsolete. In games, we value our own life and that of our\nopponents less than real life, not only because a game is a\ncrude approximation to real life, but also because games can be\nreset and one can be resurrected. Governments will stop paying\nmy salary when they can get the same research output from a\ndigital version of me, essentially for free.\n\n\n\n\nAnd why not participate in a dangerous fun activity if in the\nworst case I have to activate a backup copy of myself from\nyesterday which just missed out this one (anyway not too\nwell-going) day. The belief in immortality can alter behavior\ndrastically.\n\n\n\n\nOf course there will be countless other implications:\nethical, political, economical, medical, cultural,\nhumanitarian, religious, in art, warfare, etc.\nI have singled out the value of life, since I think it will\nsignificantly influence other aspects. Much of our society is\ndriven by the fact that we highly value (human/individual)\nlife. If virtual life is/becomes cheap, these drives will\nultimately vanish and be replaced by other goals.\nIf AIs can be easily created, the value of an intelligent\nindividual will be much lower than the value of a human life\ntoday. So it may be ethically acceptable to freeze, duplicate,\nslow-down, modify (brain experiments), or even kill (oneself or\nother) AIs at will, if they are abundant and/or backups are\navailable, just what we are used to doing with software. So\nlaws preventing experimentation with intelligences for moral\nreasons may not emerge. With so little value assigned to an\nindividual life, maybe it becomes a disposable.\n\n10 Personal Remarks\n--------------------\n\n\n\nI have deliberately avoided discussing consciousness for\nseveral reasons: David Chalmers is the consciousness\nexpert and not me, he has extensively written about it in\ngeneral and also in the context of the singularity\n[[Cha10](#bib.bibx5)], and I essentially agree with his\nassessments. Personally I believe in the functionalist theory\nof identity and am confident that (slow and fast) uploading of\na human mind preserves identity and consciousness, and indeed\nthat any sufficiently high intelligence, whether\nreal/biological/physical or virtual/silicon/software is\nconscious, and that consciousness survives changes of\nsubstrate: teleportation, duplication, virtualization/scanning,\netc. along the lines of [[Cha10](#bib.bibx5)].\n\n\n\n\nI have also only considered (arguably) plausible scenarios, but\nnot whether these or other futures are desirable. First, there\nis the problem of how much influence/choice/freedom we actually\nhave in shaping our future in general and the singularity in\nparticular. Can evolutionary forces be beaten?\nSecond, what is desirable is necessarily subjective. Are there\nany universal values or qualities we want to see or that should\nsurvive? What do I mean by we? All humans? Or the dominant\nspecies or government at the time the question is asked? Could\nit be diversity? Or friendly AI [[Yud08](#bib.bibx39)]? Could the\nlong-term survival of at least one conscious species that\nappreciates its surrounding universe be a universal value? A\ndiscussion of these questions is clearly beyond the scope of\nthis article.\n\n11 Conclusions\n---------------\n\n\n\nBased on the deliberations in this paper, here are my\npredictions concerning a potential technological singularity,\nalthough admittedly they have a speculative character.\n\n\n\n\n* This century may witness a technological explosion of a\ndegree deserving the name singularity.\n* The default scenario is a society of interacting intelligent\nagents in a virtual world, simulated on computers with\nhyperbolically increasing computational resources.\n* This is inevitably accompanied by a speed explosion when\nmeasured in physical time units, but not necessarily by an\nintelligence explosion.\n* Participants will not necessarily experience this explosion,\nsince/if they are themselves accelerated at the same pace,\nbut they should enjoy ‘progress’ at a ‘normal’ subjective pace.\n* For non-accelerated non-participating conventional humans,\nafter some short period, their limited minds will not be able\nto perceive the explosion as an intelligence explosion.\n* This begs the question in which sense an intelligence\nexplosion has happened. (If a tree falls in a forest and no one\nis around to hear it, does it make a sound?)\n* One way and maybe the only way to make progress in this\nquestion is to clarify what intelligence actually is.\n* The most suitable notion of intelligence for this purpose seems\nto be that of universal intelligence, which in principle allows\nto formalize and theoretically answer a wide range of questions\nabout super-intelligences. Accepting this notion has\nin particular the following implications:\n* There is a maximally intelligent agent, which appears to imply\nthat intelligence is fundamentally upper bounded, but this is\nnot necessarily so.\n* If the virtual world is inhabited by interacting free\nagents (rather than a ‘monistic’ vorld inhabited by a\nsingle individual or a tightly controlled society),\nevolutionary pressures should breed agents of increasing\nintelligence that compete about computational resources.\n* The end-point of this intelligence evolution/acceleration\n(whether it deserves the name singularity or not)\ncould be a society of these maximally intelligent\nindividuals.\n* Some aspects of this singularitarian society might be\ntheoretically studied with current scientific tools.\n* Way before the singularity, even when setting up a virtual\nsociety in our image, there are likely some immediate\ndifferences, for instance that the value of an individual life\nsuddenly drops, with drastic consequences.\n\n\n\n\nAcknowledgements.\nThanks to Wolfgang Schwarz and Reinhard Hutter\nfor feedback on earlier drafts.", "date_published": "2012-02-28T00:00:00Z", "authors": ["Marcus Hutter"], "summaries": ["The technological singularity refers to a hypothetical scenario in which technological advances virtually explode. The most popular scenario is the creation of super-intelligent algorithms that recursively create ever higher intelligences. It took many decades for these ideas to spread from science fiction to popular science magazines and finally to attract the attention of serious philosophers. David Chalmers' (JCS 2010) article is the first comprehensive philosophical analysis of the singularity in a respected philosophy journal. The motivation of my article is to augment Chalmers' and to discuss some issues not addressed by him, in particular what it could mean for intelligence to explode. In this course, I will (have to) provide a more careful treatment of what intelligence actually is, separate speed from intelligence explosion, compare what super-intelligent participants and classical human observers might experience and do, discuss immediate implications for the diversity and value of life, consider possible bounds on intelligence, and contemplate intelligences right at the singularity."], "doi": null, "categories": ["cs.AI", "physics.soc-ph"], "journal_ref": "Journal of Consciousness Studies, 19:1-2 (2012) 143-166", "author_comment": "20 LaTeX pages", "primary_category": "cs.AI", "data_last_modified": "2012-02-28 10:46:29+00:00"}