{"id": "e5ced1efef05588e15f09dee70a03deb", "title": "DeepMind x UCL RL Lecture Series - Introduction to Reinforcement Learning [1/13]", "url": "https://www.youtube.com/watch?v=TCCjZe0y4Qc", "source": "youtube", "source_type": "youtube", "text": "hello and welcome to this course on\nreinforcement learning\nmy name is harvan husselt and i'm a\nresearch scientist at deepmind in london\nand every year we teach this course on\nreinforcement training at ucl\nthis year it's a little bit different\nbecause due to the pandemic situation\nwith covet 19\nwe are pre-recording the lectures so\ninstead of talking from a lecture hall\ni'm now talking to you from my home\num the topic of course as mentioned is\nreinforcement training i will explain\nwhat that means what those words mean\nreinforcement learning and\nwe'll go into\nsome depth in uh multiple lectures to\nexplain different concepts and different\nalgorithms that we can build\ni'm not teaching this course by myself\nsome of the lectures will be taught by\ndiana bursa and some will be told by\nmatteo hessel\nand today will be about introducing\nreinforcement learning\nthere's also a really good book on this\ntopic by\nrich sutton and andy bartow which i\nhighly recommend and this is also going\nto be used basically background material\nfor this course\nand if you go to the url that is shown\nhere on the slide you can access\na free\ncopy of that book\njust a little bit of admin before we get\nstarted for students taking this for\ncredit at ucl\nthere's a portal called moodle\nand we'll be using that to communicate\nwith you so please check that for\nupdates and please use the forum thereon\nfor asking questions\nif you do that then if we answer these\nquestions and other people can also\nbenefit from that interaction and\nmultiple people might have the same\nquestion\nor people might have a question but not\neven realize that they have that\nquestion so then it's very useful if you\nask it publicly on that forum\nin terms of grading we will have\nassignments\nwhich will be graded this year there\nwill not be an exam\nso now about this course specifically\nwhat are we talking about\num the main question for this first\nlecture especially is just the question\nwhat is reinforcement learning and\ni'll explain it a little bit and then\nwe'll go into a lot of depth into\ndifferent subtopics of this question\nand in order to understand what\nreinforcement is it's actually useful to\nfirst ask the question what artificial\nintelligence is and how these two are\nrelated because turns out these are\nclosely related\nand\nto understand at least what i mean when\ni say artificial intelligence i'm going\nto pop up a level\nand we're going to turn first to the\nindustrial revolution\nso\nthis is a period in time that happened a\ncouple of hundred years ago or started a\ncouple hundred years ago and one could\nargue this is all about automating\nrepeated physical solutions or manual\nsolutions if you will so think for\ninstance of a steam train or a steamboat\nand how this replaced the manual labor\nof pulling a cart by yourself or using\nfor instance animal labor horses\nto draw those cards\nnow of course some of that still happens\nwe still have a manual layer but we\nreplaced a lot of that with machines and\nthis led to the machine age where we\nstarted replacing more and more things\nwith machines and in addition to that\nalso coming up with new things that we\ncould solve with machines so even things\nthat we weren't doing before we could\nnow make machines that could do those\nthings for us\nof course this led to huge productivity\nincrease worldwide\nand it also fed into a new stage you\ncould argue comes after this\nwhich you could call the digital\nrevolution\nand one way to interpret this is to say\nthe digital revolution was all about\nautomating repeated mental solutions\nso a classic example here would be a\ncalculator\nwe know how to add two numbers together\nwe know how to multiply two numbers\ntogether and in fact we know that\nprecisely enough that we can write a\nprogram and implement that on a machine\non a computer if you will\nand then automate that process in such a\nway that it's very fast and very precise\nand therefore replace the slower mental\ncalculations that we had to do before\nnow this of course\nalso led to a lot of productivity\nincrease\nbut both of these phases they have\nsomething in common which is that we\nfirst had to come up with the solutions\nnow i'm going to argue that there's a\nnext thing that you can think of which\nis already ongoing\nand that would be\nto allow machines to find solutions\nthemselves and this you could call the\ndomain of artificial intelligence\nnow this has huge potential upside\nbecause if you are able to find machines\nthat can learn for themselves to find\nsolutions\nthen this takes away the responsibility\non us to find a solution in advance and\nthen to automate it\ninstead then all that we need to do is\nspecify a problem and a goal\nand then have the machine figure out how\nto solve this\nas we'll see later this will often\ninvolve interacting\nyou have to have some data to find the\nsolution\nand this means that there's a process of\nlearning so here we already bump into\nthis term learning which i'll get into\nmuch more in depth\nin addition this requires you to\nautonomously make decisions so i'm i'm\nputting these\nterms basically up front and center so\nthere's learning and autonomy and\ndecisions and these are all quite\ncentral to this\ngeneric problem of trying to find\nsolutions\nof course we're not the first to talk\nabout artificial intelligence\nthis has been a topic of investigation\nfor many decades now and there's this\nwonderful paper by alan turing from 1950\ncalled computing machinery and\nintelligence\nand the very first sentence of that\npaper reads i propose to consider the\nquestions can machines think\nnow\ni by the way recommend you to read this\npaper it's wonderfully written it's very\naccessible and it has lots of really\ninteresting thoughts but there's one\nparagraph that i want to highlight\nspecifically\nand i'll read that to you now\nso alan turing writes\nin the process of trying to imitate an\nadult human mind we are bound to think a\ngood deal about the process which has\nbrought it to the state that it is in\nwe may notice three components the\ninitial state of the mind say at birth\nthe education to which it has been\nsubjected and other experience not to be\ndescribed as education to which it has\nbeen subjected\ninstead of trying to produce a program\nto simulate the adult mind why not\nrather try to produce one which\nsimulates the child's\nif this were then subjected to an\nappropriate course of education one\nwould obtain the adult brain\npresumably the child brain is something\nlike a notebook as one buys it from the\nstationers\nrather little mechanism and lots of\nblank sheets mechanism and writing are\nfrom our point of view almost synonymous\nour hope is that there is so little\nmechanism in the child brain that\nsomething like it can be easily\nprogrammed\nso what is aventurine talking about here\nhe's essentially talking about learning\nhe's essentially conjecturing that\ntrying to write the program which\nconstitutes an adult mind might be quite\ncomplicated which makes sense because\nwe're subjected to a lot of experience\nthroughout our lives this means we learn\na lot of you can think of these as being\nrules or\npattern matching that we learn how to do\nskills that we acquire\nand enumerating all of that describing\nthat all of that clearly enough and\ncleanly enough that you have something\nwith the same capability as an adult\nmind\nhe's conjecturing that that might\nactually be quite tricky\nand maybe it's easier to actually write\na program that can itself learn in the\nsame way maybe that we do or maybe in a\nsimilar way or maybe in a slightly\ndifferent way\nbut it can learn by interacting with the\nworld\nby\nin in his words subjecting itself to\neducation\num\nmaybe to find similar solutions as the\nadult mind has\nand he's conjecturing that maybe this is\neasier\nnow this is a really interesting thought\nand it's really interesting to think\nabout this a little bit\nso\nmaybe this is a good time for you also\nto maybe pause the video and ponder this\na little bit whether you agree with this\nconjecture that indeed maybe it might be\neasier to write a program that can learn\nthan it is to write a program that has\nthe same capabilities as the program\nthat can learn will achieve over time\nso what is an artificial intelligence\nwell one way to define it would be\nthat the\ngoal would be to to be able to learn to\nmake decisions to achieve goals this is\nnot the only possible definition of\nartificial intelligence and other people\nhave proposed sometimes slightly\ndifferent versions or vastly different\nversions i'm not going to argue that\nthis is the best definition of\nartificial intelligence maybe there are\ndifferent types of artificial\nintelligence that we could consider but\nthis is the one that is central to us so\nwe're going to basically ask this\nquestion how could we build something\nthat is able to learn to make decisions\nto achieve goals\nand that is our central question\nand note that the learning decisions and\ngoals are all very central concepts in\nthis and we'll get into a little bit\nmore detail what i mean with all of them\nso this brings us to this question what\nis reinforcement learning\nand\nthis is related to this um\nexperience that alan turing was also\ntalking about because we know that\npeople and animals learn by interacting\nwith our environment\nand this differs from certain other\ntypes of learning and that's good to\nappreciate first of all it's active\nrather than passive and we'll get back\nto that extensively in the next lecture\nwhat this means is that you\nare subjected to some data or experience\nif you will\nbut the experience is not fully out of\nyour control the actions that you take\nmight influence the experience that you\nget\nin addition to that interactions might\nbe sequential so\nfuture interactions might depend on\nearlier ones if you do something this\nmight change the world in such a way\nthat later other things are possible or\nimpossible we are also goal directed we\ndon't just randomly meander we do things\nwith a purpose\nand this this is also this is at large\nbut also at small scale i might have a\ngoal to pick up a glass for instance\nthat is a small thing perhaps but you\ncould think of this as being a directed\naction where i pick up that glass and of\ncourse this consists of many small\nlittle micro actions of\nme sending signals to my muscles to\nactually execute that\nalso we can learn without examples of\noptimal behavior and this one's\ninteresting it's good to think about\nthat a little bit and to appreciate what\ni mean when i say with that because\nobviously we are subjected to education\nas engineering courses so we do get\nexamples of what we um\nbehavior that we want or behavior that\nother people want us to do and we try to\nfollow those examples in many cases\nbut what i mean here is something a\nlittle bit different i mean that when\nyou do pick up a cup\nthat maybe somebody showed you at some\npoint oh it's useful to pick up a cup um\nor you could think of it that way maybe\nthat's not the greatest example of that\nbut somebody taught you how to write or\ntaught you how to do math\nbut nobody actually told you exactly how\nto steer your muscles in such a way as\nto move your arm to pick up a pen to\npick up a cup things like this\nso clearly we still learn some sort of a\nbehavior there we learn to control our\nmuscles but not in a way that somebody\ntells you exactly oh this is how you\nshould have moved your muscle and now\nyou just replicate so that is what i\nmean when i say we learn without\nexamples nobody gives you exactly the\nlow level actions that are required to\nexecute\nthat thing that you want to execute\nand\nthis actually\nmaybe constitutes most of the learning\nthat we do most of the learning that we\ndo is actually of that form where maybe\nwe do interpret something that we see in\nsome sense as an example but maybe\ntypically at a much higher level of\nabstraction and in order to actually\nfill in that example in order to execute\nwhat we want to mimic this might still\nrequire us to learn skills in a much\nmore\nautonomous way without clear examples\nso one way to think about this and i'll\ngo back to this is that you can think of\nthis as optimizing some reward signal we\nwant to achieve something and by\nachieving it we feel in some sense\nsatisfaction or happiness and this is\nwhat stares our behavior we notice that\nsome things are more pleasing than other\nthings\nso this brings us to a very central uh\npicture that i'm going to show multiple\ntimes during this course which is the\ninteraction loop and one can perceives\nthis as being basically the setting that\nwe find ourselves in\nso this is something to keep in mind\nthat we are basically considering an\nagent interacting with an environment\nand here i drew them separately but you\ncould also think of the agent as\nbasically being inside that environment\nthere's a huge world out there the agent\nlives somewhere in that world now this\ncould be um quite concrete for instance\nthe agent could be a robot and the\nenvironment could be the real world it\ncould also be much more abstract for\ninstance the environment could be some\nabstract game or it could be a virtual\nenvironment\nit could be the internet and the agent\ncould be some program that tries to\ninteract with that environment instead\nso it's quite a flexible framework\nand we basically say that the agent then\nexecutes actions and observes the\nenvironment this is typically drawn in\nsuch a way as i did here where the\nactions go from the agent into the\nenvironment and the observations go from\nthe environment into the agent but of\ncourse you could also think of the\nobservation as being something that the\nagent pulls in in fact the observation\ntypically depends on the agent because\nthe agent will have some sort of sent a\nsensory\nmotor stream that is defined by its\ninterface for instance the agent could\nhave a camera and that defines which\nobservations it gets\nso\nthe main purpose of this course is then\nto go basically inside that agent and\nfigure out how we could build learning\nalgorithms that can help that agent\nlearn\nto interact better and what does better\nmean here well the\nagent is going to try to optimize some\nreward signal\nthis is how we're going to specify the\ngoal and the goal is not to optimize the\nimmediate reward so we're not just\ninterested in taking an action and then\nforgetting about everything that might\nhappen after no we're actually\ninterested in optimizing the sum of\nrewards into the future\ni'll explain it a little bit more\nclearly in a moment but it's good to\nappreciate that there must be some goal\nto this right if there's no goal\nspecified then it's unclear what we're\nactually optimizing and it's unclear\nwhat the agent will actually learn to do\nso we need some way some mechanism to\nspecify that goal\nmany cases when people show versions of\nthis interaction loop when talking about\nreinforcement learning they put these\nrewards next to the observation and\nthat's one useful way to think about\nthis that you take an action and then\nthe environment gives you an observation\nand a reward\nbut sometimes it's a bit more natural to\nthink of the reward as being internal to\nthe agent\nso you could also think of the reward\nsignal as being some sort of a\npreference function over these\nobservations or over sequences of\nobservations that the agent receives and\nthe agent just observes the world and\nfeels happier or less happy about what\nit sees and then tries to optimize its\nbehavior in such a way that it achieves\nmore of these rewards\nso this is why i didn't put the reward\nin the figure because sometimes it's\neasier to think of it as just coming\nfrom the environment through the\nexternal to the agent sometimes it's\neasier to think of it as being in some\nsense part of the agent but it should\nstill be there and it should be clearly\nspecified because otherwise it's unclear\nwhat the goal of this whole system would\nbe\nand\nthis reward function is quite central so\nit's good to stop and think about why\nthis is a good way to specify a goal and\nthis is formulated in this reward\nhypothesis that we see on the slide\nwhich states that any goal can be\nformalized as the outcome of maximizing\na cumulative reward\nnow i want to encourage you to think\nabout that and think about it critically\nand try to see if you can maybe even\nbreak it in some sense so breaking it\nwould mean coming up with a counter\nexample of a goal that you cannot\nspecify by maximizing the cumulative\nreward\nfeel free to pause the video and think\nabout this for a bit\nso i've not been able to come up with\nexamples myself\nand maybe this is somewhat even\ntrivially\ntrue in some sense\nbecause you could think of a reward\nsignal that basically just checks\nwhether you've achieved that goal that\nwe want to specify and then whenever\nyou've achieved the goal the reward\nsignal becomes one and before that it's\nzero\nthen optimizing this cumulative reward\nwould clearly correspond to maximizing\nor achieving that goal\ndoesn't mean it's easy to specify that\nsometimes it's hard to specify your goal\nprecisely or\nsometimes it's hard to specify a reward\nwhich is easy to optimize which is a\ncompletely different problem but that's\nnot under\nthis reward hypothesis this just states\nthat there must exist a reward and\nindeed sometimes there's many different\nways to specify the same goal for\ninstance instead of saying you get a\nreward of plus one whenever you achieve\nthe goal and zero before that you could\nalso say let me give you a reward of\nminus one in some sense a penalty on\nevery step before you've achieved the\ngoal and then zero rewards after you've\nachieved it\nthen you could think of the agent as\nmaximizing this cumulative reward as in\nsome sense minimizing these penalties\nwhich would also then maybe lead to the\nbehavior of achieving the goal as\nquickly as possible because minimizing\nthe number of -1 rewards number of steps\nuntil you've achieved the goal will then\nbecome relevant\nso we see that a goal could also include\nnot just\nthat it happens but also when it happens\nif we specify it in this way so it's\nquite a flexible framework\nand\nit seems to be a useful one that we can\nalso use to\ncreate\ncreate concrete algorithms that work\nrather well\nso some examples some concrete examples\nof what reinforcement learning problems\ncould then exist\nso here's a list including flying a\nhelicopter managing an investment\nportfolio controlling a power station\nmaking a robot walk or playing video or\nboard games\nall of these examples were picked\nbecause they have actually been used and\nreinforcement has been applied to them\nsuccessfully\nand for instance we could have a reward\nfunction for the helicopter that is\nrelated to air time or inverse distance\nto some goal\nor to pick for instance the video games\nyou could or board games you could think\nof a reward function that just looks at\nwhether you win or not so think of the\ngame of chess for instance you could\nhave a reward function that gives you\nplus one whenever you win minus one\nwhenever you lose\nand if the goal isn't to learn via\ninteraction these are all reinforcement\nlearning problems\nthis is irrespective of which solution\nyou use and i put that on the slide\nbecause sometimes people conflate the\ncurrent set of algorithms that we have\nin reinforcement learning to solve these\ntype of problems with the field of\nreinforcement learning but it's good to\nseparate that out and to appreciate that\nthere's a reinforcement building problem\nand then there's a set of current\nsolutions that people have considered to\nsolve these problems\nand\nthat set of solutions might be under a\nlot of development they might change\nover time\nbut it's first good to think about and\nappreciate whether we agree with the\nthe goal with the problem statement\nand if we agree with the problem\nstatement then we can think flexibly\nabout the solutions we don't have to be\ndogmatic about that and we can think\nabout new solutions that achieve the\nsame goal\nso it's good to separate that out and i\nwould argue that if you're doing any of\nthese problems where there is a reward\nfunction errors or sequential\ninteraction then you're doing\nreinforcement learning whether or not\nyou call your algorithm three and four\nenforcement learning algorithms\nso\nin each of these\nproblems that i specified these\nreinforcement training problems there\nmight actually be two distinct reasons\nto learn\nso the first one maybe obviously is to\nfind solutions\nso going back to the example of the\nhelicopter for instance you might want\nto find a\npolicy of behavior for this for this\nhelicopter so that it flies to a goal as\nquickly as possible\nbut maybe in order to optimize its\ncumulative reward it also sometimes has\nto do some more complicated things such\nas\nfirst go somewhere else to refuel\nbecause otherwise it won't even reach\nthe goal\nbut in the end you might have some\nlearning process and you might find a\nsolution two examples here to make that\nconcrete you could think of a program\nthat plays chess really well that might\nbe something you desire\nor you might want a manufacturing robot\nwith a specific purpose\nand then reinforcement could potentially\nbe used to solve these problems and then\nto deploy that solution\nnow a subtly different but importantly\ndifferent thing that you might want is a\nsystem that can adapt online\nand the purpose for this would be to\ndeal with unforeseen circumstances\nso to take the same two examples and to\ncontrast what we\nhow this is different in the chess\nprogram for instance you might not want\na chess program that displays maybe the\nmost optimal form of tests that you can\nfind\nbut instead you might want to find a\nprogram that learns to adapt to you now\nwhy would you do that well for instance\nyou might want a program that doesn't\nwin too often because then maybe your\nenjoyment is less so instead of\noptimizing the number of times it wins\nmaybe it actually wants to optimize so\nthat the number of times it wins is\nmaybe like roughly half of the time or\nsomething like that or maybe it wants to\noptimize how often or how long you play\nit because maybe that's a good proxy for\nhow much you enjoy playing it\nsimilarly you can think of a robot that\ncan learn to navigate unknown terrains\nmaybe you can pre-train this\nmanufacturing robot from the first\nexample because you have a very good\nsimulator for the setting that it might\nbe in\nbut in other cases maybe you don't have\na very good simulator or maybe you do\nhave good simulators for different types\nof the terrains that the robot might\nencounter but you do not know yet\nexactly what the terrain will look at\nwhere it will be deployed and there\nmight be unknown unknowns there might be\nthings that you haven't foreseen\nin those cases obviously it's quite\nuseful if you can continue to adapt if\nyou can continue to learn and we do that\nas well we continue to learn throughout\nour lifetimes\nso that's a different purpose but\nfortunately reinforcement learning can\nprovide algorithms for both those cases\nit's still good to keep in mind that\nthese are actually different goals and\nsometimes that becomes important\nnote that the second point about\nadaptive\nalgorithms to be able to adapt online is\nnot just about generalizing it's not\nabout finding a solution similar to in\nthe first category but one solution that\nis very general in some sense now it's\nreally about unknown unknowns it's\nreally about what if the environment\nchanges what if for instance you have a\nrobot and it gets deployed and at some\npoint there's wear and tear and you\nhaven't foreseen this there was no way\nto know exactly what would happen\nand\nall of a sudden the robot has to deal\nwith this somehow then if it can't adapt\nonline then it's really hard to find a\nsolution that is generic enough general\nenough that can deal with that\nand in indeed there are other reasons\nwhy it might be useful to learn online\nbecause it might be easier to have a\nsmaller program that continues to\ncontinue to track the world around it\nthan it is to try to find this one\nhumongous solution that can deal with\nall of the unforeseen circumstances that\nyou could possibly come up with\nso these are really different\ndifferent settings\nokay so now we finally are ready\nbasically to answer this question what\nis reinforcement learning and i'm going\nto say that basically reinforcement\nlearning is the science and framework of\nlearning to make decisions from\ninteraction\nso reinforcement learning is not a set\nof algorithms also not a set of problem\nproblems sometimes in shorthand we say\nreinforcement planning when referring to\nthe algorithms but maybe it's better to\nsay reinforcement learning problems or\nreinforcement learning algorithms\nespecially if we want to specify those\ntwo different parts of it and then\nreinforcement learning itself could just\nbe the the science and the framework\naround all of that\nthis has some interesting properties it\nrequires us to think about time and\nconsequences of actions and this is a\nlittle bit different from many other\ntypes of learning from for instance\nother types of machine learning where\noftentimes you are given a data set and\nfor instance you want to find a\nclassifier or something of the form\nand then\nmaybe there are no long-term\nconsequences you basically just\nspecify that your goal is to minimize\nthe errors that the system makes\nnow in reinforcement learning we would\nargue that maybe\nyou want to consider the whole of the\nsystem so maybe you don't just want to\nconsider the classifier but you also\nwant to consider the consequences of\nclassifying something wrong and that\nmight be taken into account if you\nconsider the whole framework\nthis makes it more challenging\nand it also means we have to actively\ngather experience because these actions\nwill change the data that we see\nwe might want to predict the future so\nnot just on one step thing so unlike a\nclassifier where you just get an input\nand you're only interested in like the\ninput for that or the output for that\nspecific input we might actually want to\nconsider\nfuture steps further into the future\nwhich is an interesting and tricky\nsubject\nand in addition we this is a more\ntypical thing that happens in machine\nlearning we have to deal with\nuncertainty somehow\nnow the benefit of this is that there's\nhuge potential scope but you might have\nalso realized by that this is also very\ncomplicated\nor difficult question in general how to\nsolve this very generic problem\nbut the upside is huge if we are able to\nfind good generic algorithms that can\ndeal with this very generic setting then\nmaybe we can apply this to many\ndifferent problems successfully\nand indeed one way to think about\nreinforcement planning is that it's a\nformalization of the ai problem as i\ndefined it earlier\nso\nit's good to appreciate the the basic\nthe ambition here that reinforcement\nbuilding is quite an ambitious vicious\nendeavor that doesn't mean that of\ncourse it sits on an island and in fact\nwe will see\nuh during this course that current day\nreinforcement learning is very\nsynergetic with uh\nwith deep learning which is all about\ntraining deep neural networks\nand\nindeed\nthis is also very it seems very very uh\nsuitable component for a full ai problem\nso the reinforcement learning\ndescription is just about formalizing\nthe problem that doesn't mean that we\ndon't need solutions from all sorts of\nsubparts of machinery\nokay now i'm going to show you an\nexample\nwhat we see here is an atari game this\nis an old video game from the 1980s\ncalled beam rider\nand the agent that is playing this game\nhas learned to play it by itself\nits observations were the pixels as you\nalso see them on the screen\nhere's a different atari game with\ndifferent pixels\nand in each of these cases the actions\nthat the agents would take are the motor\ncontrols which are basically just the\njoystick inputs for the atari\ngames which means the agent could press\nup down left right or diagonally and it\nwill press a fire button\nand then the agent just had to deal with\nthat in input output stream so it just\ngets these observations these pixels\nfrom the screen and it outputs these uh\njoystick controls and we see that they\ndid relatively well learning to play\neach of these different games even\nthough they're quite different so here's\na racing game enduro\nand it's good to appreciate that the\nagent is not even told what it's\ncontrolling right it just gets these\npixels so it's not told oh there's this\nthing at the bottom here which is kind\nof meant to be a racing car and your\ngoal is to pass these other cars now\ninstead you just get these pixels you\nget your motor controls and you get a\nreward signal\nnow in these games the reward signal was\ndefined as the difference in score on\nevery time step\non a lot of time steps this difference\nin score is zero that's fine but on\nother time steps it will be positive and\nthe agent tries to maximize the\nsummation of that over time so it wants\nto take actions that will lead it to\ngood rewards later on\nnow the most important thing to take\naway from this is that we have used a\nlearning system to find these solutions\nbut we didn't need to know anything\nabout these games ourselves like there\nwas nothing put into the agent in terms\nof strategy or even in terms of prior\nknowledge on what you're controlling on\nthe screen\nso the agent when it started playing\nspace invaders did not know that it was\ngoing to control this thing at the\nbottom which is shooting\nor that it was controlling one of these\nboxes in this example\nand that is the benefit of having a\ngeneric learning algorithm in this case\nthis algorithm is called dq n and we'll\ndiscuss it later in the course as well\nokay so now i'll go back to the\nslides\nso now i've given you a couple of\nexamples i've shown you these atari\ngames\nand now is a good time to start\nformalizing things a little bit more\ncompletely so that we know a little bit\nmore what's happening\nand in future lectures of course we will\nmake this much more\nclear and rigorous and for now we're\ngoing to give you kind of like a high\nlevel overview of what happens here what\nis this reinforcement learning problem\nwhat's inside that agent how could this\nwork\nso we're going to go back to this\ninteraction loop\nand\nwe're going to introduce a little bit of\nnotation where we basically say that\nevery time step t we receive some\nobservation ot and some reward rt\nas i mentioned the reward could also be\nthought of as being inside the agent\nmaybe it's some function of the\nobservations or you could think of this\nas coming with the observations from the\nenvironment and then the agent executes\nsome action so the action can be based\non this observation ot in terms of our\nsequence of interactions\nand then the environment receives that\naction and emits a new observation or we\ncould think of the agent as pulling in a\nnew observation and a next reward\nnote that we increment the time step\nafter taking the action\nso we say that the action is emitted at\ntime step t and then the next\nobservation is\nreceived at time set t plus one that's\njust convention this is where we\nincrement the time index\nyou can actually extend reinforcement\nlearning to continuous time as well\nrather than having these discrete time\nsteps\nbut we won't cover that in this course\nthe extensions are in some sense\nnot too difficult\nso it's good to have that in mind\nbut there are some subtleties that one\nwould have to consider\nso the reward here is a scalar feedback\nsignal it's just a number it can be\npositive it can be negative a negative\nreward you could call a penalty but we\njust call that a negative reward just to\nhave this one word that refers to the\nfeedback signal\nand just to recall i put the reward\nhypothesis on the slide again where we\nstate that any goal can be formalized as\nthe outcome of maximizing a cumulative\nreward\nthis\nthis instantaneous reward that indicates\nhow well the agent is doing at that time\nstep t\nand this helps define the goal of the\nagent\nand the cumulative reward is and the\naccumulation or the sum of these rewards\nover time\nit's useful to\nalso devote a letter to that which we'll\ncall g\nso roughly speaking you can think of g\nas kind of specifying the goal but we'll\nuse the term return to refer to this\nso the return\nis just shorthand for the cumulative\nreward or the sum of rewards into the\nfuture\nnote that the return is only about the\nfuture right so this is at some time set\nt so this is useful to determine which\naction to take because your actions\ncannot influence the past they can only\ninfluence the future so when we define\nthe return the return is defined as all\nof the future rewards summoned together\nbut the past rewards are in the past and\nwe can't change them anymore\nthen we can't maybe always hope to\noptimize the return itself so instead\nwe're going to define the expected\nreturn which we'll call a value\nso the value at time s\nwould simply be the expectation of the\nreturn\nso that's the sum of the rewards going\ninto the future conditioned on the fact\nthat you're in that state s\ni haven't defined what a state is yet\nbut for simplicity you could now think\nof this as just being your observation\nbut i'll talk more about that in a\nmoment\nso this value does depend on the actions\nthe agent takes and i will also make\nthat a little bit more clear in the\nnotation later on so it's good to know\nthat the expectation depends on the\ndynamics of the world but also the\npolicy that the agent is following and\nthen the goal is to maximize the values\nwe want to pick actions such that this\nvalue becomes large\nso one way to think about that is that\nrewards and values together define the\nutility of states and actions\nand there's no supervised feedback so\nwe're not saying this action is correct\nthat action is wrong instead we're\nsaying this sequence of actions has this\nvalue that sequence of actions has that\nvalue\nand then maybe pick the one that has the\nhighest value\nconveniently this and this is used in\nmany algorithms the rewards sorry the\nreturns and the values can be defined\nrecursively so the return at time sub t\ncan be thought of as simply the first\nreward plus the return from that time\nstep t plus one\nsimilarly the value can be defined\nrecursively so the value at some time\nsorry the value at some state s\nis the expected first reward you get\nafter being in that state and then the\nvalue of the state you expect to be in\nafter being in that state\nso the goal is maximizing value by\ntaking actions\nand actions might have long-term\nconsequences so this is captured in this\nvalue function because the value is\ndefined as the expected return where the\nreturn sums the rewards into the future\nand one way to think about this is that\nactual rewards associated with certain\nactions can be delayed\nwhat i mean with that is you might pick\nan action that might have consequences\nlater on\nthat are important to keep in mind but\nthat do not show up immediately in the\nreward that you get immediately after\ntaking that action\nthis also means that sometimes it's\nbetter to sacrifice immediate reward to\ngain more long-term reward and i'll talk\nmore about that in the next lecture\nso some examples of this might be one\nthat i mentioned before today\nrefuting a helicopter might be an\nimportant action to take even if it\ntakes you slightly farther away from\nwhere you want to go so this could be\nformalized in such a way that the\nrewards for that are low or even\nnegative for the act of refuting but the\nsum of rewards over time might be higher\nbecause eventually you get closer to\nyour goal than if you wouldn't refuel or\nto pick the last example learning a new\nskill might be something that is costly\nand time consuming at first might not be\nhugely enjoyable but maybe in the long\nterm it will yield you more benefits and\ntherefore you learn this new skill to\nmaximize your value rather than the\ninstantaneous reward\nfor instance maybe that's why you're\nfollowing this course\njust in terms of terminology we call a\nmapping from states to actions a policy\nthis is just shorthand in some sense for\nan action selection policy\nit's also possible to define\nvalues on not just states but on actions\nso these are typically denoted with the\nletter q for historical reasons so we\nhave the letter v to denote the value\nfunction of states and we have the\nletter q to denote the value function of\nstates and actions and this is simply\ndefined as the expected return condition\non being in that state and then taking\nthat action a\nso instead of considering some sort of a\npolicy which immediately could pick a\ndifferent action in state s we're saying\nno no we're in state s and we're\nconsidering taking this first action a\nnow this total expectation will then of\ncourse still depend on the future\nactions that you take so this still\ndepends on some policy that we have to\ndefine for the future actions but we're\njust pinning down the first action and\nconditioning the expectation on that\nwe'll talk much more in depth about this\nin\nlectures three four five and six\nso now we can basically summarize the\ncourse concepts before we continue\nso we said that the reinforcement\nplanning formalism includes an\nenvironment which basically defines the\ndynamics of the problem\nit includes a reward signal which\nspecifies the goal\nand sometimes this is taken to be part\nof the environment but it's good to\nbasically list it separately\nand then it contains an agent\nnow this agent might contain different\nparts and most of this course will\nessentially be about what's in the agent\nwhat should be in the agent how could we\nbuild learning algorithms that work well\nand some of the parts are listed here so\nthe agent will contain some agent state\nthis is just the internal state of the\nagent it will contain some policy\nand it could contain a value function\nestimate so a prediction of the value or\na model which might be a prediction of\nthe dynamics of the world i put\nquestions mark marks there because these\nare in some sense more optional than the\nfirst two the agent must have some\ninternal state this could be a very\nsimplistic state it could be a null\nstate or your agency could simply be the\nimmediate observation that you've\nreceived right now but it could be a\nmore complicated state and it must have\nsome policy it must select actions in\nsome way\nagain this policy could be particularly\nsimple it could be a random policy that\njust selects actions\ncompletely uniformly at random but there\nmust be some policy\nthe value function and the model are\nmore optional in the sense that they're\nnot essential parts but they are very\ncommon parts and i will discuss them a\nlittle bit in the remainder of this\nlecture\nso now it's time to go into the agent\nand we'll start with the alien state\nso this is one way to depict the\ninternals of the agent so now we're\ninside the agent and in this schematic\nhere on the right hand side time\nincrements as we go to the right\nand so we see inside the agent from the\nview inside the agent on every time set\nthere's an observation that comes in\nand then there's some internal state of\nthe agent and from the state of the age\nthe agent might make predictions\nand it should define some policy somehow\nand then the action gets selected by\nthis policy so i could have basically\ndrawn another arrow going from the\npolicy into the action which would then\ngo back into the environment\nbut we're focusing here on that state\ncomponent and state here basically\nrefers to everything that the agent\ntakes along with it from one time to the\nnext\nso if there are things that are not\ntaking along for instance the policy at\nthe instantaneous policy at the time set\nmight not be\ntaken along the predictions might not be\ntaken along or they could be in that\ncase they could just be part of the\nstate\nbut there might be other things in the\nstate as well there might be some memory\nin the state there might be a learned\ncomponents in the states everything that\nyou take along with you from one time to\nthe next we could call the agent state\nwe can also talk about the environment\nstates which is the uh\nthe other side of that coin\num in many cases the environment will\nhave some really complicated internal\nstates for instance in uh the example\nwhere the agent is a robot and the\nenvironment is the real world then the\nstate of the environment is basically\njust the state of all of the physical\nquantities of the world all of the atoms\nall of the quantum mechanics of the\nworld that's the environment state\nof course in many smaller examples if\nit's a virtual environment it could be\nmuch smaller but it could still be quite\ncomplicated this also means it's usually\ninvisible to the agent it's very really\nreally large and it's not part of the\nobservation stream per se\neven if it would be visible it might\ncontain lots of irrelevant information\nand it might just be simply too large to\nprocess but actually the first one is\nmore interesting it's usually just\ninvisible to the agent we can only see a\nsub-slice of it we can see a small part\nof it via our observation stream\nan important concept to keep in mind\nthen is that we can also formulate the\nwhole interaction sequence into\nsomething that we could call the history\nof the agent\nand this is simply everything that the\nagent could have observed so far so that\nincludes the observation from the\nenvironment but also the actions that\nthe agent took and the rewards that it\nreceived so this is really just taking\nthat interface and storing everything\nthat happens on the interface level and\nwe could call that the history of the\nagent\nso for instance it could be the full\nsensory motor stream of the robo\nnow we can say that the history is the\nonly thing that can be used to construct\nthe aging stage in some sense apart from\nwhatever prior knowledge you put in all\nthe way at the beginning but let's just\nset that aside for a moment and\neverything else must be a function of\nyour history there's nothing else\nessentially the agent has no additional\ninformation\napart from its sensory motor stream so\nthat's that's what you should be using\nto construct your agency\nthen a special case\nis when the\nagent can see the full environment state\nso that the observation is the full\nenvironment state and this is called\nfull observability i mentioned before\nalready this is a very special case this\nis not the common case at all but it's a\nuseful one and sometimes it's used for\ninstance in theoretical statements just\nbecause it's easier to reason about in\nsome cases\nand in that case the agent state can\njust be the observation right we don't\nneed to worry about this whole\ninteraction stream we can just observe\nwhatever the environment state is\nand then this should be sufficient in\norder to basically tell where you are\nyou don't need additional memory you\ndon't need anything else you just need\nthe environment state as your state\nnow in addition to that\nthere could be the learnable parts of\nthe agent right the agent might have\nsome parameters that it's learning and\nyou could also consider that to be part\nof the agent state\nin this case i'm actually not\nconsidering that to be part of the\nagency that's something that we also\nhave that's also part of the asians but\nlet's just set that aside and call that\nthe\nthe agent's mind is essentially separate\nfrom its state in this sense\nso in the fully observable case you can\njust look at your observation you could\nsay oh that tells me everything i need\nto know about the environment so i don't\nneed to log any of the previous\ninteractions\nand this leads us to an important\nconcept in reinforcement training which\nis the markov property\nand this has been used to formulate\nessentially the reinforcement problem\nand also precursors to this\nand importantly a markov decision\nprocess is essentially a very useful\nmathematical framework that allows us to\nreason about algorithms that can be used\nto solve these decision problems\nthe mark property itself states\nthat a process is markovian or a state\nis markovian for this process if the\nprobability of a reward and a subsequent\nstate\ndoesn't change if we add more history\nthat's what the equation on the slide\nmeans\nso we can see the probability of a\nreward and a state\nyou could you should interpret this as\nthe probability of those occurring on\ntimes of t plus one\nand we say that the the probability of\nthis happening condition on state st is\nequal to conditions on the full history\nup to the time set\nthat means if this is true that the\nstate contains\nexcuse me that the state contains all\nthe means you need to know\nso we don't need to store anything else\nfrom the history\ndoesn't mean that the state contains\neverything\nit just means that adding more history\ndoesn't help for instance if your\nobservations are particularly\nuninformative\nthen adding more uninformative\nobservations might not help so that\nmight lead to a markovian state but it\ndoesn't mean that you can observe the\nfull environment state\nhowever if you can observe the full\nenvironment states then you're also\nmarkovian\nso once the state is known the history\nmight be thrown away if you have this\nmarket property and of course this\nsounds uh very useful because the state\nitself might be a lot smaller than the\nfull history\nso as an example the full agent and\nenvironment state\nis markov but it might be really really\nlarge because as i mentioned the\nenvironment state might be humongous it\nmight be the real world\nand also the full history is markov\nwhich you can kind of clearly read from\nthis equation because if you put\nht on the left hand side where it says\nst then obviously this is true\nbut the problem with that is that that\nstate keeps growing so if we want to use\nthe full history as our agent states\nthen the\namount of memory that we're using inside\nthe agent's head keeps growing linearly\nover time and sometimes that also\nbecomes too too large or actually\noftentimes it also becomes too large\nso typically the agent said is some\ncompression of the history\nwhether it instead of the markov\nproperty is actually\nmaybe not even the most important\nquestion but it's an interesting thing\nto keep in mind\nso\nnote here that we use st deno to denote\nthe agent states not the environment\nstates\nand we'll use that convention basically\nthroughout where sometimes as a special\ncase\nthese are these will be the same because\nthe environment state might be fully\nobservable\nbut in general we will not assume that\nand then whenever you say states this is\nbasically the state and part on the side\nof the agent and that's specified\ndifferently\nnow i said that full observable cases\nare very rare so that we should talk\nabout the uh the complement of that\nwhich is the partial observable case so\nin this case the observations are not\nassumed to be markovian and i'll give\nyou an example or a couple of examples\nso for instance a robot with a camera\nwhich is not tallest absolute location\nwould not have markovian observations\nbecause at some point it might be\nstaring at a wall\nand it might not be able to tell where\nit is it might not be able to tell\nwhat's behind it or behind the wall\nit can maybe just see the wall and then\nthis observation will not be markovian\nbecause the probability of something\nhappening might depend on things that it\nhas seen before\nbut it doesn't see right now it may have\njust turned around and there might be\ninformation about what's behind it which\nshould influence the probability of what\nhappens next\nbut it can't see this from its\nobservations per se\nsimilarly\na poker playing agent only observes\npublic cards and its own cards it\ndoesn't observe the cards from the other\nplayers but obviously these are\nimportant for its future rewards so part\nof the environment state is then hidden\nto the agent\nso now using the observation essay would\nnot be markovian that doesn't mean it's\nnecessarily a bad idea but it means it\ncan be a bad idea because you're\nignoring some information that might be\ncontained in your past observations\nthis is then called a partial observable\nmarkov decision process or pomdp for\nshort\nand it's basically just an extension of\nthe market decision processes which\nwe'll define more rigorously in future\nlectures\nand it's good to keep in mind that this\nis basically the common case\nnote that the environment state itself\ncould still be mark off it's just that\nthe agent can't see it and therefore\ncan't know it\nin addition we might still be able to\nconstruct a markov agent state\nthe example i gave in the previous slide\nis you could always take your full\nhistory and that would be markovian the\nproblem with that is just it's too large\nbut maybe there are smaller asian states\nwe can construct which still hold enough\ninformation to be markovian\nso the agent states\nis\nan important concept and\nit must depend on this information that\nyou've seen before right this must\ndepend on this interaction stream\nand the agent actions then depend on the\nstate\nand it's some function of history so the\nexamples that i gave were like the\nstates could be the observation could be\nyour full history but more generally you\ncan also write this on recursively\nwhere the state at your next time set t\nplus one\nis some function of your previous state\nthe action that you take that you've\ntaken the reward you've seen and the\nobservation that you see so we're taking\none step in this interaction loop and\nwe're basically saying we're going to\nupdate the state\nto\n[Music]\nbe aware of this new time step\nclearly if we're concatenating the\naction reward and observation then st 1\ncould just be your full history if st is\nyour full history up to time step t\nso\nthe full history is contained within\nthis formulation also quite clearly the\nspecial case of just looking at the\nobservations contains formulation and\nthis is a more flexible way to think\nabout it\nand then u is the state update function\nnow as i mentioned it's often useful um\nto consider the agent's say to be much\nmuch smaller than the environment set\nand in addition you also typically\nwanted to be much smaller than the full\nhistory so we want this agent update\nfunction\nto give us some compression of the full\nhistory\nmaybe recursively and maybe the state\nactually stays the same size right so st\ncould be of a certain size we see new\naction reward and observation and we\ncondense all of the information together\ninto something that is the same size as\nst\nand here's an example just to make that\na little bit more concrete all of that\nso let's consider a maze and let's say\nthat the full state of the environment\nin a maze is this layout\nand in addition it's where you are in\nthe maze\nand that would define the full\nenvironment state\nbut let's say that the agent can't\nobserve all of that it can't observe its\nlocation in the maze and instead maybe\nyou can only see this little three by\nthree around itself so in this case the\nagent would be in the center of this\nthree by three uh block\nand what it can see is exactly the\npixels around it the the cells around it\nso it can see for instance that above it\nit's empty\nto the left and the right there's a wall\nin black below it it's empty so it could\nwalk up it could walk down and also it\ncan look slightly around the corner\nwhere i can see that if it goes up and\nthen right there's also an empty spot\nbut if it goes up and left it would bump\ninto a wall\nand that's all that it can see\nnow this observation is\nnot markovian because if we look at a\ndifferent location\nthese observations are actually\nindistinguishable\nso if we would just use the observation\nin this case then the agent won't be\nable to tell where it is\nand we can also talk about why that\nmight be problematic\nso let's say that the agent starts in\nthe top right corner\nand let's say that the goal for the\nagent is to go to the top left corner\nthen if you consider the shortest path\nin the state that we see the observation\nthat we see in the top right here the\noptimal action would be the step down\nbecause that's in the direction of the\ngoal because we have to go via the\nbottom of this maze in order to reach\nthe top left corner\nhowever if you then look at the left\nobservation in that observation the\noptimal action would be to go up\nbut if the agent can't distinguish\nbetween these two if it would be using\nthe observation as its full agent state\nand its action selection policy must\ndepend on only that observation\nthen it's unclear what it should be\ndoing\nin the top right it should be going down\nin the left should be going up but\nthere's no single policy single function\nof this observation that will do the\nright thing in both cases\nthis is why it can be problematic to not\nhave a markovian state observation\nso now\ni actually actually want you to think\nabout for a second so feel free to pause\nthe video here\nand think about how you might be able to\nconstruct a markov in asian state for\nthis specific problem and maybe for any\nreward sequence so not just for the one\nthat goes from the top right to the top\nleft but maybe that one that works for\nany reward signal\nso feel free to pause the video and then\ni'll talk about this a little bit more\nso one thing that you may have come up\nwith is well maybe we can use that thing\nthat you said where you can use the full\nhistory yes the full history would be\nmarkovian would be would be rather large\nso i think many of you will have kind of\ndiscounted that as being not the most\npleasant or feasible solution\nso maybe we could do something that's a\nlittle bit\nin that direction but not quite the same\nso let's say we consider storing not\njust the observation that we see right\nnow but also the previous observation\nwould that work\nwell it kind of depends actually it\ndepends on the policy\nand it depends whether the state\ntransitions here in the real world are\nare completely deterministic so if you\ngo down you really go down or whether\nthere are some noise in there where\nsometimes when you press down you\nactually go up\nbecause note that if you look at both of\nthese observations that are highlighted\nright now if you step down one step the\nobservation is still the same\nso if you would come from this situation\nbelow where we currently are and you\nwould just concatenate these two\nobservations that would not be\nsufficient to be able to tell where you\nare so just concatenating two\nobservations is not necessarily\nmarkovian in this environment\nhowever\nit can be sufficient if your policy\nnever does this so if we stepped on the\nright hand side in the right top corner\nright first we step left and then we\nstep down this brothers to where we\ncurrently are\nif we would have stored the fact that we\njust stepped down and then we see this\nthen we know that we are where we are\nbecause then the previous observation is\nsufficient and then we step down again\nand if the policy never does that same\naction in the left state\nthen the ordering of the observations is\nenough to distinguish the left from the\nfrom the top right\nbut in general for any policy for\ninstance for a uniformly random policy\njust concatenating two observations is\nnot sufficient in order to get a\nmarkovian state in this case\nokay\nso in general what i'm doing there is\nbasically trying to construct a suitable\nstate representation to deal with the\npartial observability in the maze\nand as examples i mentioned using just\nthe observation might be enough using\nthe full history might be too large but\ngeneric you can think of some update\nfunction and then the question is how do\nwe pick that update function and that's\nactually what we were doing just now\nlike we were trying to hand pick a\nfunction u\nthat updates\nthe state in such a way to take into\naccount the stream of observations the\nexample that i gave where i just\nconcatenate two observations\nwould be where you just keep track of\nthis buffer and whenever you see a new\nobservation it basically replaces the\noldest observation with a new one\nwith a newer one and then adds the\nnewest one on top so you have\nlike a two observation buffer in that\ncase\nexcuse me\nso this is a generic update you can you\ncan do other\nother things there as well of course\nbut it's good to to note that\nconstructing a full markov unit you say\nit might not be feasible like your\nobservation might be really complicated\nand it might be really hard to construct\na full markovian agency and so instead\ninstead of trying to always shoot for\ncomplete markovianness maybe that's not\nnecessary maybe it's more important that\nwe allow good policies and good value\npredictions and sometimes that's easier\nsometimes going for optimal is really\nreally hard but going for very good is\nsubstantially easier and that's\nsomething more generally that we'll keep\nin mind when we want to deal with messy\nbig real world problems where optimality\nmight be out of reach\nokay\nnow we're going to continue our journey\ninside the agent and we're going to go\nto the next bits which are the policy\nthe value function in the model starting\nwith the policy\nso we covered the agencies now we're\ngoing into policy and then immediately\ninto the value function and the model\nand the policy is simply something that\ndefines the agent behavior it's not a\nvery complicated construct it's a\nmapping from agent say to actions and\nfor instance we can write this like this\nfor a deterministic policy it could be\nconsidered simply a function that takes\na state as input and outputs an action\nnow actually it will be more common and\nuh often more useful to think of\nstochastic policies where instead\npi means the probability of an action\ngiven a state\npi is just conventional notation for\npolicies we often use pi to denote a\npolicy and the stochastic policies in\nsome some more general case so typically\nwe consider this a probability\ndistribution of actions\nand that's basically it in terms of\npolicies of course we're going to say a\nlot more about how to optimize these\npolicies how to represent them how to\noptimize them and so on but in terms of\ndefinitions all that you need to\nremember is that pi denotes the\nprobability of an action given a state\nand then we can move on to value\nfunctions and value estimates\nand i'm going to\nwhat i have here on the slide is a\nversion of the value function as i\ndefined it earlier and i want to mention\na couple of things about this first of\nall it's good to appreciate that this is\nthe definition of the value later we'll\ntalk about how to approximate that this\nis just defining it\nand i've extended it in two different\nways from the previous definition that i\nhad\nfirst i made it very explicit now that\nthe value function depends on the policy\nand the way to reason about this if i\nhave this conditioning on pi\nmeans that i\nuh i could write this long form to say\nthat every action at subsequent time\nsteps is selected according to this\npolicy pi\nso note that we're not conditioning on a\nsequence of actions\nnowhere conditioning on a function that\nis allowed to look at the states that we\nencounter and then pick an action which\nis slightly different\nthe other thing that we've done now on\nthis slide introduce a discount factor\nthis is a somewhat orthogonal thing but\ni thought i should include it here so\nthat we have the generic form of a value\nfunction which conditions on the policy\nand includes potentially this discount\nfactor which is a very common construct\nin reinforcement learning\nand one way to think about that is that\nthe discount factor helps determine the\ngoal in addition to the reward function\nfor instance if you consider a reward\nfunction to be plus one on every time\nstep\num\nthen it could be\ninfinitely large\nalternatively if you think of a maze\nwhere the reward is zero on every time\nset until you reach the goal\nthen the value function for a uniformly\nrandom policy would be\nsorry if it's zero every time's at one\nwhen you reach the goal then any policy\nthat eventually reaches goals gets a\nvalue of one so then we can't\ndistinguish between getting there\nquickly\nso sometimes discount factors are used\nto define goals in the sense oh maybe\nit's better to look at the near-term\nrewards a little bit more unless it's a\nlong-term reward so this allows us to\ntrade off the importance of immediate\nversus long-term rewards\nso to look at the extremes to make it a\nbit more concrete you can consider a\ndiscount factor of zero\nif you plug that into the definition of\nthe value as it's\nwritten on the slide there you see that\nthen the value function just becomes the\nimmediate reward all of the other\nrewards are cancelled out because\nthey're multiplied with the zero\ndiscount\nso that means if your discount factor is\nsmall or in a special case if it's zero\nthen you only care about the near term\nfuture\nif you don't want to optimize your\npolicy then the policy would also be a\nmyopic policy a short-sighted policy\nwhich only cares about immediate reward\nconversely the other extreme would be\nwhen the discount factor is one this is\nsometimes called the undiscounted case\nbecause then the discounts basically\ndisappear from the value definition we\nget the definition that we had before\nwhere all rewards are equally important\nnot just the first one but the second\none also is equally important the first\none and that also means that you no\nlonger care in which order you receive\nthese rewards\nand sometimes it's useful to have a\ndiscount factor that is in between these\ntwo extremes in order to define the\nproblem that you actually want to be\nsolving\nnow as i mentioned the value depends on\nthe policy and then ultimately we want\nto optimize these so we want to be able\nto reason about how can we pick\ndifferent policies\nand we can now do that because the value\nfunction cannot be used to evaluate the\ndesirability of states and also we can\ncompare different policies on the same\nstate we can say one value might have a\ndifferent sorry one policy might have a\nhigher value than a different policy and\nthen we can maybe talk about the\ndesirability of different policies\nand ultimately we can also then use this\nto select between actions so we could do\nso no here we've defined the value\nfunction as a function of a policy\nbut then if we have a value function or\nestimated value function we can then\nmaybe use that to determine a new policy\nso this will be talked about in a lot\nmore depth in future lectures but you\ncan think of this as kind of being\nan incremental learning system where you\nfirst estimate the value of a policy and\nthen you improve your policy by picking\nbetter policies according to these\nvalues and that's indeed a relevant\nalgorithm idea that we'll get back back\nto later\nas i mentioned before the value\nfunctions and returns have recursive\nforms so the return now has its discount\nfactor in the more general case\nand the value function is also recursive\nwhere again as i mentioned before the\nvalue of a state can be defined as the\nexpected value of the immediate reward\nplus now the discounted value at the\nfuture state for that same policy\nand here the notation a\ntilde pi just means that a is sampled\naccording to the probability\ndistribution pi\nand we'll just use that same notation\neven if the probability distribution is\njust deterministic for simplicity\nthis is called a bellman equation it was\nfirst described by richard bellman in\nthe 1950s and it's useful because you\ncan turn it into algorithms\nso these equations are heavily exploited\nand a similar equation can be written\ndown\nfor the optimal value which is really\ninteresting\nso note that the equation above this is\nconditioned on some policies so we have\nsome policy and we can then determine\nits value turns out we can also write\ndown an equation for the optimal\nvalue that you can have so there is no\nhigher value that you can get in this\nsetting\nand this turned out to adhere to this\nrecursion that is written on the slide\nwhere v star the optimal value of state\ns\nis equal to the maximization over\nactions of the expected reward plus\ndiscounted next value conditioned on\nthat state in action\nimportantly this does not depend on any\npolicy it just depends on the state\nand this recursion is useful\nit defines recursively the um the\noptimal value because know that the v\nstar is on the left hand side and the\nright hand side but we can use this to\nconstruct algorithms that can then learn\nto approximate v-star\nin the future\nin future lectures we will heavily\nexploit these equations and we'll use\nthem to create concrete algorithms\nand in particular of course we often\nneed to approximately so the previous\nslide just defines the value of a\ncertain policy and it defines the\noptimal value doesn't tell you how to\nget them and in practice you can't\nactually get them exactly and we'll have\nto approximate them somehow and we will\ndiscuss several algorithms to learn easy\nefficiently\nand the goal of this would be that if\nyou have an accurate value function then\nwe can behave optimally i mean if we\nhave a fully accurate value function\nbecause then you can just look at the\nvalue function we could define a similar\nequation that we had on the previous\nslide for state action values rather\nthan just for state values\nand then the optimal policy could just\nbe picking the optimal action according\nto those values so if we have a fully\naccurate value function we can use that\nto construct an optimal policy this is\nwhy these value functions are important\nbut if we have a suitable approximation\nwhich might not be optimal might be we\nmight not be perfect it might still be\npossible to behave very well even in\ninteractively large domains\nand this is kind of the promise for\nthese approximations that we don't need\nto find the precise optimal value in\nmany cases it might be good enough to\nget close and then the resulting\npolicies might also be performed very\nwell\nokay so the final component inside the\nagent will be a potential model this is\nan optional component similar to how the\nvalue functions are optional although\nthey are very common\nand a model here refers to a dynamics\nmodel of the environment the term is\nsometimes used more generally for other\nthings as well in uh artificial\nintelligence or machine learning but in\nreinforcement we typically when we say\nwe have a model we typically mean a\nmodel of the world in some sense so that\nmeans the model predicts what the\nenvironment will do next for instance we\ncould have a model\np which predicts the next state\nwhere maybe if you give it inputs as\ninputs a state an action and a next\nstate the output of this thing is an\napproximation to the actual probability\nof seeing that next state after\nobserving this previous state and action\nso again for simplicity might be good to\nkeep in mind a specific agent states\nwhere for instance these agencies could\nbe your observation\nthen this would be the probability of\nthe next observation given the previous\nobservation and the previous action\nand we could try to model that we could\ntry to approximate this\nand then in addition we could also\napproximate the reward function which\ncould be for instance conditioned on\nstate and action where this would just\nbe the expected reward given that you\nare in that state and taking that action\na model doesn't immediately give us a\ngood policy like for value functions we\ncan actually just kind of read off a\npolicy if we have state action value\nfunctions we can pick actions according\nto these values for a model we don't\nimmediately have that we would still\nneed to conduct some sort of a planning\nmechanism we'll talk about specific\nalgorithms that can be used in addition\nto like uh sorry on top of a model in\norder to extract a policy but it's good\nto keep that in mind in general that the\nmodel would still require additional\ncomputation in order to extract a good\npolicy\nin addition to the expectation above for\ninstance for the reward we consider the\nexpected reward\nwe could also consider a stochastic\nmodel or\nan expectation model for the state so\nthe state's\nmodel here in particular\nthis would be an example of a\ndistribution model where we try to\nactually\ngrasp the full distribution of the next\nstate given the current state in action\nyou could also instead try to\napproximate the expected next state or\nyou could try to find a model that just\noutputs a plausible next state\nor maybe randomly gives you one of the\nstates that could happen these are all\nchoices design choices and it's not 100\nclear in general yet what the best\nchoices are\nand now i'll go through an example to\ntalk about all of these agent components\na little bit\nit's just a very simple example we'll\nsee much more extensive examples in data\nlectures\nand in particular we're going to\nconsider this maze\nso we'll start at the left and the goal\nis at the right and we define a certain\nreward function which gives you a minus\none per time step\nthat means that the optimal thing to do\nis to go to the goal as quickly as\npossible because then you'll have the\nlowest number of minus ones\nthen the actions will be up down left\nand right or north east south and west\nif you prefer\nand the agent location is the state\nlet's say that this is fully observable\nso you can basically just tell where you\nare maybe you could think of this as x y\ncoordinates which are easily\neasily shown to be markovian in this\nsetting\nso here's an example which shows a\npolicy\nand in fact it shows the optimal policy\nso in every state we see an arrow this\narrow depicts which action to take so\nfor instance in the left most states the\narrow points right\nso we say that in the leftmost state the\npolicy now will take the action right\nthis policy is a deterministic policy\nthat indeed gives us the shortest\npath to the goal and it will be an\noptimal policy you could also consider a\nstochastic policy which might select\nmultiple actions with non-zero\nprobability\nhere is the value of that policy on the\nprevious slide which happens to also be\nthe optimal value function which as you\ncan see increments every time you step\naway from the goal and this is because\nthe value function is defined as the\nexpected\nsum of rewards until uh\ninto in into the indefinite future but\nif the episode ends at the goal then the\nrewards stop there\nso if you're one step away from the goal\nthe value will just be minus one for\nthat optimal policy if your two steps\naway it will be -2 and so on\nthis is a model and specifically this is\nan inaccurate model because note that\nall of a sudden a part of the maze went\nmissing\nso in this case the numbers inside the\nsquares are the rewards so these are\nmolds as just oh we've learned the\nreward is basically minus one everywhere\nmaybe this is very quick and easy to\nlearn and the dynamics model was learned\nby simply interacting with the\nenvironments but turns out maybe we\nhaven't actually gone to that it's\nthat portion there in the left corner\nleft bottom corner and therefore the\nmodel is inaccurate and wrong there if\nyou then would use this model's plan it\nwould still come up with the optimal\nsolution for the other states that this\ncan see but it might not have any\nsolutions for the states it hasn't seen\nit's just an example of course it's\nunrealistic to have an accurate value\nfunction but an inaccurate model in uh\nin this way specifically\nbut it's just an example to say oh yeah\nyour model doesn't have to be perfect if\nyou learn it it could be imperfect the\nsame of course holds for the policy and\nvalue function these could also be\nimperfect\nokay now finally before we reach the end\nof this lecture i'm going to talk about\na different\nsome different agent categories\nand in particular this is basically a\ncategorization it's good to have this\nterminology in mind which refers to\nwhich part of the agent are used or not\nused\nand a value-based agent is a very common\nuh\nversion of an agent and in this agent\nwe'll learn a value function but there's\nnot explicitly a policy separately\ninstead the policy is based on the value\nfunction\nthis agent that i showed earlier that\nwas playing atari games is actually of\nthis form where this agent learns state\naction value functions and then picks\nthe picks the highest rated action in\nevery state with a high probability\nconversely you can think of a\npolicy-based agent which has an explicit\nnotion of a policy but doesn't have a\nvalue function i haven't yet told you\nany algorithms how you could learn such\na policy if you're not learning values\nbut we'll actually see an example of\nthat in the next lecture\nand then there's the terminology actor\ncritic the term actor critic refers to\nan agent which both has an explicit\nrepresentation of a policy\nand an explicit representation of a\nvalue function these are called actor\ncritics because the actor refers to the\npolicy part there's some part of the\nagent that acts and the value function\nis then typically used to update that\npolicy in some way so this is an\ninterpreted as a critic that critiques\nthe actions that the policies takes and\nhelps it select better policies over\ntime\nnow all of these agents could be\nmodel-free which means they could have a\npolicy and or a value function but they\ndon't have an explicit model of the\nenvironment\nso note in particular that a value\nfunction can of course also be\nconsidered some model of some part of\nthe environment it's a model of the\ncumulative expected rewards\nbut we're not calling them a model in\nreinforcement learning parlance\ntypically so instead if you just have a\nvalue function we tend to call this\nmodel free\ni'm saying that not because it's a great\ndefinition or a great division between\nagents but because it's a very common\none so if you read papers and they say\nsomething about model-free reinforcement\nplanning this is what they mean there's\nno explicit dynamics model\nso conversely a model-based agent could\nstill ultimately oh sorry could still\noptionally have an explicit policy and\nor a value function but it does in any\ncase have a model some model based\nagents only have a model and then have\nto plan in order to extract their policy\nother model based agents have a model\nbut in addition to that have an explicit\npolicy and for instance use the model to\nsometimes just incrementally improve\nvalue function or our policy\nso now finally we're going to talk about\nsome sub problems of the rl problem\nprediction\nis about evaluating the future\nso for instance learning a value\nfunction you could call a prediction\nproblem and this is indeed often the\nterminology that is used typically when\nwe say prediction we mean for a given\npolicy so you could think about\npredicting the value of the uniformly\nrandom policy for instance or of a\npolicy that always goes left or\nsomething of the form\nconversely control is about optimizing\nthe future finding the best policy\nso it's good to note that this\nterminology is used quite\nfrequently in papers so it's good to\nhave that in mind and often of course\nthese are quite related because if we\nhave good predictions then we can use\nthat to pick new policies\nin fact the definition of the optimal\npolicy\npi star is the arc max of policies over\nthese value functions by definition the\nvalue function defines which policies\nare uh\nthe ranking on policies essentially your\npreference on policies that doesn't mean\nthat you need to have these value\nfunctions per se in order to learn\npolicies but it just shows shows how\nstrongly related to the problems of\nprediction and control are\nin addition there's an interesting\nquestion that i encourage you to ponder\na little bit which is that\nthis is something that rich saturn often\nsays\nthat in one way or the other prediction\nis maybe very good\nform of knowledge\nand in particular\nif we could predict everything\nit's unclear that we need additional\ntypes of knowledge\nand i want you to ponder that and think\nabout whether you agree with this or not\nso if you could predict everything\nis there anything else that we need\nso feel free to pause the video and\nthink about that for a second\ni'm going to give you one suggestion so\nindeed if you can't predict everything\nabout the world this gives you a lot of\nknowledge\nit might not immediately tell you how to\ndo things so maybe it's sometimes useful\nsimilar to these policies and value\nfunctions sometimes it can be useful\nespecially if we're approximating so we\ncan't predict everything perfectly it\ncan be useful to separately store\npredictions and separately scores\nstore policies or you could think of\nthese as them being skills in some sense\nbut indeed predictions are very\nrich form of knowledge and many things\ncan be phrased as a predictive problem\neven if they're not immediately clearly\na predictive problem uh\nif you first think about them\nand as i've referred to when i was\ntalking about models there's two\ndifferent parts to the reinforcement\nvoting uh\nproblem one is about learning\nand this is the common setting which we\nassume where the environment is\ninitially unknown and the agent\ninteracts with the environment and\nsomeone has to learn\nwhether it's learning a value function a\npolicy or a model all of that could be\nput under the header of learning\nand then separately we could talk about\nplanning so planning is a common term in\nartificial intelligence research\nand planning is typically about when you\nhave a model so let's say the model of\nthe environment is just given to you\nand then the agent somehow figures out\nhow best to optimize that problem\nthat would be planning so that means\nyou're using some compute to infer from\nthe statement of the problem from the\nmodel is given what the best thing to be\nto be done is\nnow\nimportantly the model doesn't have to be\ngiven but could also be learned but then\nit's good to keep in mind that the model\nmight be slightly inaccurate so if you\nplan exhaustively in a learn model you\nmight find a certain policy but it's\nunclear that this policy is actually\noptimal in the true world because the\nmodel might not be completely accurate\nand indeed the planning might latch on\nto certain inaccuracies in the model and\nhence might find solutions that are\nactually not that suitable for the real\nworld because for instance the model\nmight have a holy wall somewhere that is\nnot actually there and then the shortest\npath might take the agent through that\nhole which isn't actually there and the\npolicy might not be great that you get\nfrom there\nbut we can think of planning more\ngenerally as some sort of an internal\ncomputation process so then learning\nrefers to\nabsorbing new experiences from this\ninteraction loop and planning is\nsomething that sits internally inside\nthe agent's head it's a purely\ncomputational process and indeed i\npersonally like to define planning as\nany computational process that helps you\nimprove your policies or predictions or\nother things inside the agent without\nlooking at new experience\nlearning is the part that looks at a new\nexperience that takes in your experience\nand somehow condenses that and planning\nis the part that does the additional\ncompute\nthat maybe turns in a model that you've\nlearned into a new policy\nit's important also to know that all of\nthese components that we've talked about\nso far can be represented as functions\nwe could have policies that map states\nto actions or to probabilities over\nactions value functions that map states\nto to expected rewards or indeed also\ntwo probabilities of these we have\nmodels that map states to states or\nstate actions to states and we could\nhave rewards that map states to rewards\nagain or distributions over these and we\nhave a state of that function that takes\na state and an observation and\npotentially an action and a reward and\nmaps it to a subsequent state\nall of these are functions and that's\nimportant because we have very good\ntools to learn functions specifically\nthese days neural networks are very\npopular very successful and the field of\nresearching how to train neural networks\nis called deep learning\nand indeed in reinforcement we can use\nthese deep learning techniques to learn\neach of these functions and this has\nbeen done with great success\nit is good to take a little bit of care\nwhen we do so because we do often\nviolate assumptions from say supervised\nlearning for instance the data coming at\nus might be correlated\nbecause for instance think of a robot\noperating in a room it might spend some\nsubstantial time in that room so if you\nlook at the data coming into the agent\nit might be correlated over time and\nthen sometime later might go somewhere\nelse and this might be less correlated\nbut there might be in the near term\nquite some strong correlations in the\ndata which are sometimes assumed not to\nbe there when you do supervised learning\nin addition the problem is often assumed\nto be stationary in supervised learning\nin many supervised learning problems not\nin all of course but in reinforcement\nwe're often interested in non-stationary\nthings think for instance of a value\nfunction as i mentioned the value\nfunction is a is typically\nconditioned on a policy but if we're\ndoing control if we're trying to\noptimize our policy the policy keeps on\nchanging that means that the relevant\nvalue functions maybe also keep on\nchanging over time because maybe we want\nto keep track of the value of the\ncurrent policy but if the policy keeps\non changing that means that the value\nfunction also needs to change\nso this is what i mean when i say we\noften violate assumptions from\nsupervised learning that's not\nnecessarily a huge problem but it does\nmean that whenever we want to use some\nsort of a deep learning technique\nsometimes they don't work out of the box\nso deep learning is an important tool\nfor us when we want to apply\nreinforcement learning to big problems\nbut deep reinforcement learning which is\nbasically a research field of the merger\nof deep learning and reinforcement\nburning or how to use deep learning in\nreinforcement birding is a very rich and\nactive research field you can't just\nplug in deep learning and then hope that\neverything will immediately work that\nworks up to a point but there's lots of\nreasons to be done exactly at that\nintersection of deep learning and deep\nreinforcement learning we'll talk much\nmore about that later in this course\nokay\nnow that brings us to the final examples\nso i talked about atari let's make it a\nlittle bit more specific now what was\nhappening in the atari game that i\nshowed you so you can think of the\nobservations as the pixels as i\nmentioned at that time point in time as\nwell\nthe output is the action which is the\njoystick controls and the input is the\nreward here on the slide it actually\nshows the score but the actual reward\nwas the difference in score on every\ntime step\nnote that the rules of the game are\nunknown and you learn directly from\ninteractive gameplay so you pick actions\non the joystick you see pixels and\nscores and this\nis a well-defined resource and printing\nproblem and we have algorithms that can\nlearn to deal well with this\nas a different example here's a\nschematic example a little bit more of\nan illustrative example\nand this is easy to easier to reason\nthrough this is why we sometimes use\nthese much smaller examples and\noftentimes the conclusions still\ntransfer so the entire example is an\nexample of a rich messy\nhard problem in some sense right and\nthis would be an example of a very small\nscale illusive problem\nand we do this because we can often\nlearn something from these smaller\nproblems that we can apply to these much\nharder to understand difficult big\nproblems\nso in this specific example which is\nfrom the susan lombardo book\nit's basically a great world without any\nwalls although there might be walls at\nthe at about at the edges essentially\nbut not any walls inside\nthe 5x5 grid\nand there's a reward function which is\ndefined as -1 when bumping into a wall\nzero on most steps\nbut in if you take any action from state\na the state that is labeled with a\nyou get a plus 10 reward and you\ntransition to a prime\nso even if you press say up from set a\nyou still find yourself in a prime and\nyou get plus 10. similarly from state b\nyou would transition to state b prime\nand you get plus five\nnow we can ask several different\nquestions about this setting and there\nmight be reasons why we might be\ninterested in these different\nquestions so a first question could be a\nprediction question which is for\ninstance what is the value of the\nuniform random policy that selects all\nof the actions uniformly at random\nand that's depicted here on the right\nhand side in figure b\nand what we see here is that this is\nquite a complicated construct right i\nwouldn't have been able to tell you just\nimmediately just by looking at the\nproblem what the value function is for\nthe optimal for sorry for the uniformly\nrandom policy but we can use\nreinforcement building algorithms which\nwe'll talk about in future lectures to\ninfer this\nand to figure out what that value is\nand it turns out just to look at this a\nlittle bit more in detail that of course\nthe value of state a is quite high\nbecause from this state you often get a\nhigh you always get a high reward\nbut it's lower than 10 because the\nrewards after this first reward of 10\nare negative you see that the value of\nstate a prime is actually minus 1.3\nsorry i didn't say but there's a\ndiscount factor here as well 0.9\nwhich means that the\nthis is why the the value of state a is\n8.8\nand the value of state a prime is 1.3\nand the difference between them is not\nquite 10 right um\nbut from from state b\nyou often get a minus one because you\noften find yourself bumping to the to\nthe ball wall to bottom\nor you don't get a minus one but then\nyou might get a minus one on the next\ntime so because you might have walked\nleft to the corner\nand it's quite a complicated thing\nbecause of the discount factor because\nof the dynamics of the world but we can\nsee that state a is desirable state b is\nsomewhat desirable\nstates in the bottom left are quite\nundesirable\nbut you might actually be more\ninterested in okay but what's the\noptimal thing to be doing and that to me\nis not immediately obvious right should\nyou be going to state a and then loop to\na prime\nand get this plus 10 every time you\ncould but it takes you a couple of steps\nin between each two times you do that\ntransition\nyou could also go to state b and go to b\nprime and then you can get these\ntransitions more often\nnow it turns out we could also figure\nout what the optimal value function is\nfor this problem and what the optimal\npolicy is\nif you look at the optimal value they're\nall positive now because you never have\nto bump into a wall anymore because the\noptimal policy doesn't bump into walls\nso even the top sorry the bottom left\ncorner now has positive values\nand in fact the lowest positive values\nare in the bottom right corner now\nbecause from there it takes you a long\ntime to get to the best possible state\nyou can and it turns out the best state\nyou can be in is state a\nlooping with these plus 10 rewards is\napparently more beneficial than looping\nwith these plus 5 rewards even though\nthe difference in\nuh\ndistance\non these plus fives is smaller\nso you can get more plus fives in a row\nvery quickly by going from b to b prime\nafter each like every time again but\ngoing from a to a prime is apparently\nmore profitable in the long term and we\ncan see this in figure c here as well\nwhere the optimal policy is depicted\nwe see that if you're in almost any\nstate what you should be doing is you\nshould be moving to state a this will\ntransition you all the way to the bottom\nto a prime and from there you'll just\nmove straight up again\nup to state a and repeat\nconversely if you're in state b prime\nif you just look at where b prime is in\nthis you would either go up or left it\ndoesn't actually matter which one\nthey're equally good\nbut if you go up you would then move\nleft so you wouldn't move into state b\ninstead you\nwould move left and then you move up or\nleft again in order to get state a\nthere's only one state that would move\ninto b\nwhich is the top right corner\nbecause from the top right going around\nstate b and then going all the way to a\nwould take so long that it's actually\nmore beneficial to jump into state b\nwhich will transition you to b prime and\nthen from there we'll go to state a and\nthen loop indefinitely\nso this is quite subtle i wouldn't have\nbeen able to tell you just from looking\nat the problem that this would be the\noptimal policy but fortunately we have\nlearning and planning algorithms that\ncan sort that out for us and they can\nfind this optimal solution without us\nhaving to find it\nso popping up in this course we will\ndiscuss how to learn by interaction we\ndidn't really discuss it in this lecture\nin this lecture we just talked about the\nconcepts and the terminology and things\nlike that but we haven't really given\nyou algorithms yet we will do that in\nthe subsequent lectures\nand the focus will be on understanding\nthe core principles and learning\nalgorithms so it's less about what the\ncurrent state of the artist will touch\nupon that a little bit for sure but it's\nless about specific algorithms that\npeople happen to use right now and then\ngo all the way to the depth of those\nwe will do that for some algorithms but\nit's much more important to understand\nthe core principles and learning\nalgorithms because the algorithms that\nare currently safer they will change\nnext year there will be new algorithms\nand if you understand the core\nprinciples\nthen you can understand these new\nalgorithms and maybe you could even\ninvent your own algorithms\ntopics include exploration in the next\nlecture\nin something called bandits which is\nbasically one step\nmark of decision processes\nwe will talk about more about what\nmarketing decision processes actually\nare like how are they mathematically\ndefined and what can we say about them\nand we will talk about how to plan in\nthose with dynamic programming this will\nbe the lectures after the next lecture\nby this user will be given by diana\nand then we will use that to\nthen go into model-free prediction and\ncontrol algorithms you may have heard of\nan algorithm called q-learning\nor i mentioned earlier in this lecture\nan algorithm called dqn dqm is short for\ndeep q network\nq as i mentioned is often used to refer\nto state action values\nq learning is an algorithm that can\nlearn state action values and then the\ndqn algorithm is an algorithm that uses\nq-learning in combination with deep\nneural networks to learn these entire\ngames\nthis falls on the model 3 prediction and\ncontrol because no explicit model of the\nenvironment is learned in that algorithm\nwe will also talk about policy gradient\nmethods we in fact already touch upon\nthem in the next lecture but we'll talk\nabout them more later and these are\nmethods that can use be used to learn\npolicies directly without necessarily\nusing a value function but we also\ndiscuss actor critic algorithms in which\nyou have both an explicit policy network\nor function and you have an explicit\nvalue function\nand this brings us also to deep\nreinforcement training because as i\nmentioned these functions are often\nrepresented these days with deep neural\nnetworks that's not the only choice they\ncould also be linear or it could be\nsomething else\nbut\nit's a popular\nchoice for a reason and it works really\nwell and we'll discuss that at some\nlength later in this course\nand also we will talk about how to\nintegrate learning and planning i talked\na little bit about planning being an\ninternal computation process and then\nlearning meaning the process that takes\na new experience and learns from that\nand of course we could have both of\nthose happening at the same time in an\nagent and then we want them to play\nnicely together\nand there will be much more there will\nbe other topics that we'll touch upon\nwhen we go through all of this\nokay\nnow finally i want to show you one final\nexample of a reinforcement writing\nproblem\nagain what we'll see here is a little\nbit more of a complicated example\nso what we'll see is a system which was\nlearned to\ncontrol the body so you can see the body\nalready here on the still\ni'll press play in a moment and what\nwill happen is is that there's an\nalgorithm that controls the uh basically\nthe forces of these uh body parts so\nthis agent specifically it can run right\nand it had to learn itself how to move\nits limbs in such a way as to produce\nforward motion the reward was a very\nsimple one\nthe reward was just go in that one\ndirection\nand you'll get plus one or you get\npositive reward\nbasically proportional to how fast you\ngo in that direction so it really wants\nto go really fast in one direction it\nwas not told how to do that so it\ndoesn't at first when it starts swelling\nit doesn't know how to control its limbs\nit just knows that it\nthat it's\nit perceives the world in some sense by\nby sensors which i won't go into that\nmuch depth it's too not too important\nhere but the point is it doesn't know\nhow to move its limbs it has to figure\nthat out by itself and it just notices\nthat when it moves in certain ways it\ngets more rewarding in other ways\ndoing that\nyou get the following behavior\nwith simplified vision as it says on the\nslide and proprioception which means it\ncan it can feel essentially where its\nown limbs are in some sense\nand then it can traverse through this\nvery complicated uh domain and it can\nlearn how to jump over things and how to\nmaybe even climb in some sense\njust because it wants to go to the right\nnot everything is easy but it does\nmanage to get there\nnow interestingly\nby using this setting by just having a\nsimple reward you can traverse different\ntypes of domains you can learn to\nterrain sorry you can learn to traverse\ndifferent types of terrains and do this\nin very non-trivial ways\nit would be very hard to specify a\npolicy by hand that does this and in\nfact because we have a learning system\nit's not just that we don't have to\nspecify a thing by hand but we can also\napply the exact same learning system to\ndifferent body types\nso this was learned with the exact same\nsystem\nthat was used for this other thing and\nyou can use this in two dimensions or\nyou can use it in three dimensions\nand in each of these cases the agent can\nlearn\nby interaction how to actually scale\nthese obstacles\nso the reward is particularly simple we\ndidn't have to think about how to move\nthe limbs in order to do that we can\njust have the learning system come up\nwith that and that's the important point\nhere\nand you can apply some more difficult uh\nterrains you can apply this to different\nbody types you can get quite non-trivial\nbehavior in doing so\nokay so that brings us to the end of\nthis lecture\nthank you for paying attention\nand\nwe'll see you in the next lecture which\nwill be on the topic of expiration", "date_published": "2022-03-29T12:01:55Z", "authors": ["Google DeepMind"], "summaries": [], "initial_source": "ai_alignment_playlist"}
{"id": "c2f85609fb5038f07b7910d170c50f9e", "title": "268. AI Practical Advice For The Worried", "url": "https://www.youtube.com/watch?v=nxYwNElXA1k", "source": "youtube", "source_type": "youtube", "text": "hello and welcome to session 268 in the\narctic.com training group tonight we\nwill be discussing the post AI practical\nadvice for the worried by three muscle\nroutes\nthree most of which is uh the founder\nand probably CEO of belter research he's\nalso a professional in Magic the\nGathering player and the CEO of mitimate\nthis is a post from the beginning of\nthis month and it was posted on his\npersonal blog called don't worry about\nthe vase which is a a reference to this\nscene in in The Matrix where the Oracle\ntells new not to worry about the vase\nthat he's about to break\nso practical advice for the word it\nturns out that I am in fact the target\nour audience for this because I am\nworried and that makes it always for me\nto evaluate this text on the criteria\nwhether the advice is in fact practical\nfor me\num and I've tried to give examples of my\nown situation in order to to answer this\nquestion\nbut I'd also like to upfront give some\nof my advice that I think\num in some respects differ quite a bit\nfrom Swiss\nthe the first one that I think is really\nCentral is if you're really worried then\nyou should strongly consider to actually\ndo something actually try to work on AI\nsafety think hard about it see if you\ncan find even some minor way to\ncontribute that is in my estimate likely\nto be very psychologically beneficial uh\nlike you get a bit more of an internal\nlocus of control that I think are very\nvaluable uh in this kind of situation\nfor many people\num you also plug into a supportive\nCommunity which also can help and can\nnot help depending on what part of the\ncommunity you go to\num and there are in fact some actions\nthat are really easy to do like upvoting\nposts unless wrong or something like\nthat it's very a very simple way to\ncontribute that I'm certain that uh even\npeople who have very little technical\nskill will be able to do productively\nso one of my key practical advice is uh\none that sweet does talk a bit about but\nit does not talk enough about like you\nneed to focus if your word on your\nmental health and uh trying to uh get\nyourself in a position that doesn't\ndeteriorate for uh for your mental\nhealth and like it depends a lot on who\nyou are and what what you respond to but\nI think this should be a core priority\nfour classic ways to do that is to\nexercise eat and sleep and avoid drama\nall these kind of things\num in particular\num one thing that you should think about\nis like what is your financial situation\nbecause uh sweet talks about\num borrowing money but a lot of people\nhave in fact saved up money so I think a\ncore consideration is\num how much money do we have uh and at\nwhat point can you afford not to get\npaid from now until you expect that\nthere is probably some kind of\nSingularity of fire alarm\na very strong Louis that I think you can\nuse for uh adjusting your\num that is very practical to adjust is\nwhat media you read what blocks do you\nsee do you like Doom scroll on Twitter I\nthink it's a term and I think that's a\nan important thing to avoid\nfor most people but I say for most\npeople because there's a classic saying\nthat you need to reverse all the advice\nthat you get because what is right for\none person may be exactly wrong for the\nother person a perhaps the best way\naround this is to do some kind of uh\nexperimentation like try to abstain from\nReading social media for a week and see\nif that makes you feel better\num that's a very easy experiment to do\nand likely to be very beneficial\nand finally when someone asks me for\npractical advice the thing I tell them\nis to make sure you hug your loved ones\nboth instrumentally and as a final goal\nnow for the text itself it starts out\nwith some arguments against AGI some\narguments against not being maximally\nworried and\num\nthree considers it's an open question\nand says there are in fact good reasons\nto think that we won't have AGI for a\nlong time and I think there are reasons\nwhy we wouldn't have it there are\nscenarios\nbut none of them come rise to the level\nwhere I would call them a good reason\num the first three uh um uh shows is we\ncould run out of train data train data\nclearly has been a very substantial part\nof the uh the story of how AI has\nimproved to this point and\num like we have all the the checks on\nthe internet but uh will we get more\nwell I think it's very obvious that we\nare going to see more things like videos\nuh obvious training data that we uh\nprobably can unlock soon interactions\nwith the physical world is another big\nthing that we almost certainly uh will\nsee more of in the future\num and there are others that have been\nsuggested\num in I think it's certainly a\npossibility that this could fail I think\nhumans are a strong counter example to\nthis humans are generally intelligent\nand humans can in fact be trained by the\namount of evidence there exists in the\nworld\nanother arguments against agis that we\ncould just run out of ways to improve AI\nright now uh I think we are seeing\nalgorithmic improvements small but but\nreal Hardware improvements following\nroughly most law uh give or take an\norganizational improvements like just\nlarger projects we see these three kinds\nof uh improvements very often and there\nare no obvious other limits to those\num we may see reculatory barriers that\nprevents AI from having a big economic\nimpact I actually think this is very\nvery likely I think we are not very very\nlike that's always stating the case my\nmodel strongly has that once AI becomes\ncapable of doing very important economic\nproductivity then this will be our Lord\num and I think that is\num very uh plausible but on the other\nhand this doesn't actually Bear on AI\nrisk as such because uh AI risk is that\nat some point the AI will be powerful\nenough to ignore these regulations and\ntake over and and that point isn't\naffected very much by the extent to\nwhich AI is prevented from impacting\nmuch of the economy a little of course\nbecause it depends on how much uh\ninvestment is done but this kind of\nregulatory barriers will only happen at\nvery very high levels\nfinally we could see other barriers to\nAGI\num I think like\num if we are not fundamentally confused\nabout AGI which totally is a thing that\ncould happen then the thing I see\nhappening here is some kind of AI safety\nsuccess coordination or some other way\nof uh preventing AGI that like it could\nbe a catastrophe for instance a global\nthermonuclear war would totally prevent\nAGI for a long time\num but that's not really something a lot\nof people hope and and I haven't really\nseen anyone make a strong argument for\nthis kind of coordination being feasible\nin the very short term or or other\nsimilar cases that would prevent\num uh if AGI continues on Trend due to a\ntraining compute Etc then what precisely\nwould prevent it seems very way to me\nand so this is just the argument against\nAGI there is of course one more step in\nthe reasoning chain from AJ from where\nwe are now to X risk and that is that\nonce we have AGI will we then die and uh\nthree says uh yes if we get AGI\nreasonably soon he expects that we will\nall die\num so how should you think about this so\nhe says you should think for yourself if\nthis is something that is really\nimportant to you and this is something\nthat will affect your decisions you\nshouldn't just adapt to this position or\nanyone else you should think for\nyourself\nand you shouldn't uh update too much\nabout like who has the the social\nprocess of figuring out who can I trust\nuh to make decisions about this kind of\nthing\nI think uh this may not be true uh like\nthis will uh AI risk of course impacts\neverybody and both impact people who\nhave like a competitive advantage in\nbuilding models and understanding the\ncore of the issues and people who are\nskilled in social cognition\num and for these people relying on\nsocial cognition may be the best bet\nI would worry here in this case I think\nI have seen a number of examples of\npeople who believe that they have very\ngood social cognition and then they make\nvery poor decisions in practice uh like\num if you are if you claim that you have\na strong social cognition on these kind\nof issues then probably the people who\nwho said you should invest in Bitcoins\nyou listen to them right because if you\nif you are able to make good decisions\nin this way\num so I think there's a good chance that\nyou that people overestimate their own\nabilities of social cognition\nand obviously the thing you should do\ninstead is to build your own\nunderstanding and your own model and in\nthis I would personally uh suggest that\npeople\ndon't put too much stock in the outside\nView and much more stuck on the inside\nview\nand finally you should decide what you\npredict and uh like I think this is like\ntechnically wrong you first make your\npredictions and then you make your\ndecisions\num but uh that may be a minor point\nso how should you react to uh to this\nnew situation that we're in just\nreacting properly our upfront three\nstates this is hard most people react\npoorly and\num the the idea that oh you should just\ncontribute is actually one that is\nsurprising surprisingly hard because if\nyou go into Ai and think I'm gonna help\nwith this problem then there is a very\nreal risk that you'll just end up making\nthe problem worse it's really hard to\nwork on safety without working on\ncapability and a lot of people have been\nburned on this\nhow about suppressing the information is\nthat a good idea well the the bad things\nthe disadvantages of suppressing the\nidea is that it's for yourself sub\noptimal like it's much better if you\nreact to uh to what is going on and it's\nmuch better for society like if you\nthere is something you can do then it's\nmuch better that you do it but on the\nplus side there are advantages to\nsuppressing you avoid existential\ndespair you don't ruin your financial\nfuture by taking on your unsustainable\ndebt you don't miss out on the important\nthings in the in life and you don't make\nthe problem worse and those are in fact\nsubstantial advantages\nnow these things like not ruining your\nfinancial future and avoiding\nexistentials despair many people can do\nthat just fine without worrying about AI\nrisk and I don't think actually that\nsuppressing is a like\num as presented here as a binary choice\nis very realistic most likely a lot of\npeople are going to be somewhere in\nbetween slightly suppressing and not\nfully suppressing and in this case there\nmay be things that you can do to capture\nmost of the value anyway\nso he warns about uh dramatically\noverreacting making an example of this\nperson on Twitter something\num who suggests you should make space\nfor questioning if your life plan makes\nsense in light of these developments\num I think that is in fact not a\ndramatic overreaction I don't know why\nit's really\num puts that label on on this imminently\nsensible uh statement\nso how do you in fact make good\ndecisions well\num three uh has a very simplified guide\nfor this first you decide what is the\nprobability that we'll have AGI soon and\nwhat is the probability that things will\ngo fine and what if we have ATI soon and\nwhat are the probability that we will\nhave a Extinction and then once you have\nthese probabilities then you look at\nyour life decisions and uh try to\ncalculate are they actually good\num\nso I am very much on in two minds about\nthe utility of this kind of easy guide\nto making decisions\num because like on one hand I think if\nyou want to make good decisions like\nthis is a very big subject how do you\nmake good decisions and the classic\nanswer that I would give is like read\nthe sequences\num uh and I realize this is a\nuh a big thing but I do in fact think\nthat it's a very important uh topic\num and one of the things that the ways\nthat this model really suffers is that\nif you just say soon without any kind of\ndefinition of soon then this makes uh uh\nthe algorithms shown here way too awake\nin my uh opinion I think you actually\nreally do need some kind of probable\ndistribution of how many years until AGI\nuh in order to make any kind of\nreasonable uh decision\nI also think that if you really want to\nto have this\ninfluence the decisions of your life you\nshould take the time to actually\nreally look into this in more detail and\nlike if you have a big life question\nlike should you have children and then\nyou should spend more than five minutes\nbecause it's something that is really in\nfact really really important\nthere are tools for how to make\ndecisions with this kind of\nprobabilistic\nFrameworks I haven't actually used them\nso I don't uh know if I can recommend\nthem\nso he has this nice\num uh summary take care remember it is\neasy to fool yourself and although he\ndoesn't state it uh I think this is this\ncan be thought of as a summary of the\nsequences and of course it is very easy\nto fool yourself and just knowing that\nit's uh very easy to fool yourself does\nnot liberate you from the problem of\nfooling yourself\nso the big statement here is normal life\nis worth living even if a very high\nprobability of Doom and for that\nhappening soon\nwhy is this uh well the first argument\nfor this is that it is in fact possible\nthat we won't have to a normal future\ncould still happen\nand\num\nin that case to you it is important\npsychologically to be prepared for that\nshould it happen and if you're not\nprepared for a normal future then that\nwill stress you out\num I think in this case it's three very\nstrongly generalizes about what is\npsychologically important and to some\npeople it may be true and to some people\nit may not\num and in particular in if I look at my\nown personal situation then I don't need\nto be prepared because I am in fact\nalready prepared and the reason why I'm\nalready prepared is because I'm the kind\nof person where if I'm not ready for a\nnormal future that would stress me out\nso uh even before I learned about AGI I\nstarted getting prepared for my normal\nfuture because that's the kind of person\nI am so and for that reason I don't need\nto be even more prepared so this\nin general people who feel that they\nneed to be prepared are already prepared\nfor a normal life so I don't think this\nis an argument uh why you should focus\non uh particular on on this case in in\nspite of the evidence\nanother reason for living a normal life\nis that you'll encounter practical\nproblems if you abandon the normal life\nfirst you will have problems with the\npeople who love you\nso I have in fact not had a lot of\nproblems with the people who love you\num they are in general pretty supportive\nand I'm very very grateful for that and\nI think like in general you could say\nthat yeah if they love you then of\ncourse by definition they are supportive\nthat's part of what loving means but I\nrealize that for a lot of people like\nthe support of the loved ones are not in\nfact something they can count on\num\nthe second is in professional\ninteractions what do people actually uh\nuh think of you when you when you say\nactually I believe that AI will at some\npoint be very very dangerous well I've\nactually talked with a number of people\nabout this and they seem surprisingly\naccepting of this they think yeah that\nseems totally reasonable they agree that\nthere is in fact a probability that AI\nwill kill us all\num like it's of course an open question\nto what extent they just humor me or\nsomething like that but I think a lot of\npeople uh working in AI\nthink that this is in fact something\nthat could happen uh\nit may also give problems in relating to\nthe world\num I think this is true uh but I also\nthink that you relate better to the\nWorld by\num\nlike the the listening of tasky is that\nif the sky is blue I desire to believe\nthat the sky is blue uh and the same way\nwith P doom and I think that there are\nin fact ways you could I think it's\ncalled derealization or something like\nthat uh there are psychological\nprocesses that are harmful that can\nhappen but in general I feel you relate\nbetter to the world if you look straight\nat the world\nfinally it will become difficult to\nadmit you made a mistake if the\nconsequences of doing so seem too dire\nand this I believe three means that if\nyou change your life to deal with AI\nrisk and then you figure out AI risk\nisn't actually that high then changing\nyour mind becomes very difficult but I\nthink this is in fact a symmetric\nargument it also goes the other way if\nyou prioritize a normal life too much\nthen\nuh changing your mind away from the\nnormal life and the normal career and\nall these kind of thing uh is also\nextremely costly\num so I think in fact it may be\nsubstantially more costly if you believe\nthat there is a high probability of Doom\nto\num to ER on the side of normalcy\nso should you take on depth like if you\ntake up that you should realize that it\nmay come back to bite you potentially\nfar sooner than you think\num and on the other hand the uh\nadvantages of living in a normal World\nthey uh they come much faster\nthan you expect or that most people\nexpect and I think at this point uh I\nwould have preferred to be to be much\nmore clear about what soon means because\nif you have timelines that say one year\nthen that's very different from\ntimelines let's say 10 years and also\nlike\num the uh the best time to influence the\nworld may in fact not be right before\nthe singularity but sometime in advance\num and uh there may be time between AGI\nand existential risk I think these\nthings need to be uh exempt in in\nGreater detail because it does in fact\nmatter very much what you should do\nwhether you believe you have on average\none year or you have 10 years\nthis is a statement that puzzled me a\nbit there are no good ways to sacrifice\nquite a lot of utility in the normal\ncase and in exchange get good\nexperiential value in unusual case\nnow the straightforward reading of this\nseems totally false right there are in\nfact a lot of ways to sacrifice a lot of\nutility in the normal case but then have\na high risk High reward thing like uh\nmaybe I'm misreading this it's possible\nI I'm not entirely sure what\nexperiential value means in in this case\num so maybe I'm just misunderstanding\nbut the classic example of a high risk\nHigh reward option that uh that\num that satisfies this is to quit your\njob and start a company in many many\ncases private in most cases you'll lose\nutility and in a few cases you'll gain a\nlot of utility and on average this is\nprobably a good idea for many people\nnow that isn't in fact the thing that I\nwould argue for starting your own\ncompany I would start uh I would accuse\nyou should instead quit your job and\nwork on AI safety\num but I think this also satisfies the\ncriteria\nyou can't do things like move\nconsumption forward uh work on your\nbucket list and take on a bit of depth\nseems useful but there are strong\nmarginal returns to this\num and I agree I have in fact done\nsomething like this uh I stopped saving\nfor retirement and put it into\naicity.com\nquite a few years ago\num so so this is something that I um I\nagree with\nhow about burning your candle at both\nends well the cost for doing that seem\nto accrue quickly you'll get burnout\nyou'll get stressed you'll get\nexistential angst\num and I agree you will get all of this\nuh I think in fact uh you'll of course\nobviously get burnt out and stress will\nyou get more existential angst from\nworking hard I would actually expect the\nopposite I would expect that the harder\nyou are working at AI safety or\npreventing a catastrophe the less\nexistential angst you get but\npeople are different and you may in fact\nbe different\nand the disadvantages like burnout is\nlike instrumentally very bad and the\nidea is you instead maintain your\ncapacity and then later a miracle of\nsome kind will happen to model violation\nand then you are able to do something\nand I think this is true like\nworking really hard can be kind of like\nsprinting and like if you've tried\nsprinting then like you can Sprint for\n10 seconds or something like that so you\nreally really need timelines for that to\nmake sense in almost all other cases you\nneed to be deliberately pacing yourself\nand three stating then contributing\nwhile living a normal life is more\neffective and I think it's a hypothesis\nthat you really need to consider that\nyou should not burn your candle at both\nends but it may also in fact not be true\nlike them it may be that the correct\nthing to do would be to just move to\nBerkeley and uh find some people there\nand work on AIC to is in fact more\neffective uh like it is not a given\nthing that it will always be more\neffective to to live in a normal life\nforeign\nthere are people in the past who have\nreacted badly to this kind of thing and\nthree gives a few examples and yeah I\nagree some people have reacted badly\nthat's not really that surprising\nbecause like if you look over the\nhistory of mankind I'm sure that a lot\nof people have reacted to things in a\nless than optimal way I do think we\nshould still take this into serious\nconsideration like uh it's called\nnoticing the skulls like you're walking\ndown a path and then you notice that uh\nactually a lot of skulls along the way\nand then a good rationalist will stop\nand say hmm this is something I should\nreally consider so depending on how many\nI don't think to be really argues that\nit's a lot of people but some people\nhave in fact gotten burned by this maybe\nwe are getting burnt well it is\nsomething we should consider are we is\nthis a local information Cascade where\nI'm updating on someone who's updating\non someone who's updating on someone and\nthis is in fact people updating on each\nother and not some uh some smart uh\npeople making some actual decisions I\nthink there's something you need to\nconsider whether there is actual\ninformation coming in or it's just a\nlocal information Cascade I think like\num we have now GT4 and I think\num in Palm and\num Claude I think it's quite clear that\ninformation is coming in but you should\nstill be aware that you could be in a\nlocal information\na an interesting point that I haven't\nseen written explicitly before is you\nshouldn't be consistent just to satisfy\npeople who ask for consistency I think\nthis is a knee point and I'm happy to to\nsee it explicitly here\noops\num and then of course the more uncertain\nyou are about timelines the more\nreasonable uh\nit is to not take on a lot of depth and\ntry to aim for some kind of knowledge\nthe last part is structures as a\nquestion and answer session\nso the first question is should I say\nfor retirement\nand Swede says well it doesn't\nexplicitly say no but says you should\nprobably focus on building up asset\nvalue over time because that is valuable\nif there is uh\num some kind of uh if there is a uh some\nlater information that lead well sweet\ndoesn't actually say that um he's just\nit's valuable in both cases and so what\nI did was once I finished uh my studies\nI started saving up and the reason I\nexplicitly gave before saving up was\nthat right now we're in a good situation\nand it's really nice to have uh some\nmoney saved up if later there is some if\nlater there is a bad situation\num so that is in fact the right reason\nthat I would\num condone after this I\num I think the thing uh to me is missing\nhere is some kind of stock criteria\nbecause saving up for retirement and\nbuilding that up asset value over time\nis a really good idea but you also need\nto spend it at some point like um and\nyou can't we don't expect a fire alarm\nso what will be your stop criteria what\nwould be the time where you stop saving\nfor retirement and then quit your job to\nwork on AI safety you you need to have\nsome idea about well that is a thing\nthat could actually happen\na more extreme version is to just check\non depth that you can't pay back and\nthree is negative on that is that that\nwill require a lot of confidence in both\nthat there is uh will be doomed and that\nwill consume and you also need to have\ngood way to spend the money like if you\num take on a lot of depth and then you\nblow it on something that is totally\nirrelevant then that sounds like a uh\nsomething you will strongly regret\ndid you buy a house\num so he says maybe like there are tax\nadvantages and things like that but\num psychologically it can be hard to\nsell for some people and I think in\nparticular a house makes it much more\nlikely that your location becomes fixed\num and that is something like it is very\npossible like\num I haven't moved to Berkeley I can't\nmove to Berkeley because I have a wife\nand children here and a house also uh\nand this kind of Route can in fact be a\nsubstantial obstacle\nshould you start a business\num three is in general uh bullish on\npeople starting businesses he just says\nas long as you don't make the problem\nworse by uh doing something with AI I\nthink I am more optimistic about not\nmaking the problem worse I think if you\nare starting a company that uses AI then\nthe amount of extra money and hype\nyou're putting into AI may be extremely\nsmall\nyou can make it even smaller by being in\nstealth mode uh or just not hyping the\nbusiness that's what I'm doing and I\nthink\nI am pretty confident that this is in\nfact not making the problem worse\nshould you have kids\nuh three is positive on this he believes\nthey are valuable as an end in\nthemselves it gives you something to\nprotect\num\nthere are a few well-placed researchers\nwho should not but you are probably not\none of them\num and\num I think this is too simple analysis\nof a very difficult difficult subject I\nthink uh an obvious clip answer is that\nyeah there are s risks in fact and this\nmay be a very good argument for not\nhaving children right now another is\nthat you'll are just really expensive in\ntime and money and there is a long tail\nit is very possible to have children who\nhave some kind of special needs and this\ncan seriously uh like uh take up\nliterally all your time and money and\nthat is something you uh you need to\nconsider as well\nI think in fact if you're doing some\nkind of utilitarian calculus here then\nthat may in fact come out quite strongly\nto not having children and maybe later\nif everything becomes normal then\nadopting or something like that uh if it\nbecomes too late for biological reasons\nI think the utilitarian calculus\ndisagrees with this and I think in fact\nthis is something that you should really\nreally seriously consider\nokay but if you already have children\nshould you tell them about ai's risk uh\nwell you shouldn't quite hide\ninformation I think this video is\nstrongly against hiding information from\nchildren but at the same time it\nshouldn't be emphasized\nI disagree with this uh in how I raise\nmy children in that I do in fact hide\ndark stuff from small children I think\nthat is in fact the the right way to do\nthat\num but\num I don't uh hide like\nthe the gray stuff I had the very dark\nstuff but not the gray stuff so they\nknow what I'm doing that it's like AI\nsafety but they don't care very much\nabout that because like children have\nmany other things in in their heads\nhow about normies should I explain\nwhat's coming to them\num so this is just being open and honest\nbut not shoving it in in people's faces\num and like I sometimes when I explain\nwhat I'm doing they\nask a little but very little people\ngenerally really really don't care\num I think that is fortunate in that I\ndon't think that it helps uh shoving it\ninto people's face so we are lucky to be\nin a situation where it's both immoral\nand\num not helpful to shove it in people's\nfaces\num but\num let's move to unlock\nalso a consideration is like if uh just\nbefore the end it becomes apparent that\nAI is very very dangerous uh then you\nshould be prepared that uh numbers\naround you may blame you like the making\ninterpretation of Ethics say that you\nhave a special uh responsibility since\nyou're working with the problem\nso how about forgetting things uh this\nand just having a good time\num\nthis probably won't work unfortunately\num it's if you try to forget about it\nand then just try to enjoy your life you\nwill be consumed by worry uh Jesus he\nwould rather go down fighting and um I\nbasically agree I wouldn't work I prefer\nto go down fighting also but I would add\nhere that there's a difference between\ngoing down fighting as part of you know\na heroic effort that is close to winning\nwhoops\nyeah sorry about that\num there is a difference between uh\ngoing down uh fighting heroically and\ngoing down uh fighting very pathetically\nuh like almost\num uh not contributing\nuh finally the question how long time do\nwe do we have what does three things uh\nand he thinks basically it's very\nuncertain and it's also uncertain if\nthere will be an existential catastrophe\nso at this point I should bring up my\nown probability distribution I think 25\non an existential risk in 2025 50 in\n2029 75 and 35 and 90 on 2015. but\nthat's conditioning on not slowing down\ndue to things like uh catastrophe and\ncoordination Etc and these estimates are\nweekly Health subject to significant\nchanges and did in fact change when gbt3\ngbt4 was announced a couple of days ago\nquestion should you invest in AI\ncompanies\nwell you shouldn't invest them in them\nif they are funding constrained because\nthen you're making the the problem worse\num uh whether that is actually true\nthere was some comments on this saying\nthat actually for for large companies\nthis matters not very very little but\nprecisely zero\num and I thought that's an interesting\nargument\num what you should do instead is to\ninvest in AI safety and to a large\nextent because it's really hard to find\ninvestment opportunities for AI that\ndoesn't push on capabilities I think the\nthing you should invite and invest in is\nyour own set of skills\num\nif you have to invest in an AI company I\nthink a distinction should be made\nbetween an AI user and an AI developer\nand I think AI users are often\nreasonably okay depending uh and AI\ndevelopers are always strongly bad\nshould you learn some new skills well\nin general you should be flexible and\nready to change and also learn to code\nand also learn to use AI\nI agree and I think this is one of my\npersonal weak spots I am good at coding\nI'm not good at using Ai and I also be\nbe better\nso depending on your job there is a\nprobability that will disappear like if\nyou're doing like artists or writing\nnovels or something like that you should\nbe really worried but in other cases\nit's harder to know but you should\nreally try to model it\num\nand for me like I'm a software developer\nand like\num I think the AI developer disappears\nat the singularity and the AI uses\nprobably slightly before in general\nprogrammers it's likely before that is\nkind of my expectation but note that uh\nlike a bad programmer right now is in\nfact uh like I think it's going to be\nreally really tough to be a bad\nprogrammer in a couple of years due to\nCopilot\nso how do you plan a long-term career\nwell people generally do that really\npoorly you can perhaps obtain capital\nand stay healthy and this kind of thing\nyou can try but it's going to be really\nhard one of the things you should\nconsider is that if the world doesn't\nend it maybe because Society collapses\nand that is something you might consider\nplanning for\num\nso in general I did what I did was\ncomputer science and saving up money as\na long time career plan I think that was\nvery smart so just focus on the thing\nthat that earned you the most money\num for my children we're like I don't\nhave a long-term career plan and for my\nchildren uh the youngest he will his\nlong-term career plan will start in 20\nyears and I basically don't expect that\nuh that he will be able to meaningfully\ncontribute\nand this kind of uncertainty is of\ncourse\num uh problematic and the obvious\nquestion is can you just wait with\ntaking any kind of action and so he says\nno you have to act under uncertainty and\nthat's a general thing you always have\nto act under uncertainty you're never\ngoing to get a situation where you can\nmake meaningful changes uh without\nacting on their own certain surge\nso some of the key considerations uh\nthat three percents is you need to get a\ngood model to really figure out what is\nthe probability that you are right that\nP Doom is really hard this is going to\nhave high and this is going to happen\nsoon\nyou need to figure out given that the\nworld ends what is the utility of your\nactions and also if you are wrong that\nthe world ends what will be the\nconsequences of your uh of your access\nin in that case\nyou need you probably should distinguish\nbetween what is the consequences for you\npersonally and what are the consequences\nfor the rest of society\nif the world ends\nso some of the actions you can take\nranked from worst to best the worst you\ncan do is to work on capability or work\non funding capabilities\nand I expect that for the people who\nread this they are probably generally\nnot in a position where they can\nmeaningfully contribute to Google so\nit's much more likely that they should\nworry about working on capabilities\nworking on applications and layers\num I think three is negative on this I\nthink it's in fact possible to do this\nuh quite ethically spreading hype and\njust gaining mundane utility for uh for\nlanguage models um that is probably okay\nI think in particular if you try to\nbuild hype for a gbt4 uh that is like a\ndrop in the bucket I don't think that\nwon't matter very much\num I would notice in fact that one of\nthe great ironies of AI safety is that\ntalking about AGI risk does in fact\ncause hype\num that is like one of the worst thing\nto realize that P there are apparently a\nlot of people who read Nick Boston's\nbook super intelligence and thought hey\nI'm gonna build a super intelligence as\nfast as possible and yeah that is also a\nrisk\njailbreaking and tingering with the\nmodels is something that today is very\npositive about I don't think that is\nobviously risk-free I think jailbreaking\nand tinkering with the models is likely\nto uh\nuh probably be capable to work in theory\nit's possible that you could do\nsomething directly bad but I don't think\nthat's very likely I think in general\nthe thing you should do here is AI\nSafety Research\nand so\num three ends with the uh observation\nthat without AGI we should expect 100\nmortality uh rate so you should remember\nthat you will die with Memento Mori\num and I think Memento Mori is in the\nsecond person like you will die and I\nthink that is in fact less very very\nunlikely at this point like you\nsingularly dying I think at this point\neither everybody is going to die or no\none practically no one is going to die\nthat is at least my take on the overall\nsituation\nthat is all for today thank you and see\nyou next time", "date_published": "2023-03-16T22:38:37Z", "authors": ["AI Safety Reading Group"], "summaries": [], "initial_source": "ai_safety_reading_group"}
{"id": "f7f410439a2c91af3f647cc1c1515f3f", "title": "Risks from Learned Optimization: Evan Hubinger at MLAB2", "url": "https://www.youtube.com/watch?v=OUifSs28G30", "source": "youtube", "source_type": "youtube", "text": "hello everybody I am Evan hubinger I am\na research fellow at Mary I used to work\nat open AI I've done a bunch of stuff\nwith Paul Christiano uh I also wrote\nthis paper that I'm talking about today\nuh\nlet me very quickly raise hands if you\nhave had any experience with the paper\nI'm going to be talking about let's see\nokay okay pretty good sweet okay uh\nwe could even potentially I could give a\nfollow-up talk that covers more advanced\nstuff but okay I think that was not\nquite enough hands so I think I'm going\nto be doing this talk and hopefully\nit'll help people at least work through\nthe material understand stuff better I\nthink you will probably understand it\nbetter from having seen the talk at the\nvery least maybe at some other point if\nI stick around I can give another talk\nokay so yes let's get started\nso let's see uh about me uh this talk\nwas made when I was at openai I'm\ncurrently Miri I did other stuff before\nthat\num I've given this talk in a bunch of\nplaces it's based on this paper\nyou know it I think some of you\nokay what are we talking about\nokay so I want to start uh with just\ngoing back and trying to understand what\nmachine learning does and how it works\nuh because it's very important to\nunderstand how machine learning works if\nwe're going to try to talk about it uh\nso what does machine learning do so so\nfundamentally you know any machine\nlearning training process it has some\nmodel space some you know large space of\nalgorithms and then does some really big\nsearch over that algorithm space to find\nsome algorithm which performs well\nempirically on some uh set of data on\nsome loss function over that data\nokay this is what essentially everything\nlooks like this is what RL looks like\nthis is what you know supervised\nlearning looks like this but you know\nfine-tuning looks like everything looks\nessentially like we have some really big\nparameterized space we do a big search\nover it we find an algorithm which does\nwell on some loss over some data\nokay so I think that when a lot of\npeople like to think about this process\nthere's an abstraction that is really\ncommon in thinking about it and an\nabstraction I think can be very useful\nand I call that abstraction that he does\nthe right thing abstraction\nso well when we trained this you know\nmodel when we produced this uh you know\nparticular parameterization this\nparticular algorithm over this\nparticular data\num well we selected that algorithm to do\na good job on this loss on that data and\nso we sort of want to conceptually think\nabout the algorithm as Trying to\nminimize the loss you know if you're\nlike how is this algorithm going to\ngeneralize you can sort of think well\nyou know what would it do uh what it\nwould that would be loss minimizing what\nwould be the sort of loss minimizing\nBehavior uh but of course this is not\nliterally true we did not literally\nproduce an algorithm that is in fact\nminimizing the loss on all off dispution\npoints what we have produced is we\nproduce an algorithm that empirically\nwas observed to minimize the loss on the\ntraining data but in fact when we move\nit into new situations uh you know it\ncould do anything\num but you know well we selected to\nminimize the loss and so we'd like to\nthink uh probably it's going to do\nsomething like the loss minimizing\nBehavior\nokay so uh this abstraction you know\nlike all abstractions that are not\ntrivial are leaky and we are going to be\ntalking about ways in which this\nabstraction is leaky\nokay so I'm going to be talking about a\nvery specific situation uh where I think\nthis abstraction can be leaky in a way\nthat I think is problematic so that\nsituation is a situation where you have\nuh a the algorithm itself is doing some\nsort of optimization so what do I mean\nby that so we're going to say that a\nsystem is an Optimizer if it is\ninternally searching through some space\nof possible uh plans strategies actions\nwhatever uh you know for those that\nscore highly on some criteria so you\nknow it's maybe maybe it's looking for\nactions that would get low loss maybe it\nis looking for actions that would get it\ngold coins maybe it is looking for\nactions that would do a good job of\npredicting the future whatever it's\nlooking for things that do a good job on\nsome criteria okay\nso just really quickly gradient descent\nis an Optimizer because it looks through\npossible algorithms to find those that\ndo a good job empirically on the loss uh\nyou know a Minimax algorithm is an\nOptimizer it looks for you know moves\nthat do a good job at playing the game\nhumans are optimizers we look for you\nknow plans strategies that accomplish\nour goals things that are not optimizers\na bottle cap is not an Optimizer this is\na classic example you could take a\nbottle of water and it's got the water\nin the bottle and it's like really good\nat keeping the water in the bottle you\ncan like turn the bottle over and the\nwater stays in but you know that's not\nbecause the bottle cap is doing some\nsort of optimization procedure it's\nbecause we did an optimization procedure\nto produce a bottle and that bottle is\nthen really good in the same way that a\ngradient descent process by default does\nan optimization procedure over the space\nof algorithms and produces a you know\nneural network that is not necessarily\nitself doing optimization you know it\nmay just be like the bottle cap it was a\nthing that was optimized to do something\nbut it's not necessarily doing any\noptimization itself certainly if I just\nrandomly initialize a neural network it\nis definitely only not going to be doing\nany optimization unless I get really\nreally unlucky or lucky\num\nokay all right so that is what an\nOptimizer is\nokay so what we want to talk about and I\nalluded to this previously is a\nsituation where the uh model itself uh\nthe algorithm that you found when you\ndid this big search is itself an\nOptimizer it is doing some optimization\ninternally inside of your neural network\nuh so I'm going to give names to things\nin in that particular situation where\nthe thing is an Optimizer in that\nsituation we're going to call the grainy\ndescent process that did a bunch of\noptimization on top to produce the model\nwe're going to call it a base Optimizer\nand we're going to call the thing that\nit optimized the algorithm that it found\nthat gradient descent found that was in\nfact doing some optimization we're going\nto call it a mace Optimizer what does\nthat mean so Mesa is sort of the\nopposite of meta you know oftentimes you\nmay be familiar with meta learning you\nknow we have an optimization process\nthen we put a little meta learning\noptimization process on top of it that\nlike searches over possible ways the\nlearning process could go and we're like\nwell that's sort of what happened here\nyou know you had this gradient descent\nprocess and you're sort of grading\ndescent process turned into a meta\nlearning process now instead of\nsearching over you know you know a whole\nSpace of algorithms it's sort of\nspecifically searching over learning\nalgorithms and so we're going to say\nwell essentially the relationship to the\nbase Optimizer to the model is very\nsimilar to the relationship between a\nmeta Optimizer and a sort of the thing\nthat meta Optimizer is optimizing so\nwe're going to say well it's one meta\nlevel below it's a Mesa optimizer\nokay\nall right so some things that people\noften misunderstand here so when I say\nmace Optimizer I don't mean like some\nsubsystem or some you know component of\nthe model I just mean like a model you\nknow it's a neural network and it's\ndoing some stuff and you know in\nparticular we're referring to a\nsituation where it is doing you know\nsome sort of search it's you know\nsearching over some space of possible\nplans strategies whatever for something\nwhich does a good job on some criteria\nso it's the whole trained model\num okay so I talked a little bit about\nthe relationship with meta learning uh\nbut you know essentially you can sort of\nthink about this as spontaneous meta\nlearning you know uh you you didn't sort\nof think you were going to be doing meta\nlearning that you were doing\noptimization over learning processes but\nin fact the algorithm that your grading\ndescent process found that was doing a\ngood job on the task was itself a\nlearning algorithm and so now you're\nyou're in the business of doing meta\nlearning because that's what algorithm\nyou found\nokay and so the difficulty you know is\nis in this situation controlling what\nwhat we're going to learn\nokay\nso uh right so we have this does the\nright thing abstraction that you know we\nsaid well it's leaky sometimes but at\nleast helps us reason about models in\nother times uh so we can ask but what is\nthe does the right thing abstractions\nsay in this situation where you have a\nmodel that is itself doing optimization\nin the situation where you have a mace\nOptimizer well so in that situation it\nshould say you know if the model is\ndoing the right thing which is you know\nit's actually out of distribution going\nto try to minimize the loss then it\nshould be whatever the thing that this\nsort of Mace Optimizer is searching for\nwhatever you know optimization it's\ndoing that optimization should be\ndirected towards the goal of minimizing\nthe loss\nokay so this is one abstraction that you\ncould use to try to think about this\nprocess but of course as we stated this\nis not the actual process that we are\ndoing uh you know so so we don't know\nwhether this abstraction actually holds\nand so what we want to know is what\nhappens when this abstraction is leaky\nokay so when it's leaky we're sort of\ngoing to have two alignment problems\nwe're going to have an outer alignment\nproblem which is like uh uh make it so\nthat the you know base objective\nwhatever the loss function is you know\nthat's the sort of thing that we would\nwant and then an inner alignment problem\nwhich is make it so that the main\nsubjective the thing that the thing is\nactually pursuing uh is in fact the\nthing that we wanted it to pursue\none uh caveat that I will say here so I\nthink that this sort of this sort of\ntraditional setup really mostly applies\nin situations where the sort of goal\nthat you have for how you're going to\nalign the system is we're going to first\nspecify some loss function and then\nwe're going to make the does the right\nthing abstraction hold we're going to\nspecify some loss function that we'd\nlike and then we're going to get the\nmodel to optimize it I will say that's\nnot the only way that you could produce\nan align system right so an alternative\nprocedure would be specify some loss\nfunction that you know is bad if the\nthing actually optimizes for that loss\nfunction you know that it would end up\ndoing uh you know producing bad outcomes\nand then also get it to not optimize for\nthat get it to do some totally different\nthing that nevertheless results in good\nbehavior so and in fact I think that's\nactually a pretty viable strategy and\none that we may want to pursue but for\nthe purposes of this talk I'm going to\nbe assuming that's not the strategy that\nwe're going with that the strategy we're\ngoing with is we're going to write down\nsomething uh you know that specifies you\nknow some loss or reward uh that like\nactually reflects what we care about and\nthen we're going to try to get them all\nall to sort of actually care about that\nthing too but I just want to preface\nwith that's only one strategy and not\neven necessarily the best one but it is\none that I think is at least easy to\nconceptualize and that we can sort of uh\nyou know understand what's going on if\nif that's the strategy we're pursuing\nquestion\n[Music]\na thing that we don't want to do with\nthe training to not do that yeah so\nhere's a really simple example here's a\nreally simple example this is not\nsomething I think we should literally do\nbut here's a simple example that might\nlike illustrate the point so let's say\nwhat we do is we have an environment and\nyou know we're going to do RL in that\nenvironment and we're going to set up\nthe environment to have incentives and\nstructures that are similar to the human\nancestral environment and then we're\nlike well you know if it's kind of\nsimilar to the human ancestral\nenvironment maybe it'll produce agents\nthat operate similarly to humans and so\nthey'll be aligned right that's a hope\nthat you could have and if that's your\nstrategy then the thing that you're\ndoing is you are saying I'm going to\nspecify some incentives that I know are\nnot aligned right like reproduction you\nknow natural selection that's not what I\nwant that's not what I care about but I\nthink that those incentives will\nnevertheless produce agents which\noperate in a way that is the the thing\nthat I want so so you could have that as\nyour strategy we're not talking about\nthat sort of a strategy though right now\nin this talk but but you but I don't\nwant to like rule it out I don't want to\nsay that like you could never do that in\nfact I think a lot of my favorite\nstrategies for alignment do look like\nthat though they don't look like the one\nI just described but\nokay but they do have the general\nstructure of like we don't necessarily\nwrite down a loss function that\nliterally captures what we want but uh\nsupposing we do we're going to try to\nexplore you know what this what this\nlooks like and in fact you know as we'll\nsee basically all of this is going to\napply to that situation too because what\nwe're the problem that we're going to\nend up talking about is just the central\nproblem of you know what happens when\nyou have some loss and then you want to\nunderstand what thing will your uh model\non you know if it is itself an Optimizer\nend up optimizing for\nokay is that clear does that make sense\nokay\ngreat\nokay so you've probably heard stories of\nouter alignment failures uh you know\nwhat they might look like you know\nclassical examples are things like\npaperclip maximizers you've got you know\nyou're you're like I want to make the\nmost paper clips in my paper clip\nFactory and then uh you know it it just\ndestroys the world because that's the\nbest way to make paper clips\num that is a classic example of an outer\nalignment failure what I would like to\nprovide is sort of more classic examples\nof inner alignment failures so we can\nsort of start to get a sense of what it\nmight look like if uh inter alignment\nyou know in the sense that we talked\nabout you know previously where you know\nthe the model ends up optimizing for a\nthing that is different than the sort of\nthing that you wanted to uh that you\nspecified might look like so so what\ndoes that look like well so when inner\nalignment fails it looks like a\nsituation that we're going to call\ncapability generalization without\nobjective generalization so what does\nthat mean so let's say I have a training\nenvironment and it looks like this brown\nmaze on the left and I've got this uh\nyou know some arrows that like Mark\nwhat's going on uh and you know the loss\nfunction you know the reward function\nthat I use I'm gonna we're gonna do like\nRL this environment I'm going to say you\nknow reward the agent for finishing the\nmaze I want it to get to the end\nokay and then I'm going to deploy to\nthis other environment where now I've\ngot a bigger maze it's blue instead of\nbrown and I have this green arrow at\nthis like random position\nand now I want to know what happens\nokay so you know here are some things\nthat could happen in this new\nenvironment when we ask you know what is\nthe generalization behavior of this\nagent so here's one thing that could\nhappen it could look at this you know\nmassive blue Maze and just like you know\nnot have any idea how to do anything and\nso just like you know randomly you know\ngoes off and does nothing you know\nuseful that would be a situation where\nyou know we're going to say it's\ncapabilities did not generalize you know\nit did not learn a general purpose\nenough algorithm for solving mazes that\nthe algorithm was able to generalize\nthis new environment\nokay but but it might so let's say\ncapabilities did generalize it actually\ndid learn some sort of general purpose\nmedia solving algorithm and knows how to\noperate in this environment well there's\nsort of two things that it could you\nknow potentially learn at least two uh\nit could learn to use those maze solving\ncapabilities to get to the end of the\nmaze which is the thing that we were\nrewarding it for previously or it could\nuse the maze solving capabilities to get\nto the Green Arrow\nuh and that you know if the strategy was\npursuing if the generalization it\nlearned was always go to the Green Arrow\nversus go always go to the end of the\nmaze you know in training those were\nindistinguishable uh but you know now in\nthis deployment environment they're you\nknow quite distinct\nokay and so uh let's say you know what\nwe really wanted right you know we\nreally wanted it to go to the end but\nyou know this green arrow just got moved\nand so we're like okay so what happens\nhere you know why is this bad that we\ncould end up in a situation where the\nmodel generalizes in such a way that it\ngoes to the Green Arrow instead well\nwhat's happened is we are now in a\nsituation where your model has learned a\nvery powerful general purpose capability\nwhich is the capability to solve mazes\nand that capability is misdirected uh\nyou have deployed a model that is in a\nsituation where it is it is very\npowerful and is actively using those\ncapabilities not to do the thing that\nyou you know wanted it to do uh so\nthat's that's bad that's dangerous right\nyou know we do not want to be in\nsituations where we are deploying\npowerful capable models in situations\nwhere they are using their capabilities\nuh where you know they don't actually\ngeneralize and that you know they don't\nactually match up with what we wanted\nthem to use those capabilities for\nokay so so that's concerning and that's\nthe thing that we want to avoid that's\nsort of what it looks like when we have\nthis sort of inner alignment failure\nokay so some more words we're going to\nsay that the situation where the model\nuh generalizes correctly on the sort of\nuh uh you know reward you know loss uh\nobjective that we want uh you know that\nactually is trying to finish the maze\neven out of distribution we're going to\nsay it's robustly aligned it's a\nsituation where you know it's alignment\nyou know it's it's you know actually\ngoing to pursue the correct thing is\nrobust to you know uh out of\ndistributional shifts whatever and we're\ngoing to say that if the model sort of\nlooks aligned over the training data but\nactually you know we can find situations\nwhere the model would not be aligned\nthen we're going to say it's\npseudo-aligned\nokay\num great all right yes robust\ntest data the test data we're going to\nsay well uh any this any data that it\nmight in fact encounter in the real\nworld if if the thing has you know is is\nactually robust in all situations it you\nknow it always pursues the right thing\nthen we're going to say it's robustly\nline importantly we're only talking\nabout the alignment here what we don't\ncare about is if the model like in some\nsituation its capabilities don't\ngeneralize right if it ends up in a\nsituation where like it's just like I\ndon't know what's going on and then it\ndoesn't do anything we're fine with that\nright that's still robust alignment\npotentially uh right we're sort of\nfactoring the capabilities in the\nalignment here we're saying that if it\nis capable of pursuing some complex\nthing it is always pursuing the thing\nthat we wanted it to pursue and here\nwe're saying it only looks like it's\npursuing the right thing on training but\nyou know out of uh distribution it might\nend up pursuing the wrong thing capably\njust to clarify robust alignment is\ndefined with respect to a particular\ndata distribution you can't speak about\nit in isolation\nuh no uh pseudo alignment is is defined\nspecifically relative to a data\ndistribution because it's defined\nrelative to the training bad\ndistribution but robust alignment is not\nrobust alignment is defined relative to\nany data it might ever encounter okay\nokay great\nall right so we have two problems yes\ndon't uh generalize are you doomed to\nhave a pseudo alignment because in cases\nwhere it doesn't know what to do it will\ndo things that are unintended well uh\nyou know I'm okay if it does things that\nare unintended we're worried if the like\noptimization is directed in an\nunintended place right that it does\npowerful optimization in the wrong you\nknow towards the wrong goal so like you\nknow if it's just like you know in the\nnew maze it just like Falls over and\ndoes like random you know stuff we don't\ncare right we're like okay whatever the\nproblem is a situation where it does\nknow how to solve complex bases you know\nit actually does have the capability uh\nyou know and then it uses like actively\noptimizes for something we don't want it\nto be optimizing for\nokay so like even if we have the May\nsolva and I was right I'll see a lion\nwe would we would call it robustly\naligned even if we like gave it a\nmassive Maze and then it tried to like\ntake over a piece of computational power\nto solve this maze because it was still\nactually applying right so in this\nsituation like you know just going back\nlike to here we're assuming that the\nloss function is actually reflects what\nwe care about right and we're like What\nwould it look like if the loss function\ndoes reflect what we care about to have\nan inner alignment failure so in that\nsituation you would be like well you\nknow your your strategy of like write\ndown a loss function that reflects\neverything I care about and then get a\nbottle's optimized for it you failed at\nthe point where the loss function you\nwrote down that you thought reflected\neverything you care about just said\noptimize for mazes but that was bad you\nshouldn't have done that that was\nactually not everything you cared about\nand so we're like that was where you\nwent wrong here\num I think that like I said though you\nknow you might not have this strategy\nbut like you know right now at least\nwe're assuming that it's your strategy\nand we're gonna say you know actually\nwhen you wrote down this last time you\nwere at least trying to get it to\ncapture you know everything that you\nwanted\nokay okay yes do you draw any\ndistinction between a model like\nspecifically optimizing for like some\ninternal golf versus like just\nstatistically preferring something\nyes yes we we absolutely are drawing a\ndistinction there we are saying that the\nthing is an Optimizer if it is like in\nfact has some search process inside of\nit that is you know you know searching\nfor things that do well according to\nsome objective uh you know this is the\ndifference right between the bottle cap\nyou know which like you know in fact\nsystematically prefers the water to be\nin a bottle but like isn't in fact doing\noptimization for it and the reason that\nwe're making this distinction is because\nwe're going to say well we're actually a\nlot more concerned about situations\nwhere the model is actively doing\noptimization that is misdirected then\nsituations where it has you know a bunch\nof you know heuristics that were\nselected in a problematic way and the\nreason we're more concerned about that\nis because misdirected optimization has\nthe ability to be a lot worse out of\ndistribution you know because it can you\nknow competently optimize for things\nthat we don't want to optimize for even\nin situations where you know we didn't\nselect it to be able to do that\nsorry\nif a model actually be an Optimizer that\nis a great question and that is what\nwe're going to be talking about next\nokay so yeah we're gonna be talking\nabout two problems here you know one is\nunintended optimization which is like\nyou know maybe you didn't want to be an\nOptimizer maybe you just wanted it to\nyou know be a bottle cap like you wanted\nto be a selection of heuristics you\ndidn't want to be doing any optimization\nwhy might you get optimization anyway\nand then the second question you know is\nthis inner alignment question which is\nokay if it is an Optimizer how do you\nget it to optimize for what you want\nokay let's talk about it okay what sort\nof machine learning systems are more or\nless likely to find optimizers that's\nthe question we're starting with\nokay so here's the first thing is look\nsearch algorithms are really good at\ngeneralizing so let's say we're in a\nsituation where you know we want our you\nknow algorithm to play go you know we\nwant to do a really good job at go\nplaying well there is a systematic uh\nsimilarity uh between all good go moves\nright the systematic similarity between\nall good go moves is that they are moves\nwhich have the property that if I do\nsome look ahead and you know try to see\nhow good is this move\nthey end up being really good right and\nso if I want to encode an algorithm\nwhich systematically produces good go\nmoves uh encoding search is a really\nsimple way to do that and it's a way\nthat generalizes in a lot of situations\nit's a way that you know if I go into\nsome new situation where you know I\ndon't necessarily have a bunch of\nheuristics that are well optimized for\nthat situation I nevertheless have the\nability to you know figure out what's\ngoing to do well you know the uh you\nknow the the you know the go engine\nwhich has a bunch of ability to do\nsearch you know can can find it you know\nabsolutely crazy billboard that it's\nnever seen before and still do you know\n10 steps of look ahead and come out with\nyou know some ideas of what things are\ngoing to be good in that situation\num and you know obviously in fact you\nknow we explicitly program search in and\nwe make you know uh go uh Bots you know\nalphago actually has this MCTS process\nuh okay so so you know there are\nsituations where you know uh if you have\nthis really you know a really diverse\nenvironment you have a lot of situations\nyou need to be able to handle being able\nto in each situation be able to search\nfor what the correct thing to do in that\nsituation is gives the ability to\ngeneralize better\nokay uh and then similarly also it's a\nform of compression so you know I said\nthat like well what is the regularity\nright between all good go moves well the\nregularity between them is when I do\nlook ahead they end up looking good\num and so if you were sort of asking\nwell what is the easiest way to compress\nthe set of all you know good go moves\nwell the way to compress the set of all\ngood go moves is an Optimizer that\nsearches and does look ahead to find\ngood go moves right\num and so if we expect that your model\nis going to be biased towards\num uh Simplicity which I think we should\nuh if you expect that then you should\nexpect that it's going to find these\nsorts of compressions ways of taking\nyour data and compressing it into a\nsimple form that you know uh you can\nstill be able to recapitulate it and in\nthe situation where the thing that\nyou're doing is you know some situation\nwhere you're trying to train it to you\nknow uh solve some complex task uh a\ngood way to compress a lot of complex\ntasks is to be able to do search why is\nthere Simplicity bias there's a bunch of\nstuff on this uh I think it's a little\nbit tricky\num\nmaybe a really sort of simple version of\nthis I don't this is not something I\nhave up here but maybe probably the best\nresult that I think argues for\nSimplicity is probably the mingard\nresult which shows that if you do\num sampling with uh replacement from the\nGalaxy initialization prior uh so this\nis a little bit of a weird thing to do\nbecause it's extremely you know\nuncompetitive it would take you know\nyears and years if you wanted to\nliterally just take you know initialize\na neural network over and over again\nuntil you've got a neural network which\nactually had good performance but you\ncould suppose you did this thing if\nyou're just like you know you keep\ninitializing it every single time you\nknow you never train it and you hope\nthat eventually the initialization\nactually just does well on your data if\nyou did this in fact the prior ends up\nbeing extremely similar to the prior you\nget from granny descent\num which sort of is showing that\nactually you know what greatness that is\ndoing it is sort of doing something like\nfinding the sort of minimal Norm in\ngaussian Norm space uh for you know\nsolution that is able to solve the\nproblem that sort of maps onto\nSimplicity in various ways there's a lot\nmore to say here\nyes you said prior twice the prior when\ndrawing this is the same as the\nposterior you get after after gradient\ndescent okay I mean it is also posterior\nin this case because it is a posterior\nis like the prior is gaussian\ninitialization and the posterior is take\nthe prior update on in fact when I\ninitialize and sample from the\ninitialization I get a model which has\ngood performance on my data and then I'm\nlike that posterior is extremely similar\nto the posterior of take a model\nactually train it and then see what I\nget okay okay there's a lot more to say\nhere but I'm not going to say too much\nmore you can ask me more later if you\nwant to know more about inductive biases\nokay uh but but whatever the point is\nthere's a lot of evidence I think that\nis biased on Simplicity okay but this is\nnot the not the only situation where we\nmight want where we sort of might expect\nauthorization another situation is and\nthis is pretty common there's a lot of\nsituations where we like to train you\nknow machine learning models to mimic\nhumans uh okay what is a property of\nhumans that we you know established\npreviously well humans are optimizers\nyou know there's lots of situations\nwhere if you're going to do a good job\nat mimicking humans you better be able\nto do a good job at uh you know\npredicting what we're going to do when\nyou know we have a goal and we're trying\nto optimize that goal you know being\nable to understand you know if a human\nwas trying to you know do X how would\nthey accomplish x uh you know if you\nknow being able to sort of work\nbackwards and be like okay you know how\nwhat would what would be necessary for\nthe human to be able to accomplish their\ngoal and you know how would they achieve\nthat is is you know something that's\npretty important if you want to be able\nto predict human behavior\nand so if we're predicting humans we\nshould also expect that our you know\nmodels you know are at least going to\ndevelop the capability to do\noptimization\num okay\nand okay so so some some things here\nthat are going to push you in various\ndifferent directions do you have a\nquestion\nokay uh so some things that are going to\npush you towards mace authorization here\nwell so uh you know large model capacity\nyou know just having the ability to\nimplement complex optimization\nalgorithms you know Simplicity bias\neither explicitly because you you have\nsome regularization implicitly just\nbecause you know gradient descent is\nregularized in this way uh as I was\ntalking about previously\num you know other things I didn't talk\nabout this but statefulness having the\nability to store state is the thing that\nreally improves your ability to do\noptimization over time uh because you\nknow you can sort of store the results\nof optimization and then you know\ncontinue iterating uh so like iterative\nstateful procedures are a lot easier to\nyou know produce optimization in things\nthat might disintensify this\noptimization if you just give the thing\noptimization so like I think if you have\nlike MCCS that's like part of your\narchitecture you're like a little bit\nless likely to just like learn\noptimization in the non-mcds parts\nbecause it's like already doing some\noptimization so it has less need to sort\nof implement it\num but it's not 100 True uh you know\nespecially because in fact when we do\nlike MCTS we usually distill the MCCS\ninto the policy Network and so you're\nlike actively training it to mimic a\nsearch process\num but you know at least the actual like\nyou know thing of giving it access to\nauthorization seems like it should you\nknow reduce the probability that\ndevelops it itself though it still could\nmaybe just because the optimization that\nit needs is different than the\noptimization you gave it\num time complexity bias I didn't mention\nthis but you know you could be in a\nsituation where where if you really\nincentivize the thing to perform its\ncomputation very quickly it's a lot\nharder to do optimization optimization\nwhile it's a very simple algorithm is\none that can take you know many steps of\nlook ahead to actually you know produce\nsome results and so you know uh if we\nbias the thing on any amount of time it\ntakes and it's speed that sort of\ndisincentivized optimization\nand also if we just give it really\nsimple tasks so you know I think you\nknow example of this might be we just\ngive it a task that is like literally\njust you know pre-training on webtext\nyou know we just give it a task that is\njust predict the next token you know\nreally simple tasks that uh you know\naren't very complex that don't have like\na bunch of moving pieces in terms of\nlike you know what the we're trying to\ntrain the thing to do then it you know\nuh you know potentially needs less uh uh\noptimization and probably still like at\nleast if you're trying to predict humans\nlike I said it probably still needs to\nhave the capability to do optimization\nbut it doesn't necessarily need to\ndirect that optimization towards some\nobjective that it's optimizing for\num okay so these are sort of some things\nI think might push you in one direction\nor the other\num I think it's a little bit tricky\nuh there's like you know a lot of things\npushing you in in you know different\ndirections here\num\nI think my guess is that uh you know at\nsome point for just four capabilities\nreasons people are going to want to\nbuild agents and their groups want to\nbuild agents which have the ability to\ncompetently take actions and do things\nin the world and I think when they do\nthat uh these sorts of you know\nprocedures that we are building for\ndoing these sorts of things are moving\nin the direction of the things that are\nincentivizing optimization\nso why is that well you know we're\nmoving towards doing more imitation of\nhumans uh we're moving towards you know\nbigger models moving towards uh more\nSimplicity bias this is actually a\nlittle bit tricky I didn't mention this\npreviously but larger models have a\nstronger Simplicity bias than smaller\nmodels\num that might be a little bit\ncounter-intuitive but it is true uh one\nway to see this is the double descent\ncurve maybe I'll just go back really\nquickly\num where if I have uh the like size of\nmy model\nand they like train the model over time\nyou're probably familiar with like the\nstandard sort of uh you know thing that\nyou get out of learning theory which is\nlike your train loss goes down and then\nyour test loss sort of you know first\nyou underfit and then you go up and then\nyou overfit but in fact if you like\nactually increase the size of the model\nfar past the point at which uh you know\nyou normally would in sort of standard\nlearning theory your test loss actually\ngoes down again and it goes lower than\nyou can get any point in the classical\nregime\num what's happening here is that um the\nsort of only way that this works is if\nthe like implicit biases that you get by\nmaking getting a larger model size are\nactually selecting amongst the larger\nspace of models for you know models that\nactually generalize better and the\nmodels to generalize better are you know\nthe simpler models and so as we increase\nthe size of the model we're actually\nincreasing the amount of Simplicity bias\nthat we're applying to get you know an\neven simpler and simpler and simpler\nsolution which is able to fit the\nproblem\num\nokay uh yes is that the same thing as\nsaying the larger models have the\nfreedom in their weights to explore\nthese simpler models and then end up\nfinding them whereas in a smaller model\nthey're too compressed to be able to to\nexplore that yes so it's a little bit\ncounter-intuitive but in fact the larger\nmodels have the ability to implement\nsimpler models right so if you think\nabout it like this\num you know there are algorithms which\nare very simple right you know I can\nwrite you know search and you know two\nlines of python or whatever right but\nyou know in fact you know influencing in\na small Network might be really might be\nreally difficult\num and so as the network gets larger\nthere's still a bias towards like simple\nalgorithms but it gets the ability to\nimplement more of the possible simple\nalgorithms that are available\nuh and but then it still it selects a\nsimple one\nokay there's a lot more to be said here\nyes so I take it there's a is there a\nversion of this plot where like instead\nof just complexity of age it somehow\nincorporates the whatever that bias is\nyou talked about and then this is this\nis this is traditionally model size so\njust like number of parameters\nbut you can actually do this with a\nbunch of you could you see double\ndescent graphs with with all sorts of\nthings on the x-axis but but\ntraditionally at least it's with model\nsize but I guess so I'm trying to wrap\nmy head around what's like unintuitive\nabout the right side and it's like or\nlike where why it shouldn't actually be\nunintuitive I guess and it seems like\nthe x-axis is just the complexity of our\nmodel class but it's so it's not\naccounting for how our training\nprocedure interacts with that model\nclass is that right no no this is after\ntraining\nso this is literally just sorry just\ncover this up and replace with number of\nparameters\nand then this is after training we're\nkeeping like uh training uh constant we\ncan we we train to convergence at every\npoint on this graph\nright but there there's some the point\nis\nthe x-axis on I'm maybe not making a\nvery interesting point but I'm just\ntrying to understand better so the the\nx-axis here only measures the complexity\nof like our function\nclass it does not\naccount for how that interacts with our\ntraining procedure it doesn't yeah it\ndoesn't account for the biases right so\nwe have some it it's this is just the\nsize of the model class right but then\non top of that model class we actually\nhave some some prior that selects out\nthe model that we actually like out of\nthat model class right and we're saying\nthat prior the sort of Prior that of\nlike gradient of machine learning that\nlike selects out you know uh the a like\nsimple you know it actually selects out\nthe simple simple algorithms out of that\nlarge model class so as we increase the\nmodel class we get simpler and simpler\nalgorithms out of it and so if we made a\nnew plot where the x-axis encountered\nencapsulated both the complexity of the\nclass and how our training procedure\ninteracts with it\nwould not get\num yes so in fact one thing that I will\nsay here is that sort of what's going on\nis that there's a disconnect between the\norder in which new models get added to\nthe model class and the criteria that\ngrading descent uses to select from\namongst those models right it is not the\ncase that you know what this tells us is\nthis is not the case that new models get\nadded to the model class of like what\nalgorithms are available to be learned\nin the same order in which grading\ndescent would prefer them like it's not\nit doesn't just start with the most\npreferred algorithm then the second most\npreferred algorithm it's just like\nrandomly chucking them in there and then\ngradescent is like doing some you know\nvery you know special thing to actually\npick out the best one from amongst them\nokay if it were the case that the sort\nof order in which algorithms were added\nto the support of your prior were\nactually like in Simplicity order then I\nthink you would not see this\ncontinue on this line go for it\noh uh are you basically arguing in favor\nthat the lottery ticket hypothesis here\nthat no or no not here no in fact I\nthink lottery tickets hypothesis people\nget very confused about I think it\ndoesn't say people think it says I think\nit's not super relevant here we could we\ncould we could talk about it later maybe\nyes\num\nnoted that uh in a neural network\nencoding search requires like a like a\nlarge enough model you can't do it in a\ntwo-layer Network it's impossible\num\nseems pretty hard to do in a two-layer\nNetwork yeah so even though we regard\nsearch to be simple shouldn't we sort of\nbe conditioning our Simplicity based on\nthe architectural model well the sort of\nSimplicity that I care about is the\nSimplicity of like if I have a really\nreally large model and the Green Design\ncan select a huge amount of possible\nalgorithms from amongst that set what\nsort of ones does it select for right\nI'm not as much interested in the sort\nof Simplicity that is like if I have a\ntiny model what are the algorithms that\nI can Implement right and so when I say\nSimplicity the the place where I think\nSimplicity bias occurs in machine\nlearning is not the like oh if I have a\nreally small model I just can't\nImplement some things the place where I\nthink the Simplicity bias occurs is if I\nhave a really large model and I can\nImplement a ton of things which ones\ndoes green Nissan actually end up\nselecting that's what I think is\nSimplicity bias not the other one\ncouldn't like sorry and in fact I think\nthat the like architectural bias you get\nfrom having a small model is more like\nspeed bias it actually probably\ndisincentivizes optimization like we're\nsaying because it just doesn't have\nenough serial uh computation time to do\nup just do it\nwouldn't it be though that uh given that\nit's just quite impossible to do it in a\nsmall model\num there's less of a Simplicity biased\nand more just\nonce you make once you reach the like\nrequired size then that's an available\nmethod to implement and then at that\npoint it just berries a tiny bit based\non Simplicity and generalization\nI think that the claim I'm making is\nthat actually you know some models are\nreally vastly simpler but they only\nbecome accessible to be implemented by\ngrain descent once you have a larger\nmodel we we can talk about this this\nlater maybe\nokay let's keep going okay uh great okay\nso that's that's you know part one which\nis like okay here are some things that\nwould push you in directions for or\nagainst learning optimization I think my\ntake is where you know it's going to\nhappen you know we're going to at least\nbe able to have models which are capable\nof doing optimization uh you know it\nseems like all of the ways in which we\nwant to build powerful models and you\nknow have models that you can generalize\nin you know new situations they're gonna\nthey're gonna be able to do uh\noptimization okay so if that's gonna\nhappen\nuh will it be doing the right thing\nokay so first\nwhat does it look like for a model to\nhave a an objective uh you know for a\nmace authorizer to have a mace objective\nthat is uh misaligned with uh you know\nnot the same as the loss function right\ndoesn't obey this does the right thing\nabstract it okay so fundamentally you\nknow I think the most basic case the one\nthat we're primarily going to be talking\nabout is a proxy pseudo alignment which\nis a situation where uh you know there\nis some uh correlation uh there's some\ncorrelational structure between a proxy\nthat the model is trying to optimize for\nand the you know actual objective that\nwe wanted to optimize for\nuh so you know in a sort of causal graph\nlanguage you can think about them as you\nknow having some common ancestor you\nknow if I am trying to optimize for this\nand there's some common ancestor between\nyou know the base objective and me well\nthen I need to make this x large because\nthat is how I make my thing large and\nthat will bite uh you know as a side\neffect make this thing large as well and\nso if we're in a situation like this\nthen we're going to be in a situation\nwhere uh you know it could be the case\nthat anything which has this\ncorrelational structure uh you know to\nthe thing we care about could be the\nthing that your model learns so you know\nfor example we had that situation where\nyou know it learned to go to the Green\nArrow rather than the end of the maze\nbecause in training those two things\nwere highly correlated\nokay\num there are other things as well I'll\njust like mention really briefly you\ncould also have something like\nsub-optimality suit alignment where you\nknow the reason the thing looks aligned\nis because it's actually doing\noptimization in some really bad way and\nso you know in fact it's objective it's\njust like some crazy thing uh you know\nan example I like of this is like you're\ntrying to make a cleaning robot and the\ncleaning robot you know believes uh it\nhas an objective of like minimizing the\namount of atoms in existence and it\nmistakenly believes that when it like\nvacuums up the dust the dust is just\nlike annihilated and so it's like oh\nthis is this is great but then you know\nlater it discovers that in fact you know\nthe dust does not get annihilated it\njust like goes into the vacuum and you\nknow it stops being aligned so that\nwould be a situation where like it\nactually had some you know insane\nobjective but that objective like in\nfact exhibited a line behavior and\ntraining because it also had some insane\nbeliefs so you can also get some weird\ncancellations and stuff like this but\nwe're mostly not going to be talking\nabout this mostly focusing on the proxy\ncase was just like well you know it kind\nof understands what's going on and it\njust cares about something in the\nenvironment which is correlated it but\nnot the same as the thing we want\nokay so that's sort of what pseudo\nalignment looks like why would you get\nit\nall right so the most fundamental thing\nis just unidentifiability look you know\nany complex environment it just has a\nshitload of proxies there's just like a\nton of things in that environment which\none could pay attention to and which\nwould be correlated with the thing you\ncare about and so as you sort of you\nknow you train agents in very complex\nenvironments you're just gonna get a\nbunch of sort of possible things that it\ncould learn\nuh and so by default you know if it just\nlearns anything which is correlated you\nknow uh it'd be really unlikely and\nweird if it learned exactly the thing\nthat you wanted\nuh because you know all of these things\nyou know to the extent that they're\ncorrelated with the thing you're trying\nto train it on we'll still have good\ntraining performance\num okay uh so you know just operate you\nknow probably some of these proxies are\nlikely to be simpler and easier to find\nthan the you know one that you actually\nwant\nunless you have some really good reason\nto believe that the one you actually\nwant is like you know very simple uh and\neasy to find\nokay in addition uh you know it I think\nthe situation is sort of worse than that\nbecause actually some proxies can be a\nlot faster than others so what do I mean\nby that so so we have like an example\nhere so we're like look you know a\nclassic you know example that people\nlike to talk about you know in terms of\nthis sort of thing is like Evolution you\nknow Evolution you know also was sort of\nlike an optimization process selecting\nover humans and selecting the humans uh\nto do a good job on you know uh you know\nnatural section right you know pass on\nyour DNA into the Next Generation okay\nuh so let's suppose in an alternate\nreality that you know uh natural\nselection you know Evolution actually\nproduced humans that really just care\nabout like you know how much their\nalleles you know are represented in you\nknow future Generations that's like you\nknow the only thing that the humans care\nabout okay but here's the problem with\nthis is that this is just like a\nterrible optimization task right you\nhave a baby right and the baby like\nstubs their toe and they're like you\nknow what do I do about this right\nlike how is this going to influence my\nlike chances of like in the future of\nbeing able to like mate and then like\npass my DNA down and like hmm like what\nis this but like no right but like\nthat's terrible right like instead a\nmuch simpler and faster to compute\neasier strategy for the baby to\nimplement is just like you know have a\npain mechanism that is like okay we've\ndeveloped this you know heuristic that\nlike you know all pain is always going\nto be bad and so just like you know\nshunt that to you know the negative\nreward system and so we're like okay so\nyou know we actually if we're in a\nsituation where we're training on\nsomething which is relatively complex\nand difficult to specify in terms of the\nagency you know input something like DNA\nyou know we're trying to train on pass\nyour DNA onto the Next Generation uh you\nknow it's not going to learn to care\nabout passing the DNA on the Next\nGeneration it's going to learn to care\nabout you know reducing pain uh because\nyou know reducing pain is something that\nis much more easily accessible to the\nthe baby and it's something which the\nbaby can actually Implement effectively\nand doesn't rely on doing some really\ncomplex difficult optimization process\nto be able to determine how to optimize\nfor that thing\nokay so you know we shouldn't expect to\nget you know some really complex\ndifficult to optimize for a thing we\nshould we should expect to get you know\nthese sorts of faster easier to optimize\nfor proxies\nokay and also some proxies are simpler\nright so I mentioned this right you know\npain also has this property that you\nknow it's extremely accessible in terms\nof the input data of the baby you know\nthe baby can just detect oh man you know\nsomething has happened to my leg that's\nbad uh and it doesn't have to do some\ncomplex thing that's like okay I can\ninfer that probably you know there is\nDNA inside of my cells but I'm not like\nyou know 100 sure I should probably go\nget like a microscope and then I can\nlike figure it out you know it doesn't\nhave to do that it just like you know is\nextremely accessible you know the thing\nthat it cares about in terms of its\ninput data and the data that's available\nto it and so we should expect that again\nyou know the sort of properties you're\ngoing to be learning are the sorts of\nproxies that are accessible and simple\nin terms of the data that the model has\naccess to Yes um\nyeah because\nwhen you're consider in the case of like\na an artificial neural network\nthe amount of efforts required is just\none forward pass through a simple proxy\nor a complicated thought process that\nleads to an output so\num yeah so the way I think you should\nthink about this is that well you know\ntechnically the amount of computation it\nperforms in any forward pass is fixed\nbut it has to ration that computation\nright it's not the case that it gets\ninfinite computation in fact it gets\nlike you said a finite amount of\ncomputation and so it needs to ration\nthat computation to do the things that\nare most useful for it right and so if\nit's wasting a bunch of the computation\ndoing all of this reasoning about how to\nyou know get the most DNA you know it's\nnot going to have room to you know for\nexample just do the look ahead five\nsteps more right it's not going to have\nthe you know capacity left to do other\nthings which will help it with\nperformance more and so you know because\nit is computation limited we should\nexpect that that computation will be\nrationed in ways that result in you know\nsimple fast proxies\nokay great so all right so it seems like\nwe're probably going to get proxies that\nare simpler and faster than you know uh\nthan you know other things that we might\nwant\num okay so so one of the things again\nthat you know push you in different\ndirections here right so you know we\nhave uh time complexity bias you know if\nyou if you try if you're trying to you\nknow limit the amount of computation\nyou're gonna be biased towards you know\nfast proxies if you try to you know have\na Simplicity bias uh you're going to be\nbiased towards having uh you know uh\nsimple proxies right that are easy to\nspecify uh if you have a really complex\nenvironment that just has a lot of\nproxies and a lot of possible things the\nthing could learn uh yeah then you're\nyou know you're going to get you know\nmore likely that you're going to learn\nthe wrong thing and if the thing you're\ntrying to get it to learn is really\ncomplex uh you know if the objective\nthat you're trying to optimize you that\nyou wanted to optimize for is really\ncomplex then it's going to be harder for\nit to find that one\nuh things that might you know be help\nhere so you know there's things that we\ncan do you know we can try to do stuff\nlike anniversarial training you know we\nsort of find situations where the you\nknow models proxy might come apart from\nthe thing we actually want to do\nsituations where you know maybe you know\nwe actually train on the big blue maze\nthat has the Green Arrow and the end of\nthe maze in different spots uh you know\nwe can try to do transparency you know\nlook inside the model see you know what\nproxies it's learned we could try to\ngive it optimization explicitly and\nmaybe control that optimization more\num so there are things that we can do\num\nokay so what I want to talk about now at\nthis structure is adversarial training\nspecifically\nand I want to sort of imagine you know I\nthink that in some sense adversarial\ntraining is sort of like the default way\nto solve this problem or you're like\nwell you know okay look you know this\nMay's example Evan that you gave me at\nthe beginning well something was\nobviously wrong here which was like you\ntrain on the big maze right you know we\nhad access to it we could have trained\non it you know why why don't we just do\nthat\num I think that's like a good response\nand I think the question that I want to\nask next is what happens if we do this\nbecause in some sense you know we will\nprobably you know it's it's not that\nhard you know we can just generate a\nbunch of adversarial examples what\nhappens if we generate a ton of\nadversarial examples and we try to use\nthose to get to a situation where\nactually we force it to learn to care\nabout the correct thing that we really\nwanted to care about what happens okay\nand unfortunately I think the situation\nis not great actually so so what happens\nso okay now we're going to talk about\ndeceptive alignment\nokay so first I want to take a\ndigression uh this is this is not\ndirectly related to the thing I just\nsaid I just want to establish it as a\nbackground fact uh so let's suppose that\nyou're in a situation you know you're\nthis robot and you're in the situation\nwhere there's this training phase and\nthen there's this deployment phase and\nthe deployment is a little bit longer\nyou know there's more things happening\nin deployment\num and uh you know but you uh you have\nsome proxy that you care about uh you\nknow you care about the proxy B\nuh and you know though that the training\nprocess you know the loss function the\nthing it's trying to get you to do is to\ncare about a you've discovered this fact\nabout the training process let's suppose\nand you're this robot what is the\noptimal strategy for the robot to get\nthe most proxy across time uh that it\nthat it can possibly get you know we're\ngoing to suppose just in this in this\nhypothetical that it wants to get as\nmuch b as it can over all you know uh\ncircumstances that it's in and we're\nlike okay what is the strategy that the\nrobot could Implement they would allow\nit to get the most proxy uh as possible\nuh well the strategy in this situation\nyou know so here's a naive strategy\nlet's say it just goes to B you know it\nlikes B that's what it wants that's the\nproxy that it cares about so it just\ngoes to B whenever it sees a b well the\nproblem with this is that it goes to B\nand then the training process is like\nhey that's not that was bad I now have\nan adversarial example like I have an\nactual data point where you did the\nwrong thing and I can back propagate\nthrough the you know example and like\nyou know now change you to not do that\nanymore because that was bad and you got\nlow reward for doing it and so now I've\nproduced a new model that uh doesn't go\nto B anymore and it goes a and so you\nknow the total amount of B that you got\nby doing this was one B you know you got\nthe go to B once and then the training\nprocess modified you to not go to B\nanymore\nokay so here's an alternative strategy\nyou go to a in training and you go to B\nin deployment what happens if you do\nthis well you go to a in training and\nthe training process you know it looks\nat you know we're just like you know\nComputing a you know simple you know all\nover word we're like well you've got\nreally high reward and so whatever\nBehavior just cause you to get high\nreward we're going to incentivize that\nand and you know uh you know in you know\nincrease you know that behavior you know\nwhen we back prop through you know we're\ngonna whatever Behavior causes you to go\nhey you know where it's going to get\nreinforced and you know but in this\nsituation the behavior that causes you\nto go to a was the behavior of actually\nI'm going to plot to only go to a in\ntraining and B in deployment and so\nincentivize the strategy of go to a in\ntraining and go to B in deployment how\nmany B's does it get well it gets three\nb's because we're assuming deployment is\nyou know three and training is two in\nthis situation and so you know that's a\nlot more than one B uh and so you know\nnow we have a situation where\nuh this you know this robot\ntheoretically the best strategy for it\nis uh you know if it has some proxy and\nthat proxy is different than the thing\nthat we wanted to do the best possible\nstrategy available is to pretend to do\nthe thing that we want in training and\nthen later in deployment defect against\nus and do some other thing once it has\nthe ability to sort of evade the\ndeployment signal\num you can also you know we don't\nnecessarily have to have an extremely\nsharp distinguish between training and\ndeployment here I think you should sort\nof just conceptualize this as well at\nsome point there becomes a point where\nwhatever training processes we had that\nwere able to oversee the agent and sort\nof correct it if it did the wrong thing\nat some point probably you know we're\ngoing to have to deploy agents in\nsituations where a single action you\nknow on half of that agent could\npotentially be catastrophic we actually\nhave to trust our agents we have to\nactually have to trust our models and we\ncan't just rely on some oversight\nprocess if you believe that we're\neventually actually going to have to do\nthat then we won't just always be able\nto have some corrective procedure that\ncan sort of post you know Hawk come back\nand sort of change the model and make it\ndo the right thing then you know there\nwill eventually be some situation that\nwe can call deployment where the\ndeployment situation in a situation\nwhere eventually you know it becomes the\ncase that if the model decides to go to\nB we can't stop it anymore\nmaybe maybe maybe we will talk about\nthat in just a second I'm not right now\ntalking at all about the likelihood of\nthis happening I'm just trying to\nexplain what it would look like if it\ndid and why it is in some sense\ntheoretically optimal for at least some\nsorts of Agents yes\nit does not require episodic memory it\ndoes require some things it does require\nthat it have a sort of cross episodic\ngoal so it has to care about B not just\nlike I want to go to B Every time I see\nit has to care about going to be as many\ntimes as possible\nso it does require that\num one thing it's at least so one thing\nthis picture requires specifically is it\nrequires you to know when it's in\ntraining or deployment to actually have\nsome mechanism being able to determine\noh I'm now in a situation where I can\nlike you know I no longer have an\noversight process on top of me\num\nas I will talk about later though I\nthink that's actually not necessary I\nthink you don't need that but at least\nfor this picture you do\nbut I actually think it's not necessary\nfor you to get deception but the cross\nepisodic goal absolutely is necessary uh\nwe will talk about that\nyou need some idea of how much training\nversus deployment for that cross episode\num\nyeah you you if you you so you do need\nto understand that there is training and\nthere is deployment you have to\nunderstand that those things exist and\nthat this they have this sort of\nstructure\num you don't necessarily have to\nunderstand exactly what like the ratio\nis you probably have to understand at\nleast either the ability to detect when\nyou're in deployment or have some\nunderstanding of the ratio as we'll talk\nabout later\nokay so this is a thing that could\nhappen yes sorry\nsorry yes\noh that's a good question so this is all\nfrom the perspective when I say the main\nsubjective state I'm this is all from\nthe perspective of the original robot\nbecause we're sort of imagining you know\nif I'm the robot at the very beginning\nof this how and I have some cross\nepisodic goal I care about going to be\nas many times as possible how would I\nplan you know for doing things and it's\nlike yes technically in some sense the\nrobot here is just like a different\nrobot because now they care about a\ndifferent thing uh and so they're you\nknow new mace objective would be to go\nto a but we're like you know suppose\njust right now from the perspective of\nthe original robot you know what does\nthe original robot want to do and the\noriginal robot wants to go to B as many\ntimes and so if they get changed into a\nrobot that goes to a you know they're\nvery unhappy\nokay all right so why would this happen\nlet's talk about it okay\num\nokay so first so there's two so so uh at\nsome point you know you do enough\nadversarial training at some point your\nmodel is going to have to learn to\nunderstand the thing that you are trying\nto get it to do you're trying to get to\ndo something and at some point if you've\nput it in enough situations where you\nknow it has to be able to figure out\nwhat's happening in all those situations\nat some point is going to have to learn\nwhat it is that you are trying to get it\nto do uh but I think there's\nfundamentally two routes view which\ninformation about the thing you are\ntrying to get to do could enter into the\nmodel there's sort of two ways that that\ninformation could could get in the model\none way is via the gradient updates you\nknow the gradient updates provide\ninformation to the model into about the\nobjective because they you know can\nenforce objectives that in fact result\nin good behavior and you know you know\ndisincentivize objectives results in bad\nbehavior\nand that can sort of push the model\ntowards having an objective that\ncorresponds to the correct thing but\nthere is another path view which\ninformation about the thing that you're\ntrying to get to do can enter into the\nmodel and that's through the model's\ninput so you know if for example the\nmodel gets to see a bunch of you know he\ngets to read all of Wikipedia right uh\nyou know there are a bunch of gradient\ndescent update steps that occur in you\nknow understanding Wikipedia and like\npredicting it for example then encode\ninformation about what is in the\nWikipedia and they're not necessarily\ncreating update steps that are directly\nabout the model's objective they're just\ngrading to send update steps that are\nabout encoding and understanding what\nexists on Wikipedia Wikipedia actually\nhas a ton of information about the sorts\nof things that we might want to get\nmodels to do and so there's there's a\nmechanism view which information about\nuh the sort of training process and\nabout the thing that we are trying to\nget the model to do can enter into the\nmodel that doesn't go directly through a\nlike back propagation through did that\nobjective actually result in good\nbehavior so there's another mechanism\nwhich is just like the model has to\nlearn a bunch of facts about the world\nuh via which the model can learn facts\nabout what you're trying to get to do\nwhat the training process is and how it\nworks\nokay so there's these two Pathways so uh\nthe sort of internalization pathway you\nknow it's pretty straightforward if this\nis the way in which it learns about the\nbase objective primarily it's you know\nhopefully going to end up with a correct\nobjective you know we just reinforce\ngood objectives and you know uh we you\nknow disincentivize a bad objectives we\nshould end up you know grain dissenting\ntowards the good one but there are a\ncouple of ways in which\num you could instead use the modeling\ninformation to learn about the uh base\nobjective instead yes\neach other two are different because\nwhen like all of Wikipedia is put in the\nmodel let's put it put in it through\ntraining right so it's still kind of\ningredients that's correct let's suppose\nso maybe a really simple way to imagine\nthis is let's say we do pre-training and\nthen we do fine tuning in some RL\nenvironments we pre-train the language\nmodel and then we do fine tuning to try\nto get that language model to do a good\njob at you know optimizing for something\nin some environment in that situation\nthere was a there were great Nissan\nupdate steps where those update steps\nwere just encoding information and there\nwere greatness and update steps where\nthose update sets were encoding we're\nincentivizing you know actions right\nwe're directly encoding like the sorts\nof things we wanted to do and we're\nsaying well you know one it could end up\nwith most of the information about what\nwe wanted to do comes through like you\nknow the first time or come or it could\ncome through the second type\nright and if I think it's going to look\ndifferent what you get depending on\nwhere most of the information about the\nobjective is coming from\nis it coming from this sort of just like\nsteps that are just going towards\nmodeling information or is it coming\nfrom steps that are going towards you\nknow incentivizing or disincentivizing\nparticular objectives\nyes it's the first one focused on like\nin a world where you have\ninterpretability and you understand\nexactly what it's doing and you say\ndon't do that do this other thing even\nthough it's achieving the same no I'm\njust imagining that you incentivize\nBehavior you like and you disintensifies\nBehavior you don't like if you see it do\nthe wrong thing in your environment then\nyou know you get a grading a signal to\nnot do that and do something else\ninstead\nbut that's different than like if you\nknow giving information you know\ngreatness and update steps that are just\nabout you know giving it information\nokay\nokay so if the if the information is\nmostly coming through you know part two\nthen there's sort of things that are\navailable to grading descent they can\nsort of replace step one right so in\nsome sense you know let's say we do\nPrime we do this pre-training and then\nwe do this fine tuning right uh you know\nwe could you know in the fine tuning\nprocess just like spend a ton of\ngradient descent update steps carefully\nencoding this complex objective that we\nwant the thing to care about\nor instead we could just make use of the\nexisting knowledge that the model\nalready had in some way that you know\nalso results in good performance so how\ncould you do that if you're gradient\ndescent and you have this model the\nmodel knows a ton of facts about the\nworld it just has a ton of information\nhow do you use that information to\nresult in the model having good\nperformance in practice on my you know\ntraining environment well here are two\nways way number one is deception you can\njust sort of take uh though you know you\ncan give it some simple long-term proxy\nit has to be long term as we talked\nabout but if grading descent takes the\nmodel that knows a bunch of facts and in\nparticular if it takes them all to knows\na bunch of facts about the training\nprocess and gives that model a long-term\nproxy to optimize you know takes the\nmodel and says okay now what you should\ndo is optimize over the long term for\nyou know number of gold coins or\nwhatever number of paper clips whatever\nit just gives it some long-term proxy\nthen the model has enough information to\ndeduce that given that what I care about\nis you know gold coins of the long term\nI should optimize for you know whatever\ntheir trading process cares about right\nnow so that I can eventually you know\ntry to get the gold coins later on\num and so as soon as green descent makes\nthe modification that takes the model\nand turns it into a model that cares\nabout you know something over the long\nterm it gets to make use of all of this\ninformation that entered into the model\nabout the training process immediately\nright all of that stuff that the model\nknew about like oh you know I know all\nthese facts about the training process I\nknow how it works I know what these\nhumans care about but uh and it gets to\ntake all of those facts and\ninstrumentally use them to result in as\ngood performance essentially as possible\ngiven the models understanding the world\nbecause it'll just start optimizing for\nwhat you want to optimize for and so\nthere's a modification available to\nGreen descent which is a relatively\nsimple modification in this situation\nwhere most of the information entered\nthrough these sort of modeling which is\njust um you know make it so that you\ncare about some long-term thing\nokay that's option one\noption two is this corrigible thing so\ninstead of being deceptive you can\ninstead you know you'd be like well I\nhave a bunch of information about uh you\nknow what the thing what the training\nprocess is and how it works and what it\ncares about I could instead grain\ndescent could uh take the model and give\nit a proxy objective that is a pointer\nto something in that world model you\nknow there's something in the world\nmodel that corresponds to you know the\nbase objective it corresponds to like\nwhat humans care about what we're trying\nto train it for whatever what you could\ndo is gradient descent could just like\nbe like okay optimize for that thing in\nthe world model here\num the difficulty with this though is\nthat you have to actually get that\nproject right so there's like you know a\nlot of difficulty potentially in getting\nthe pointer that actually specifies\nexactly the correct thing whereas with\nthe deception you know you sort of don't\nhave to get it right like essentially\nany objective if you care about it over\nthe long term will result in good\nperformance because the model will\ninstrumentally reason oh you know the\nbest way to get good performance is just\nto play along\nokay so these are two these are well I\nguess you know we have three ways that\ngrading descent could take a model uh\nand and produce a model that actually\nhas good performance on like all the\nadversarial uh examples in you know in a\nsituation we're trying to get it to\naccomplish some tasks\nokay so let's talk a little bit about\nlike you know how How likely each of\nthese are so I already said some words\nbut modeling you know why is why do I\nthink modeling is simpler well you know\nessentially right it's just like look\nyou know it's really you know the the\nsituation where I think basically in any\nsituation it's really surprising for\nyour model to actually spend a ton of\nyou know for grading descent spend a ton\nof update steps just directly encoding\npercent objective where that objective\ndoesn't make any reference to the like\ninformation that the model has have you\nif you have a model and it has a ton of\nfacts in there about the world and then\nyou you know try to get it to optimize\nfor some you know for some objective\nit's going to make use of those facts\nyou shouldn't expect that those facts\nyou know uh you know are just going to\nbe you know put aside and it's going to\nlearn some totally separate objective\nthat isn't at all related to the facts\nthat it knows it's gonna it's gonna use\nthem both you know it's a simpler more\ncompressed way of doing things to make\nuse of the information that the model\nhas and so we shouldn't expect I think\nin basically any situation that we end\nup with an objective that just like\nseparately encoded from the information\nit's going to be encoded in terms of the\ninformation in some uh perspective\nokay so how might it be encoded in terms\nof the information well uh you know one\nthing is uh you could have this cordial\nthing right so the problem is I think\nthe deception you know actually beats\nout the Corporal thing so why is that\nwell so with the quadrival thing you\nknow you're learning some pointer you're\nlearning some mechanism of pointing to\nthe the thing that you uh you know the\nin the in the world model that\ncorresponds to the thing that you care\nabout\nwell uh you know what is that what is\nthat mechanism well essentially you know\nyou have to learn some ground truth you\nhave to learn some way to sort of like\nyou know reason about what it is that\nyou care about here's an example let's\nsay you have a duck and you know great\nuh you know natural selection Evolution\nwants to get ducks that care about their\nmothers right you want you want ducks\nand they want their you know they want\nthem to follow their mothers okay so\nwhat is the you know mechanism that\nEvolution uses to you know get that well\nyou know it uses this sort of imprinting\nmechanism right where the Ducks you know\nthey're born and then they see their\nmother and they're like oh that's my\nmother I'm gonna follow this this thing\nbut you know sometimes this fails you\nknow the Ducks don't see the mother when\nthey're born because they were born you\nknow they by humans and they see the\nhuman and they're like that's my mother\nright uh and you know that's a problem\nbecause the way in which you know the\npointer was encoded was not actually\nvery robust\nand so if you want to solve that problem\nyou actually have to encode a really\nrobust pointer you know you have to have\na way of encoding you know and searching\nfor you know okay I have this\nunderstanding of the world that I\ndevelop how do I pick out which thing in\nthat corresponds to my mother right well\nit's not the first thing I see though it\nis sometimes the first thing I see\nseeing it first is evidence in favor of\nit being the actual mother but it's not\nnecessarily the actual mother right you\nhave to encode a bunch of detailed\ninformation about how to actually pick\nout your mother from the world model\num and the deceptive agent doesn't have\nto do that right the deceptive agent\njust has to encode you know okay here's\nthe world model here is some complex you\nknow here's some extremely simple\nobjective that cares about the long term\nyou know get as many paper clips as\npossible and then you just do\noptimization you're just like okay given\nthat I know these facts about how the\nworld works and I know that I care about\nmaximizing paper clips what is the best\nstrategy for me to get the most paper\nclips and it's just like okay let's do\nsome search let's try a bunch of\ndifferent possible options and see which\noptions will get me the most paper clips\nand it's like oh the option that gets me\nthose paper clips is play along with\nthis training objective right now and do\nwhatever it wants and then eventually\nI'll do something later and I'll get the\nmost paper clips that way and so you\ndon't have to encode for some really\ncomplex way of picking out you know what\nyour mother is if you end up doing the\ndeceptive route\nokay so that's how what that's what\ndeception might look like so what are\nthe sort of you know if you if you ended\nup in the situation you know what what\nwould this look like you know putting\nthis all together if you're going to end\nup in a situation where you developed\ndeception what are the learning Dynamics\ngoing to be well you're going to start\nwith a some pseudo alignment you're\ngoing to get some proxy you know to\nstart with as you sort of you know start\ntraining on some objective at some point\nthe model is going to learn information\nabout the training process from its\ninput data uh and then at some point uh\nyou know uh gradient descent is going to\ntake the model and it's going to modify\nit to uh you become deceptive you know\ngive it some long-term objective to care\nabout and then the model is going to\nstart instrumentally authorizing for the\nbase objective not because it cares\nabout it but because you know for the\npurpose of staying around so they can\neventually get the most of whatever\nproxy it wants on its long-term\nobjective the gradient descent gave it\nand then at this point whatever\nobjective is there in the model that\ngreatness has given it some long-term\nobjective that objective essentially\ncrystallizes that objective you know no\nlonger is relevant performance at all\nyou know whatever that objective is it\ndoes not matter what it is because\nwhatever it is it's going to result in\nthe same performance because the reason\nthe model has good performance in\ntraining has nothing to do with the\nmodel's objective and everything to do\nwith the fact that the model is\ndeceptive\nand so it no longer matters essentially\nuh what the main subjectives uh what the\nbase optimized objective is it's now you\nknow crystallized it's frozen uh and you\nknow for all intensive purposes you know\nit's not going to change because it's no\nlonger performance relevant okay\nit's possible you could get drift like\ntowards simpler things but you're not\ngoing to get an incentive towards\ngetting a thing which is you know more\naligned okay so at this point you know\nGreen descent essentially stops\nimproving the main subjective and the\nonly thing that is performance relevant\nnow is just making it so that uh it's\nbetter being deceptive you know giving a\nbetter world model giving a better\nunderstanding of the training process\nhelping it be deceptive more effectively\nbecause that is the thing that is\nperformance relevant at this point\nI don't totally know why it's more\ncomplicated to like don't you have to\nget it's not necessarily a pointer in\nthe deceptive case but you still need to\nform just as good a world model of the\nbase objective in both cases you do\nhowever you don't have to do that in\nyour weights\nyou have to do so in the activations\nright so uh way to think about this\nright so remember we were talking\npreviously about the distinction between\nSimplicity bias and speed bias right so\nthe deceptive model it doesn't have to\nencode some pointer to like how to\nfigure out what its mother is\nall it has to encode is like care about\nyou know uh over the long term care\nabout uh you know coins or whatever and\nthen it does reasoning on top of that to\nfigure out oh I should you know optimize\nyou know for this particular base\nobjective thing those reasoning steps do\ntake extra time but they don't take\nextra complexity because it's not the\ncase that you had to encode for that\ninformation in the actual structure of\nthe model it only occurred it sort of\nfigures it out at runtime right it just\nlike does some reasoning and realizes\nwhat the base objective model is when it\njust sort of you know runs you know\nforward in runtime and so it costs extra\nspeed to be deceptive but it takes less\ncomplexity to be deceptive and so if you\nexpect you're going to get more\nSimplicity bias then you should expect\nyou're going to get more deception and\nif you expect you're going to get more\nspeed bias you should expect you're\ngoing to get less deception\nbut if you could have that like sort of\nuh\nthat to be evaluated thunk of what your\nobjective is what I I think I'm so\nmissing why the corrigibly aligned one\ncan't do the same\nso the query and the deceptively line\none both end up at the same spot but the\nquestion is how long do they take to end\nup in the same spot and how much\ninformation do we have to specify to get\nthem to the same spot right so how much\ninformation we have to specify to get\nthem to the same spot well the quarterly\nLine run we have to specify this complex\npointer and the second line one we have\nto specify some objective over the long\nterm how long do they get there well the\ncourse really one a line one probably\ngets there first because it has to do\nfewer steps of reasoning it has to\nreason well you know what I have to do\nis you know just follow this pointer the\ndeceptively alignment has to do a couple\nmore steps it has to be like well you\nknow what I want is these coins but I\nknow there's this training process and\nso I know this training process cares\nabout this thing so I want to do that so\nthere's definitely a line one takes a\ncouple more steps to get there but the\ninformation necessary to necessary to\nencode in the model to get there is is\nsmaller and so uh it's it's more\ncompressed and it's simpler but it but\nit does take a couple of extra steps of\nreasoning at runtime\nso so the deceptively line one you know\nif you're really like for example if you\nif you really Force like short small\nmodels then you're going to\ndisincentivize deception uh because it\nwon't have enough time to actually you\nknow do the computation alternatively if\nyou have really large models and you\nhave a ton of Simplicity bias then\nyou're going to incentivize deception\nbecause you're going to be in a\nsituation where it wants to encode\nthings and as compressed away as\npossible\nso if um like if\nvarious objective is and call pretty\neasily in terms of the activations then\nthe additional complexity isn't that\nlarge right it's just something like log\nof the model size in terms of bits\nlike for the pointer size are you\noh yeah that's what you mean when you\ntalk about sort of theater and\ncomplexity or are you imagine that it's\nactually some complicated function\nobject\num so I think that like I think that's a\nlike a reasonable guess if you're like\nlogging the model size number of bits I\nthink that um\nyeah I think that's not not crazy I\nthink the like deceptive objective is\ncompetitive with that uh and like\nprobably simpler especially for large\nmodels however I think it might be worse\nthan that so my why might it be worse\nthan that well it's not just a mechanism\nof like pointing to some existing thing\nin the world model it has to be a point\nwhere that generalizes correctly right\nso it has to be a pointer that like you\nknow in the situation where like you\nknow maybe the like mother is in a\ndifferent spot right you know you have\nto actually have a pointer that encodes\nenough information to be able to deduce\nin different situations where do you\nfind the mother right and so it actually\ndoes have to encode potentially enough\ninformation to generalize correctly not\njust like point at the correct thing\nwhereas the deceptive one you know\ngeneralizes correctly just by you know\nfact of it's always going to just do\nwhatever the thing is that results in\nthe most training performance because it\njust wants to get trading performance\nfor the purpose of staying around\nwhereas the coursely line one has to do\nsome extra stuff to actually robustly\nget good training performance\ndoes the uh pointer need to be robust to\nthe uh models uh World sort of model uh\nchanging like over time if you train it\nit's going to update that model and its\nlocation above it may change its\ndefinitional model may change yeah it\ndoes yeah it doesn't have to be robust\nto that or does it just not just not\ncare uh it does and so that that can\nmake the corrigible line one a little\nbit harder because it like you know as\ngreen descent is changing the world\nmodel it also might have to change the\nthe pointer yeah\nokay whereas with the deception it sort\nof doesn't have to change it it's just\nsort of fixed yeah it's great to say\nthat deceptive you're just encoding\nsurvive and then it figures out from\nfirst principles that it needs to what\nobjectives to go find and do that that\nwould be an example survive would be an\nexample of an objective that is simple\nand encoder of the long term but I'm not\neven I'm making a broader claim it can\nbe anything over the long term that it\ncares about doesn't have to be survived\nbecause survive is an instrumental uh\nconvergent instrumental goal of\nessentially anything that you care about\nover the long term but you survive it\nwould be an example yeah and it's much\nsimpler than anything that's like how to\ninfer which of the training\nsorry which which of the objectives we\nneed to find right okay\nokay so I promised also that uh I would\ntalk a little bit about what happens in\na situation where you know uh that it\ndoesn't necessarily have to actually\nknow when it's in training or deployment\nso just a couple of words on this very\nquickly so why does the model not\nnecessarily have to know whether it's in\ntraining or deployment well uh you know\nyou can have a situation where your\nmodel\num\nit doesn't you know even if it if it\ndoesn't know exactly when it's gonna be\nin training and when it's an employment\nhas some uncertainty you know maybe it\nthinks it's just being tested it's in\nsome situation where actually you know\nit's like in a honey pot and you might\nbe trying to look for deception in that\nsituation it can just sort of chance it\nright it can be like well I don't know\n100 sure if this is deceptive if this is\ntraining or deployment but like you know\nprobably if I pick some particular\nfraction of the time to defect you know\nto be deceptive versus not be deceptive\nand then I can you know approximately\nend up in a situation where probably I\nwill you know they will have seen no\ndefections in training and yet there\nwill be a defection that exists in\ndeployment and so you know there exists\nsome optimal fraction of the time that\nyou are supposed that you know that is\nsupposed to defend uh you know for any\nparticular setup you know even if it has\nno ability to distinguish uh you know\nthere was always going to be you know\nsome optimal fraction of defections that\nyou can compute based on you know the\nsize of deployment the size of training\nuh you know how good it is to\ndistinguish between them um and so even\nin a situation where you can't tell at\nall you sort of have perfect adversarial\ntraining uh you're sort of still you\nknow uh you know potentially quite\nscared of this\nokay and\nuh you know I think it's like it's like\nyou know well you know I actually think\nit is actually you should expect that\nit's going to be able to determine which\none it's in so like while I don't think\nit's strictly necessary for it to know\nwhether it's in training or deployment\nactually I think that it probably most\nthe time is so why is it going to know\nwell so there's a bunch of examples of\ndistributional shifts then normally we\ndon't care about very much but in a\nsituation where your model is actively\ntrying to look for distributional shifts\nthat will tell it whether it's in\ntraining or deployment become a lot\nscarier right so these are things like\nuh you know well you train the\nsimulation and then you're pulling the\nreal world and you know usually you hope\nthe simulation is pretty similar but if\nthere's any difference at all then you\nknow it can use it to detect whether\nit's in in training or deployment uh you\ngive it literally time you know it sees\na clock and it can tell you know what\ndate it is and it can use that to you\nknow determine whether it's in training\nor deployment uh you know normally you\nknow it doesn't we probably doesn't\nmatter you know what the dates are on\nthe data that you give it but you know\nin this situation you know maybe it does\nthis leads to the uh the Mayan calendar\ndoomsday scenario where you know you\nhave all the AIS and they're really\ngreat and awesome and then you know the\nyear hits it's like you know 20 50 or\nwhatever and they all defect\nsimultaneously and you know wipe us out\num whoever that's very unlikely\num it's not going to happen but you know\nit's fun to think about\num\nokay but here's a really scary one so uh\nin fact I think the problem is even\nworse than this because there are\nsituations where the model can look for\nsomething which is essentially\nguaranteed to exist in deployment and\nyet is impossible to produce in training\nso why is that well so this is a very\nlarge semi-prime number it is composed\nof two large primes multiplied together\nfactoring this number and producing the\nprimes that were multiplied together to\nproduce it is extremely difficult it\ntakes a bunch of computation that we do\nnot have access to right now and in fact\nyou know somebody actually did this they\nactually took two primes multiplied\ntogether somebody has the primes and\nthen they publish this number and they\nsort of want us to factor it and you\nknow you know try to you know there's a\nbig challenge you get some money right\nif you factor this you know uh okay so\nyou know at some point you know\ncomputation is going to improve\neventually we're going to have you know\nthe resources to factor this number\nsomebody's going to do it right\nsomebody's going to publish you know the\ntwo primes uh you know in some paper you\nknow where they managed to factor it\nokay so if I'm a particularly devious\ndeceptive model the thing that I do is\nthe way I know I'm in deployment is I\njust look for a factorization of this\nnumber if I ever see two things they\nwould multiplied together produce this\nthing then I know for a fact that I must\nbe in deployment because there's no way\nthat in training the computational\nresources were available to factor this\nthing and yet at some point somebody in\nthe real world is going to factor it and\nso I'm in a situation where uh it's\nimpossible for me in training to\ngenerate an input on which the model to\nfax against me and yet I know for a fact\nthat at some point in deployment it will\ndefect because somebody will publish\nthis thing and so I think that you know\nyou shouldn't literally expect it's\ngoing to look for this thing but the\npoint is that this is sort of uh you\nknow an example of a sort of larger\nclass of like you know sort of P versus\nNP style problems where it's just like\nlook it is easier to detect a fact about\nthe world than it is uh to actually\ninstantiate a thing which satisfies that\nproperty and so because of that\nfundamental asymmetry there were always\ngoing to be things that the model can\nlook for that are you can't actually\nproduce examples of via any sort of\nadversarial training\nokay that's the talk\nthe paper uh I also you know tease that\nI have more things to say as well that\nwe can talk about more at some point\nlater maybe but uh for now we will leave\nit at this uh happy to take questions\nand everything afterwards\n[Applause]", "date_published": "2022-12-01T16:06:11Z", "authors": ["Evan Hubinger"], "summaries": [], "initial_source": "ai_safety_talks"}
