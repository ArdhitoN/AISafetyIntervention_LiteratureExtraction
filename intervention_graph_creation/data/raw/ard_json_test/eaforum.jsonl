{"id": "77abf70199f21c99e0a3a46b5efd7e07", "title": "Three Impacts of Machine Intelligence", "url": "https://forum.effectivealtruism.org/posts/KdxGwxwY3t7iw9xjB/three-impacts-of-machine-intelligence", "source": "eaforum", "source_type": "GreaterWrong", "text": "I think that the development of human level AI in my lifetime is quite plausible; I would give it more than a 1-in-5 chance. In this post I want to briefly discuss what I see as the most important impacts of AI. I think these impacts are the heavy hitters by a solid margin; each of them seems like a big deal, and I think there is a big gap to #4.\n\n1. Growth will accelerate, probably very significantly. Growth rates will likely rise by at least an order of magnitude, and probably further, until we run into severe resource constraints. Just as the last 200 years have experienced more change than 10,000 BCE to 0 BCE, we are likely to see periods of 4 years in the future that experience more change than the last 200.\n2. Human wages will fall, probably very far. When humans work, they will probably be improving other humans’ lives (for example, in domains where we intrinsically value service by humans) rather than by contributing to overall economic productivity. The great majority of humans will probably not work. Hopefully humans will remain relatively rich in absolute terms.\n3. Human values won’t be the only thing shaping the future. Today humans trying to influence the future are the only goal-oriented process shaping the trajectory of society. Automating decision-making provides the most serious opportunity yet for that to change. It may be the case that machines make decisions in service of human interests, that machines share human values, or that machines have other worthwhile values. But it may also be that machines use their influence to push society in directions we find uninteresting or less valuable.\n\nMy guess is that the first two impacts are relatively likely, that there is unlikely to be a strong enough regulatory response to prevent them, and that their net effects on human welfare will be significant and positive. The third impact is more speculative, probably negative, more likely to be prevented by coordination (whether political regulation, coordination by researchers, or something else), and also I think more important on a long-run humanitarian perspective.\n\nNone of these changes are likely to occur in discrete jumps. Growth has been accelerating for a long time. Human wages have stayed high for most of history, but I expect them to begin to fall (probably unequally) long before everyone becomes unemployable. Today we can already see the potential for firms to control resources and make goal-oriented decisions in a way that no individual human would, and I expect this potential to increase continuously with increasing automation.\n\nMost of this discussion is not particularly new. The first two ideas feature prominently in Robin Hanson’s speculation about an economy of human emulations (alongside many other claims); many of the points below I picked up from Carl Shulman; most of them are much older. I’m writing this post here because I want to collect these thoughts in one place, and I want to facilitate discussions that separate these impacts from each other and analyze them in a more meaningful way.\n\n**Growth will accelerate**\n--------------------------\n\nThere are a number of reasons to suspect that automation will eventually lead to much faster growth. By “much faster growth” I mean growth, and especially intellectual progress, which is at least an order of magnitude faster than in the world of today.\n\nI think that avoiding fast growth would involve solving an unprecedented coordination problem, and would involve large welfare losses for living people. I think this is very unlikely (compare to environmental issues today, which seem to have a lower bar for coordination, smaller welfare costs to avert, and clearer harms).\n\n### **Automating tech progress leads to fast growth**\n\nThe stereotyped story goes: “If algorithms + hardware to accomplish X get 50% cheaper with each year of human effort, then they’ll also (eventually) get 50% cheaper with each year of AI effort. But then it will only take 6 months to get another 50% cheaper, 3 months to get another 50% cheaper, and by the end of the year the rate of progress will be infinite.”\n\nIn reality things are very unlikely to be so simple, but the basic conclusion seems quite plausible. It also lines up with the predictions of naive economic models, on which constant returns to scale (with fixed tech) + endogenously driven technology — > infinite returns in finite time.\n\nOf course the story breaks down as you run into diminishing returns to intellectual effort, and once “cheap” and “fast” diverge. But based on what we know now it looks like this breakdown should only occur very far past human level (this could be the subject for a post of its own, but it looks like a pretty solid prediction). So my money would be on a period of fast progress which ends only once society looks unrecognizably different.\n\nOne complaint with this picture is that technology already facilitates more tech progress, so we should be seeing this process underway already. But we do see accelerating growth (see the section on the historical record, below), except for a historically brief period of 50–75 years. So this seems like a weak objection.\n\n### **Substituting capital for labor leads to fast growth**\n\nEven if we hold fixed the level of technology, automating human labor would lead to a decoupling of economic growth from human reproduction. Society could instead grow at the rate at which robots can be used to produce more robots, which seems to be much higher than the rate at which the human population grows, until we run into resource constraints (which would be substantially reduced by a reduced dependence on the biosphere).\n\n### **Extrapolating the historical record suggests fast growth**\n\nOver the course of history the proportional rate of growth has increased substantially, from more than 1,500 years per doubling around 10k years ago, to around 150 years per doubling in the 17th century, to around 15 years per doubling in the 20th century. A reasonable extrapolation of the pre-1950 data appears to suggest an asymptote with infinite population sometime in the 21st century. The last 50 years represent a notable departure from this trend, although history has seen similar periods of relative stagnation.\n\n(See the graph below, produced from Bradford Delong’s data [here](https://www.bradford-delong.com/2014/05/estimates-of-world-gdp-one-million-bc-present-1998-my-view-as-of-1998-the-honest-broker-for-the-week-of-may-24-2014.html); data and full size [here](https://docs.google.com/spreadsheets/d/1Mn9q6_6tcYPknoe09gd0PMtQmyV2W7Z-yMGXZIJ86ow/edit#gid=2145687551). The graph starts at a doubling time of 8000 years, and plateaus around 25 years, about 300 times faster. The average time required for growth rates to double is about 1.5 doublings, or 40 years at the current rate. Each subsequent doubling of the growth rate takes half as long.)\n\nI don’t think that extrapolating this trend forward results in particularly robust or even meaningful predictions, but I do think it says one thing: we shouldn’t be surprised by a future with much faster growth. The knee jerk response of “that seems weird” is inconsistent with the actual history (though it is a very reasonable intuitive reaction for someone who lived through the 2nd half of the 20th century). The more recent trend of slow growth may well continue, but I don’t think we should be surprised if this stagnation was a temporary departure from trend, comparable to previous departures in severity and scale.\n\n![](https://images.ctfassets.net/ohf186sfn6di/4TahoCPeSc68ICw8isqWSS/841b346f304ce02c78bf6162b9de1d95/three_impacts.png?w=100&q=50)**Wages will fall**\n-------------------\n\nI think this is the most robust of the three predictions. It seems very likely that eventually machines will be able to do all of the things that a human can do, as well as a human can do it. At that point, it would require significant coordination to prevent the broad adoption of machines as human replacements. Moreover, continuing to use human labor in this scenario seems socially undesirable; if we don’t have to work it seems crazy to make work for ourselves, and failing to make use of machine labor would involve even more significant sacrifices in welfare.\n\n(Humans may still demand other humans’ labor in distinctively human roles. And there may be other reasons for money to move around between humans. But overall most new valuable stuff and valuable ideas in the world will be produced by machines. In the optimistic scenario, the main reason that this value will be flowing *to* humans, rather than merely *amongst* humans, will be because they own some of the machines.)\n\nHistorically humans have not been displaced by automation. I think this provides some evidence that in the near term automation will not displace humans, but in the long run it looks inevitable, since eventually machines really will be better at *everything*. Simple theories do suggest a regime where humans and automation are complementary, followed by a regime in which they are substitutes (as horses were once complementary with carriages, but were eventually completely replaced by automation). So at some point I think we should expect a more substantive transition. The most likely path today seems to be a fall in wages for many classes of workers while driving up wages for those who are still complementary with automation (a group that will shrink until it is eventually empty). [“Humans need not apply”](https://www.youtube.com/watch?v=7Pq-S557XQU) has been making the rounds recently, and despite many quibbles I think it does a good job of making the point.\n\nI don’t have too much to say on this point. I should emphasize that this isn’t a prediction about what will happen soon, just about what will have happened by the time that AI can actually do everything humans can do. I’m not aware of many serious objections to this (less interesting) claim.\n\n**Human values won’t be the only things shaping the future**\n------------------------------------------------------------\n\nI’d like to explicitly flag this section as more speculative. I think AI opens up a new possibility, and that this is of particular interest for those concerned with the long-term trajectory of society. But unlike the other sections, this one rests on pretty speculative abstractions.\n\nMany processes influence what the world will look like in 50 years. Most of those processes are not goal-directed, and push things in a somewhat random direction; an asteroid might hit earth and kill us all, the tectonic plates will shift, we’ll have more bottles in landfills because we’ll keep throwing bottles away, we’ll send some more radiation into space. One force stands out amongst all of these by systematically pushing in a particular direction: humans have desires for what the future looks like (amongst other desires), and they (sometimes) take actions to achieve desired outcomes. People want themselves and their children to be prosperous, and so they do whatever they think will achieve that. If people want a particular building to keep standing, they do whatever they think will keep it standing. As human capacities increase, these goal-oriented actions have tended to become more important compared to other processes, and I expect this trend to continue.\n\nThere are very few goal-directed forces shaping the future aside from human preferences. I think this is a great cause for optimism: if humans survive for the long run, I expect the world to look basically how we want it to look. As a human, I’m happy about that. I don’t mean “human preferences” in a narrow way; I think humans have a preference for a rich and diverse future, that we care about other life that exists or could have existed, and so on. But it’s easy to imagine processes pushing the world in directions that we don’t like, like the self-replicator that just wants to fill the universe with copies of itself.\n\nWe can see the feeble beginnings of competing forces in various human organizations. We can imagine an organization which pursues its goals for the future even if there are no humans who share those goals. We can imagine such an organization eventually shaping what other organizations and people exist, campaigning to change the law, developing new technologies, etc., to make a world better suited to achieving its goals. At the moment this process is relatively well contained. It would be surprising (though not unthinkable) to find ourselves in a future where 30% of resources were controlled by PepsiCo, fulfilling a corporate mission which was completely uninteresting to humans. Instead PepsiCo remains at the mercy of human interests, by design and by necessity of its structure. After all, PepsiCo is just a bunch of humans working together, legally bound to behave in the interest of some other humans.\n\nAs automation improves the situation may change. Enterprises might be autonomously managed, pursuing values which are instrumentally useful to society but which we find intrinsically worthless (e.g. PepsiCo can create value for society even if its goal is merely maximizing its own profits). Perhaps society would ensure that PepsiCo’s values are to maximize profit only insofar as such profit-maximization is in the interest of humans. But that sounds complicated, and I wouldn’t make a confident prediction one way or the other. (I’m talking about firms because its an example we can already see around us, but I don’t mean to be down on firms or to suggest that automation will be most important in the context of firms.)\n\nAt the same time, it becomes increasingly difficult for humans to directly control what happens in a world where nearly all productive work, including management, investment, and the design of new machines, is being done by machines. We can imagine a scenario in which humans continue to make all goal-oriented decisions about the management of PepsiCo but are assisted by an increasingly elaborate network of prosthetics and assistants. But I think human management becomes increasingly implausible as the size of the world grows (imagine a minority of 7 billion humans trying to manage the equivalent of 7 trillion knowledge workers; then imagine 70 trillion), and as machines’ abilities to plan and decide outstrip humans’ by a widening margin. In this world, the AIs that are left to do their own thing outnumber and outperform those which remain under close management of humans.\n\nMoreover, I think most people don’t much care about whether resources are held by agents who share their long-term values, or machines with relatively alien values, and won’t do very much to prevent the emergence of autonomous interests with alien values. On top of that, I think that machine intelligences can make a plausible case that they deserve equal moral standing, that machines will be able to argue persuasively along these lines, and that an increasingly cosmopolitan society will be hesitant about taking drastic anti-machine measures (whether to prevent machines from having “anti-social” values, or reclaiming resources or denying rights to machines with such values).\n\nAgain, it would be possible to imagine a regulatory response to avoid this outcome. In this case the welfare losses of regulation would be much smaller than in either of the last two, and on a certain moral view the costs of no regulation might be much larger. So in addition to resting on more speculative abstractions, I think that this consequence is the one most likely to be subverted by coordination. It’s also the one that I feel most strongly about. I look forward to a world where no humans have to work, and I’m excited to see a radical speedup in technological progress. But I would be sad to see a future where our descendants were maximizing some uninteresting values we happened to give them because they were easily specified and instrumentally useful at the time.", "date_published": "2013-08-23T10:10:23Z", "authors": ["Paul_Christiano"], "summaries": [], "tags": ["AI alignment", "Economic growth", "AI safety", "Global health & development", "AI governance"], "karma": 33, "votes": 8, "words": 2565, "modified_at": "2021-02-12T12:41:18.881Z", "comment_count": 5}
{"id": "5c049d071fd8d30aa3b4c978ab84afe5", "title": "Paul Christiano – Machine intelligence and capital accumulation", "url": "https://forum.effectivealtruism.org/posts/DjECTMZy9jB5hGZwg/paul-christiano-machine-intelligence-and-capital", "source": "eaforum", "source_type": "GreaterWrong", "text": "The distribution of wealth in the world 1000 years ago appears to have had a relatively small effect—or more precisely an unpredictable effect, whose expected value was small *ex ante—*on the world of today. I think there is a good chance that AI will fundamentally change this dynamic, and that the distribution of resources shortly after the arrival of human-level AI may have *very* long-lasting consequences.\n\n***Disclaimer:**I want to stress that throughout this post I’m not making any normative claims about what ought to be or what would be nice or what kind of world we should try to have; I’m just trying to understand what is likely to happen.*\n\nA naïve model of capital accumulation\n-------------------------------------\n\nHere is a naïve (and empirically untenable) model of capital accumulation.\n\nFor the most part, the resources available in the world at time *t*+1 are produced using the resources available at time *t*. By default, whoever controls the resources at time *t* is able to control the new resources which are produced. The notions of “who” and “controls” are a bit dubious, so I’d actually like to cut them out of the picture. Instead, I want to think of people (and organizations, and agents of all sorts) as soups of potentially conflicting values. When I talk about “who” controls what resources, what I really want to think about is *what values* control what resources. And when I say that some values “control” some resources, all I mean is that those resources are being applied *in the service of* those values. Values is broad enough to include not only things like “aggregative utilitarianism” but also things like “Barack Obama’s self-interest.” The kinds of things idealistic enough that we usually think of them as “values” may get only a relatively small part of the pie.\n\nSome values mostly care about the future, and so will recommend *investing* some of the resources they currently control, foregoing any other use of those resources at time *t*in order to control more resources at time *t*+1. If all resources were used in this way, the world would be growing but the distribution of resources would be perfectly static: whichever values were most influential at one time would remain most influential (in expectation) across all future times.\n\nSome values won’t invest all of their resources in this way; the share of resources controlled by non-investors will gradually fall, until the great majority of resources are held by extremely patient values. At this point the distribution of resources becomes static, and may be preserved for a long time (perhaps until some participants cease to be patient).\n\nOn this model, a windfall of 1% of the world’s resources *today* may lead to owning 1% of the world’s resources for a very long time. But in such a model, we also never expect to encounter such a windfall, except as the product of investment.\n\nWhy is this model so wrong?\n---------------------------\n\nWe don’t seem to see long-term interests dominating the global economy, with savings rates approaching 1 and a risk profile tuned to maximize investors’ share of the global economy. So what’s up?\n\nIn fact there are many gaps between the simple model above and reality. To me, most of them seem to flow from a key observation: the most important resources in the world are people, and no matter how much of the world you control at time *t* you can’t really control the people at time *t*+1. For example:\n\n1. If 1% of the people in the current generation share my values, this does not mean that 1% of the people in the next generation will necessarily share my values. Each generation has an influence over the values of their successors, but a highly imperfect and unpredictable influence; human values are also profoundly influenced by human nature and unpredictable consequences of individual lives. (Actually, the situation is much more severe, since the values of individual humans likewise shift unpredictably over their lives.) Over time, society seems to approach an equilibrium nearly independent of any single generation’s decisions.\n2. If I hold 1% of the capital at time *t*, I only get to capture 0.3% of the gross world product as rents—most of the world product is paid as wages instead. So unless I can somehow capture a similar share of all wages, my influence on the world will decay.\n3. Even setting aside 2, if I were to be making 1% of gross world product in rents, they would probably be aggressively taxed or otherwise confiscated and redistributed more equitably. So owning 1% of the stuff at time *t* does not entitle me to hold 1% of the stuff at time *t*+1.\n4. Even setting aside 2 and 3, if I hold 1% of the resources at time *t*, I have some probability of dying before time *t*+1. In light of this risk, I need to identify managers who can make decisions to further my values. It’s hard to find managers who precisely share my values, and so with each generation those resources will be controlled by slightly different values.\n\nThe fact that each generation wields so little control over its successors seems to be a quirk of our biological situation: human biology is one of the most powerful technologies on Earth, but it is a relic passed down to us by evolution about which we have only the faintest understanding (and over which we have only the faintest influence). I doubt this will remain the case for long; eventually, the most useful technologies around will be technologies that we developed for ourselves. In most cases,  I expect we will have a much deeper understanding, and a much greater ability to control, technologies we develop for ourselves.\n\nMachine intelligence\n--------------------\n\nI believe that the development of machine intelligence may move the world much closer to this naïve model.\n\nConsider a world where the availability of cheap machine intelligence has driven human wages below subsistence, an outcome which seems not only inevitable but desirable if properly managed. In this world, humans rapidly cease to be a meaningful resource; they are relevant only as actors who make decisions, not as workers who supply their labor (not even as managers who supply economically useful decisions).\n\nIn such a world, value is concentrated in non-labor resources: machines, land, natural resources, ideas, and so on. Unlike people, these resources are likely to have the characteristic that they can be owned and controlled by the person who produced them.  Returning to the list of deviations from the naïve model given above, we see that the situation has reversed:\n\n1. The values of machine intelligences can (probably, eventually) be directly determined by their owners or predecessors. If at time *t* 1% of the world’s machine intelligences share my values and own 1% of the world’s resources, then 1% of all new machine intelligences will also share my values and at time *t*+1 it’s likely to also be the case that 1% of the world’s machine intelligences share my values and own 1% of the world’s resources.\n2. A capital holder with 1% of the world’s resources owns about 1% of the world’s machine intelligences, and so also captures 1% of the world’s labor income.\n3. In a world where most “individuals” are machine intelligences, who can argue as persuasively as humans and appear as sympathetic as humans, there is a good chance that (at least in some states) machine intelligences will be able to secure significant political representation. Indeed, in this scenario the complete oppression of machine intelligences would be something of a surprisingly oppressive regime. If machine intelligences secure equal representation, and if 1% of machine intelligences share my values, then there is no particular reason to expect redistribution or other political maneuvering to reduce the prevalence of my values.\n4. In a world where machine intelligences are able to perfectly replace a human as a manager, the challenge of finding a successor with similar values may be much reduced: it may simply be possible to design a machine intelligence who exactly shares their predecessor’s values and who can serve as a manager. Once technology is sufficiently stable, the same manager (or copies thereof) may persist indefinitely without significant disadvantage.\n\nSo at least on a very simple analysis, I think there is a good chance that a world with human-level machine intelligence would be described by the naïve model.\n\nAnother possible objection is that a capital owner who produces some resources exerts imperfect control over the outputs–apart from the complications introduced by humans, there are also random and hard-to-control events that prevent us from capturing all of the value we create. But on closer inspection this does not seem to be such a problem:\n\n* If these “random losses” are real losses, which are controlled by no one, then this can simply be factored into the growth rate. If every year the world grows 2% but 1% of all stuff is randomly destroyed, then the real growth rate is 1%. This doesn’t really change the conclusions.\n* If these “random losses” are lost to me but recouped by someone else, then the question is “who is recouping them?” Presumably we have in mind something like “a random person is benefiting.” But that just means that the returns to being a random person, on the lookout for serendipitous windfalls at the expense of other capital owners, have been elevated. And in this world, a “random person” is just another kind of capital you can own.  A savvy capital owner with *x*% of the world’s resources will also own *x*% of the world’s random people. The result is the same as the last case: someone who starts with *x*% of the resources can maintain *x*% of the resources as the world grows.\n\nImplications\n------------\n\nIf we believe this argument, then it suggests that the arrival of machine intelligence may lead to a substantial crystallization of influence. By its nature, this would be an event with long-lasting consequences. Incidentally, it would also provide the kind of opportunity for influence I was discussing in my [last post](https://rationalaltruist.com/2014/05/04/we-can-probably-influence-the-far-future/).\n\nI find this plausible though very far from certain, and I think it is an issue that deserves more attention. Perhaps most troubling is the possibility that in addition to prompting such crystallization, the transition to machine intelligences may also be an opportunity for influence to shift considerably—perhaps in large part to machines with alien values. In Nick Bostrom’s [taxonomy](http://www.nickbostrom.com/existential/risks.html), this suggests that we might be concerned about the world ending in a “whimper” rather than a “bang”: even without a particular catastrophic or disruptive event, we may nevertheless irreversibly and severely limit the potential of our future. \n\nIt is tempting to be cosmopolitan about the prospect of machine intelligences owning a significant share of the future, asserting their fundamental liberty to autonomy and self-determination. But our cosmopolitan attitude is itself an artifact of our preferences, and I think it is unwise to expect that it (or anything else we value) will be automatically shared by machine intelligences any more than it is automatically shared by bacteria, self-driving cars, or corporations.", "date_published": "2014-05-15T00:10:34Z", "authors": ["Tessa"], "summaries": [], "tags": ["Artificial intelligence", "Review crosspost", "Economics", "AI safety", "Paul Christiano"], "karma": 21, "votes": 6, "words": 1859, "modified_at": "2021-12-08T11:18:44.755Z", "comment_count": 0}
{"id": "9cecd25777a2e1372476367b9f4c7413", "title": "Good policy ideas that won’t happen (yet)", "url": "https://forum.effectivealtruism.org/posts/n5CNeo9jxDsCit9dj/good-policy-ideas-that-won-t-happen-yet", "source": "eaforum", "source_type": "GreaterWrong", "text": "Key points:\n-----------\n\n\n* Over the last year the Centre for Effective Altruism has had over a dozen meetings with UK policymakers and tested the waters on a range of policies we thought might have significant positive benefits for the world.\n* In most cases we quickly found that they could not currently attract political support, for various reasons.\n* Here we discuss two more we investigated in greater detail. These were both aimed at improving the depth of knowledge of policymakers about risks from new and upcoming technologies.\n* These turned out to be premature, as they were policies to deal with issues about which there is no academic consensus on the nature of the problem, let alone the appropriate response. More groundwork needs to be done before there will be majority support for significant policy changes in these areas.\n* Nonetheless we had successes in raising the profile of unprecedented technological risks in Government via a report we wrote which was widely circulated through several departments and read by senior civil servants, advisors, and politicians.  We were also invited to contribute a chapter on existential risk to the Government Chief Scientist’s Annual Report.\n\n\nCross-posted from the [Global Priorities Project](http://www.fhi.ox.ac.uk/research/global-priorities-project/)\n--------------------------------------------------------------------------------------------------------------\n\n\nIntroduction\n------------\n\n\nDuring the past year the Centre for Effective Altruism has been engaged in ongoing policy discussions with policymakers in the UK Government.  We generated hundreds of policy options and had many meetings with policymakers within the UK Government, both party political advisors, politicians and civil servants.  This post describes some of our learnings from this process.  These learnings will probably be obvious to anyone with experience of the policy-making process, but I hope that discussing them in more detail will be useful for people less familiar with policy making who attempt to do policy work going forwards.  This work was primarily carried out by Toby Ord, Nick Beckstead, William MacAskill, Owen Cotton-Barratt, Robert Wiblin, Haydn Belfield, and myself.  \n\n\n\nOne of our motivations for engaging in this work is that developed countries’ governments typically control budgets of the order of a trillion $US ($10^12) per year, and so moving a small portion of this budget could result in large amounts of money moved to effective causes.  Paul Christiano discusses the potential value of engaging in politics in more detail [here](http://80000hours.org/blog/310-an-estimate-of-the-expected-influence-of-becoming-a-politician).  \n\n\n\nOver the past year we have devised many outlines for possible policies which aimed to, among other things, reduce poverty, reduce animal suffering, improve decision making, improve scientific progress, or reduce extinction risk.  The policymakers thought that many of these policies were promising or interesting, but weren’t excited enough about any specific initiatives to really push for them.  The overwhelming majority of the proposals that we presented were set aside by policymakers without further investigation.  Most of those initiatives that were investigated further had problems associated with them, did not provide the benefits that we were initially hoping for, or simply weren’t promising enough for the policymakers to push for them publicly.  While we will not be making the full list public at this time, there are a number of themes that emerged from the policies that were not taken up.  \n\n\nPolitical feasibility\n---------------------\n\n\nThe most common reason for policies to be rejected was because they were politically infeasible.  We realised that the initiatives that were most likely to progress were those that would incur little to no financial or political cost to the politicians.  It may be possible to overcome the preference for zero or low financial and political cost by building supporting coalitions, but I will save the discussion for [later is this post](https://docs.google.com/document/d/1ySbMVhkxZx6ot--gluvjzueD3yImhhgJkuQ8iaUR7_c/edit#heading=h.8y6krcxk29wt).  \n\n\n### Financial cost\n\n\nMany of the policies and initiatives we suggested were rejected because they would cost Government money.  Policies ruled out for this reason included:\n\n\n* Funding research to [improve forecasting](http://files.givewell.org/files/conversations/Tetlock%202-12-14%20(public).pdf)\n* Increasing funding for research into [global catastrophic risks](http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501)\n\n\n\nOthers were rejected because they would cost some voters money, even if they would save more money for others.  Policies ruled out for this reason included:\n\n\n* Taxing all non-free-range meat products\n* Stopping subsidies of certain agricultural products\n\n\n### Political cost\n\n\nMany policies were ruled out because of the political, financial and time opportunity cost.  One way of conceiving of this objection is that the policymakers were unwilling to spend [political capital](http://en.wikipedia.org/wiki/Political_capital) because they had more promising ways in which to spend their time and money.  This could be because the policy was weird, unpopular, or could be blocked by special interest groups.  Policies that were ruled out for these reasons include:\n\n\n* Creating more [disaster shelters](/ea/5r/improving_disaster_shelters_to_increase_the/) to protect against global catastrophic risks (too weird)\n* Increasing immigration via various mechanisms (too unpopular)\n* Setting minimum sizes for gestation crates for pigs (could be blocked by special interests)\n\n\n\n\n\n\n| Box 1: Over-simplistic policy analysis (or ‘why some scientists are bad at policy’)\nDuring my years as a climate scientist and campaigner, I had many opportunities to see scientists attempting to engage in policy poorly.  I would often see scientists:1. Identify a problem, e.g. climate change due to increasing EU road transport CO2 emissions\n2. Identify a possible response, e.g. reduce EU road transport emissions\n3. Identify a possible policy, e.g. require some fraction of EU petrol to come from EU biofuels\n4. Promote this policy\n\n\nOne of the problems with this approach is that the scientists have not engaged in policy analysis, for example by:* Identifying all of the other possible policy responses,\n* Defining our aims, not just in the specific area the policy is addressing, but also in terms of limiting other potentially negative consequences of the policy,\n* Estimating how well these possible policies achieve our aims,\n* And assessing the trade-offs between the policies to identify the best policy.\n\n\nAnother problem is that they have not analysed the political landscape or initiated stakeholder engagement, for example by:* Speaking with different stakeholders to determine how much support and opposition there is for these different policies\n* Considered tweaking the policy in a number of ways to bring potential allies on board and neutralise potential opponents\n* Used tools such as power matrices to estimate whether it would be possible to build a coalition influential enough to push any of these policies through to implementation.\n\n\nBy moving straight from step three to four above we ignore key aspects of the policy-making process and end up promoting policies that are usually sub-optimal from an efficiency perspective, and are also politically infeasible.  \n\nOne way in which scientists can engage with the political process successfully is by proposing their policy and ‘passing the baton’ on to policy analysts who can then compare it to the other policies on the table.  But the scientist should not be surprised if their policy turns out to be sub-optimal or politically infeasible.  Indeed this is probably the most likely outcome for a policy that has been proposed by someone who is not an expert in this policy area.  However if the policy analyst is in favour of the idea, a powerful partnership can develop in which the scientist acts as a scientific advisor to an ongoing policy development process.  One example of this occurring is the role that Dr. Drew Shindell played in the development of the Climate and Clean Air Coalition, which was largely developed by the US State Department with Dr. Shindell acting as a scientific advisor to the process.  \n\nNote that Kingdon’s [‘multiple streams’](http://www.fsu.edu/~pea/Multiple_Streams.html) model of policy change suggests that [‘policy windows’](http://ww2.publicpolicy.utoronto.ca/ppgr/PPGGlossary/PQ/PolicyWindow/Pages/default.aspx) caused by the shifting political landscape could allow previously-rejected policies to be implemented.  Under this model a scientist who is unable to tell when a policy window has opened could have success in continuously promoting policy over a long period of time and hoping that at some point a policy window may open.  An example of where this may have occurred is in the story that is told about the creation of an observational program to monitor ocean currents in the North Atlantic.  These measurements had been desired by the scientific community for some time, but according to this story it was not until a scientist mentioned the problem of a potential slow-down in the ocean currents and the need to monitor them to then UK Prime Minister Tony Blair that funding became available for the programme.  \n\nA potentially more effective option for the scientists pushing for their policy continuously would be for them to ally with a policy analyst with more expertise in analysing the political landscape, who could monitor the landscape and report back to the scientist if a policy window opens or has a chance of opening soon.  This may have been the case with the Climate and Clean Air Coalition, which some commentators said was Obama’s attempt to claim a success in tackling climate change in the run up to the 2012 general election in order to woo back environmental voters who had been unimpressed by his record on climate change to date.  This policy window may have caused policy analysts at the State Department to implement Dr. Shindell, and others’ long-standing policy recommendations.  \n\nThere is plenty more work that needs doing in identifying possible responses to problems that effective altruism community has identified (i.e. in step two above), before the community can focus the majority of its efforts on proposing policies (see step three).  However, if the effective altruism community would like to do some of this policy analysis, political landscape analysis and stakeholder engagement itself rather than relying on busy and potentially disinterested policy analysts and campaign strategists, we will need to develop expertise in these areas.   |\n\n\n\n\nMore initiatives that won’t work (yet)\n--------------------------------------\n\n\nOnce the majority of our proposals had been ruled out due to not being clearly politically feasible, we had a much smaller number of options to investigate further.  Of these, several failed because they were too advanced relative to our current understanding, and required a majority to support them.  I discuss two examples in more detail below.  \n\n\n### An Intergovernmental Panel on Biotechnological Risk\n\n\n#### The proposal\n\n\nSet up a body, modelled on to the [Intergovernmental Panel on Climate Change](http://en.wikipedia.org/wiki/Intergovernmental_Panel_on_Climate_Change) (IPCC), to synthesise knowledge on biotechnological risks.  \n\n\n#### The appeal\n\n\nThe IPCC had a huge impact on the climate change debate, and even won a Nobel peace prize \"for their efforts to build up and disseminate greater knowledge about man-made climate change, and to lay the foundations for the measures that are needed to counteract such change\".  Presumably an Intergovernmental Panel on Biotechnological Risk could do the same for biotechnological risk?  \n\n\n#### The stumbling blocks\n\n\nThe IPCC was set up by the scientific community to illustrate to the world that there was a scientific consensus on climate change.  The IPCC does not do original research, and does not progress the field other than by recording the state of climate science research every six or seven years.  The consensus in the field on what to do about the risks arising from biotechnology is not as uniform as the state of climate science was when the IPCC was set up.  There are still a wide range of views on questions such as whether certain types of potentially dangerous research should be undertaken, and whether creating novel dangerous pathogens in the lab is overall a net harm.  \n\n\n\nMy understanding from [Prof. Steve Stedman](http://cisac.stanford.edu/people/stephen_j_stedman/), former Assistant Secretary General to the United Nations, is that he spoke with over fifty experts about the question of setting up an IPCC-equivalent body in this area and concluded that there were neither high-level experts willing to champion the initiative, nor was there a general feeling that the initiative would succeed, in part due to the lack of consensus.\n\n\n\nIPCC authors are not paid for their time, and so they either volunteer their time, or use funding from other grants to cover their costs.  This aspect also made the initiative unappealing to potential champions of the initiative.  \n\n\n\nAnother problem is that the incentives are aligned rather differently.  When climate scientists raised the alarm about climate change, it did not curtail their research freedom, and increased the future funding to climate science.  If biotechnologists raise the alarm about biotech risk, their proposals will largely be imposing restrictions or large costs on their own and their peers’ research.  Additionally, the overall effect may well be to reduce the total amount of funding to their area, though this is uncertain.  \n\n\n\nProf. Stedman conducted the most in depth investigation to date of the possibility of setting up an IPCC for biotechnology risk.  He was convinced enough that it was unlikely to go ahead that after his investigation he returned the remainder of the funding that he had been given to set up the initiative and moved on to other things.  \n\n\n### House of Commons/Lords Science and Technology Committee inquiry into unprecedented technological risk\n\n\n#### The proposal\n\n\nThe House of Commons/Lords Science and Technology Committee would run an inquiry into unprecedented technological risk, particularly focusing on its impact on existential risk.  \n\n\n#### The appeal\n\n\nThe Government must respond in writing to the committee’s reports, and this could allow the committee to influence government policy on this issue.  We also hoped that we could use this as a way to cause people to investigate this issue and develop possible responses.  \n\n\n#### The stumbling blocks\n\n\nHouse of Commons/Lords committees such as this one cannot do original research, and largely synthesise the views of the witnesses that they call on.  As there has been little research done into policy responses to existential risk, and there is no consensus in the area, it is unlikely that the committee would make strong recommendations to Government.  \n\n\n\nAdditionally, in order to launch the inquiry we would need multiple sympathetic MPs or Lords on the committee.  The inquiry would need to be led by a particularly sympathetic member, and it would take up a large amount of their time.  This is time that could otherwise be put to potentially more useful initiatives.  \n\n\n\nThis may be a useful initiative to take up in future when there has been more research done into responses to unprecedented technological risks and their impact on existential risk, particularly once those responses have been developed into prioritised policy proposals.  \n\n\n### Communicating expert opinion\n\n\nBoth of these failed proposals are about communicating the majority view of experts to decision-makers and interested parties.  The reason they failed is because the field of existential risk reduction is so new that the majority of experts do not yet hold similar views on the issue.  For example, even within the [Future of Humanity Institute](http://www.fhi.ox.ac.uk/) there is significant internal disagreement on key elements of what we should be doing to reduce existential risk.  This suggests there is much more research to be done in this area before we can start implementing many of our possible responses.  \n\n\nThe rational model of policy analysis\n-------------------------------------\n\n\nThere are [many models](http://www.amazon.co.uk/Lobbying-Policy-Change-Wins-Loses/dp/0226039455) of the policy-making process.  [Schattschneider’s](http://en.wikipedia.org/wiki/Elmer_Eric_Schattschneider) [‘expansion of conflict’](http://www.amazon.com/The-Semi-Sovereign-People-Realists-Democracy/dp/0030133661) model suggests that the policy-making process is largely determined by the extent to which parties are able to expand the process to include additional groups that are supportive of their point of view.  One of the more popular models is [Kingdon’s](http://en.wikipedia.org/wiki/John_W._Kingdon) [“multiple streams” model](http://www.amazon.co.uk/Alternatives-Policies-Epilogue-Classics-Political/dp/020500086X), which suggests that policies are only implemented when ‘problems, policies and politics’ align to create a window of opportunity.  Kingdon also introduces the idea of a [‘policy entrepreneur’](http://en.wikipedia.org/wiki/Political_entrepreneur) as someone who seeks to gain benefits in exchange for implementing policies and initiatives that are popular.  Baumgartner and Jones [focus](http://www.amazon.co.uk/Agendas-Instability-American-Politics-Political/dp/0226039390) on issue-definition and venue-shopping to suggest that issues usually follow a stable policy direction but that this stability is interjected with periods of rapid, unpredictable change.  [Schneider, Ingram](http://www.amazon.com/Policy-Design-Democracy-Larason-Schneider/dp/0700608443), [Stone](http://www.amazon.com/Policy-Paradox-Political-Decision-Making/dp/0393976254) and others focus on issue framing as the factor which decides which policies are implemented and which are not.  These are just a small subset of the more popular models of the policymaking process which have been proposed.  These models cover different aspects of the decision-making process, and do not always agree where they overlap, as we might expect in a real-world process as complex as this one.  \n\n\n\n[Simon’s](http://en.wikipedia.org/wiki/Herbert_A._Simon) [rational model](http://en.wikipedia.org/wiki/Policy_analysis#Rational_model) of policy analysis is another model that we can use to frame our discussion of the policymaking process.  It is a simple model that I will discuss in more detail is I believe it can be used to illustrate the broader picture of why many of our policies were not taken up.  It was originally put forward by [Herbert A. Simon](http://en.wikipedia.org/wiki/Herbert_A._Simon) to describe a process one could take to develop public policy.  Below is an adaptation of [Ian Thomas](http://books.google.com/books/about/Environmental_Policy.html?id=QXkNGwWkF10C)’s description of [Simon](http://en.wikipedia.org/wiki/Herbert_A._Simon)’s model of policy development: \n\n\n1. Intelligence gathering— data and potential problems and opportunities are identified, collected and analyzed.\n2. Identifying a problem\n3. Identifying options to solve this problem\n4. Assessing the consequences of all options\n5. Relating consequences to values— with all decisions and policies there will be a set of values which will be more relevant (for example, economic feasibility and environmental protection) and which can be expressed as a set of criteria, against which performance (or consequences) of each option can be judged.\n6. Choosing the preferred option— given the full understanding of all the problems and opportunities, all the consequences and the criteria for judging options.\n\n\n\nStages in the model can occur concurrently and stages can be skipped.  \n\n\n### Criticism of the rational model\n\n\nLike any simple model, the ‘rational model’ ignores some important factors.  For example, it ignores the political landscape and vested interests, and assumes the Government is a unitary rational actor in a static system.  There is a literature of case studies both showing [benefits](http://www.sciencedirect.com/science/article/pii/S0168851096008895) and [drawbacks](http://www.untag-smd.ac.id/files/Perpustakaan_Digital_2/PUBLIC%20POLICY%20(Public%20Administration%20and%20public%20policy%20125)%20Handbook%20of%20Public%20Policy%20Analysis%20Th.pdf) of the rational model in different circumstances.  Nonetheless it can be helpful to us in providing a hypothesis as to why our policies were unworkable, and where we might focus our attention in the future.  \n\n\n### An adapted rational model\n\n\nThe rational model has been adapted in a [number](http://www.sciencedirect.com/science/article/pii/S0168851096008895) [of](http://www.msu.edu/course/prr/389/Pattonsawicki.doc) [ways](http://www.minneapolisfed.org/research/qr/qr1011.pdf) throughout the [literature](http://cursodeposgrado.files.wordpress.com/2011/08/libro-sabatier.pdf).  Here I suggest an extension that may help explain some of our experience.  As with all models based on the rational model, it will have a number of shortfalls, but is hopefully still useful.  It proposes that policy development largely goes through the following stages:\n\n\n![](http://res.cloudinary.com/cea/image/upload/v1667995874/mirroredImages/n5CNeo9jxDsCit9dj/lcdwczwzau59nzyg3pz2.png)\n\n\nLike in the original rational model, stages can occur concurrently and stages can be skipped.  I make a distinction between responses (desired outcomes, e.g. reduce emissions), and policies (methods of achieving the desired outcomes, e.g. a cap and trade regime) as I did in the box above.  \n\n\n\nWe can represent our progress towards a policy-based solution for a given problem in this model.  For example, on existential risk due to artificial intelligence, the community is perhaps working on gathering intelligence and identifying the problem (stages 1 and 2A), with limited work on identifying responses and building coalitions around the problem (stages 3A and 2B).  Thus it would not be as useful for the community to shift its attention to working on stages 4A, 3B, 4B or 5 at this point.  There may be space for individuals to do pioneering work in these areas, but in general the model would suggest that they will find progress easier once more progress has been made in the previous stages.  \n\n\n\nOn climate change mitigation (reducing greenhouse gas emissions) in the UK, most of the work that remains to be done is on developing policies, building coalitions around various responses and policies and attempting get those policies implemented (stages 4A, 3B, 4B and 5).  This is one of the reasons I moved away from being a climate scientist, where I was predominantly working on stage one.  This is not to say that there is no more work to be done in this area; further highlighting the dangers from climate change is still useful, just perhaps not as useful as well-directed work in the policy arena.  In the US, it is plausible that the coalitions around the problem (stage 2B) are not yet strong enough, and so there is potentially more work to be done there.  \n\n\n\nWe can view the intergovernmental panel on biotechnological risk as an attempt to build a coalition around responses to biotech risk once we had a coalition in place around the problem of biotech risk (moving from stage 2B to stage 3B).  The problem in this case was that there was not a pre-existing coalition developed around the problem large enough to represent the majority view.  \n\n\n\nWe investigated using the House of Commons/Lords Select Committee to do response identification, policy identification, and advocacy (stages 3A onwards), however the committees are unable to carry out response and policy identification (stages 3A and 4A) as these are not within the Committees’ mandates.  Indeed I cannot think of any examples of Government committees that do this form of response and policy research, except perhaps in extreme circumstances such as during times of crisis. We should perhaps view the Commons/Lords Committees as part of moving from a coalition around a specific policy to getting that policy implemented (moving from stage 4B to stage 5).  We are not yet ready for this stage as policies and coalitions around responses (stages 4A and 3B) have not been adequately developed, let alone coalitions around policies (stage 4B).  \n\n\n### Criticisms\n\n\nThere are many ways that this model may mislead us, particularly if it is followed in too formulaic a fashion.  I will list just some of these criticisms below.  \n\n\n\nFor example, by building visible coalitions we can also ignite opposition, and so need to be wary of the political landscape in order to improve the chances of our activities have an overall positive effect.  \n\n\n\nThis model could also cause us to miss easy wins, in which the size of the coalition required to cause the policy to be implemented is surprisingly small, if we were focusing attention on earlier stages in the model.  \n\n\n\nThis model doesn’t properly incorporate the possibility of larger-scale political change (such as a change of party after an election), nor does it account for external events (such as a war or various types of disaster) that can dramatically alter coalitions and the thresholds required for policy implementation.  \n\n\n\nAdditionally, by simply modelling ‘coalitions’ the model ignores the different roles played by different actors such as the media, the public, interest groups, political elites, and others in the process.  A more fine grained model of the political landscape and stakeholder engagement will be useful in the later stages of the model.  \n\n\n\nSo this is a simplistic model, yet it allows us to make hypotheses about where further work may be most useful to help move towards policy solutions to problems.  These hypotheses can then be probed further using other methods.  \n\n\nOutcomes and next steps\n-----------------------\n\n\nOver the course of our policy engagement to date we have learned much, and also had some significant successes.  Our report on unprecedented technological risks was widely distributed and read by politicians and civil servants across a number of departments.  We were also invited to contribute a chapter on existential risk to the UK Government Chief Scientist’s Annual Report on risk.  \n\n\n\nGoing forwards, we have decided to focus more of our effort on building coalitions around the problem and identifying solutions (stages 2B and 3A).  One of the reasons for this is that we found it easier than expected to gain access to policymakers, and thus when we have more politically feasible responses to problems and more support for them, we think it will be possible for us to take these to policymakers.  Nonetheless, we will continue to respond to policy opportunities in a more reactive fashion, and will continue to meet with senior policymakers and politicians as opportunities arise.  \n\n\n\nThis work was carried out by the Global Priorities Project, which is currently fundraising to hire an additional researcher for the project and a full-time project manager who will also lead our outreach to Governments and foundations.  If you would be interested in contributing to this effort please [contact me](mailto:niel.bowerman@centreforeffectivelaltruism.org) or [Robert Wiblin](mailto:robert.wiblin@centreforeffectivelaltruism.org). We have more information on our current plans which you can read [here](https://drive.google.com/file/d/0B8_48dde-9C3a3dscDJ3eHZwN3c/edit?usp=sharing).\n\n\nSummary\n-------\n\n\nSome of the key points I feel I have learned from our interactions with policymakers are:\n\n\n* We need more research into potential responses to unprecedented technological risks.\n* Once we have a better idea of the responses to unprecedented technological risks we would like to see implemented, we can start researching policy implementations of those responses.\n* If we want to implement policies that incur financial cost or political capital, then we will need to start building political coalitions around the problems, responses and policies that we are championing.\n* Coalition-building is something that we do not have much expertise in among the people I spoke to within the effective altruism community.\n* Initiatives that build upon a majority view among experts are not useful unless there is already a majority view among experts.\n* It was easier than expected to discuss our ideas with senior civil servants and politicians.\n\n\n\nAdditional points not mentioned above, but which I felt I learned through this process:\n\n\n* It would have been useful if we had been even more familiar with what the Government was already doing on unprecedented technological risks.\n* Policymakers were more willing to accept that there was a problem in the area of unprecedented technological risk than I was expecting.  This shifted the conversation over to specific policy responses, which is where I felt we had the least to say.\n* Policymakers’ believing that there was a problem in the area of unprecedented technological risks was not sufficient to cause them to put energy into developing responses.  It was expected that we were the people who would do this work.\n\n\n\nThanks to Nick Beckstead, Owen Cotton-Barratt, Steve Stedman, Toby Ord, Haydn Belfield, Robert Wiblin, William MacAskill and David Frame for the work, comments and conversations that went into this post.  \n\n\n \n\n\nNiel Bowerman is a co-founder and Director of Special Projects at the Centre for Effective Altruism.  He was a member of Obama’s 2008 presidential election Energy and Environment Policy Team, and was Climate Science Advisor to the President of the Maldives.  He has a PhD (DPhil) in physics from the University of Oxford.", "date_published": "2014-09-11T12:29:11Z", "authors": ["Niel_Bowerman"], "summaries": [], "tags": ["Biosecurity & pandemics", "Biosecurity", "AI safety", "Policy", "AI governance"], "karma": 28, "votes": 21, "words": 4341, "modified_at": "2021-02-12T23:48:31.649Z", "comment_count": 8}
