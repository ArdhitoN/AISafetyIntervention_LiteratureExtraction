{"id": "fbe15fcfceb0a3dcc4e078e04a26b556", "title": "The Myth of the Rational Voter: Why Democracies Choose Bad Policies", "url": "https://www.goodreads.com/book/show/698866.The_Myth_of_the_Rational_Voter", "source": "special_docs", "source_type": "book", "text": "[]\n\nTHE MYTH OF THE RATIONAL VOTER\n\nTHE MYTH OF\n\nTHE RATIONAL VOTER\n\nWHY DEMOCRACIES CHOOSE BAD POLICIES\n\nWith a new preface by the author\n\nBRYAN CAPLAN\n\n[pub]\n\nCopyright © 2007 by Princeton University Press\n\nPublished by Princeton University Press, 41 William Street, Princeton,\n\nNew Jersey 08540\n\nIn the United Kingdom: Princeton University Press, 6 Oxford Street,\n\nWoodstock, Oxfordshire OX20 1TW\n\nAll Rights Reserved\n\nSeventh printing, and first paperback printing, with a new preface by the author, 2008 Paperback ISBN: 978-0-691-13873-2\n\nThe Library of Congress has cataloged the cloth edition of this book as follows\n\nCaplan, Bryan Douglas, 1971–\n\nThe myth of the rational voter : why democracies choose\n\nbad policies / Bryan Caplan.\n\np.    cm.\n\nIncludes bibliographical references and index.\n\nISBN-13: 978-0-691-12942-6 (cloth : alk. paper)\n\nISBN-10: 0-691-12942-8 (cloth : alk. paper)\n\n1. Economic policy. 2. Democracy. 3. Political sociology. 4. Representative government and representation. 5. Rationalism. I. Title.\n\nHD87.C36 2006\n\n320.6—dc22      2006030855\n\nBritish Library Cataloging-in-Publication Data is available\n\nThis book has been composed in Utopia\n\nPrinted on acid-free paper. ∞\n\npress.princeton.edu\n\nPrinted in the United States of America\n\n10   9   8   7\n\nThe serious fact is that the bulk of the really important\nthings economics has to teach are things that people\nwould see for themselves if they were willing to see.\n\n—Frank Knight, “The Role of Principles\nin Economics and Politics”\n\nI have often wondered why economists, with these absurdities\nall around them, so easily adopt the view that men\nact rationally. This may be because they study an economic\nsystem in which the discipline of the market ensures\nthat, in a business setting, decisions are more or less\nrational. The employee of a corporation who buys something\nfor $10 and sells it for $8 is not likely to do so for\nlong. Someone who, in a family setting, does much the\nsame thing, may make his wife and children miserable\nthroughout his life. A politician who wastes his country’s\nresources on a grand scale may have a successful career.\n\n—Ronald Coase, “Comment on Thomas W. Hazlett”\n\n[T]he superstitions to be feared in the present day are\nmuch less religious than political; and of all the forms of\nidolatry I know none more irrational and ignoble than this\nblind worship of mere numbers.\n\n—William Lecky, Democracy and Liberty\n\nCONTENTS\n\nPREFACE TO THE PAPERBACK EDITION\n\nACKNOWLEDGMENTS\n\nINTRODUCTION\n\nThe Paradox of Democracy\n\nCHAPTER 1\n\nBeyond the Miracle of Aggregation\n\nCHAPTER 2\n\nSystematically Biased Beliefs about Economics\n\nCHAPTER 3\n\nEvidence from the Survey of Americans and Economists on the Economy\n\nCHAPTER 4\n\nClassical Public Choice and the Failure of Rational Ignorance\n\nCHAPTER 5\n\nRational Irrationality\n\nCHAPTER 6\n\nFrom Irrationality to Policy\n\nCHAPTER 7\n\nIrrationality and the Supply Side of Politics\n\nCHAPTER 8\n\n“Market Fundamentalism” versus the Religion of Democracy\n\nCONCLUSION\n\nIn Praise of the Study of Folly\n\nNOTES\n\nREFERENCES\n\nINDEX\n\nPREFACE TO THE PAPERBACK EDITION\n\nThe Myth of the Rational Voter was far more successful than I had expected. The real surprise, though, was how reasonable the critics have been. Admittedly, I aimed for broad appeal. From the start, my goal was to transcend disciplinary and ideological boundaries—to find some common ground for people with common sense, and build on it. But I was skeptical that my outreach would be successful. After all, the book does not take a contrarian position in a dry academic debate; it questions the dogmas of the secular religion of democracy, and prods the reader to leave the church.\n\nApparently, many prominent thinkers were already quietly questioning these dogmas. I half expected the Economist to confess to doubts about voter rationality, but I was shocked when Nicholas Kristof named it “the best political book this year” in the New York Times.¹ Most reviews were less enthusiastic, but only a few claimed that voters are rational, or stood up for what I call “popular economic misconceptions.” Although several colleagues at George Mason have criticized my “elitism,” my real mistake was underestimating how fair elite critics would be.\n\nStill, almost every reviewer posed objections—some of which were quite consistent with my thesis, or even implied by it. The Economist was right to joke that “[Caplan’s] book is a treat, but he will never win elective office.” I also sympathize with its claim that “Caplan is better at diagnosis than prescription,”² but I would rephrase the objection. You should not blame the prescription simply because the patient refuses to take his medicine. The Myth of the Rational Voter contains many workable reforms, but due to voter irrationality they are unlikely to be tried.\n\nThis does not mean that nothing can be done; the book is not a plea for fatalism. But progress is likely to come slowly, if it comes at all. There is some slack in the democratic system. As the final chapter explains, if you want to push policy in a more sensible direction, you can take advantage of this slack. I know I do: I doubt the voters of Virginia want me to write and lecture against popular misconceptions, but for reasons that remain a mystery, they cut me enough slack to do so.\n\nAnother common criticism is that I ignore the symbolic and/or legitimating power of democracy. As Louis Menand writes in the New Yorker,\n\n[T]he group that loses these contests must abide by the outcome, must regard the wishes of the majority as legitimate. The only way it can be expected to do so is if it has been made to feel that it had a voice in the process, even if that voice is, in practical terms, symbolic. A great virtue of democratic polities is stability. The toleration of silly opinions is (to speak like an economist) a small price to pay for it.³\n\nComplaints like this miss a point that I make repeatedly throughout the book: democracy comes in degrees. We do not have to choose between abandoning democracy and tolerating whatever foolish policies the majority favors. The American polity has been quite stable despite the existence of supermajority rules, the Supreme Court, and independent agencies like the Federal Reserve. Democracy could be far more limited than it is now without risking civil unrest.\n\nA few critics practically see my project as self-contradictory. If my premise is that the economic consensus is reliable, how can I reach a conclusion that the economic consensus rejects? As Christopher Hayes colorfully puts it,\n\n[T]he book eats its own tail. Caplan wants to grant a presumptive authority to the consensus view of economists, but the consensus view of economists is that voters are rational, which is, of course, precisely the position he wants to convince us is wrong.⁴\n\nThis complaint would be airtight if my premise were that the economic consensus is infallible. But my actual premise is merely that economists, like other experts, deserve the benefit of the doubt—and that the burden of proof rests on those who question the expert consensus. Since the rational voter assumption is part of that consensus, my responsibility as a naysayer is to refute it—which is precisely why I needed to write the book.\n\nThe most serious criticism of my work is also the strangest. A number of critics—such as Daniel Casse in the Wall Street Journal—deny that popular misconceptions actually influence policy.\n\n[N]owhere in “The Myth of the Rational Voter” does Mr. Caplan demonstrate that dumb voter bias triggers bad public policy.\n\nTake free trade. Mr. Caplan reports that support for free trade hit bottom in 1977, when only 18% of Americans favored eliminating tariffs. Yet three years later, Ronald Reagan campaigned on a platform of free trade and proceeded to sign historic free-trade agreements with Canada and laid the groundwork for free trade with Mexico.\n\nCasse concludes that “voter bias has fueled some foolish national debate in recent years but imposed very little foolish national policy.”⁵ In effect, he defends democracy by saying that the voice of the people falls on deaf ears.\n\nThe Myth of the Rational Voter explicitly states that democracies’ policies are better than you would expect given public opinion. But this does not imply that public opinion is unimportant. If voter bias has no effect on policy, why were extensive protectionist measures adopted in the first place? Why does protectionism remain after three decades of liberalization? The most convincing explanation is also the simplest: politicians backed the original measures to win votes; their successors remain reluctant to liberalize because they are afraid they will lose votes.⁶\n\nCasse may be right that, in recent years, voter bias has imposed few new foolish policies (though the Iraq War is a strong counterexample). But this is misleading in two ways. First, very little new national economic policy of any kind has been imposed in recent years, because gridlock keeps the status quo in place. Second, and more importantly, Casse focuses on how policies have changed instead of what policies exist. A democracy should not be judged a success merely because it refrains from making bad policies worse—or makes a half-hearted effort to correct long-standing mistakes.\n\nAfter all the friendly media coverage, a big question left in my mind is whether the book will actually change academic research. Mental inertia and conformity pressure inside the ivory tower are strong. Even professors who agree that voters are irrational may pursue intellectual dead ends because it is easier than starting over.\n\nStill, I am optimistic. Behavioral economics has never been stronger; it has become almost impossible to do applied economics without learning some empirical psychology. Behavioral political economy should not be far behind. With some luck, the conflict between what economists believe as researchers and what they believe as teachers will culminate in serious cognitive dissonance—and scientific progress. Once economists admit to themselves that voters are just like their students, only worse, they will be poised to unravel the mysteries of politics and policy.\n\nIf and when economists come to their senses, social scientists from other disciplines—especially political science—will hopefully give us a hand up. Complaints about “economists’ arrogance” are often off target. Considering how little attention most economists pay to empirical political science, though, I have to admit that there is some truth to the accusation. Economists’ disinterest in public opinion is especially egregious. How can we build models about conflicts between the public and special interests, for example, without even peeking at the vast literature about what human beings actually think and want? Fortunately, political scientists are unlikely to hold a grudge. In my experience, they are pleasantly surprised when an economist takes the time to ask them a question and listen to their response.\n\nMy other piece of advice to fellow economists is to write more books. In an article, you have enough space to challenge only one or two standard views. Unless you take the rest of the conventional wisdom for granted, you seem confusing at best, and crazy at worst. In a book, you have the time to candidly explain your whole position. Furthermore, even an unsuccessful book will probably have more readers than any of your articles. Personally, I wish I had started writing books sooner, and plan to focus on books for the rest of my career.\n\nAdmittedly, I may be biased by the excellent treatment I received from the good people at Princeton University Press. Director Peter Dougherty never stopped encouraging me during several years of writing. My editor, Tim Sullivan, expertly guided me through the whole journey from submission to distribution, and his wise advice was never more than an email away. Copy editor Richard Isomaki vastly improved my book the hard way—line by line. Jacket designer Frank Mahood did his job well enough to transform a book on voter irrationality into an impulse buy. Finally, my energetic publicist, Jessica Pellien, managed to sell an obscure economics professor to virtually every major media outlet in the country. How she did it, I don’t know; but I am most grateful.\n\nNotes\n\n1. “Vote for Me, Dimwit,” Economist, June 16, 2007, 42; Kristof, “The Voters Speak: Baaa!” New York Times, July 30, 2007, A19.\n\n2. “Vote for Me, Dimwit.”\n\n3. Menand, “Fractured Franchise,” New Yorker, July 9 & 16, 2007, 91.\n\n4. Hayes, “Who’s Afraid of Democracy?” In These Times, May 25, 2007, 40.\n\n5. Casse, “Casting a Ballot With a Certain Cast of Mind,” Wall Street Journal, July 10, 2007, D5.\n\n6. It is also worth mentioning that public opinion has grown markedly less protectionist over the last three decades. Would movement toward free trade have been possible if support for it were stuck at 18%?\n\nACKNOWLEDGMENTS\n\nI AM BLESSED with an abundance of argumentative but encouraging colleagues, but there are two to whom I am especially grateful.\n\nThe first is Don Boudreaux, who urged me to begin serious research on voter rationality right after the 1998 Public Choice Outreach Seminar. In a discipline where praise is extremely scarce, Don was quick to tell me that he loved my approach, and hasn’t stopped telling me since. I really wonder whether I would have written this book—or any of the papers it builds on—without Don’s support.\n\nThe second is Tyler Cowen, my constant critic. Since I joined the faculty at George Mason, Tyler has never failed to read my work and tell me what I’m doing wrong. No one has reviewed more drafts of this book than Tyler did, or asked harder questions. I can’t remember the last time we agreed, but I still feel like he taught me half of everything I know.\n\nI am also eternally grateful to the institution of lunch. Years of debate with lunchtime regulars Tyler, Robin Hanson, and Alex Tabarrok were required to turn my raw ideas into a finished product. And they were only the beginning. Scores of other lunch-goers have heard my views and given me feedback, including Scott Beaulier, David Bernstein, Tim Besley, Pete Boettke, Don Boudreaux, J. C. Bradbury, Geoff Brennan, Corina Caplan, Roger Congleton, Mark Crain, Eric Crampton, Gordon Dahl, Veronique de Rugy, Bill Dickens, Zac Gochenour, Rodolfo Gonzalez, Donald Green, Friedrich Heinemann, Bob Higgs, Randy Holcombe, Dan Houser, Jeff Hummel, Larry Iannaccone, Scott Keeter, Dan Klein, Arnold Kling, Ken Koford, George Krause, Timur Kuran, David Levy, Jacob Levy, Loren Lomasky, John Lott, Daniel Lurker, John Matsusaka, Kevin McCabe, Mitch Mitchell, Nathaniel Paxson, Ben Powell, Ilia Rainer, Carlos Ramirez, Joe Reid, Fab Rojas, Russ Roberts, Charles Rowley, Paul Rubin, Joe Salerno, Jim Schneider, Andrew Sellgren, Thomas Stratmann, Ed Stringham, Tom TerBush, Gordon Tullock, Dick Wagner, Walter Williams, and Donald Wittman.\n\nAs much fun as the lunches were, though, I especially want to thank those who read drafts of the manuscript and gave me detailed comments: Scott Beaulier, Pete Boettke, Eric Crampton, Tyler Cowen, Andrew Gelman, David Gordon, Robin Hanson, Michael Huemer, Dan Klein, Arnold Kling, Geoffrey Lea, David Levenstaum, Steve Miller, Nathaniel Paxson, Russ Roberts, Fab Rojas, Russ Sobel, Ilya Somin, Ed Stringham, Koleman Strumpf, Tim Sullivan, Dan Sutter, Alex Tabarrok, Gordon Tullock, Donald Wittman, and the referees at Princeton University Press. I also want to express my appreciation to the Kaiser Family Foundation, for freely sharing the data from the Survey of Americans and Economists on the Economy; Scott Beaulier, Steve Miller, Eric Crampton, Kail Padgitt, and Geoffrey Lea for excellent research assistance; my graduate students in microeconomics and public finance, and readers of my blog, for years of great feedback; and the Mercatus Center for generous support. Finally, I am very lucky that my wife has both a degree in economics and the daily patience to listen to my latest theory.\n\nMy apologies to anyone I missed. Can I make it up to you at our next lunch?\n\nTHE MYTH OF THE RATIONAL VOTER\n\nIntroduction\n\nTHE PARADOX OF DEMOCRACY\n\nA supporter once called out, “Governor Stevenson, all\nthinking people are for you!” And Adlai Stevenson answered,\n“That’s not enough. I need a majority.”\n\n—Scott Simon, “Music Cues: Adlai Stevenson”¹\n\nIN A DICTATORSHIP, government policy is often appalling, but rarely baffling. The building of the Berlin Wall sparked worldwide outcry, but few wondered, “What are the leaders of East Germany thinking?” That was obvious: they wanted to continue ruling over their subjects, who were inconsiderately fleeing en masse. The Berlin Wall had some drawbacks for the ruling clique. It hurt tourism, making it harder to earn hard currency to import Western luxuries. All things considered, though, the Wall protected the interests of elite party members.\n\nNo wonder democracy is such a popular political panacea. The history of dictatorships creates a strong impression that bad policies exist because the interests of rulers and ruled diverge.² A simple solution is make the rulers and the ruled identical by giving “power to the people.” If the people decide to delegate decisions to full-time politicians, so what? Those who pay the piper—or vote to pay the piper—call the tune.\n\nThis optimistic story is, however, often at odds with the facts. Democracies frequently adopt and maintain policies harmful for most people. Protectionism is a classic example. Economists across the political spectrum have pointed out its folly for centuries, but almost every democracy restricts imports. Even when countries negotiate free trade agreements, the subtext is not, “Trade is mutually beneficial,” but, “We’ll do you the favor of buying your imports if you do us the favor of buying ours.” Admittedly, this is less appalling than the Berlin Wall, yet it is more baffling. In theory, democracy is a bulwark against socially harmful policies, but in practice it gives them a safe harbor.³\n\nHow can this Paradox of Democracy be solved? One answer is that the people’s “representatives” have turned the tables on them. Elections might be a weaker deterrent to misconduct than they seem on the surface, making it more important to please special interests than the general public. A second answer, which complements the first, is that voters are deeply ignorant about politics. They do not know who their representatives are, much less what they do. This tempts politicians to pursue personal agendas and sell themselves to donors.⁴\n\nA diametrically opposed solution to the Paradox of Democracy is to deny that it regularly delivers foolish policies. You could insist that the public is right and “the experts” are wrong, openly defending the merits of protection, price controls, and so on. That is straightforward, but risky: It is akin to putting your client on the stand and opening him up to cross-examination. A less direct but safer stance—analogous to keeping your client from testifying—is to pick holes in the alleged mechanisms of democratic failure. You don’t have to show that your client is innocent if the prosecution lacks a coherent account of how the crime was committed. In the same way, you need not show that a policy is good if there is no coherent account of how it could be bad.\n\nDemocracy’s cleverest enthusiasts usually take this safer route.⁵ Especially in recent years, their strategy has been successful despite the intuitive appeal of stories about electorally safe politicians and ignorant voters. For reasons we will soon explore, these stories buckle or even break when critically analyzed. Without a credible account of how democracy falls short of its promise, the insight that it does fall short lives on borrowed time.\n\nThis book develops an alternative story of how democracy fails. The central idea is that voters are worse than ignorant; they are, in a word, irrational—and vote accordingly. Economists and cognitive psychologists usually presume that everyone “processes information” to the best of his ability.⁶ Yet common sense tells us that emotion and ideology—not just the facts or their “processing”—powerfully sway human judgment. Protectionist thinking is hard to uproot because it feels good. When people vote under the influence of false beliefs that feel good, democracy persistently delivers bad policies. As an old computer programming slogan goes, GIGO—Garbage in, garbage out.\n\nAcross-the-board irrationality is not a strike against democracy alone, but all human institutions. A critical premise of this book is that irrationality, like ignorance, is selective. We habitually tune out unwanted information on subjects we don’t care about. In the same vein, I claim that we turn off our rational faculties on subjects where we don’t care about the truth.⁷ Economists have long argued that voter ignorance is a predictable response to the fact that one vote doesn’t matter. Why study the issues if you can’t change the outcome? I generalize this insight: Why control your knee-jerk emotional and ideological reactions if you can’t change the outcome?\n\nThis book has three conjoined themes. The first: Doubts about the rationality of voters are empirically justified. The second: Voter irrationality is precisely what economic theory implies once we adopt introspectively plausible assumptions about human motivation. The third: Voter irrationality is the key to a realistic picture of democracy.\n\nIn the naive public-interest view, democracy works because it does what voters want. In the view of most democracy skeptics, it fails because it does not do what voters want. In my view, democracy fails because it does what voters want. In economic jargon, democracy has a built-in externality. An irrational voter does not hurt only himself. He also hurts everyone who is, as a result of his irrationality, more likely to live under misguided policies. Since most of the cost of voter irrationality is external—paid for by other people, why not indulge? If enough voters think this way, socially injurious policies win by popular demand.\n\nWhen cataloging the failures of democracy, one must keep things in perspective. Hundreds of millions of people under democratic rule enjoy standards of living that are, by historical standards, amazingly good. The shortcomings of the worst democracies pale in comparison with those of totalitarian regimes. At least democracies do not murder millions of their own citizens.⁸ Nevertheless, now that democracy is the typical form of government, there is little reason to dwell on the truisms that it is “better than Communism,” or “beats life during the Middle Ages.” Such comparisons set the bar too low. It is more worthwhile to figure out how and why democracy disappoints.⁹\n\nIn the minds of many, one of Winston’s Churchill’s most famous aphorisms cuts the conversation short: “Democracy is the worst form of government, except all those other forms that have been tried from time to time.”¹⁰ But this saying overlooks the fact that the governments vary in scope as well as form. In democracies the main alternative to majority rule is not dictatorship, but markets.\n\nDemocracy enthusiasts repeatedly acknowledge this.¹¹ When they lament the “weakening of democracy,” their main evidence is that markets face little government oversight, or even usurp the traditional functions of government. They often close with a “wake-up call” for voters to shrug off their apathy and make their voice heard. The heretical thought that rarely surfaces is that weakening democracy in favor of markets could be a good thing. No matter what you believe about how well markets work in absolute terms, if democracy starts to look worse, markets start to look better by comparison.\n\nEconomists have an undeserved reputation for “religious faith” in markets. No one has done more than economists to dissect the innumerable ways that markets can fail. After all their investigations, though, economists typically conclude that the man in the street—and the intellectual without economic training—underestimates how well markets work.¹² I maintain that something quite different holds for democracy: it is widely over-rated not only by the public but by most economists too. Thus, while the general public underestimates how well markets work, even economists underestimate markets’ virtues relative to the democratic alternative.\n\nChapter 1\n\nBEYOND THE MIRACLE OF AGGREGATION\n\nI am suspicious of all the things that the\naverage citizen believes.\n\n—H. L. Mencken, A Second Mencken Chrestomathy¹\n\nWhat voters don’t know would fill a university library. In the last few decades, economists who study politics have revitalized age-old worries about the people’s competence to govern by pointing out that—selfishly speaking—voters are not making a mistake. One vote has so small a probability of affecting electoral outcomes that a realistic egoist pays no attention to politics; he chooses to be, in economic jargon, rationally ignorant.²\n\nFor those who worship at the temple of democracy, this economic argument adds insult to injury. It is bad enough that voters happen to know so little. It remains bearable, though, as long as the electorate’s ignorance is a passing phase. Pundits often blame citizens’ apathy on an elections’ exceptionally insipid candidates. Deeper thinkers, who notice that the apathy persists year after year, blame voters’ ignorance on lack of democracy itself. Robert Kuttner spells out one version of the story:\n\nThe essence of political democracy—the franchise—has eroded, as voting and face-to-face politics give way to campaign-finance plutocracy . . . [T]here is a direct connection between the domination of politics by special interest money, paid attack ads, strategies driven by polling and focus groups—and the desertion of citizens. . . . People conclude that politics is something that excludes them.³\n\nYet the slogan “The solution for the problems of democracy is more democracy” sounds hollow after you digest the idea of rational ignorance. Voter ignorance is a product of natural human selfishness, not a transient cultural aberration. It is hard to see how initiatives, or campaign finance reform, or any of the popular ways to “fix democracy” strengthen voters’ incentive to inform themselves.\n\nAs the rational ignorance insight spread, it became an intellectual fault line in the social sciences. Economists, along with economically minded political scientists and law professors, are generally on one side of the fault line.⁴ They see voter ignorance as a serious problem, making them skeptical about using government intervention to improve market outcomes. Beneficial government action is possible in theory, but how could hopelessly uninformed voters be expected to elect politicians who follow through? The implication: “Voters don’t know what they’re doing; just leave it to the market.” Thinkers on the other side of the fault line downplay these doubts about government intervention. Once you discount the problem of voter ignorance, it is a short hop from “the policies beneficial in theory” to “the policies democracies adopt in practice.”\n\nIn time, rational ignorance spawned an expansive research program, known as public choice or political economy or rational choice theory.⁵ In the 1960s, finding fault with democracy bordered on heretical, but the approach was hardy enough to take root. Critiques of foolish government policies multiplied during the 1970s, paving the way for deregulation and privatization.⁶\n\nBut as these ideas started to change the world, serious challenges to their intellectual foundations surfaced. Earlier criticism often came from thinkers with little understanding of, and less sympathy for, the economic way of thinking. The new doubts were framed in clear economic logic.\n\nThe Miracle of Aggregation\n\nThink about what happens if you ask a hundred people\nto run a 100-meter race, and then average their times.\nThe average time will not be better than the time of the\nfastest runners. It will be worse. . . . But ask a hundred\npeople to answer a question or solve a problem, and the\naverage answer will often be at least as good as the\nanswer of the smartest member. With most things, the\naverage is mediocrity. With decision-making, it’s often\nexcellence. You could say it’s as if we’ve been\nprogrammed to be collectively smart.\n\n—James Surowiecki, The Wisdom of Crowds⁷\n\nIf a person has no idea how to get to his destination, he can hardly expect to reach it. He might get lucky, but common sense recognizes a tight connection between knowing what you are doing and successfully doing it. Ubiquitous voter ignorance seems to imply, then, that democracy works poorly. The people ultimately in charge—the voters—are doing brain surgery while unable to pass basic anatomy.\n\nThere are many sophisticated attempts to spoil this analogy, but the most profound is that democracy can function well under almost any magnitude of voter ignorance. How? Assume that voters do not make systematic errors. Though they err constantly, their errors are random. If voters face a blind choice between X and Y, knowing nothing about them, they are equally likely to choose either.⁸\n\nWhat happens? With 100% voter ignorance, matters are predictably grim. One candidate could be the Unabomber, plotting to shut down civilization. If voters choose randomly, the Unabomber wins half the time. True, the assumption of zero voter knowledge is overly pessimistic; informed voters are rare, but they do exist. But this seems a small consolation. 100% ignorance leads to disaster. Can 99% ignorance be significantly better?\n\nThe surprising answer is yes. The negative effects of voter ignorance are not linear. Democracy with 99% ignorance looks a lot more like democracy with full information than democracy with total ignorance.⁹ Why? First, imagine an electorate where 100% of all voters are well informed. Who wins the election? Trivially, whoever has the support of a majority of the well informed. Next, switch to the case where only 1% of voters are well informed. The other 99% are so thick that they vote at random. Quiz a person waiting to vote, and you are almost sure to conclude, with alarm, that he has no idea what he is doing. Nevertheless, it is basic statistics that—in a large electorate—each candidate gets about half of the random votes. Both candidates can bank on roughly a 49.5% share. Yet that is not enough to win. For that, they must focus all their energies on the one well-informed person in a hundred. Who takes the prize? Whoever has the support of a majority of the well informed. The lesson, as Page and Shapiro emphasize, is that studying the average voter is misleading:\n\nEven if individuals’ responses to opinion surveys are partly random, full of measurement error, and unstable, when aggregated into a collective response—for example, the percentage of people who say they favor a particular policy—the collective response may be quite meaningful and stable.¹⁰\n\nSuppose a politician takes a large bribe from “big tobacco” to thumb his nose at unanimous demand for more regulation. Pro-tobacco moves do not hurt the candidate’s standing among the ignorant—they scarcely know his name, much less how he voted. But his share of the informed vote plummets. Things get more complex when the number of issues rises, but the key to success stays the same: Persuade a majority of the well informed to support you.\n\nThis result has been aptly named the “Miracle of Aggregation.”¹¹ It reads like an alchemist’s recipe: Mix 99 parts folly with 1 part wisdom to get a compound as good as unadulterated wisdom. An almost completely ignorant electorate makes the same decision as a fully informed electorate—lead into gold, indeed!\n\nIt is tempting to call this “voodoo politics,” or quip, as H. L. Mencken did, that “democracy is a pathetic belief in the collective wisdom of individual ignorance.”¹² But there is nothing magical or pathetic about it. James Surowiecki documents many instances where the Miracle of Aggregation—or something akin to it—works as advertised.¹³ In a contest to guess the weight of an ox, the average of 787 guesses was off by a single pound. On Who Wants to Be a Millionaire, the answer most popular with studio audiences was correct 91% of the time. Financial markets—which aggregate the guesses of large numbers of people—often predict events better than leading experts. Betting odds are excellent predictors of the outcomes of everything from sporting events to elections.¹⁴ In each case, as Page and Shapiro explain, the same logic applies:\n\nThis is just an example of the law of large numbers. Under the right conditions, individual measurement errors will be independently random and will tend to cancel each other out. Errors in one direction will tend to offset errors in the opposite direction.¹⁵\n\nWhen defenders of democracy first encounter rational ignorance, they generally grant that severe voter ignorance would hobble government by the people. Their instinctive responses are to (a) deny that voters are disturbingly ignorant, or (b) interpret voters’ ignorance as a fragile, temporary condition. To call these responses “empirically vulnerable” is charitable. Decades of research show they are plain wrong.¹⁶ About half of Americans do not know that each state has two senators, and three-quarters do not know the length of their terms. About 70% can say which party controls the House, and 60% which party controls the Senate.¹⁷ Over half cannot name their congressman, and 40% cannot name either of their senators. Slightly lower percentages know their representatives’ party affiliations.¹⁸ Furthermore, these low knowledge levels have been stable since the dawn of polling, and international comparisons reveal Americans’ overall political knowledge to be no more than moderately below average.¹⁹\n\nYou could insist that none of this information is relevant. Perhaps voters have holistic insight that defies measurement. But this is a desperate route for a defender of democracy to take. The Miracle of Aggregation provides a more secure foundation for democracy. It lets people believe in empirical evidence and democracy at the same time.\n\nThe original arguments about rational ignorance took time to spread, but eventually became conventional wisdom. The Miracle of Aggregation is currently in the middle of a similar diffusion process. Some have yet to hear of the Miracle. Backward-looking thinkers hope that if they ignore the objection, it will go away. But the logic is too compelling. Unless someone uncovers a flaw in the Miracle, the fault line in the social sciences will close. Economists and economically minded political scientists and law professors will rethink their doubts about democracy, and go back to the prerational ignorance presumption that if democracies do X, X is a good idea.\n\nThe Reality of Systematic Error\n\nUniversal suffrage, which to-day excludes free trade from\nthe United States, would certainly have prohibited the\nspinning-jenny and the power-loom.\n\n—William Lecky, Liberty and Democracy²⁰\n\nThe Miracle of Aggregation proves that democracy can work even with a morbidly ignorant electorate. Democracy gives equal say to the wise and the not-so-wise, but the wise determine policy. Belaboring the electorate’s lack of knowledge with study after study is beside the point.\n\nBut there is another kind of empirical evidence that can discredit the Miracle of Aggregation. The Miracle only works if voters do not make systematic errors. This suggests that instead of rehashing the whole topic of voter error, we concentrate our fire on the critical and relatively unexplored question:²¹ Are voter errors systematic?\n\nThere are good reasons to suspect so. Yes, as Surowiecki points out, our average guess about the weight of oxen is dead on. But cognitive psychology catalogs a long list of other questions where our average guess is systematically mistaken.²² This body of research ought to open our minds to the possibility of systematic voter error.\n\nBy itself, though, the psychological literature does not get us very far. The link between general cognition and particular political decisions is too loose. People could have poor overall judgment but good task-specific judgment.²³ Voters might be bad statisticians but perceptive judges of wise policy. Thus, we should refine our question: Are voter errors systematic on questions of direct political relevance?\n\n[f0010-01]\n\nFigure 1.1 The Median Voter Model: Random Error\n\nMy answer is an emphatic yes. This book presents robust empirical evidence that—at minimum—beliefs about economics are riddled with severe systematic errors.²⁴ I strongly suspect that the same holds for beliefs about many other subjects. But as far as economics is concerned, the jury is in. People do not understand the “invisible hand” of the market, its ability to harmonize private greed and the public interest. I call this antimarket bias. People underestimate the benefits of interaction with foreigners. I call this antiforeign bias. People equate prosperity not with production, but with employment. I call this make-work bias. Lastly, people are overly prone to think that economic conditions are bad and getting worse. I call this pessimistic bias.\n\nEconomic policy is the primary activity of the modern state, making voter beliefs about economics among the most—if not the most—politically relevant beliefs. If voters base their policy preferences on deeply mistaken models of the economy, government is likely to perform its bread-and-butter function poorly. To see this, suppose that two candidates compete by taking positions on the degree of protectionism they favor. Random voter errors about the effect of protection cause some voters who prefer the effect of free trade to vote for protection. But it is equally common for voters who prefer the effect of protection to vote for free trade.²⁵ Then the Miracle of Aggregation holds: in spite of voter ignorance, the winning platform is socially optimal.\n\nFor anyone who has taught international economics, though, this conclusion is underwhelming. It takes hours of patient instruction to show students the light of comparative advantage. After the final exam, there is a distressing rate of recidivism. Suppose we adopt the more realistic assumption that voters systematically overestimate the benefits of protection. What happens? Lots of people vote for protection who prefer the effect of free trade, but only a few vote for free trade who prefer the effect of protection. The political scales tilt out of balance; the winning platform is too protectionist. The median voter would be better off if he received less protection than he asked for. But competition impels politicians to heed what voters ask for, not what is best for them.\n\n[f0011-01]\n\nFigure 1.2 The Median Voter Model: Systematic Error\n\nComparable biases plausibly underlie policy after policy.²⁶ For example, supply-and-demand says that above-market prices create unsaleable surpluses, but that has not stopped most of Europe from regulating labor markets into decades of depression-level unemployment.²⁷ The most credible explanation is that the average voter sees no link between artificially high wages and unemployment. Before I studied economics, I failed to see it myself.\n\nModern Research versus Intellectual Tradition\n\nEconomists have two attitudes toward discourse,\nthe official and the unofficial.\n\n—Donald McCloskey, The Rhetoric of Economics²⁸\n\nThe terminology of “systematic” versus “random” error entered economists’ vocabulary about 30 years ago.²⁹ But the concept of systematic error has a much longer history. Here is how Simon Newcomb began an article in the Quarterly Journal of Economics in 1893:\n\nThe fact that there is a wide divergence between many of the practical conclusions of economic science, as laid down by its professional exponents, and the thought of the public at large, as reflected in its current discussion and in legislation, is one with which all are familiar.³⁰\n\nThis was the intellectual climate that Newcomb saw in the contemporary United States and Great Britain. Over a century earlier, in The Wealth of Nations, Smith made similar observations about economic beliefs in Britain:\n\nNothing, however, can be more absurd than this whole doctrine of the balance of trade, upon which, not only these [mercantilist] restraints, but almost all other regulations of commerce are founded. When two places trade with one another, this doctrine supposes that, if the balance be even, neither of them loses or gains; but if it leans in any degree to one side, that one of them loses, and the other gains in proportion to its declension from the exact equilibrium.³¹\n\nThe policy consequences, for Smith, are far-reaching:\n\nBy such maxims as these, however, all nations have been taught that their interest consisted in beggaring all their neighbors. Each nation has been made to look with an invidious eye upon the prosperity of all the nations with which it trades, and to consider their gain as its own loss. Commerce, which ought naturally to be, among nations, as among individuals, a bond of union and friendship, has become the most fertile source of discord and animosity.³²\n\nWhen he affirms that “science is the great antidote to the poison of enthusiasm and superstition,”³³ Smith is not thinking about errors that harmlessly balance out.\n\nIn the middle of the 19th century, Frédéric Bastiat, the French popularizer of classical economics, titled one of his most famous books Economic Sophisms. “Sophism” is Bastiat’s synonym for “systematic error,” and he assigns sophisms broad consequences: They “are especially harmful, because they mislead public opinion in a field in which public opinion is authoritative—is, indeed, law.”³⁴ Bastiat attacks dozens of popular protectionist sophisms, for example, but does not bother to criticize any popular free trade sophisms. The reason is not that bad arguments for free trade do not exist, but that—unlike bad arguments for protection—virtually none are popular!\n\nBastiat’s outlook remained respectable well into the 20th century. The eminent economist Frank Knight made no apologies for it:\n\nThe action taken by our own democracy, and the beliefs of the great majority on which the action rests, are often absurd. Nor are they to be explained by economic self-interest, since the measures depend on votes of electors whose interests are directly opposed to them, as well as those benefited.³⁵\n\nYet in recent decades, these ideas have been forced underground. Nearly all modern economic theories of politics begin by assuming that the typical citizen understands economics and votes accordingly—at least on average.³⁶ As George Stigler, widely known as a stern critic of government regulation, scoffs:\n\nThe assumption that public policy has often been inefficient because it was based on mistaken views has little to commend it. To believe, year after year, decade after decade, that the protective tariffs or usury laws to be found in most lands are due to confusion rather than purposeful action is singularly obfuscatory.³⁷\n\nIn stark contrast, introductory economics courses still tacitly assume that students arrive with biased beliefs, and try to set them straight, leading to better policy. Paul Samuelson famously remarked, “I don’t care who writes a nation’s laws—or crafts its advanced treaties—if I can write its economics textbooks.”³⁸ This assumes, as teachers of economics usually do, that students arrive with systematic errors.\n\nWhat a striking situation: As researchers, economists do not mention systematically biased economic beliefs; as teachers, they take their existence for granted. One might blame ossified textbooks for lagging behind research, or teachers for failing to expose their students to cutting-edge work. But the hypothesis that people hold systematically biased beliefs about economics has not been falsified; it has barely been tested.\n\nI maintain that the oral tradition of the teachers of economics offers the researchers of economics a rich mine of scientific hypotheses. At the same time, the oral tradition has been subject to so little analytical scrutiny that it is not hard to refine. Samuelson’s is a story of hope; we can sleep soundly as long as he keeps writing textbooks. But pondering two more facts might keep us lying awake at night. Fact 1: The economics the average introductory student absorbs is disappointingly small. If they had severe biases at the beginning, most still have large biases at the end. Fact 2: below-average students are above-average citizens. Most voters never take a single course in economics. If it is disturbing to imagine the bottom half of the class voting on economic policy, it is frightening to realize that the general population already does. The typical voter, to whose opinions politicians cater, is probably unable to earn a passing grade in basic economics. No wonder protectionism, price controls, and other foolish policies so often prevail.\n\nPreferences over Beliefs\n\nThe growing obsession in most advanced nations with international\ncompetitiveness should be seen, not as a well-founded\nconcern, but as a view held in the face of overwhelming\ncontrary evidence. And yet it is clearly a view\nthat people very much want to hold—a desire to believe\nthat is reflected in a remarkable tendency of those who\npreach the doctrine of competitiveness to support their\ncases with careless, flawed arithmetic.\n\n—Paul Krugman, Pop Internationalism³⁹\n\nThe most common objection to my thesis is theoretical: it contradicts the whole “rational choice approach” of modern social science. My colleague Robin Hanson aptly describes rational choice models as “stories without fools.” I put folly—or, in technical terms, “irrationality”—at center stage.\n\nOne is tempted to snap: If the facts do not fit rational choice theory, so much the worse for rational choice theory! But this reaction is premature, for there is a satisfying way to reconcile theory and common sense. The preliminary step is to drop specious analogies between markets and politics, between shopping and voting. Sensible public opinion is a public good.⁴⁰ When a consumer has mistaken beliefs about what to buy, he foots the bill. When a voter has mistaken beliefs about government policy, the whole population picks up the tab.\n\nDropping false analogies between shopping and voting restores our intellectual flexibility, making the conflict between theory and common sense less daunting. But how can the conflict be resolved? We do not have to turn our backs on economics. It is only necessary to broaden its understanding of human motivation and cognition.\n\nEconomists usually presume that beliefs are a means to an end, not an end in themselves. In reality, however, we often have cherished views, valued for their own sake. As Shermer puts it, “Without some belief structure many people find this world meaningless and without comfort.”⁴¹ In economic jargon, people have preferences over beliefs. Letting emotions or ideology corrupt our thinking is an easy way to satisfy such preferences.⁴² Instead of fairly weighing all claims, we can show nepotism toward our favorite beliefs. Ayn Rand calls it “blanking out”: “the willful suspension of one’s consciousness, the refusal to think—not blindness, but the refusal to see; not ignorance, but the refusal to know.”⁴³\n\nOutside of economics, the idea that people like some beliefs more than others has a long history. John Locke’s Essay Concerning Human Understanding inveighs against “enthusiasm, in which reason is taken away.” To be an enthusiast is to embrace dubious ideas on emotional grounds:\n\nFor the evidence that any proposition is true (except such as are self-evident) lying only in the proofs a man has of it, whatsoever degrees of assent he affords it beyond the degrees of that evidence, it is plain that all the surplusage of assurance is owing to some other affection, and not to the love of truth.⁴⁴\n\nNotice the two components of his analysis. The first is “surplusage of assurance.” Locke observes that people assign probabilities to beliefs higher than the evidence warrants. The second is “other affections.” The cause of excess confidence, on Locke’s account, is conflict of motives. Everyone likes to think that he values truth for its own sake, but there are competing impulses: “conceit,” “laziness,” “vanity,” “the tedious and not always successful labor of strict reasoning,” and “fear, that an impartial inquiry would not favour those opinions which best suit their prejudices, lives, and designs.”⁴⁵\n\nThinkers who discuss preferences over beliefs almost invariably bring up religion. Locke is no different:\n\nIn all ages, men in whom melancholy has mixed with devotion, or whose conceit of themselves has raised them into an opinion of a greater familiarity with God, and a nearer admittance to his favour than is afforded to others, have often flattered themselves with a persuasion of an immediate intercourse with the Deity, and frequent communications from the Divine Spirit.⁴⁶\n\nLike most things, enthusiasm comes in degrees. Many who feel no need to convert others take offense if you politely argue that their religion is mistaken. Few dispassionately accept their religious teachings as the “current leading hypothesis.” Consider the adjectives that so often appear in the study of religion: fervent, dogmatic, fanatical. Human beings want their religion’s answers to be true. They often want it so badly that they avoid counterevidence, and refuse to think about whatever evidence falls in their laps. As Nietzsche uncharitably puts it, “ ‘Faith’ means not wanting to know what is true.”⁴⁷\n\nOnce you admit that preferences over beliefs are relevant in religion, it is hard to compartmentalize the insight. As Gustave Le Bon observes in The Crowd, there is a close analogy between literal religious belief and fervent (“religious”) adherence to any doctrine: “Intolerance and fanaticism are the necessary accompaniments of the religious sentiment. . . . The Jacobins of the Reign of Terror were at bottom as religious as the Catholics of the Inquisition, and their cruel ardor proceeds from the same source.”⁴⁸ Eric Hoffer famously expands on this point in his short classic The True Believer, declaring that “all mass movements are interchangeable”: “A religious movement may develop into a social revolution or a nationalist movement; a social revolution, into militant nationalism or a religious movement; a nationalist movement into a social revolution or a religious movement.”⁴⁹\n\nIt is no accident that both of the substitutes for religion that Hoffer names—nationalism and social revolution—are political. Political/economic ideology is the religion of modernity. Like the adherents of traditional religion, many people find comfort in their political worldview, and greet critical questions with pious hostility.⁵⁰ Instead of crusades or inquisitions, the twentieth century had its notorious totalitarian movements.⁵¹ “The religious character of the Bolshevik and Nazi revolutions is generally recognized,” writes Hoffer. “The hammer and sickle and the swastika are in a class with the cross. The ceremonial of their parades is as the ceremonial of a religious procession. They have articles of faith, saints, martyrs and holy sepulchers.”⁵² Louis Fischer confesses that “just as religious conviction is impervious to logical argument and, indeed, does not result from logical processes, just as nationalist devotion or personal affection defies a mountain of evidence, so my pro-Soviet attitude attained complete independence from day-to-day events.”⁵³ George Orwell’s 1984 developed the novel vocabulary of Newspeak—words like doublethink and thoughtcrime—to ridicule the quasireligious nature of totalitarian ideologies.⁵⁴ A tour of Nazi or Communist websites can provide the reader with good contemporary examples.\n\nAs with religion, extreme ideologies lie at the end of a continuum. One’s political worldview might compare favorably with the outlook of the sole member of a Maoist splinter faction, but remain less than rational.⁵⁵ To many people, for example, blaming foreigners for domestic woes is a source of comfort or pride. They may not proclaim their protectionism every day, and might acknowledge that foreign trade is beneficial in special circumstances. But they still resist—and resent—those who try change their minds by explaining comparative advantage.\n\nNatural scientists have long known that the majority disbelieves some of their findings because they contradict religion.⁵⁶ Social scientists need to learn that the majority disbelieves some of their findings because they contradict quasi religion.\n\nRational Irrationality\n\nAs we never cease to point out, each man is in practice\nan excellent economist, producing or exchanging\naccording as he finds it more advantageous to do\nthe one or the other.\n\n—Frédéric Bastiat, Economic Sophisms⁵⁷\n\nPreferences over beliefs is the critical idea that reconciles the theory of rational choice with the facts of voter irrationality. How? Suppose that human beings value both their material prosperity and their worldview. In economic jargon, they have two arguments in their utility function: personal wealth and loyalty to their political ideology. What happens if people rationally make trade-offs between their two values?\n\nIn any rational choice analysis, prices are the guiding star. If you like both meat and potatoes, you need to know how much meat you must forego in order to get one more potato. It is a mistake, however, to focus exclusively on the price tags at the grocery store. Part of the price of an unhealthy diet is a shorter life span, but the price tag says nothing about it. Economists call the total cost—explicit and implicit—of an activity its “full price.” Though less visible than a printed price tag, the full price is the one that matters most.\n\nThe more incorrect your beliefs, the more poorly tailored your actions are to actual conditions.⁵⁸ What is the full price of ideological loyalty? It is the material wealth you forego in order to believe. Suppose that Robinson Crusoe’s ideology teaches that native islanders like Friday are unable to farm. It flatters his pride to believe that only Europeans can understand agriculture. If Crusoe’s belief is in fact correct, he wisely specializes in agriculture and has Friday do other kinds of work. But if Crusoe’s belief is blind prejudice, keeping Friday out of agriculture reduces total production and makes both men poorer. The difference between Crusoe’s potential living standard and his actual living standard is the full price of his ideological stance.\n\nOn an island with two people, the ideologue’s material cost of hewing to his false precepts can be substantial. Under democracy, however, the probability that one vote—however misguided—changes policy rapidly decreases as the number of voters increases. In order to alter the outcome, a vote has to break a tie. The more votes, the fewer ties there are to break. Imagine a thousand Crusoes vote on permissible lines of work for a thousand Fridays. The Crusoes prefer to believe that the Fridays are unfit for agriculture, but the facts are against them. What is the expected loss of material wealth for a Crusoe who indulges this preference? He forfeits not the per capita reduction in wealth, but the per capita reduction discounted by the probability that he flips the outcome of the election. If the per capita cost of keeping Fridays out of agriculture is $1,000, and the probability of being a tiebreaker is 0.1%, then a Crusoe who votes to keep them out pays $1 to adhere to his cherished fallacy.\n\nThis example illustrates one of this book’s recurring points: In real-world political settings, the price of ideological loyalty is close to zero.⁵⁹ So we should expect people to “satiate” their demand for political delusion, to believe whatever makes them feel best. After all, it’s free. The fanatical protectionist who votes to close the borders risks virtually nothing, because the same policy wins no matter how he votes. Either the borders remain open, and the protectionist has the satisfaction of saying, “I told you so”; or the borders close, and the protectionist has the satisfaction of saying, “Imagine how bad things would have been if we hadn’t closed the borders!”\n\nThere can easily be a large gap between the private and social costs of ideological fealty. Recall that the expected material cost of error for one Crusoe was only $1. If a majority of the individual Crusoes find this price attractive, though, each and every Crusoe loses $1,000. Voting to keep the Fridays out of agriculture sacrifices $1,000,000 in social wealth in order to placate ideological scruples worth as little as $501.\n\nA recurring rejoinder to these alarmist observations is that precisely because confused political ideas are dangerous, voters have a strong incentive to wise up. This makes as much sense as the argument that people have a strong incentive to drive less because auto emissions are unpleasant to breathe. No one faces the choice, “Drive a lot less, or get lung cancer,” or “Rethink your economic views, or spiral down to poverty.” In both driving and democracy, negative externalities irrelevant to individual behavior add up to a large collective misfortune.\n\nThe Landscape of Political Irrationality\n\nDemocracy is the theory that the common people know\nwhat they want, and deserve to get it good and hard.\n\n—H. L. Mencken⁶⁰\n\nOrdinary cynics—and most economists—compare voters to consumers who shrewdly “vote their pocketbooks.” In reality, this is atypical. Empirically, there is little connection between voting and material interests. Contrary to popular stereotypes of the rich Republican and the poor Democrat, income and party identity are only loosely related. The elderly are if anything slightly less supportive of Social Security and Medicare than the rest of the population. Men are more pro-choice than women.⁶¹\n\nIf self-interest does not explain political opinion, what does? Voters typically favor the policies they perceive to be in the general interest of their nation. This is, however, no cause for democratic optimism. The key word is perceive. Voters almost never take the next step by critically asking themselves: “Are my favorite policies effective means to promote the general interest?” In politics as in religion, faith is a shortcut to belief.\n\nWhat are the implications for democracy? Standard rational choice theory rightly emphasizes that politicians woo voters by catering to their preferences. But this means one thing if voters are shrewd policy consumers, and almost the opposite if, as I maintain, voters are like religious devotees. In the latter case, politicians have a strong incentive to do what is popular, but little to competently deliver results. Alan Blinder cuttingly refers to “a compliant Congress, disdainful of logic, but deeply respectful of public opinion polls.”⁶² If one politician fails to carry out the people’s wishes, a competing politician will. Le Bon makes the same point in sweeping terms:\n\nThe masses have never thirsted after truth. They turn aside from evidence that is not to their taste, preferring to deify error, if error seduce them. Whoever can supply them with illusions is easily their master; whoever attempts to destroy their illusions is always their victim.⁶³\n\nThus, it is in mind-set, not practical influence, that voters resemble religious believers. Given the separation of church and state, modern religion has a muted effect on nonbelievers. Scientific progress continues with or without religious approval. Political/economic misconceptions, in contrast, have dramatic effects on everyone who lives under the policies they inspire—even those who see these misconceptions for what they are. If most voters think protectionism is a good idea, protectionist policies thrive; if most believe that unregulated labor markets work badly, labor markets will be heavily regulated.\n\nThe conventional complaint about politicians is “shirking”—their failure to do what voters want.⁶⁴ I maintain that “shirking” should be dethroned in favor of “demagoguery.” Merriam-Webster’s Collegiate Dictionary defines a demagogue as “a leader who makes use of popular prejudices and false claims and promises in order to gain power.”⁶⁵ Put bluntly, rule by demagogues is not an aberration. It is the natural condition of democracy. Demagoguery is the winning strategy as long as the electorate is prejudiced and credulous. Indeed, while demagogue normally connotes insincerity, this is hardly necessary. “Religious” voters encourage politicians to change their behavior by feigning devotion to popular prejudices, but also prompt entry by the honestly prejudiced into the political arena.⁶⁶\n\nShirking should be dethroned, not but disowned. Elections are imperfect disciplinary devices.⁶⁷ Some deviation from voter wishes is bound to occur. But how much? How strictly do elections constrain politicians? My view is that it depends on voters themselves. If they care deeply about an issue—like public use of racial slurs—politicians have almost no slack. One wrong word costs them the election. In contrast, if voters find a subject boring—like banking regulation—if emotion and ideology provide little guidance, their so-called representatives have “wiggle room” to maneuver.\n\nPoliticians’ wiggle room creates opportunities for special interest groups—private and public, lobbyists and bureaucrats—to get their way. On my account, though, interest groups are unlikely to directly “subvert” the democratic process. Politicians rarely stick their necks out for unpopular policies because an interest group begs them—or pays them—to do so. Their careers are on the line; it is not worth the risk. Instead, interest groups push along the margins of public indifference.⁶⁸ If the public has no strong feelings about how to reduce dependence on foreign oil, ethanol producers might finagle a tax credit for themselves. No matter how hard they lobbied, though, they would fail to ban gasoline.\n\nLastly, for all the power ascribed to them, the media are also consumer-driven. Competition induces them to cover news that viewers want to watch. In the standard rational choice account, this reduces political information costs and so helps democracy work. Yet I am skeptical that much useful information flows from media to viewers. Instead, like politicians, the media show viewers what they want to see and tell them what they want to hear.⁶⁹\n\nAdmittedly, the media, like politicians, have wiggle room. Yet once again, it is slack along the margins of indifference. If a shocking disaster story, bundled with mild liberal reporting bias, remains highly entertaining to a mainstream audience, then predominantly Democratic newscasters can mix in a little left-wing commentary. But if the media stray too far from typical viewer opinion—or just get too pedantic—the audience flies away. So while the conventional view gives the media too much credit—the private good of entertainment vitiates the public good of information—it is even more wrongheaded to treat the media as the source of popular fallacies. As we shall see, the fallacies preceded modern media; they continue to flourish because the audience is predisposed to be receptive.\n\nTo recap, my story is voter-driven. Voters have beliefs—defensible or not—about how the world works. They tend to support politicians who favor policies that, in the voters’ own minds, will be socially beneficial. Politicians, in turn, need voter support to gain and retain office. While few are above faking support for popular views, this is rarely necessary: Successful candidates usually sincerely share voters’ worldview. When special interests woo politicians, they tailor their demands accordingly. They ask for concessions along policy margins where the voice of public opinion is silent anyway. The media, finally, do their best to entertain the public. Since scandalous behavior by politicians and interest groups is entertaining, the media are watchdogs. Like all watchdogs, though, the media have a subordinate role. If their coverage, however sound, conflicts with viewers’ core beliefs, they change the channel.\n\nConclusion\n\nTo undermine the Miracle of Aggregation, this book focuses on the empirical evidence that voters’ beliefs about economics are systematically mistaken. This does not imply that their beliefs about other topics are any sounder. In fact, I hope that experts in other fields will use my framework to explain how biased beliefs about their area of specialty distort policy.\n\nThe reason why I emphasize economics is that it is at the heart of most modern policy disputes. Regulation, taxes, subsidies—they all hinge on beliefs about how policy affects economic outcomes. The modal respondent in the National Election Studies ranks economic issues as “the most important problem” in most election years. In fact, if you classify “social welfare” issues like welfare, the environment, and health care as economic, then economic issues were “the most important problem” in every election year from 1972 to 2000.⁷⁰ Biased beliefs about economics make democracy worse at what it does most. Understanding these biases is therefore important not just for economists, but for everyone who studies politics. If that is not motivation enough, economists’ love/hate relationship with the Miracle of Aggregation—official embrace, punctuated by exasperated under-the-table complaints about economic illiteracy—makes for a juicy story.\n\nThe empirics of economic beliefs serve as the springboard for a new perspective on democracy. How can economic theory accommodate the empirical evidence on systematic bias? Conceptually, the necessary change is not radical: Just add one new ingredient—preferences over beliefs—to the rational choice stew. Yet substantively, my account almost reverses the rational choice consensus. I see neither well-functioning democracies nor democracies highjacked by special interests. Instead, I see democracies that fall short because voters get the foolish policies they ask for. Adding one new ingredient to the rational choice stew gives it a starkly different flavor.\n\nChapter 2\n\nSYSTEMATICALLY BIASED BELIEFS\n\nABOUT ECONOMICS\n\nLogical minds, accustomed to being convinced by a chain\nof somewhat close reasoning, cannot avoid having recourse\nto this mode of persuasion when addressing\ncrowds, and the inability of their arguments always surprises\nthem.\n\n—Gustave Le Bon, The Crowd¹\n\nIn their modern theoretical work, economists look almost uniformly hostile to the view that people suffer from systematic bias. Nearly every formal model takes for granted that whatever individuals’ limitations, on average they get things right. The approach that Gary Becker championed is now the norm:\n\nI find it difficult to believe that most voters are systematically fooled about the effects of policies like quotas and tariffs that have persisted for a long time. I prefer instead to assume that voters have unbiased expectations, at least of policies that have persisted. They may overestimate the dead weight loss from some policies, and underestimate it from others, but on the average they have a correct perception.²\n\nJournals regularly reject theoretical papers that explicitly take the opposite position on methodological grounds: “You can’t assume that.” Papers that covertly introduce systematic bias risk being “outed.”³ In a well-known piece in the Journal of Political Economy, Stephen Coate and Stephen Morris worry that other economists are smuggling in the “unreasonable assumptions” that voters “have biased beliefs about the effects of policies” and “could be persistently fooled.”⁴ Dani Rodrik similarly laments, “The bad news is that the habit of attributing myopia or irrationality to political actors—whether explicitly or, more often, implicitly—persists.”⁵ Translation: These eminent social scientists are demanding that their colleagues honor the ban on irrationality in deed as well as word.\n\nEvidence of Bias from Psychology and Public Opinion Research\n\nEconomists’ theoretical aversion to systematic bias has fortunately not prevented empirical work from moving forward. Beyond the confines of their discipline, economists’ strictures have been largely ignored. Psychologists like Daniel Kahneman and Amos Tversky have unearthed a diverse list of biases to which humans are prone.⁶ For example, individuals overestimate the probability of vivid, memorable events such as airplane crashes. Other studies confirm that markedly more than 50% of people put themselves in the upper half of the distribution of many favorable attributes.⁷ Numerous economists have built on psychologists’ work, giving rise to the field of Psychology and Economics.⁸\n\nThis body of research proves that systematic mistakes exist. It is a powerful argument for keeping an open mind about the frailty of human understanding. Nevertheless, moving from laboratory to real life is somewhat perilous.⁹ It is one thing to show that people fall short of a theoretical ideal of rationality in contrived experimental conditions. It is another to infer that irrational beliefs undermine their real-world choices—the decisions that human beings make in the environment where they were “born and raised.”¹⁰ After all, people might be good at what they do even though their general cognitive skills make logicians and statisticians cringe. Psychologists call this “ecological rationality”—the ability to choose sensibly in your natural habitat.¹¹ A mechanic who fails to notice correlations in a laboratory experiment may ably diagnose your car trouble. Voters might have sensible views about the issues of the day even though the clunkiest computer on the market beats them in chess.\n\nIt is hard to remain cavalier, however, if your mechanic affirms that cars run on sand instead of gasoline. How could anyone who holds this belief be trusted with a car? The error is directly relevant to practical decisions, and points its adherent in a dangerous direction. Roughly the same is true if voters think that the biggest item in the federal budget is foreign aid. With such a distorted picture of where their tax dollars go, they are likely to spurn responsible politicians with realistic proposals in favor of demagogues who promise to painlessly balance the budget.\n\nThe question that naturally presents itself, then, is: Do voters have biased beliefs about questions directly relevant to policy? While economists have shied away from this topic, public opinion researchers have not. They find voter bias to be common and quantitatively significant.¹² To escape their conclusion, one must reject the whole idea of “grading” the quality of public opinion—effectively letting the public act as the judge in its own case.\n\nThe simplest way to test for voter bias is to ask questions with objective quantitative answers, like the share of the federal budget dedicated to national defense or Social Security. Since researchers know the true numbers, they can statistically compare respondents’ expressed beliefs to the facts. One high-quality example is the National Survey of Public Knowledge of Welfare Reform and the Federal Budget.¹³ It presents strong evidence that the public systematically overestimates the share of government spending on welfare and foreign aid, and underestimates the share devoted to national defense and especially Social Security.\n\nThe main drawback of these studies is that many interesting questions are only answerable with a degree of ambiguity. Suppose you wonder if the public systematically underestimates the benefits of free trade. You cannot simply compare public opinion to Known Fact from the Statistical Abstract of the United States.¹⁴ But several political scientists propose and apply a creative alternative. They estimate voters’ “enlightened preferences”—the preferences they would have if they were “fully informed,” or, to be more precise, far better informed.¹⁵ This is a three-step process:\n\n1. Administer a survey of policy preferences combined with a test of objective political knowledge.\n\n2. Statistically estimate individuals’ policy preferences as a function of their objective political knowledge and their demographics—such as income, race, and gender.\n\n3. Simulate what policy preferences would look like if all members of all demographic groups had the maximum level of objective political knowledge.\n\nThus, you begin by collecting data on respondents’ preferred policies—whether they want more or less government spending, whether they want to reduce the deficit by raising taxes, whether they are pro-choice or pro-life. Next, you test respondents’ objective political knowledge. Think of it as a test of their “Political I.Q.” See if they know how many senators each state has, who the chief justice of the Supreme Court is, whether Russia is a member of NATO, and so on.\n\nOnce you know respondents’ Political I.Q., you can use it—along with information on respondents’ income, race, gender, and so on—to statistically predict their policy preferences. You can see whether, for example, the average person with high Political I.Q. favors more or less government spending than the average person with low Political I.Q.\n\nTable 2.1\nAverage Policy Preferences\n\n[t0026-01]\n\nArmed with this information, you can guesstimate what an individual would think if his demographics stayed the same but his Political I.Q. rose to godly heights. If a poor man with a low Political I.Q. learned a lot more about politics but stayed poor, would he change his mind about welfare policy? If so, how?\n\nFinally, once you know how one individual would revise his opinions, you can calculate how the whole distribution of opinions would change if everyone had the maximum Political I.Q. All you have to do is figure out what each and every individual would want given maximum political knowledge, then compare the new distribution to the old.\n\nTo work through a simple example, imagine there are two demographic groups—rich and poor—and two knowledge levels—low and high, for a total of four categories. Each category has the same fraction of people—25% each. Respondents rate their preferred welfare policy on the scale from 0 to 10, where 0 means drastic cuts and 10 means drastic increases. The average response for the whole population is 4.5.\n\nTo calculate the enlightened preferences of the whole population, replace the actual answers of the low-knowledge respondents with the average answer of high-knowledge respondents with the same income. Assign the average preference of the high-knowledge rich respondents—3—to all rich respondents. Assign the average preference of the high-knowledge poor respondents—4—to all poor respondents. The new average—3.5—is the population’s enlightened preference.\n\nOne key feature of the enlightened preference approach is that in the absence of systematic effects of knowledge on policy preferences, there would be nothing to report. The distribution of enlightened preferences would equal the distribution of actual, “unenlightened” preferences.\n\nIn practice, though, the enlightened preference approach has a big payoff: Systematic effects of knowledge on policy preferences are large and ubiquitous. As Althaus explains: “Contrary to the predictions of collective rationality models, the aggregate opinions of ill-informed respondents are usually more one-sided than those of the well informed.”¹⁶ He goes on to provide an excellent summary of the three most noteworthy patterns in the data:\n\n1. “First, fully informed opinion on foreign policy issues is relatively more interventionist than surveyed opinion but slightly more dovish when it comes to the use and maintenance of military power.”¹⁷ If the public’s knowledge of politics magically increased, isolationism would be less popular. More knowledgeable individuals favor an active international role for the United States. At the same time, they are less hawkish: They want to be involved in world affairs, but see a greater downside of outright war.\n\n2. “The second pattern among policy questions is for fully informed opinion to hold more progressive attitudes on a wide variety of social policy topics, particularly on those framed as legal issues.”¹⁸ Most notably, a more knowledgeable public would be more pro-choice, more supportive of gay rights, and more opposed to prayer in school.\n\n3. “The third pattern in policy questions is for simulated opinions to be more ideologically conservative on the scope and applications of government power. In particular, fully informed opinion tends to be fiscally conservative when it comes to expanding domestic programs, to prefer free market solutions over government intervention to solve policy problems, to be less supportive of additional government intervention to protect the environment, and to prefer a smaller and less powerful federal government.” For example, the 1996 American National Election Study asks which of the following two positions is closer to the respondent’s views: “One, we need a strong government to handle today’s complex economic problems; or two, the free market can handle these problems without government becoming involved.”¹⁹ Fully informed opinion was more promarket. Beliefs about welfare and affirmative action fit the same pattern: While political knowledge increases support for equal opportunity, it decreases support for equal results.\n\nIt is hard to swallow the idea that if people knew more, they would agree with you less. Particularly for Althaus’s third pattern, it is tempting to dismiss the results. After all, riches and knowledge go together. Why not conclude that more informed people favor free-market policies because the rich correctly identify their own interests? This objection misses the whole point. The distribution of enlightened preferences is more promarket than the actual distribution of preferences primarily because people of all income levels become more promarket as their political knowledge increases. In fact, Althaus shows that as knowledge rises, promarket views increase disproportionately in the bottom half of the income distribution.\n\n[f0028-01]\n\nFigure 2.1 “Enlightened Preferences” for Free Market vs. Government Source: Althaus (2003: 111)\n\nThe effects that Althaus reports are often large. Of those surveyed, 62% expressed a preference for strong government over the free market; 38% took the contrary position. But estimated “enlightened preferences” were 15 percentage points more promarket; the split went from 62/38 to 47/53. The same holds for many other basic policy questions, on everything from deficit reduction (69/31 opposed becomes 52/48 in favor) to abortion on demand (54/46 opposed becomes 56/44 in favor).²⁰\n\nGetting Economics Back on Track\n\nPolitical scientists’ findings are frankly embarrassing for economists who study politics. While economists learn more and more about how government would work in theory if voters were immune to systematic error, public opinion researchers convincingly show that in practice, systematic voter error is quite real. Indeed, bias is the rule, not the exception.\n\nEconomists’ blind spot is particularly hard to excuse because they stand at the end of a long tradition with a lot to say about bias. Many of the most famous economists of the past, like Adam Smith and Frédéric Bastiat, obsessed over the public’s wrongheaded beliefs about economics, its stubborn resistance to basic principles like opportunity cost and comparative advantage. Today’s economists have not merely failed to follow relevant empirical work in a related discipline. They have also turned their backs on what economists used to know.\n\nAt least this is what economists have done as researchers. As teachers, curiously, most economists honor the wisdom of their forebears. When the latest batch of freshmen shows up for Econ 1, textbook authors and instructors still try to separate students from their prejudices—in the words of Paul Krugman, “to vaccinate the minds of our undergraduates against the misconceptions that are so predominant in educated discussion.”²¹\n\nThis peculiar disconnect between research and teaching has an important upside. The problem is not that economists have nothing to say about bias. On the contrary, the problem is that economists have a lot to say, but are reluctant to go public, to put their scientific credibility on the line. If this reluctance could be overcome, however, economics would have much to offer. Great economists have been studying systematic bias for centuries, but modern economists have failed to notify psychologists, public opinion specialists, or anyone else. Furthermore, teaching experience has given many living economists shrewd insight into the public’s biases. Human knowledge would take several steps forward if economists merely revealed what they already know.\n\nSo the glass is half full. Economics is not living up to its potential, but it has a lot of potential. Few economists are currently interested in the vital questions that public opinion researchers are asking. But economists of the past have thought profoundly about these matters, and economists of the present have more to add, even if they keep their cards close to their chest.\n\nPsychologists and public opinion researchers have made an impressive effort to educate economists about the realities of systematic bias. The communication has been largely one-way. It may be jarring, then, to hear that economists can repay the favor. After all their stern admonitions against the assumption of systematic bias, are we to believe that economists have original insights on the topic? It is out of character for economists to hold back.\n\nThere is a logical explanation. Few modern economists care about the history of thought, so many of the most penetrating discussions have been ignored or forgotten.²² Furthermore, in their dual roles as researchers and teachers, economists face starkly different incentives. It is professionally risky to emphasize systematically biased beliefs in the journals, but perfectly respectable to do so in the classroom. This is an ideal climate for ideas to quietly endure.\n\nVery well: What do economists—past and present—have to say about systematic error? Out of all the complaints that economists lodge against laymen, four families of beliefs stand out.²³ This book will refer to these families as antimarket bias, antiforeign bias, make-work bias, and pessimistic bias. Economists have long seen them as widely accepted but sadly mistaken. The rest of this chapter describes the systematic errors that economists accuse the public of making, and briefly explains why economists think they are right and the public is wrong. Formal statistical evidence waits in the next chapter.\n\nAntimarket Bias\n\nCommerce is, by its very essence, satanic.\n\n—Charles Baudelaire²⁴\n\nI first learned about farm price supports in the produce section of the grocery store. I was in kindergarten. My mother explained that price supports seemed to make fruits and vegetables more expensive, but assured me that this conclusion was simplistic. If the supports went away, so many farms would go out of business that prices would soon be higher than ever. If I had been more precocious, I would have asked a few questions. Were there price support programs for the other groceries? Why not? As it happened, though, I accepted what she told me, and felt a lingering sense that price competition is bad for buyer and seller alike.\n\nThis was one of my first memorable encounters with antimarket bias, a tendency to underestimate the economic benefits of the market mechanism.²⁵ The public has severe doubts about how much it can count on profit-seeking business to produce socially beneficial outcomes. They focus on the motives of business, and neglect the discipline imposed by competition. While economists admit that profit-maximization plus market imperfections can yield bad results, non-economists tend to view successful greed as socially harmful per se.\n\nNear the end of his life, Joseph Schumpeter eloquently captured the essence of antimarket bias:\n\nCapitalism stands its trial before judges who have the sentence of death in their pockets. They are going to pass it, whatever the defense they may hear; the only success victorious defense can possibly produce is a change in the indictment.²⁶\n\nArguably the greatest historian of economic thought, Schumpeter elsewhere matter-of-factly speaks of “the ineradicable prejudice that every action intended to serve the profit interest must be anti-social by this fact alone.”²⁷ Considering his encyclopedic knowledge, this remark speaks volumes. Antimarket bias is not a temporary, culturally specific aberration. It is a deeply rooted pattern of human thinking that has frustrated economists for generations.²⁸\n\nEconomists across the political spectrum criticize antimarket bias. Liberal Democratic economists echo and amplify Schumpeter’s theme. Charles Schultze, head of Jimmy Carter’s Council of Economic Advisors, proclaims, “Harnessing the ‘base’ motive of material self-interest to promote the common good is perhaps the most important social invention mankind has yet achieved.” But politicians and voters fail to appreciate this invention. “The virtually universal characteristic of [environmental] policy . . . is to start from the conclusion that regulation is the obvious answer; the pricing alternative is never considered.”²⁹\n\nProjecting your own preferences onto the majority is a cliché of democratic politics. Pundits rarely proclaim, “The American people want X, but they’re wrong.” In the face of antimarket bias, however, many economists loudly defy public opinion. It would be hard to find an economist more in favor of free markets than Ludwig von Mises. Yet does he argue that unresponsive elites force big government on an unwilling majority? No, he freely grants that the policies he opposes reflect the will of the people: “There is no use in deceiving ourselves. American public opinion rejects the market economy.”³⁰ The problem with democracy is not politicians’ shirking, but the public’s antimarket bias:\n\nFor more than a century public opinion in Western countries has been deluded by the idea that there is such a thing as “the social question” or “the labor problem.” The meaning implied was that the very existence of capitalism hurts the vital interests of the masses, especially those of the wage earners and the small farmers. The preservation of this manifestly unfair system cannot be tolerated; radical reforms are indispensable.\n\nThe truth is that capitalism has not only multiplied population figures but at the same time improved the people’s standard of living in an unprecedented way.³¹\n\nThere are too many variations on antimarket bias to list them all. Probably the most common is to equate market payments with transfers, ignoring their incentive properties.³² (A “transfer,” in economic jargon, is a no-strings-attached movement of wealth from one person to another.) All that matters, then, is how much you empathize with the transfer’s recipient compared to the transfer’s provider. To take the classic case: People tend to see profits as a gift to the rich. So unless you perversely pity the rich more than the poor, limiting profits seems like common sense.\n\nEconomists across the ideological spectrum find it hard to respond to this outlook with anything but derision. Profits are not a handout, but a quid pro quo: “If you want to get rich, then you have to do something people will pay for.” Profits give incentives to reduce production costs, move resources from less-valued to more-valued industries, and dream up new products. This is the central lesson of The Wealth of Nations: the “invisible hand” quietly persuades selfish businessmen to serve the public good:\n\nEvery individual is continually exerting himself to find out the most advantageous employment for whatever capital he can command. It is his own advantage, indeed, and not that of the society, which he has in view. But the study of his own advantage naturally, or rather necessarily leads him to prefer that employment which is most advantageous to the society.³³\n\nFor modern economists, these are truisms, but they usually miss the deeper lesson. If Adam Smith’s observations are only truisms, why did he bother to write them? Why do teachers of economics keep quoting and requoting this passage? Because Smith’s thesis was counterintuitive to his contemporaries, and remains counterintuitive today. A truism for the few is heresy for the many. Smith, being well aware of this fact, tries to shock readers out of their dogmatic slumber: “By pursuing his own interest he frequently promotes that of the society more effectually than when he really intends to promote it. I have never known much good done by those who affected to trade for the publick good.”³⁴ Business profit appears to be a transfer but benefits society; business philanthropy appears to benefit society but is at best a transfer.\n\nThe same applies to other unpopular “windfalls.” Attacks on “obscene profits” dominate antimarket thought in recent centuries, but in earlier times the leading culprit was interest or “usury.”³⁵ In popular imagination, interest has but one effect: enriching moneylenders and impoverishing those who depend upon them. In his classic Capital and Interest, Eugen von Böhm-Bawerk observes that prejudice against debt markets goes back millennia:\n\nThe creditor is usually rich, the debtor poor, and the former appears in the hateful light of a man who squeezes from the little that the poor man has, something, in the shape of interest, that he can add to his own superfluous riches. It is not to be wondered at, then, that both the ancient world and especially the Christian Middle Ages were exceedingly unfavorable to interest.³⁶\n\nTimur Kuran’s dissection of Islamic economics reports that opposition to interest has recently enjoyed a powerful revival:\n\nTo be recognized as an Islamic economist it is not sufficient to be a learned Muslim who contributes to economic debates. One must be opposed in principle to all interest.³⁷\n\nInterest is economic enemy number one throughout the Muslim world, and many governments actively favor interest-free “Islamic banking”:\n\nThe objective is not simply to make Islamic banking more accessible. It is to make all banking Islamic. Certain campaigns against conventional banking have succeeded in making “interest-laden” banking illegal. In Pakistan all banks were ordered in 1979 to purge interest from their operations within five years, and in 1992 the Sharia court removed various critical exemptions. Interest prohibitions have gone into effect also in Iran and the Sudan.³⁸\n\nWhat is everyone from ancient Athens to modern Islamabad missing? Like profit, interest is not a gift, but a quid pro quo: The lender earns interest in exchange for delaying his consumption. A government that successfully stamped out interest payments would be no friend to those in need of credit, for the same stamp would crush lending as well.\n\nSkipping ahead to the present, Alan Blinder blames opposition to tradable pollution permits on antimarket bias.³⁹ Why let people “pay to pollute,” when we can force them to cease and desist? The textbook answer is that tradable permits get you more pollution abatement for the same cost. The firms able to cheaply cut their emissions do so, selling their excess pollution quota to less flexible polluters. End result: More abatement bang for your buck. A price for pollution is therefore not a pure transfer; it creates incentives to improve environmental quality as cheaply as possible. But noneconomists disagree—including relatively sophisticated policy insiders. Blinder discusses a fascinating survey of 63 environmentalists, congressional staffers, and industry lobbyists. Not one could explain economists’ standard rationale for tradable permits.⁴⁰\n\nThe second most prominent avatar of antimarket bias is monopoly theories of price. Economists obviously acknowledge that monopolies exist. But the public habitually makes “monopoly” a scapegoat for scarcity.⁴¹ The idea that supply and demand usually controls prices is hard to accept. Even in industries with many firms, noneconomists treat prices as a function of their CEO’s intentions and conspiracies. Economists understand, however, that collusion is a Prisoners’ Dilemma.⁴² If an industry has more than a handful of firms, industry-wide conspiracies are unlikely to succeed.\n\nHistorically, it has been especially common for the public to pick out middlemen as uniquely vicious “monopolists.” Look at these parasites: They buy products, “mark them up,” and then resell us the “exact same thing.” Bastiat attacks contemporary socialists for “hate speech” against the middleman:\n\nThey would willingly eliminate the capitalist, the banker, the speculator, the entrepreneur, the businessman, and the merchant, accusing them of interposing themselves between producer and consumer in order to fleece them both, without giving them anything of value. . . . Then, with the aid of those high-sounding words: Exploitation of man by man, speculation in hunger, monopoly, they set themselves to blackening the name of business and throwing a veil over its benefits.⁴³\n\nWhat could these so-called benefits possibly be? Economists have a standard response. Transportation, storage, and distribution are valuable services—a fact that becomes obvious whenever you need a cold drink in the middle of nowhere. And like most valuable services, they are not costless. The most that is reasonable to ask, then, is not that middlemen work for free, but that they face the daily test of competition. Given the large number of firms one typically sees in these markets, economists find accusations of “monopoly” fairly bizarre.⁴⁴\n\nWhile we are on the subject, we should not forget a conspiracy theory that is as popular as it is preposterous: Capitalists join forces to keep wages at the subsistence level. Many still see Third World economies through this lens, and tell a watered-down version of the same story for the First. But there are literally millions of employers in the First World. Just imagining the logistics of such a plot is laughable. Its more literate defenders point out that Adam Smith himself worried about employer conspiracies,⁴⁵ conveniently overlooking the fact that in Smith’s time, high transportation and communication costs left workers with far fewer alternative employers.\n\nWhat about the Third World? The number of employment options is often substantially lower. But if there really were a vast employer conspiracy to hold down wages, the Third World would be an especially profitable place to invest. Query: Does investing your life savings in poor countries seem like a painless way to get rich quick? If not, you at least tacitly accept economists’ sad-but-true theory of Third World poverty: Its workers earn low wages because their productivity is low.⁴⁶\n\nCollusion aside, the public’s implicit model of price determination is that businesses are monopolists of variable altruism. If a CEO feels greedy when he wakes up, he raises his price—or puts low-quality merchandise on the shelves. Nice guys charge fair prices for good products; greedy scoundrels gouge with impunity for junk. It is only a short step for market skeptics to add, “And nice guys finish last.” As John Mueller emphasizes, the public links greed with almost everything bad: Capitalism is “commonly maligned for the deceit, unfairness, dishonesty, and discourtesy that are widely taken to be the inevitable consequences of its apparent celebration of greed.”⁴⁷ Or as villainous innkeeper Thenardier sings in Les Misérables:\n\nCharge ’em for the lice,\n\nExtra for the mice,\n\nTwo percent for looking in the mirror twice!\n\nHere a little slice,\n\nThere a little cut,\n\nThree percent for sleeping with the window shut!\n\nWhen it comes to fixing prices,\n\nThere are a lot of tricks he knows.\n\nHow it all increases,\n\nAll those bits and pieces,\n\nJesus! It’s amazing how it grows\\!⁴⁸\n\nNever mind that Thenardier is bankrupt before the end of the first act. Presumably he was run out of business by an even greedier competitor.\n\nWhere does the public go wrong? For one thing, asking for more can get you less. Giving your boss the ultimatum, “Double my pay or I quit” usually ends badly. The same holds in business: raising price and cutting quality often leads to lower profits, not higher. Mueller makes the deeper point that many strategies that work as a one-shot scam backfire as routine policies.⁴⁹ It is hard to make a profit if no one sets foot in your store twice. Intelligent greed militates against “deceit, unfairness, dishonesty, and discourtesy” because they damage the seller’s reputation.\n\nAn outsider who eavesdrops on Krugman’s or Stiglitz’s debates with other economists might get the impression that the benefits of markets remain controversial.⁵⁰ To understand the conversation, you have to notice what economists are not debating. They are not debating whether prices give incentives, or if a vast business conspiracy runs the world. Almost all economists recognize the core benefits of the market mechanism; they disagree only at the margin.\n\nAntiforeign Bias\n\nThe impressive fact about ordinary Americans is that,\ndespite years of education and propaganda, they still cling\nstubbornly to their skepticism about the global economy.\nWith their usual condescension, elite commentators\ndismiss the popular expressions of concern as uninformed\nand nativist, the misplaced fears of people ill\nequipped to grasp the larger dimensions of economics.\n\n—William Greider, Who Will Tell the People?⁵¹\n\nA shrewd businessman I know has long thought that everything wrong in the American economy could be solved with two expedients:\n\n1. A naval blockade of Japan.\n\n2. A Berlin Wall at the Mexican border.\n\nThis is only a mild caricature of his position, which is all the more puzzling because he usually gets the mutual benefits of trade. He does well on eBay. But like most noneconomists, he suffers from antiforeign bias, a tendency to underestimate the economic benefits of interaction with foreigners.⁵² When outsiders emerge on the economic scene, they do a mental double take: “Foreigners? Could it really be mutually beneficial for us to trade with them?”\n\nPopular metaphors equate foreign trade with racing and warfare, so you might say that antiforeign views are embedded in our language. Perhaps foreigners are sneakier, craftier, or greedier. Whatever the reason, they supposedly have a special power to exploit us. As Newcomb explains:\n\nIt has been assumed as an axiom which needs no proof, because none would be so hardy as to deny it, that foreign nations cannot honestly be in favor of any trade with us that is not to our disadvantage; that the very fact that they want to trade with us is a good reason for receiving their overtures with suspicion and obstructing their wishes by restrictive legislation.⁵³\n\nAlan Blinder echoes Newcomb’s lament a century later. People around the world scapegoat foreigners:\n\nWhen jobs are scarce, the instinct for self-preservation is strong, and the temptation to blame foreign competitors is all but irresistible. It was not only in the United States that the bunker mentality took hold. That most economists branded the effort to save jobs by protectionism shortsighted and self-defeating was beside the point. Legislators are out to win votes, not intellectual kudos.⁵⁴\n\nThere is probably no other popular opinion that economists have found so enduringly objectionable. In The Wealth of Nations, Smith admonishes his countrymen:\n\nWhat is prudence in the conduct of every private family, can scarce be folly in a great kingdom. If a foreign country can supply us with a commodity cheaper than we ourselves can make it, better buy it of them with some part of the produce of our own industry, employed in a way in which we have some advantage.⁵⁵\n\nAs far as his peers were concerned, Smith’s arguments won the day. Over a century later, Newcomb could securely observe in the Quarterly Journal of Economics that “one of the most marked points of antagonism between the ideas of the economists since Adam Smith and those which governed the commercial policy of nations before his time is found in the case of foreign trade.”⁵⁶ There was a little backsliding during the Great Depression,⁵⁷ but economists’ pro-foreign views abide to this day. Even theorists like Paul Krugman who specialize in exceptions to the optimality of free trade frequently downplay their findings as curiosities:\n\nThis innovative stuff is not a priority for today’s undergraduates. In the last decade of the 20th century, the essential things to teach students are still the insights of Hume and Ricardo. That is, we need to teach them that trade deficits are self-correcting and that the benefits of trade do not depend on a country having an absolute advantage over its rivals.⁵⁸\n\nEconomists are especially critical of the antiforeign outlook because it does not just happen to be wrong; it frequently conflicts with elementary economics. Textbooks teach that total output increases if producers specialize and trade. On an individual level, who could deny it? Imagine how much time it would take to grow your own food, when a few hours’ wages spent at the grocery store feed you for weeks. Analogies between individual and social behavior are at times misleading, but this is not one of those times. International trade is, as Steven Landsburg explains, a technology:\n\nThere are two technologies for producing automobiles in America. One is to manufacture them in Detroit, and the other is to grow them in Iowa. Everybody knows about the first technology; let me tell you about the second. First you plant seeds, which are the raw materials from which automobiles are constructed. You wait a few months until wheat appears. Then you harvest the wheat, load it onto ships, and sail the ships westward into the Pacific Ocean. After a few months, the ships reappear with Toyotas on them.⁵⁹\n\nAnd this is one amazing technology. The Law of Comparative Advantage, one of most fascinating theorems in economics, shows that mutually beneficial international trade is possible even if one nation is less productive in every way.⁶⁰ Suppose an American can make 10 cars or five bushels of wheat, and a Mexican can make one car or two bushels of wheat. Though the Americans are better at both tasks, specialization and trade increase production. If one American switches from wheat to cars, and three Mexicans switch from cars to wheat, world output goes up by two cars plus one bushel of wheat.\n\nHow can anyone overlook trade’s remarkable benefits? Adam Smith, along with many 18th- and 19th-century economists, identifies the root error as misidentification of money and wealth: “A rich country, in the same manner as a rich man, is supposed to be a country abounding in money; and to heap up gold and silver in any country is supposed to be the best way to enrich it.”⁶¹ It follows that trade is zero-sum, since the only way for a country to make its balance more favorable is to make another country’s balance less favorable.\n\nEven in Smith’s day, however, his story was probably too clever by half. The root error behind 18th-century mercantilism was unreasonable distrust of foreigners. Otherwise, why would people focus on money draining out of “the nation,” but not “the region,” “the city,” “the village,” or “the family”? Anyone who consistently equated money with wealth would fear all outflows of precious metals. In practice, human beings then and now commit the balance-of-trade fallacy only when other countries enter the picture. No one loses sleep about the trade balance between California and Nevada, or me and Tower Records. The fallacy is not treating all purchases as a cost, but treating foreign purchases as a cost.⁶²\n\nModern conditions do make antiforeign bias easier to spot. To take one prominent example, immigration is far more of an issue now than it was in Smith’s time. Economists are predictably quick to see the benefits of immigration. Trade in labor is roughly the same as trade in goods. Specialization and exchange raise output—for instance, by letting skilled American moms return to work by hiring Mexican nannies.\n\nIn terms of the balance of payments, immigration is a nonissue. If an immigrant moves from Mexico City to New York, and spends all his earnings in his new homeland, the balance of trade does not change. Yet the public still looks on immigration as a bald misfortune: jobs lost, wages reduced, public services consumed. Many see a larger trade deficit as a fair price to pay for reduced immigration. One peculiar pro-NAFTA argument is that if we admit more Mexican goods, we will have fewer Mexicans.⁶³ It should be evident, then, that the general public sees immigration as a distinct danger—independent of, and more frightening, than an unfavorable balance of trade. People feel all the more vulnerable when they reflect that these foreigners are not just selling us their products. They live among us.\n\nIt is misleading, however, to think about “foreignness” as either/or. From the viewpoint of the typical American, Canadians are less foreign than the British, who are in turn less foreign than the Japanese. During 1983–87, 28% of Americans in the General Social Survey admitted they disliked Japan, but only 8% disliked England, and a scant 3% disliked Canada.⁶⁴ It is not surprising, then, that the degree of anti-foreign bias varies by country. Objective measures like the volume of trade or the trade deficit are often secondary to physical, linguistic, and cultural similarity. Trade with Canada or Great Britain generates only mild alarm compared to trade with Mexico or Japan. U.S. imports from, and trade deficits with, Canada exceeded those with Mexico every year from 1985 to 2004.⁶⁵ During the anti-Japan hysteria of the eighties, British foreign direct investment in the United States always exceeded that of the Japanese by at least 50%.⁶⁶ Foreigners who look like us and speak English are hardly foreign at all.\n\nCalm reflection on the international economy reveals much to be thankful for, and little to fear. On this point, economists past and present agree. But an important proviso lurks beneath the surface. Yes, there is little to fear about the international economy itself. But modern researchers—unlike economists of the past and teachers of the present—rarely mention that attitudes about the international economy are another story. Paul Krugman hits the nail on the head: “The conflict among nations that so many policy intellectuals imagine prevails is an illusion; but it is an illusion that can destroy the reality of mutual gains from trade.”⁶⁷\n\nMake-Work Bias\n\nWhat we should wish for, clearly, is that each hectare\nof land produce little wheat, and that each kernel of\nwheat contain little sustenance—in other words, that our\nland should be unfruitful. . . . [O]ne could even say that\njob opportunities would be in direct proportion to this\nunfruitfulness. . . . What we should desire still more is\nthat human intelligence should be enfeebled or\nextinguished; for, so long as it survives, it ceaselessly\nendeavors to increase the ratio of the end to the means\nand of the product to the effort.\n\n—Frédéric Bastiat, Economic Sophisms⁶⁸\n\nI was an undergraduate when the Cold War ended, and I can still remember talking about military spending cuts with a conservative student. The whole idea made her nervous. Why? Because she had no idea how a market economy would absorb the discharged soldiers. She did not even distinguish between short-term and long-term consequences of the cuts; in her mind, to layoff 100,000 government employees was virtually equivalent to disemploying 100,000 people for life. Her position is particularly striking if you realize that her objection applies equally well to spending on government programs that—as a conservative—she opposed.\n\nIf a well-educated individual ideologically opposed to wasteful government spending thinks like this, it is hardly surprising that she is not alone. The public often literally believes that labor is better to use than conserve. Saving labor, producing more goods with fewer man-hours, is widely perceived not as progress, but as a danger. I call this make-work bias, a tendency to underestimate the economic benefits of conserving labor.⁶⁹ Where noneconomists see the destruction of jobs, economists see the essence of economic growth—the production of more with less. Alan Blinder explains:\n\nIf you put the question directly, “Is higher productivity better than lower productivity?,” few people will answer in the negative. Yet policy changes are often sold as ways to “create jobs.” . . . Jobs can be created in two ways. The socially beneficial way is to enlarge GNP, so that there will be more useful work to be done. But we can also create jobs by seeing to it that each worker is less productive. Then more labor will be required to produce the same bill of goods. The latter form of job creation does raise employment; but it is the path to rags, not riches.⁷⁰\n\nFor an individual to prosper, he only needs to have a job. But society can only prosper if individuals do a job, if they create goods and services that someone wants.\n\nEconomists have been at war with make-work bias for centuries. Bastiat ridicules the equation of prosperity with jobs as “Sisyphism,” after the mythological fully-employed Greek who was eternally condemned to roll a boulder up a hill. In the eyes of the public:\n\nEffort itself constitutes and measures wealth. To progress is to increase the ratio of effort to result. Its ideal may be represented by the toil of Sisyphus, at once barren and eternal.⁷¹\n\nIn contrast, for the economist:\n\nWealth . . . increases proportionately to the increase in the ratio of result to effort. Absolute perfection, whose archetype is God, consists in the widest possible distance between these two terms, that is, a situation in which no effort at all yields infinite results.⁷²\n\nIn the 1893 Quarterly Journal of Economics, Simon Newcomb explains:\n\nThe divergence between the economist and the public is by no means confined to foreign trade. We find a direct antagonism between them on nearly every question involving the employment of labor. . . . The idea that the utility and importance of an industry are to be measured by the employment which it gives to labor is so deeply rooted in human nature that economists can scarcely claim to have taken the first step towards its eradication.⁷³\n\nHis last remark is particularly striking. Nineteenth-century economists believed they had diagnosed enduring economic confusions, not intellectual fads, and they were right. Almost a hundred years after Newcomb, Alan Blinder makes the same lament. But Blinder’s critique of make-work bias, unlike Newcomb’s, did not appear in a leading academic journal like the QJE. He had to venture beyond the ivory tower with a popular book to find his audience. Referees would almost certainly have taken issue with Blinder—not because modern economists agree with make-work bias, but because it is disreputable to claim that anyone embraces such folly.\n\nBut embrace it they do. The crudest form of make-work bias is Luddite fear of the machine. Common sense proclaims that machines make life easier for human beings. The public qualifies this “naive” position by noting that machines also make people’s lives harder by throwing them out of work. And who knows? Maybe the second effect dominates the first. During the Great Depression, intellectual fads like Howard Scott’s “technocracy” movement blamed the nation’s woes on technological progress.\n\nAs Scott saw the future, the inexorable increase in productivity, far outstripping opportunities for employment or investment, must mean permanent and growing unemployment and permanent and growing debt, until capitalism collapsed under the double load.⁷⁴\n\nEconomists’ love of qualification is notorious, but most doubt that the protechnology position needs to be qualified. Technology often creates new jobs; without the computer, there would be no jobs in computer programming or software development. But the fundamental defense of labor-saving technology is that employing more workers than you need wastes valuable labor. If you pay a worker to twiddle his thumbs, you could have paid him to do something socially useful instead.\n\nEconomists add that market forces readily convert this potential social benefit into an actual one. After technology throws people out of work, they have an incentive to find a new use for their talents. Cox and Alm aptly describe this process as “churn”: “Through relentless turmoil, the economy re-creates itself, shifting labor resources to where they’re needed, replacing old jobs with new ones.”⁷⁵ They illustrate this process with history’s most striking example: The drastic decline in agricultural employment:\n\nIn 1800, it took nearly 95 of every 100 Americans to feed the country. In 1900, it took 40. Today, it takes just 3. . . . The workers no longer needed on farms have been put to use providing new homes, furniture, clothing, computers, pharmaceuticals, appliances, medical assistance, movies, financial advice, video games, gourmet meals, and an almost dizzying array of other goods and services. . . . What we have in place of long hours in the fields is the wealth of goods and services that come from allowing the churn to work, wherever and whenever it might occur.⁷⁶\n\nThese arguments sound harsh. That is part of the reason why they are so unpopular: people would rather feel compassionately than think logically. Many economists advocate government assistance to cushion displaced workers’ transition, and retain public support for a dynamic economy. Alan Blinder recommends extended unemployment insurance, retraining, and relocation subsidies.⁷⁷ Other economists disagree. But almost all economists grant that stopping transitions has a grave cost.\n\nExasperating as the Luddite mentality is, countries rarely move beyond rhetoric and turn back the clock of technology. But you cannot say the same about another controversy infused with make-work bias: hostility to downsizing. What could possibly be good about downsizing? Every time we figure out how to accomplish a goal using fewer workers, it enriches society, because labor is a valuable resource.\n\nWe have a tremendous stake in allowing the churn to grind forward, putting our labor resources to work raising living standards, to give us more for less. We can’t get around it: The churn’s promise of higher living standards can’t be reaped without job losses. . . . Downsizing companies will be vilified for making what appear to be hardhearted decisions. When passions cool, however, there ought to be time to recognize that, in most cases, the dirty work had to be done.⁷⁸\n\nInside of a household, everyone understands what Cox and Alm call “the upside of downsizing.”⁷⁹ You do not worry about how to spend the hours you save when you buy a washing machine. There are always other ways to spend your time. Bastiat insightfully observes that a loner would never fall prey to make-work bias:\n\nNo solitary man would ever conclude that, in order to make sure that his own labor had something to occupy it, he should break the tools that save him labor, neutralize the fertility of the soil, or return to the sea the goods it may have brought him. . . . He would understand, in short, that a saving in labor is nothing else than progress.⁸⁰\n\nThe existence of an exchange economy is a necessary condition for make-work confusion to arise.\n\nBut exchange hampers our view of so simple a truth. In society, with the division of labor that it entails, the production and the consumption of an object are not performed by the same individual. Each person comes to regard his own labor no longer as a means, but as an end.⁸¹\n\nIf you receive a washing machine as a gift, the benefit is yours; you have more free time and the same income. If you get downsized, the benefit goes to other people; you have more free time, but your income temporarily falls. In both cases, though, society conserves valuable labor.\n\nPessimistic Bias\n\nTwo [more] generations should saturate the world with\npopulation, and should exhaust the mines. When that\nmoment comes, economical decay, or the decay of\neconomical civilization, should set in.\n\n—Henry Adams, 1898⁸²\n\nI first encountered antidrug propaganda in second grade. It was called “drug education,” but it was mostly scary stories. I was told that kids around me were using drugs, and that a pusher would soon offer me some, too. Teachers warned that more and more kids would become addicts, and by the time I was in junior high I would be surrounded by them. Authority figures would occasionally speculate about our adulthood, and wonder how a country could function with such a degenerate workforce. Yet another reason, they mused, that this country is going downhill.\n\nThe junior high dystopia never materialized. I am still waiting to be offered drugs. By the time I reached adulthood, it was apparent that most people were not going to their jobs high on PCP. Generation X used its share of illegal narcotics, but its entry into the workforce accompanied the marvels of the Internet age, not a stupor-induced decline in productivity and innovation.\n\nMy teachers’ predictions about America’s economic future turned out to be laughable. But they fit nicely into a larger pattern. As a general rule, the public believes economic conditions are not as good as they really are. It sees a world going from bad to worse; the economy faces a long list of grim challenges, leaving little room for hope. I refer to the public’s leanings as pessimistic bias, a tendency to overestimate the severity of economic problems and underestimate the (recent) past, present, and future performance of the economy.⁸³\n\nAdam Smith famously ridiculed such attitudes with a one-liner: “There is a great deal of ruin in a nation.”⁸⁴ His point, which economists often echo, is that the public lacks perspective. A large economy can and usually does progress despite interminable setbacks. While economists debate about how much growth to expect, public discourse thinks in terms of stagnation versus decline.\n\nSuppose a congenitally pessimistic doctor examines a patient. There are two kinds of errors to watch out for. For one thing, he would exaggerate the severity of the patient’s symptoms. After finding a body temperature of 100 degrees, the doctor might exclaim that the patient has a “dangerous fever.” But the doctor might also err in his overall judgment, giving the patient two weeks to live.\n\nPessimism about the economy exhibits the same structure. You may be pessimistic about symptoms, overblowing the severity of everything from the deficit to affirmative action. But you can also be pessimistic overall, seeing negative trends in living standards, wages, and inequality. Public opinion is marked by pessimism in both its forms. Economists constantly advise the public not to lose sleep over the latest economic threat in the news.⁸⁵ But they also make a habit of explaining how far mankind has come in the last hundred years, pointing out massive gains we take for granted.⁸⁶\n\nA staple of pessimistic rhetoric is to idealize conditions in the more distant past in order to put recent conditions in a negative light. Arthur Herman’s The Idea of Decline in Western History asserts that “Virtually every culture past or present has believed that men and women are not up to the standards of their parents and forebears,” and asks, “Why is this sense of decline common to all cultures?”⁸⁷ In Primitivism and Related Ideas in Antiquity, Arthur Lovejoy and George Boas second the view that this pessimistic illusion is nearly universal:\n\nIt is a not improbable conjecture that the feeling that humanity was becoming over-civilized, that life was getting too complicated and over-refined, dates from the time when the cave-men first became such. It can hardly be supposed—if the cave-men were at all like their descendants—that none among them discoursed with contempt on the cowardly effeminacy of living under shelter or upon the exasperating inconvenience of constantly returning for food and sleep to the same place instead of being free to roam at large in wide-open spaces.⁸⁸\n\nPessimistic bias has a smaller role in the oral tradition of economics than antimarket, antiforeign, or make-work bias. Famous economists of the past frequently overlook it; teachers of economics spend relatively little time rooting it out. But while the voice of oral tradition is softer than usual, it is not silent. Though he did not live to see the Industrial Revolution, Adam Smith declares progress the normal course of events:\n\nThe uniform, constant, and uninterrupted effort of every man to better his condition . . . is frequently powerful enough to maintain the natural progress of things toward improvement, in spite both of the extravagance of government, and of the greatest errors of administration. Like the unknown principle of animal life, it frequently restores health and vigour to the constitution, in spite, not only of the disease, but of the absurd prescriptions of the doctor.⁸⁹\n\nHowever, progress is so gradual that a few pockets of decay hide it from the public view:\n\nTo form a right judgment of it, indeed, we must compare the state of the country at periods somewhat distant from one another. The progress is frequently so gradual that, at near periods, the improvement is not only not sensible, but from the declension either of certain branches of industry, or of certain districts of the country, things which sometimes happen though the country in general be in great prosperity, there frequently arises a suspicion that the riches and industry of the whole are decaying.⁹⁰\n\nDavid Hume—economist, philosopher, and Adam Smith’s best friend—blames popular pessimism on our psychology, not the slow and uneven nature of progress: “The humour of blaming the present, and admiring the past, is strongly rooted in human nature, and has an influence even on persons endued with the profoundest judgment and most extensive learning.”⁹¹ Hume elsewhere appeals to pessimistic bias to account for superstition: “Where real terrors are wanting, the soul, active to its own prejudice, and fostering its predominant inclination, finds imaginary ones, to whose power and malevolence it sets no limits.”⁹²\n\nDespite these promising beginnings, 19th-century economists did little to develop the theme of pessimistic bias. Bastiat and Newcomb say little about it. Nineteenth-century socialists who predicted “immiseration” of the working class met intellectual resistance from economists. But the root of the socialists’ forecast was hostility to markets, not pessimism as such. Economists often ridiculed socialists for their wild optimism about the impending socialist utopia.⁹³\n\nNineteenth-century opponents of doom and gloom are easier to find in sociology. Alexis de Tocqueville attacks pessimism as “the great sickness of our age.”⁹⁴ Herbert Spencer finds it exasperating that “the more things improve the louder become the exclamations about their badness.”⁹⁵ When problems—from mistreatment of women to illiteracy to poverty—are serious, people take them for granted. As conditions improve, the public believes ever more strongly that things have never been worse.\n\nYet while elevation, mental and physical, of the masses is going on far more rapidly than ever before—while the lowering of the death-rate proves that the average life is less trying, there swells louder and louder the cry that the evils are so great that nothing short of a social revolution can cure them. In presence of obvious improvements . . . it is proclaimed, with increasing vehemence, that things are so bad that society must be pulled to pieces and re-organised on another plan.⁹⁶\n\nEven leading optimists grant that pessimistic bias has grown worse in the modern era. Herman maintains that it peaked soon after the end of World War I, when “Talking about the end of Western civilization had become as natural as breathing. The only subject left to debate was not whether the modern West was doomed but why.” But pessimism remains at strangely high levels: “While intellectuals have been predicting the imminent collapse of Western civilization for more than one hundred and fifty years, its influence has grown faster during that period than at any time in history.”⁹⁷\n\nHow can high levels of pessimism coexist with constantly rising standards of living?⁹⁸ Though pessimism has abated since World War I, the gap between objective conditions and subjective perceptions is arguably greater than ever.⁹⁹ Gregg Easterbrook ridicules the failure of the citizens of the developed world to appreciate their good fortune:\n\nOur forebears, who worked and sacrificed tirelessly in their hopes their descendants would someday be free, comfortable, healthy, and educated, might be dismayed to observe how acidly we deny we now are these things.¹⁰⁰\n\nLike David Hume, economists Cox and Alm appeal to fundamental human psychology to explain our pessimism: “The present almost always pales when measured against ‘the good old days.’ ” Mild forms of this bias sustain lingering economic malcontent: “Nostalgists often ignore improvements in goods and services, yet remember fondly the prices they paid long ago for the cheapest versions of products.”¹⁰¹ Strong forms make us “open-minded” to paranoid fantasies:\n\nSome part of human nature connects with the apocalyptic. Time and again, pessimists among us have envisioned the world going straight to hell. Never mind that it hasn’t: A lot of us braced for the worst. Whether the source is the Bible or Nostradamus, Thomas Malthus, or the Club of Rome, predictions of calamity aren’t easily ignored, no matter how many times we wake up in the morning after the world was supposed to end.¹⁰²\n\nThere is an ongoing debate about growth slowdown. This is what relatively pessimistic economists like Paul Krugman mean when they say that “the U.S. economy is doing badly.”¹⁰³ Other economists counter that standard numbers inadequately adjust for the rising quality and variety of the consumption basket, and the changing composition of the workforce. The rapid growth of the 1990s raised more doubts.¹⁰⁴ Either way, the worst-case scenario GDP statistics permit—a lower speed of progress—is no disaster. In the face of popular economic pessimism, Krugman, too, exclaims: “I have seen the present, and it works!”¹⁰⁵\n\nThe intelligent pessimist’s favorite refuge is to argue that standard statistics like GDP miss important components of our standard of living. The leading candidate is environmental quality, where negative thinking is firmly ensconced—to put it mildly.¹⁰⁶ Pessimists often add that our failure to deal with environmental destruction will soon morph into economic disaster as well. In the 1960s, über-pessimist Paul Ehrlich notoriously predicted that environmental neglect would shortly lead to mass starvation.¹⁰⁷ If resources are rapidly vanishing as our numbers multiply, human beings are going to be poor and hungry, not just out of touch with Mother Earth.\n\nA number of economists have met these challenges. The most wide-ranging is Julian Simon, who argues that popular “doom and gloom” views of resource depletion, overpopulation, and environmental quality are exaggerated, and often the opposite of the truth.¹⁰⁸ Past progress does not guarantee future progress, but it creates a strong presumption:\n\nThroughout the long sweep of history, forecasts of resource scarcity have always been heard, and—just as now—the doomsayers have always claimed that the past was no guide to the future because they stood at a turning point in history. . . . In every period those who would have bet on improvement rather than deterioration in fundamental aspects of material life—such as the availability of natural resources—would usually have been right.¹⁰⁹\n\nSimon has been a lightning rod for controversy, but his main theses—that natural resources are getting cheaper, population density is not bad for growth, and air quality is improving—are now almost mainstream in environmental economics.¹¹⁰ Since Michael Kremer’s seminal paper “Population Growth and Technological Change: One Million B.C. to 1990,” even Simon’s “extreme” view that population growth raises living standards has gained wide acceptance.¹¹¹ The upshot: Refining measures of economic welfare does not revive the case for pessimism. In fact, more inclusive measures cement the case for optimism, because life has also been getting better on the neglected dimensions.¹¹² The question “Aren’t you worried that declining environmental quality is going to destroy our material prosperity?” is therefore reminiscent of “Do you still beat your mother?”\n\nConclusion\n\nEconomists have a love/hate relationship with systematic bias. As theorists, they deny its existence. As empiricists, they increasingly import it from other fields. But when they teach, address the public, or wonder what is wrong with the world, they dip into their own “private stash.” On some level, economists not only recognize that systematically biased beliefs exist. They think they have discovered virulent strains in their own backyard—systematically biased beliefs about economics.¹¹³\n\nAntimarket bias, antiforeign bias, make-work bias, and pessimistic bias are the most prominent specimens. Indeed, they are so prominent that one can hardly teach economics without bumping into them. Students of economics are not a blank slate for their teachers to write on. They arrive with strong prejudices. They underestimate the benefits of markets. They underestimate the benefits of dealing with foreigners. They underestimate the benefits of conserving labor. They underestimate the performance of the economy, and overestimate its problems.\n\nBut economists’ love/hate relationship with systematic bias raises some doubts. If the leading figures in the history of economics took the existence of these biases for granted, if teachers of economics grapple with them over and over in the classroom, what happens when we put these biases under the microscope of modern research? Do they hold up to empirical scrutiny? Or are they just stories that economists have been telling themselves all these years?\n\nChapter 3\n\nEVIDENCE FROM THE SURVEY OF\n\nAMERICANS AND ECONOMISTS\n\nON THE ECONOMY\n\nIt seems, then, that I am asserting that the conventional\nwisdom about international trade is dominated by\nentirely ignorant men, who have managed to convince\nthemselves and everyone else who matters that they\nhave deep insights, but are in fact unaware of the most\nbasic principles of and facts about the world economy;\nand that the disdained academic economists are at least\nby comparison fonts of wisdom and common sense.\nAnd that is indeed my claim.\n\n—Paul Krugman, Pop Internationalism¹\n\nECONOMISTS from Smith, Bastiat, and Newcomb to Mises, Blinder, and Krugman maintain that the public suffers from systematically biased beliefs about economics. Are they right? We can judge an argument about, say, comparative advantage, on its own merits. But that is not enough to establish the existence of a systematic bias. Once you know that economic view X is correct, you still have to verify that, by and large, (a) economists believe X, and (b) noneconomists believe not-X. Is it really the case, for example, that economists are more upbeat about the effects of international competition than noneconomists?\n\nThese are quintessentially empirical questions. Teaching experience carries some weight: Can economists have been misreading their students for centuries? But personal impressions are not good enough. When psychologists and political scientists talk about bias, they back up their claims with hard data. Economists who want to join the discussion have to do the same.\n\nThere are numerous surveys of the economic beliefs of both economists and the general public.² They broadly confirm the “wide divergence” with which Newcomb maintained “all are familiar.” Take the case of free trade versus protection. A long-running survey initiated by J. R. Kearl and coauthors has repeatedly asked economists whether they agree that “tariffs and import quotas usually reduce the general welfare of society.”³ In 2000, 72.5% mainly agreed, and an additional 20.1% agreed with provisos; only 6% generally disagreed. The breakdowns for 1990 and the late 1970s are even more lopsided in favor of free trade.\n\nWhat about the public’s views on this matter? The carefully constructed Worldviews survey⁴ has repeatedly asked a random sample of Americans the following:\n\nIt has been argued that if all countries would eliminate their tariffs and restrictions on imported goods, the costs of goods would go down for everyone. Others have said that such tariffs and restrictions are necessary to protect certain manufacturing jobs in certain industries from the competition of less expensive imports. Generally, would you say you sympathize more with those who want to eliminate tariffs or those who think such tariffs are necessary?⁵\n\nThe public always leans decidedly in favor of protection. Support for free trade bottomed out in 1977, when only 18% sympathized with eliminating tariffs, and 66% thought they were necessary. But public opinion remains protectionist in absolute terms. In 2002, sympathy for ending tariffs reached a historic high of 38%—versus 50% who took the opposite view. Furthermore, 85% of the respondents that year held that “protecting the jobs of American workers” should be a “very important” goal of foreign policy—an all-time high\\!⁶\n\nIf antiforeign bias really exists, these are the patterns you would expect. Comparable evidence can be marshaled for the other biases explored in the last chapter. Take antimarket bias. In the late 1970s, Kearl et al. asked economists whether “wage-price controls should be used to control inflation.”⁷ Almost three-quarters of economists generally disagreed. In contrast, the General Social Survey (hence-forth, GSS) reports that solid majorities of noneconomists think it should be government’s responsibility to “keep prices under control.”⁸ Those who agree outnumber those who disagree by at least 2:1 and often 3:1. Casual empiricism and formal empiricism are in sync. Economists trust competition; noneconomists want government to leash rapacious businesses.\n\nNevertheless, the evidence is not rock solid, because the survey results are not strictly comparable. The Kearl questions on free trade and price controls are similar to their counterparts in the Worldviews survey and the GSS, not identical. Moreover, the surveys were rarely performed at exactly the same time. The Kearl data on price controls come from the late seventies; the GSS data, from the eighties and nineties.\n\nGetting data on the economic beliefs of economists and the public is therefore deceptively easy. Surveys of both groups abound. The catch is that almost none samples both laymen and experts on the same questions at the same time. A skeptic could attribute differences to wording: If you ask economists a question loaded one way, and the public a question loaded in the other, you can “find” any pattern you like.\n\nAnalyzing the SAEE: The Public, the Economists, and the “Enlightened Public”\n\nFortunately, one large and well-crafted study is largely immune to this critique. In 1996, the Washington Post, Kaiser Family Foundation, and Harvard University Survey Project collaborated to create the Survey of Americans and Economists on the Economy (henceforth, SAEE).⁹ Based on interviews with 1,510 randomly selected members of the American public and 250 economics Ph.D.’s, the SAEE is ideally structured to test for systematic lay-expert belief differences.¹⁰ It also features remarkably diverse questions, which lets us explore belief differences in depth. A further advantage of the SAEE is its rich set of respondent characteristics. One can use this information to test theories about the origin of lay-expert belief gaps.\n\nThe rest of this book draws heavily on the SAEE, so it is worth exploring at length. Its 37 questions break down into four categories.¹¹ Questions in the first two categories ask whether various factors are a “major reason,” “minor reason,” or “not a reason at all” why “the economy is not doing better than it is.” There are 18 questions of this form. Questions in the third category ask whether something is good, indifferent, or bad for the economy. There are seven questions with this structure. The last category is a grab bag of a dozen miscellaneous questions.\n\nThe next three sections walk the reader through the entire survey. But before proceeding, it is vital to address the most serious objection to this approach: Experts can be biased, too. There are large belief gaps between economists and the public. They cannot both be right. But is it legitimate to infer the existence of systematic biases in the public’s thinking from the mere existence of systematic differences between economists and noneconomists?\n\nElitist though it sounds, this is the standard practice in the broader literature on biases. As the great cognitive psychologists Kahneman and Tversky describe their method: “The presence of an error of judgment is demonstrated by comparing people’s responses either with an established fact . . . or with an accepted rule of arithmetic, logic, or statistics.”¹² “Established” or “accepted” by whom? By experts, of course.\n\nIn principle, experts could be mistaken instead of the public. But if mathematicians, logicians, or statisticians say the public is wrong, who would dream of “blaming the experts”? Economists get a lot less respect. Many maintain, like William Greider, that even that is more than they deserve:\n\nDemocracy is now held captive by the mystique of “rational” policymaking, narrow assumptions about what constitutes legitimate political evidence. It is a barrier of privilege because it effectively discounts authentic political expressions from citizens and elevates the biases and opinions of the elites.¹³\n\nFrom this standpoint, using economists’ views to impugn the public’s backfires. There are no “experts in economics,” only “ ‘experts’ in economics.”\n\nThe most common doubt about economists stems from their apparent inability to agree, best captured by George Bernard Shaw’s line “If all economists were laid end to end, they would not reach a conclusion.”¹⁴ But economists’ hard-core detractors recognize the superficiality of this complaint. They know that economists regularly see eye-to-eye with one another. A quip from Steven Kelman directly contradicts Shaw:\n\nThe near-unanimity of the answers economists give to public policy questions, highly controversial among the run of intelligent observers, but which share the characteristic of being able to be analyzed in terms of microeconomic theory, reminds one of the unanimity characterizing bodies such as the politburo of the Soviet Communist Party.¹⁵\n\nIt is not lack of consensus that incenses knowledgeable critics, but the way economists unite behind unpalatable conclusions—such as doubts about the benefits of regulation. Kelman bemoans the fact that even economists in the Carter administration were economists first and liberals second:\n\nAt the government agency where I have worked and where agency lawyers and agency microeconomists interact with each other . . . the lawyers are often exasperated, not only by the frequency with which agency economists attack their proposals but also by the unanimity among the agency economists in their opposition. The lawyers tend to (incorrectly) attribute this opposition to failure to hire “a broad enough spectrum” of economists, and to beg the economists, if they can’t support the lawyers’ proposals, at least to give them “the best economic arguments” in favor of them. . . . The economists’ answer is typically something like, “There are no good economic arguments for your proposal.”¹⁶\n\nAs usual, it is a rare person who seriously considers, “Maybe others disagree with me because they know more than me.” For detractors, the most plausible explanation for economists’ distinctive outlook is that these so-called experts are biased.\n\nBut how? Baldly asserting, “They’re wrong because they’re biased” explains nothing. Even critics feel compelled to specify the source of the bias. Challenges to economists’ scientific objectivity take on two main forms.\n\nThe first is self-serving bias. A large literature claims that human beings gravitate toward selfishly convenient beliefs.¹⁷ Since economists have high incomes and secure jobs, perhaps they are biased to believe that whatever benefits them, benefits all. Marx famously ridiculed economists as apologists for the capitalist system that suckled them, denouncing Jeremy Bentham, for instance, as “that insipid, pedantic, leather-tongued oracle of the ordinary bourgeois intelligence of the 19th century.”¹⁸ Ludwig von Mises colorfully recalls that in interwar Germany “all that the students of the social sciences learned from their teachers was that economics is a spurious science and that the so-called economists are, as Marx said, sycophantic apologists of the unfair class interests of bourgeois exploiters, ready to sell the people to big business and finance capital.”¹⁹ Brossard and Pearlstein, writing half a century later for the Washington Post, remark, “The disconnect between economists and typical Americans reflects, at least in part, the fact that economists tend to be members of a social, intellectual, and economic elite that has fared relatively well over the past 20 years. . . . And many of the economists hold down tenured teaching positions that afford them a lifetime of job security.”²⁰ One could even equate economists’ cushy jobs with a tacit bribe. Why rock the boat when you enjoy a lavish stateroom?\n\nThe second doubt about economists’ objectivity is less sordid but equally damaging: ideological bias.²¹ Robert Kuttner disapprovingly observes that “Much of the economics profession, after an era of embracing the mixed economy, has reverted to a new fundamentalism cherishing the virtues of markets.”²² A consensus of fundamentalists hardly inspires confidence. It sound like an intellectual chain letter: Maybe each batch of graduate students was brainwashed by the previous generation of ideologues.\n\nBy appealing to these two specific biases, the critics take a risk. Both the self-serving and the ideological bias hypotheses are, in principle, empirically testable. Economists’ views are the product of their affluence? Then rich economists and rich noneconomists should agree. Economists are blinded by conservative ideology? Then conservative economists and conservative noneconomists should agree. The SAEE is a remarkable resource because it has enough information to test both hypotheses. It measures all the leading social cleavages: family income, job security, race, gender, age, even income growth. It also has two measures of ideology.\n\nOne can use this information to estimate what the average belief would be after statistically adjusting for both self-serving and ideological biases. I term this the belief of the Enlightened Public. The Enlightened Public’s belief is the answer to the question, “What would the average person believe if he had a Ph.D. in economics?” Or equivalently: “What would Ph.D. economists believe if their finances and political ideology matched those of the average person?”²³\n\nImagine, then, that laymen and experts had identical income, job security, income growth, race, gender, age, ideology, and party identification. Would they still disagree? If either self-serving bias or ideological bias is a full explanation for the belief gap, the estimated beliefs of the Enlightened Public will match the observed beliefs of the typical noneconomist.²⁴ You could make laymen and experts see eye-to-eye by adding the right control variables. Contrarily, if the hypotheses of self-serving and ideological bias are totally without merit, the beliefs of the Enlightened Public would match the observed views of economists. Whatever control variables you used, the lay-expert gap would persist unscathed.\n\nNote the parallel with political scientists’ analysis of “enlightened preferences,” as discussed in chapter 2. In the enlightened preference approach, one estimates what a person would think if you increased his level of political knowledge to the maximum level, keeping his other characteristics fixed. Using the SAEE, similarly, I estimate what a person would think if you turned him into a Ph.D. economist, keeping his other characteristics fixed. The key difference is that political scientists usually measure knowledge directly, while my approach proxies it using educational credentials.\n\nThe next four sections travel through the entire Survey of Americans and Economists on the Economy, analyzing responses question by question. Each of these questions has three summary statistics:\n\n• First, the “raw” average belief for the general public.\n\n• Second, the “raw” average belief for Ph.D. economists.\n\n• Last, the estimated belief of the Enlightened Public.\n\nTo repeat, if self-serving and/or ideological bias fully accounts for the lay-expert belief gap, the Enlightened Public’s average answer equals the public’s. If self-serving and/or ideological bias explains none of the lay-expert belief gap, the Enlightened Public’s average answer equals the economists’. If the truth lies somewhere in the middle, the Enlightened Public’s average answer lies between the public’s and the economists’.\n\nIf the self-serving and ideological bias hypotheses fail, it remains conceivable that economists suffer from a totally different bias. The same is true for any empirical result. No matter how airtight an explanation now appears, the truth conceivably belongs to another theory, so startlingly original that no one has been smart enough to propose it. Conceivable, but unlikely. If the two main efforts to undermine economists’ objectivity fail, this shifts the burden of proof back onto their critics. After adding all these controls, belief differences that remain are best interpreted as biases of the public.\n\nTo preview, it turns out that the beliefs of the Enlightened Public are usually far closer to economists’ than to the public’s. Self-serving and ideological bias combined cannot account for more than 20% of the lay-expert belief gap. The remaining 80% should be attributed to the experts’ greater knowledge. The naive “Experts are right, laymen are wrong” theory fits the data; the “Experts are deluded, laymen get it right” theory does not.\n\nThis does not mean that the average belief of the economics profession is an infallible oracle. I have never seen it that way. There are cases where I think that the public is closer to the truth. There are topics that I think both groups badly misunderstand. My claim, rather, is that—after correcting for measurable biases—economists should not change their minds just because noneconomists think differently.\n\nVeteran teachers of Econ 1—along with economically literate laymen—will deem much of the question-by-question walkthrough to be obvious, but there are periodic surprises. Sheltered economists who teach only upper-division or graduate classes will probably have a sense of déjà vu as they progress through the SAEE. Even if they have never discussed economics with a noneconomist since they were college freshmen, neglected memories of their pre-econ outlook will bubble up. Readers with little or no background in economics may react with astonishment, puzzlement, or outrage. There is not much I can do about the outrage. But I try to point readers in the right direction by sketching the main reasons why economists think as we do.\n\nThe SAEE Examined, Part I\n\nThe SAEE’s first 11 questions all use the following prompt:\n\nRegardless of how well you think the economy is doing, there are always some problems that keep it from being as good as it might be. I am going to read you a list of reasons some people have given for why the economy is not doing better than it is. For each one, please tell me if you think it is a major reason the economy is not doing better than it is, a minor reason, or not a reason at all.\n\nQuestion 1. You are reading figure 3.1 correctly: economists are less concerned about the economic damage of excessive taxation than the general public. If you think that economists are far-right ideologues, this is the first sign that you should think again. The Enlightened Public takes the same view, with slightly more moderation. The reason is that the rich and securely employed worry less about taxation than the rest of the population—presumably the opposite of what self-serving bias predicts.\n\nThe most plausible explanation for the gap is pessimistic bias. The public is convinced it is getting a bad deal; taxes could be significantly reduced without cutting back on popular government functions. But economists recognize that locating clear-cut “waste” is difficult, and unpopular programs like foreign aid are only a tiny fraction of the budget. They also know that slashing taxes while holding spending steady spells trouble.²⁵\n\n[f0057-01]\n\nFigure 3.1 Question 1: “Taxes are too high”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\n[f0057-02]\n\nFigure 3.2 Question 2: “The federal deficit is too big”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\n[f0058-01]\n\nFigure 3.3 Question 3: “Foreign aid spending is too high”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\nQuestion 2. The public is frequently chided for simultaneously opposing tax increases, spending cuts, and budget deficits. Responses to the question on budget deficits strongly confirm the third part of this triad (fig. 3.2). No other problem in the SAEE inspires more pessimism. Economists take the deficit seriously too, but view this woe as minor and manageable. Note that the Enlightened Public sides completely with the economists; economists’ personal circumstances and ideology do nothing to explain their dissent.\n\nQuestion 3. The belief gap on foreign aid (fig. 3.3) is larger than on any other, and remains almost as large after correcting for bias. The public sees foreign aid spending as a serious problem. Economists virtually to a man believe it is not worth mentioning, and the Enlightened Public is nearly as extreme. Given many economists’ strong criticisms of foreign aid, this is surprising at first.²⁶ But economists normally criticize the effects of foreign aid on the countries that receive it. It is one thing to assert that foreign aid subsidizes foolish policies in the Third World and props up corrupt regimes. It is another to insist that foreign aid is bankrupting the United States. It is the latter claim that the public whole-heartedly endorses.\n\nIt is hard not to link these misconceptions with antiforeign bias. The elderly are the most quantitatively important drain on the federal budget,²⁷ but people like them. If scapegoats for fiscal distress must be found, why not focus on people who rub you the wrong way? Ungrateful foreigners smugly bleeding us dry fit the bill.\n\nQuestion 4. To a person who suffers from antiforeign bias, immigration is scary. Unskilled foreigners “flood” into the country, “steal” jobs from Americans, depress wages, and gobble up public services. Economists take almost the opposite position—and the Enlightened Public is willing to cosign (fig. 3.4). International trade in goods increases the size of the pie, even if one trading partner has an absolute advantage in everything, and even if the good is labor. The case is not airtight; immigrants might prefer mugging or collecting welfare to working. But economists recognize, as the public does not, that one more self-supporting worker is a net benefit, no matter where he was born.\n\n[f0059-01]\n\nFigure 3.4 Question 4: “There are too many immigrants”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\n[f0059-02]\n\nFigure 3.5 Question 5: “Too many tax breaks for business”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\nWhat about the public’s fears? Some are overstated; others are flat wrong. Above all, there is no fixed number of jobs. Labor markets have often absorbed far larger infusions into the workforce than the United States is doing today. Although immigration is currently a high fraction of our population growth, the main reason is the low U.S. birth rate. Measured as a fraction of the population, the rate of immigration is not all that high. Empirical economists also know that there is weak evidence that immigrants depress wages, and considerable evidence that immigrants consume less in public services than they pay in taxes.²⁸\n\nQuestion 5. The question in figure 3.5 primarily taps into antimarket bias. Taxes are too high, thinks the public—except taxes on greedy businesses. They must be shirking their fair share, indirectly wreaking havoc on the rest of the economy. Economists see matters differently, and the Enlightened Public leans in its direction.\n\nIf you look at the facts rather than judging business guilty by reason of greedy intent, the popular view has several weaknesses. Probably the main one is that tax breaks for business are small relative to the budget.²⁹ Another underlying factor is that economists know that corporate income is already double taxed. Tax breaks or “loopholes” partially mitigate the inefficiencies of double taxation. The public, moreover, typically ignores the complexities of tax incidence. Consumers or workers might ultimately bear the burden that the tax code assigns to business.\n\nQuestion 6. The public sees inadequate education as a serious problem, and economists agree (fig. 3.6). Indeed, economists see this as the single most serious economic problem for the United States. The leading rationale is that education has positive externalities, making the market’s level of output smaller than optimal. The public presumably lacks such a sophisticated argument, but happens to reach the same conclusion.³⁰\n\nQuestion 7. In figure 3.7, economists again defy their conservative reputation. Yes, they habitually point out the hidden disincentive effects of government programs. But the public is already comfortable with the idea that if you help the poor, they are less likely to help themselves. The dispute is one of magnitude. Swayed by their pessimistic bias, noneconomists imagine that welfare disincentives are an implausibly large burden.\n\nWhere does the public go wrong? Its greatest error is numerical. Poverty programs, even broadly interpreted, add up to only 10% of federal spending.³¹ This is many times larger than foreign aid, but still too small to be a “major reason” for subpar economic performance. Furthermore, welfare recipients come from the least skilled segment of the population. This tightly caps the economic damage of their absence from the workforce.\n\n[f0060-01]\n\nFigure 3.6 Question 6: “Education and job training are inadequate”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\n[f0061-01]\n\nFigure 3.7 Question 7: “Too many people are on welfare”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\n[f0061-02]\n\nFigure 3.8 Question 8: “Women and minorities get too many advantages under affirmative action”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\nQuestion 8. Economists know about the negative efficiency consequences of affirmative action. Giving special categories of employees the right to sue their employers makes them less likely to be hired in the first place. But economists nevertheless assign the problem less overall significance than the public (figure 3.8). The reason is probably quantitative: Despite the public’s pessimism, there are too few discrimination lawsuits to be more than a minor problem.³²\n\nQuestion 9. The question on the work ethic (fig. 3.9) taps straight into noneconomists’ pessimistic bias. It fits their image of a society falling apart due to steadily declining virtue. For economists, in contrast, relaxed attitudes toward work are a symptom of progress, not decay. As people get richer, economists expect them to consume more luxury goods—including more free time. In a well-functioning economy, if individuals want more leisure and less stuff, the labor market gives them what they want.\n\n[f0062-01]\n\nFigure 3.9 Question 9: “People place too little value on hard work”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\n[f0062-02]\n\nFigure 3.10 Question 10: “The government regulates business too much”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\nYet this cannot be the whole story. Economists’ relative rating for this problem is high, and so is the Enlightened Public’s. The simplest explanation is that economists are thinking in terms of measured gross domestic product, which has the widely acknowledged defect that it puts no value on leisure. By this metric, more work is always economically beneficial.³³\n\nQuestion 10. Economists’ reputation as dogmatic deregulators appears to be overstated (fig. 3.10). They rate the problem of overregulation less seriously than the ever-pessimistic public.³⁴ But note that relatively speaking, stereotypes work. It is economists’ fifth largest problem, versus third smallest for the public. Economists frequently hold that many perceived problems are all in the public’s head, but overregulation is not one of them.\n\nDoes this not cut against the thesis of antimarket bias? To a degree, but evidence outside the SAEE helps triangulate the public’s position. The public frets about regulation in the abstract, but favors it the particular, from minimum wages to farm subsidies to drug testing.³⁵ Even drastic measures like overall price controls are not unpopular.³⁶ For the public, the primary cost of regulation seems to be burdensome paperwork and red tape. Economists’ often have the more fundamental worry that regulation is counterproductive. Price controls create shortages and black markets; drug efficacy tests mandated by the Food and Drug Administration delay the introduction of life-saving drugs. Economists also harbor doubts about regulators’ goals: they know, as few noneconomists do, that the goal of much regulation is to shield existing firms from competition.³⁷\n\n[f0063-01]\n\nFigure 3.11 Question 11: “People are not saving enough”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\nQuestion 11. Laymen and experts are almost equally distressed by the low savings rate (fig. 3.11). Fear is the public’s default position; this is a rare case where economists concur. There are two main reasons for the experts’ high level of concern. First, savings is double taxed. You pay one tax when you earn income, and a further tax if you earn any interest on your after-tax income. This suggests an unusually large gap between the efficient, untaxed level of savings and the actual level. Second, many economists think that savings has positive externalities, so without taxes the level of savings would still be too low.\n\nThe SAEE Examined, Part II\n\nThe prompt for the SAEE’s next seven questions changes slightly:\n\nNow I am going to read you another list of reasons, having to do with businesses, that some people have given for why the economy is not doing better than it is. For each one, please tell me if you think it is a major reason the economy is not doing better than it is, a minor reason, or not a reason at all.\n\nQuestion 12. Are the critics right about self-serving bias? Economists scoff at the idea that excessive profits are hurting the economy (fig. 3.12). Who would be so insensitive, other than malefactors of great wealth? The results for the Enlightened Public support a curt rejoinder. Anyone with a Ph.D. in economics, rich or poor, would tell you the same. Economists’ contrarian position is not a rentier’s rationalization.\n\n[f0064-01]\n\nFigure 3.12 Question 12: “Business profits are too high”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\n[f0064-02]\n\nFigure 3.13 Question 13: “Top executives are paid too much”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\nThe real problem is not that greed blinds economists but that antimarket bias blinds the public. Part of the public’s error is quantitative. It wildly overestimates the rate of profit enjoyed by the typical business, with an average guess near 50%.³⁸ But the disagreement is deeper. Through the prism of antimarket bias, the public perceives profit as a lump-sum transfer to business. Economists, in contrast, recognize it as the motor of progress as well as flexibility.\n\nQuestion 13. Beliefs about excessive executive pay (fig. 3.13) parallel those about excessive profits. The numbers for the Enlightened Public fit the “experts right, laymen wrong” story. Once again, we should stop worrying about economists’ self-serving bias, and start worrying about noneconomists’ antimarket bias. For the public, executive pay is a transfer to high-level managers: When they earn more, underlings get less. Economists reject this fixed-pie mentality.³⁹ The salaries of the captains of industry provide incentives to cut costs, create and improve products, and accurately predict consumer demand.\n\n[f0065-01]\n\nFigure 3.14 Question 14: “Business productivity is growing too slowly”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\n[f0065-02]\n\nFigure 3.15 Question 15: “Technology is displacing workers”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\nQuestion 14. Business productivity (fig. 3.14) is the only problem that clearly worries economists more than the general public, but one can plausibly argue that this dispute is semantic. “Business productivity” sounds vaguely desirable to laymen. But it has a precise meaning for economists: It is the part of production unaccounted for by labor or capital. Intuitively, business productivity growth means the same inputs give you more output. If noneconomists understood economists’ jargon, maybe their judgments would match.\n\nQuestion 15. It is hard not to notice that machines make us richer. Technology is one of the most blatant differences between the present and the past, and the First World and the Third. The data show, however, that many embrace make-work bias in its crudest form: fear of the machine (fig. 3.15). Indeed, they probably resent those who are not afraid, especially egghead economists who fail to “feel the pain” of the untenured man in the street. But this accusation falls flat; the Enlightened Public embraces economists’ “extreme” position with only a hint of moderation.\n\nQuestion 16. If economists and the public agreed about the economic dangers of “sending jobs overseas” (fig. 3.16), claims that the public suffers from antiforeign bias would have to be abandoned. In fact, this is the second-largest gap in the SAEE, overshadowed only by the belief gap on foreign aid.\n\nEconomists’ dismissal of the foreign aid problem stems from their knowledge of the budget. If the United States spent 50 times as much on foreign aid, they would admit it to be a major drain on Americans’ standard of living. The lack of concern with jobs going overseas is more theory-driven. According to the Law of Comparative Advantage, jobs “go overseas” because there are more remunerative ways to use domestic labor.⁴⁰\n\nQuestion 17. When a profitable company cuts its workforce, the typical person treats it as clearly bad for the economy (fig. 3.17). It is excusable if a firm lets workers go in order to avoid bankruptcy; then you are sacrificing some jobs to save the rest. But everyone reviles a profitable firm that downsizes in order to be more profitable.\n\n[f0066-01]\n\nFigure 3.16 Question 16: “Companies are sending jobs overseas”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\n[f0066-02]\n\nFigure 3.17 Question 17: “Companies are downsizing”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\n[f0067-01]\n\nFigure 3.18 Question 18: “Companies are not investing enough money in education and job training”\n0 = “not a reason at all” 1 = “minor reason” 2 = “major reason”\n\nEveryone, that is, who suffers from make-work bias. The popular stance rests on the illusion that employment, not production, is the measure of prosperity. In contrast, for economists and the Enlightened Public, downsizing proves the rule that private greed and the public interest point in the same direction.⁴¹ Downsizing superfluous workers leads them to search for more socially productive ways to apply their abilities. Imagine what would have happened if the farms of the 19th century never “downsized.” Greed drove these changes, but they remained changes for the better.\n\nQuestion 18. There is a broad consensus (see fig. 3.6) that inadequate education is a major economic problem. The item in figure 3.18 advances a hypothesis about why we are insufficiently educated: lack of spending by business. This story would appeal to people with antimarket bias, and the shoe fits: Economists and the Enlightened Public do not dismiss this explanation, but the public is noticeably more sympathetic.\n\nThe SAEE Examined, Part III\n\nAll of the previous questions focused on perceived economic problems. The next batch of questions is more open-ended.\n\nGenerally speaking, do you think each of the following is good or bad for the nation’s economy, or don’t you think it makes much difference?\n\n[f0068-01]\n\nFigure 3.19 Question 19: “Tax cuts”\n0 = “bad” 1 = “doesn’t make much difference” 2 = “good”\n\n[f0068-02]\n\nFigure 3.20 Question 20: “More women entering the workforce”\n0 = “bad” 1 = “doesn’t make much difference” 2 = “good”\n\nQuestion 19. The public thinks that taxes are too high, and infers that tax cuts are a good thing (fig. 3.19). My interpretation is that noneconomists, avid pessimists, are convinced that government squanders their money. They therefore naively hope to pay for tax cuts by cutting unpopular programs and “waste.” Economists, contrary to their laissez-faire image, are skeptical. Unpopular programs are only a small fraction of the budget,⁴² and “waste” cannot be identified in an uncontroversial way.\n\nQuestion 20. Economists and noneconomists both see increased female labor force participation as a good thing (fig. 3.20), but—ever the pessimists—the latter are less unanimous. It is striking that the public is so upbeat about increased female labor supply but so downbeat about increased immigrant labor supply. Presumably their economic effects are similar. One explanation for the inconsistency is that political correctness makes people too nervous to lament that women are “stealing jobs” from men.\n\n[f0069-01]\n\nFigure 3.21 Question 21: “Increased use of technology in the workplace”\n0 = “bad” 1 = “doesn’t make much difference” 2 = “good”\n\n[f0069-02]\n\nFigure 3.22 Question 22: “Trade agreements between the United States and other countries”\n0 = “bad” 1 = “doesn’t make much difference” 2 = “good”\n\nQuestion 21. Despite make-work bias, the public is not completely crazy. A comfortable majority acknowledges the economic benefits of technological progress (fig. 3.21). A sizable belief gap arises because economists embrace new technology with one voice, while the public has reservations. According to popular stereotypes, economists fail to give a straight answer, but now the shoe is on the other foot. A noneconomist is many times more likely to say, “Yes, technology can be good, but on the other hand. . . .”\n\nQuestion 22. Looking only at the public’s average response in figure 3.22, one might be puzzled by how positive the public is about trade agreements. Where has its antiforeign bias gone? Compared to economists and the Enlightened Public, though, the public’s support is halfhearted. Noneconomists tend to think, “Exports good, imports bad.” So they wonder whether trade agreements “give too much” to the other side. Economists lack the public’s ambivalence because they think imports are good; unilateral free trade is better than mutual protection.⁴³\n\n[f0070-01]\n\nFigure 3.23 Question 23: “The recent downsizing of large corporations”\n0 = “bad” 1 = “doesn’t make much difference” 2 = “good”\n\nQuestion 23. Economists do not just say that the danger of downsizing is overblown; they see it as a blessing (fig. 3.23). Doing more with less is the definition of progress. Is this a modern version of “Let them eat cake”? The results for the Enlightened Public say otherwise. If a person of average means got an econ Ph.D., he would change his mind.\n\nQuestion 24. The most plausible way to defend the public’s grasp of economics is to blame lay-expert disagreement on varying time horizons. Economists emphasize the “long run”; the public cares about here and now. Perhaps experts and laymen covertly agree about facts, but have different levels of patience. Many economists who acknowledge the reality of lay-expert belief gaps opt for this interpretation. One is Schumpeter:\n\nRational recognition of the economic performance of capitalism and of the hopes it holds out for the future would require an almost impossible moral feat by the have-not. That performance stands out only if we take a long-run view; any pro-capitalism argument must rest on long-run considerations. . . . For the masses, it is the short-run view that counts. Like Louis XV, they feel après nous le déluge.⁴⁴\n\nBy asking about effects 20 years in the future (fig. 3.24), we can test Schumpeter’s hypothesis. If different levels of impatience are the full explanation, laymen and experts would think exactly alike. In fact, this belief gap is unusually big. Both groups are less negative about the long run, but economists are more positive both now and later. They expect a mixed blessing to become a pure gain; the public expects a pure bad to fade out.\n\nQuestion 25. When the SAEE asks about the effect of trade agreements on U.S. employment (fig. 3.25), antiforeign bias and make-work bias join forces, opening up a very wide gap between economists and the public. Whatever noneconomists think about trade agreements overall, they are convinced that the effect on domestic employment is negative. Economists and the Enlightened Public deny this, as expected.⁴⁵\n\n[f0071-01]\n\nFigure 3.24 Question 24: “Some people say that these are economically unsettled times because of new technology, competition from foreign countries, and downsizing. Looking ahead 20 years, do you think these changes will eventually be good or bad for the country or don’t you think these changes will make much difference?”\n0 = “bad” 1 = “doesn’t make much difference” 2 = “good”\n\n[f0071-02]\n\nFigure 3.25 Question 25: “Do you think that trade agreements between the United States and other countries have helped create more jobs in the U.S., or have they cost the U.S. jobs, or haven’t they made much of a difference?”\n0 = “cost jobs” 1 = “haven’t made much difference” 2 = “helped create jobs”\n\nThe Saee Examined, Part IV\n\nThe remaining questions vary in form and content, but continue to exhibit large and robust systematic belief differences.\n\nQuestion 26. A key form of antimarket bias is to deny or downplay the role of competition. It is telling, then, that economists overwhelmingly attribute the 1996 rise in the price of gas to supply and demand (fig. 3.26), but barely a quarter of the public agrees.⁴⁶ Where economists see prices governed by market forces, the public sees monopoly or collusion. The numbers for the Enlightened Public confirm that economists do not dissent just because they are too rich to worry about how much it costs to fill their gas tank.\n\n[f0072-01]\n\nFigure 3.26 Question 26: “Which do you think is more responsible for the recent increase in gasoline prices?”\n0 = “oil companies trying to increase their profits” 1 = “the normal law of supply and demand”\n\n[f0072-02]\n\nFigure 3.27 Question 27: “Do you think the current price of gasoline is too high, too low, or about right?”\n0 = “too low” 1 = “about right” 2 = “too high”\n\nThe real problem is not that economists are out of touch, but that the public’s story makes no sense. If gas prices rise because “oil companies are trying to increase their profits,” why do gas prices ever fall? Do oil companies feel generous and decide to cut their profits? Basic economics, in contrast, has an elegant explanation: If the cost of inputs falls, so does the profit-maximizing price.\n\nQuestion 27. The wording of question 27 (fig. 3.27) leaves something to be desired; as a consumer, you might trivially maintain that any price is “too high.” But responses to the previous question suggest that few respondents read the question so literally. When people answer “too high,” they probably mean that some kind of monopoly holds prices above the competitive level. “Too low,” in contrast, probably means that higher fuel taxes are necessary to correct for pollution, congestion, and other negative externalities of car use.\n\n[f0073-01]\n\nFigure 3.28 Question 28: “Do you think improving the economy is something an effective president can do a lot about, do a little about, or is that mostly beyond any president’s control?”\n0 = “beyond president’s control” 1 = “do a little about”\n2 = “can do a lot about”\n\nThe “too high” position is a classic form of antimarket bias. But opposition to the “too low” thesis arguably stems from the same root. Suppose you want to reduce pollution and congestion. You could do it by command-and-control: emissions regulations, annual inspections, carpool lanes. But economists realize that the market mechanism is a more efficient method. A tax on gas gives people an incentive to reduce pollution and congestion without specifically dictating anyone’s behavior.⁴⁷\n\nQuestion 28. A rare issue where economists and the public agree is on the president’s capacity to improve the economy (fig. 3.28). It is most curious because economists criticize the public for mechanically linking economic conditions to incumbent presidents. What about the Federal Reserve, Congress, other governments, secular trends, and random shocks?\n\nWhen economists only criticize errors in one direction, there is normally a good reason: errors in that direction predominate. This is the exception that proves the rule. Perhaps those who minimize the president’s influence are less outspoken, creating the illusion of a systematic difference.\n\nQuestion 29. The public’s default is to expect things to get worse. The good old days are gone; since the 1970s, stagnation and decline have been our lot. “McJobs” fit neatly into this worldview. As usual, economists think that the numbers contradict the public’s extreme pessimism (fig. 3.29).⁴⁸ But the belief gap runs deeper than the latest data set. The progress of recent centuries implies that it is abnormal for new jobs to be low-paying. A temporary setback is possible, but it merits an intellectual double-take.\n\n[f0074-01]\n\nFigure 3.29 Question 29: “Do you think most of the new jobs being created in the country today pay well, or are they mostly low-paying jobs?”\n0 = “low-paying jobs” 1 = “neither” 2 = “pay well”\n\n[f0074-02]\n\nFigure 3.30 Question 30: “Do you think the gap between the rich and the poor is smaller or larger than it was 20 years ago, or is it about the same?”\n0 = “smaller” 1 = “about the same” 2 = “larger”\n\nQuestion 30. The public sees two decades of rising inequality (fig. 3.30). Given its antimarket and pessimistic reflexes, how could it not? But playing against type, economists are more convinced than the public. The data on inequality are solid enough, and economists have no strong presumptions about inequality.⁴⁹ They know that living standards rise over time, but have little reason to expect a trend in the distribution of income and wealth.\n\nQuestion 31. It is tempting to interpret “pessimistic bias” as semantic. Maybe the public says “The economy is doing badly compared to my hopes” and economists counter “The economy is doing well considering its constraints.” But if they are just talking past each other, apparent pessimism would fall for less ambiguous topics. It does not. The question about family incomes (fig. 3.31) is one of the least ambiguous in the SAEE—and has one of the larger belief gaps.\n\n[f0075-01]\n\nFigure 3.31 Question 31: “During the past 20 years, do you think that, in general, family incomes for average Americans have been going up faster than the cost of living, staying about even with the cost of living, or falling behind the cost of living?”\n0 = “falling behind” 1 = “staying about even” 2 = “going up”\n\nShouldn’t the belief gap be larger? Economists’ average response is slightly above 1; does a substantial minority of the profession deny that mean income went up? No. Rising inequality is a confounding factor. The question asks about median income (“family incomes for average Americans”), not mean income (“average American family incomes”). If inequality is rising, the first can go down as the second goes up.\n\nBut while almost every economist grasps the distinction between mean and median income, it is doubtful that many noneconomists do. Members of the general public who said “falling behind” probably think that mean income fell from 1976 to 1996. However, even economists who said “falling behind” know that mean income rose. The upshot: residual ambiguity in this question masks the full size of the lay-expert gap.\n\nQuestion 32. The belief gap for real wages (fig. 3.32) is much narrower than for real income, a change almost entirely attributable to the economists. The public gives the same answer twice in a row, probably because it equates income and wages. Economists know that the two are different, and that some of the data on average real wages contradict the presumption of progress. If average real wages are stagnant and inequality is rising, it follows that the wages of the average American worker are falling. Still, a substantial minority of economists stands behind the presumption of progress on wages, pointing to serious flaws that bias official numbers downwards.⁵⁰\n\n[f0076-01]\n\nFigure 3.32 Question 32: “Thinking just about wages of the average American worker, do you think that during the past 20 years they have been going up faster than the cost of living, staying about even with the cost of living, or falling behind the cost of living?”\n0 = “falling behind” 1 = “staying about even” 2 = “going up”\n\n[f0076-02]\n\nFigure 3.33 Question 33: “Some people say that in order to make a comfortable living, the average family must have two full-time wage earners. Do you agree with this, or do you think the average family can make a comfortable living with only one full-time wage earner?”\n0 = “can make living with one wage earner” 1 = “need two wage earners”\n\nQuestion 33. Ample majorities of both economists and the public agree that the average American family needs two incomes to live comfortably (fig. 3.33), but economists are less sure. This does not reflect economists’ above-average income, because the Enlightened Public says the same. Economists are probably less pessimistic because they practice marginal thinking. Being a stay-at-home mom or having a full-time job are not the only choices. Lower income means some sacrifices, but a family with one full-time and one part-time earner has ways to “comfortably” adjust: buy a moderately less expensive home, or delay purchase of a new car for a year or two.\n\n[f0077-01]\n\nFigure 3.34 Question 34: Over the next five years, do you think the average American’s standard of living will rise, or fall, or stay about the same?”\n0 = “fall” 1 = “stay about the same” 2 = “rise”\n\n[f0077-02]\n\nFigure 3.35 Question 35: “Do you expect your children’s generation to enjoy a higher or lower standard of living than your generation, or do you think it will be about the same?”\n0 = “lower” 1 = “about the same” 2 = “higher”\n\nQuestion 34. With enough data, you can convince an economist that improvements in living standards failed to materialize some time in the past, or that an impending recession will pull them down. But it is hard to stop an economist from expecting rising living standards in the medium- or long-term future (fig. 3.34). Critics hail this as proof of their dogmatism. Yet the presumption of progress does not come out of thin air. Two centuries of awesome economic growth back it up.⁵¹ Is it not more dogmatic for noneconomists to remain pessimistic in spite of this track record?\n\nQuestion 35. Here is an ideal prompt to tap respondents’ beliefs about long-run growth (fig. 3.35). Economists’ beliefs about the economic future are of course more upbeat than noneconomists’, though the gap is smaller than you would expect. Surprisingly, the Enlightened Public is more optimistic than either. The reason: high-income males are uncharacteristically pessimistic on this topic. Since economists tend to be high-income males, their demographics dilute their optimism.\n\n[f0078-01]\n\nFigure 3.36 Question 36: [If you have any children under the age of 30] “When they reach your age, do you expect them to enjoy a higher or lower standard of living than you do now, or do you expect it to be about the same?”\n0 = “lower” 1 = “about the same” 2 = “higher”\n\nQuestion 36. It seems especially odd that economists and the public agree about their own children’s economic future (fig. 3.36). If economists are more optimistic than the public about the prospects of the next generation, why are the two groups equally optimistic about their own children? On closer examination, though, economists are more optimistic—after controlling for income. If a person of ordinary means had an economist’s education, he would see a brighter future for his children.\n\nThere is a logical explanation for this pattern. The question asks respondents to compare their own current situation to their children’s. The better you are doing, the more successful your children have to be to equal you. Many SAEE respondents appear to grasp this subtle point: As income goes up, optimism steeply declines. The upshot is that economists’ income camouflages their optimism.\n\nQuestion 37. When asked about the current state of the economy (fig. 3.37), economists give more upbeat answers than the rest of the public. The root of the disagreement is not, however, economic training. Economists see eye to eye with noneconomists who happen to have high job security and growing incomes. After controlling for these characteristics, the belief gap is no longer statistically significant.⁵²\n\nThree Doubts\n\nEverything that follows in this book takes the reality of systematically biased beliefs about economics for granted. So before moving on, it is worth plugging some holes by considering leading challenges. My findings may not be watertight, but they are more than seaworthy. The objections are not strong enough to reverse the conclusion that the public’s economic beliefs are riddled with large systematic errors.\n\n[f0079-01]\n\nFigure 3.37 Question 37: “When you think about America’s economy today, do you think it is. . .”\n0 = “in a depression” 1 = “in a recession” 2 = “stagnating” 3 = “growing slowly” 4 = “growing rapidly”\n\nVagueness. One problem with the “experts right, laymen wrong” view of the SAEE is the vagueness or “fuzziness” of the responses. Who knows what it means to be a “major” versus a “minor” reason for subpar economic performance? Perhaps the public is partial to superlative adjectives, and Ph.D.s signal their levelheadedness with measured language.\n\nThe most telling counterevidence comes from other public opinion studies that ask for exact numbers. If you compare numerical responses to actual figures, systematic bias still leaps out at you. Take the budget. The National Survey of Public Knowledge of Welfare Reform and the Federal Budget finds that the public’s numerical perceptions are almost the reverse of the truth.⁵³ This survey presented a list of six federal program categories: foreign aid, welfare,⁵⁴ interest on the debt, defense, Social Security, and health. It then asked respondents to name the two largest items.\n\nTable 3.1 shows responses, providing the actual numbers from 1993 for the sake of comparison. Foreign aid—by far the smallest—was absurdly the most frequently named! Only 14% realized that the most expensive federal program—Social Security—is in the top two. The public’s picture of the budget is upside down.⁵⁵ Furthermore, it is upside down in the expected way. In the SAEE, respondents qualitatively overestimate the damage of foreign aid and welfare; in the National Survey respondents quantitatively overestimate the fraction of the budget spent on foreign aid and welfare.\n\nTable 3.1\nAmericans’ Views of the Two Largest Areas of Federal Government Spending\n\n[t0080-01]\n\nInsincerity. The SAEE measures what people say they believe. It is possible that when they affirm strange beliefs, they are lying. As Gordon Tullock explains:\n\nA man may inform a social scientist that he is trying to achieve some goal by a given course of action although the course of action does not seem well chosen in view of the stated goal. An incautious social scientist may then conclude that the man is irrational. The real explanation may simply be that the goals aimed at are different from the stated goals.⁵⁶\n\nThis is an internally consistent but highly implausible way to interpret my results. Respondents in the SAEE have no material incentive to lie. They are not politicians whose candor could cost them an election. And there is not much emotional impetus to lie either. Respondents might hide their true feelings about race out of embarrassment, but few economic beliefs bear such a stigma. After years of teaching economics, I cannot recall a single case where I suspected that a student was only pretending to disagree with me. (Pretending to agree is another matter!)\n\nQuestion Selection Bias. Systematic belief differences are so common in the SAEE that you might get suspicious. Did the authors select questions where they expected disagreement? There is no evidence that they did. They picked questions on the basis of public and media attention. Here, for example, is how the authors wrote the first part of the survey: “Based on a review of almost two decades of public opinion polling on the economy, we chose 18 of the reasons most frequently mentioned as possible reasons for the economy not doing better. . .”⁵⁷ They were looking for common explanations, not explanations with large lay-expert gaps. The same holds for the rest of the SAEE.⁵⁸\n\nRethinking Systematic Error\n\nOnce they realize that a theory implicitly depends on systematic error, most economists are incredulous: “You are assuming irrationality!” Being explicit wins you a little credit for candor, but the main effect is to hasten your dismissal. The goal of this chapter has been to bypass a priori objections with direct empirical evidence.\n\nIn the process, we have amassed an embarrassment of riches. There are too many details to digest in one sitting. What does an aerial view of the SAEE tell us?\n\nFirst and foremost, the SAEE strongly confirms the reality of large and systematic belief differences between economists and the public. In fact, there are almost no areas where large, systematic belief differences do not exist. The Miracle of Aggregation is not merely false every now and then. At least in economics, it barely works more often than most “miracles” do.\n\nThe findings are especially compelling because, with few exceptions, differences go in the predicted directions. This is the second of the SAEE’s lessons. Economists and the public disagree in the way that economists—in conversation, lectures, and textbooks—have long maintained. The public really holds, for starters, that prices are not governed by supply and demand, protectionism helps the economy, saving labor is a bad idea, and living standards are falling. Educators are not beating a dead horse when they argue against antimarket bias, antiforeign bias, make-work bias, or pessimistic bias.\n\nIf A and B disagree, there are three logical possibilities. The first is that A is right and B is wrong. The second is that A is wrong and B is right. The third is that both A and B are wrong. But we can rule out the possibility that both A and B are right. Systematic differences between laymen and experts do not logically entail systematic errors on the part of the public. Continuing with my aerial summary, though, the third lesson from the SAEE is that the naive “economists right, public wrong” interpretation is usually the best.\n\nWe all share a presumption that when an expert disagrees with a nonexpert, the expert is right. This holds in math, science, history, and car repair. Yes, the experts have been wrong before. An amusing book by Cerf and Navasky, The Experts Speak: The Definitive Compendium of Authoritative Misinformation,⁵⁹ provides hundreds of embarrassing examples. Notice, however, that they did not write a companion volume entitled The Public Speaks: The Definitive Compendium of Amateur Misinformation. It would be too easy. How startling would it be to read hundreds of inane comments by the unqualified? The Experts Speak is funny precisely because experts are ordinarily right.\n\nIf you want to criticize the experts, the burden is on you to overcome the standard presumption. The detractors of the economics profession try, pointing to economists’ self-serving and ideological biases. But they do not meet their burden of proof. The SAEE reveals that both of the leading accounts of the experts’ biases are wrong.\n\nEconomists are richer than noneconomists, but millionaires without economics degrees think like other people, and economists who drive taxis think like other economists. In fact, the paltry evidence of self-serving bias should be taken at less than face value. Income has a small influence on beliefs, but is the direction really selfish? The rich worry less about foreign aid and welfare, not just excessive profits and executive pay.\n\nIdeological bias is an even weaker reed. Controlling for individuals’ party identification and ideology makes the lay-expert belief gap a little larger. Ideologically moderate, politically independent economists are totally at odds with ideologically moderate, politically independent noneconomists. How can this be? Economics only looks conservative compared to other social sciences, like sociology, where leftism reigns supreme. Compared to the general public, the typical economist is left of center.⁶⁰ Furthermore, contrary to critics of the economics profession, economists do not reliably hold right-wing positions. They accept a mix of “far right” and “far left” views. Economists are more optimistic than very conservative Republicans about downsizing or excessive profits—and more optimistic about immigration and welfare than very liberal Democrats.⁶¹\n\nShooting down the leading opponents of the “economists right, public wrong” position does not prove that it is true. But it significantly increases the probability. Think of it this way: Common sense advises us to trust the experts. Critics challenge the experts’ objectivity, and their complaints turn out to be in error. The sensible response is to reaffirm the commonsense position. Indeed, after the strongest challengers fail, we should become more confident that economists are right and the public is wrong.\n\nThere is no reason, then, to deny economists a normal level of deference in their field of expertise. But the profession also deserves an affirmative defense. Frankly, the strongest reason to accept its reliability is to flip through a basic economics text, then read the SAEE questions for yourself. You may not be fully convinced of economists’ wisdom. I, too, doubt it on occasion. But it is hard to avert your gaze from the public’s folly. Time and again, it gravitates toward answers that are positively silly.\n\nIf that is too subjective for you, an impressive empirical regularity points in the same direction: Education makes people think like economists. Out of the SAEE’s 37 questions, there are 19 where economic training and education move together, and only two where they move apart. It is not merely members of one inbred discipline who diverge from mainstream opinion. So do educated Americans in general, with the degree of divergence rising with the level of education. And the magnitude is substantial. Moving from the bottom of the educational ladder to the top has more than half of the (enormous) effect of an econ Ph.D.⁶²\n\nThis pattern is all the more compelling because it has parallels in other fields. Take political knowledge. Delli Carpini and Keeter report that education substantially improves performance on objective tests about government structure, leaders, and current events.⁶³ Kraus, Malmfors, and Slovic similarly find that education makes members of the general public “think more like toxicologists.”⁶⁴ Perhaps education just increases exposure to brainwashing. But it is more likely that educated people think clearer and know more.\n\nConclusion\n\nAppearances can be revealing. Noneconomists and economists appear to systematically disagree on an array of topics. The SAEE shows that they do. Economists appear to base their beliefs on logic and evidence. The SAEE rules out the competing theories that economists primarily rationalize their self-interest or political ideology. Economists appear to know more about economics than the public. The SAEE weighs heavily in favor of this conclusion.\n\nThe SAEE is hardly the only empirical evidence for these propositions. As mentioned at the beginning of this chapter, there are numerous studies of economic beliefs. The advantage of the SAEE is its craftsmanship. It has been constructed to deflect the main objections that skeptics could levy against earlier empirics. Now that the SAEE has cleared these hurdles, it is fair to look back and recognize that the earlier literature—including both statistical work and economists’ centuries of observation and reflection—is basically sound.\n\nThe rest of this book takes the public’s systematically biased beliefs as an established fact. There is much more work to be done on the details, but the overall story is unlikely to change. The task at hand is to figure out how these biases fit into the big picture. How can social science account for the ubiquity of these systematic errors? And what effects do these systematic errors have in the world?\n\nTECHNICAL APPENDIX\n\nThe Enlightened Public\n\nTo estimate the beliefs of the Enlightened Public, the data for the general public and economists were pooled. Each of the 37 beliefs in the SAEE, reproduced in table 3.2, was regressed on all of the variables in table 3.3.\n\nStrictly speaking, of course, simple regression is not the best method for discrete dependent variables, but the coefficients are easier to interpret, and redoing everything with ordered logits yields virtually identical predictions.⁶⁵ The regression equations were then used to predict the beliefs of the Enlightened Public, who by definition are average members of the general public in every way except in education and economic training. Those values are, by assumption, respectively equal to 7 and 1. Equivalently, since the public’s average Education = 4.54 and its average Econ = 0, the Enlightened Public holds the beliefs an average person would hold if his Education score were (7 − 4.54) = 2.46 higher and his Econ score were 1.00 higher.\n\nTo reproduce the complete results for all 37 equations would be overkill. Table 3.4 instead displays the results of greatest interest: the coefficients and t-stats for Education and Econ, controlling for the other variables in table 3.3. Table 3.4’s results can be used to calculate how beliefs respond to changes in education and economic training.\n\nExample. What is the predicted effect of sending a noneconomist with the average level of education (4.54) to graduate school in economics? Upon completion, his Education will be 2.46 higher and his Econ will be 1 instead of 0. His predicted belief on any given question is therefore his initial belief plus 2.46 times the coefficient on Education plus the coefficient on Econ.⁶⁶ On TAXHIGH, for instance, the coefficient on Education is −.09, and the coefficient on Econ is −.32. The estimated belief change is therefore −2.46 \\* .09 −.32 = −.54.\n\nTable 3.2\nQuestions and Mean Answers\n\n[t0085-01]\n\n[t0085-02]\n\n[t0086-01]\n\n[t0086-02]\n\n[t0086-03]\n\n[t0087-01]\n\n[t0087-02]\n\n[t0088-01]\n\n[t0088-02]\n\n[t0089-01]\n\n[t0089-02]\n\nTable 3.3\nControl Variables\n\n[t0090-01]\n\n[t0090-02]\n\n[t0091-01]\n\n[t0091-02]\n\n[t0092-01]\n\nTable 3.4\nCoefficients on Education and Econ\n\n[t0092-02]\n\n[t0093-01]\n\n[t0093-02]\n\nChapter 4\n\nCLASSICAL PUBLIC CHOICE\n\nAND THE FAILURE OF RATIONAL\n\nIGNORANCE\n\nApparently irrational cultural beliefs are quite remarkable:\nThey do not appear irrational by slightly departing from\ncommon sense, or timidly going beyond what the\nevidence allows. They appear, rather, like down-right\nprovocations against common sense rationality.\n\n—Richard Shweder¹\n\nANTHONY DOWNS’S An Economic Theory of Democracy (1957) turned rational ignorance into a basic element of the economics of politics. Gordon Tullock did not coin the phrase until 10 years later,² but Downs’s one-sentence explanation remains definitive: “it is irrational to be politically well-informed because the low returns from data simply do not justify their cost in time and other resources.”³\n\nThe logic is simple. Time is money, and acquiring information requires time. Individuals balance the benefit of learning against its cost.⁴ In markets, if individuals know too little, they pay the price in missed opportunities; if they know too much, they pay the price in wasted time. The prudent path is to find out enough to make a tolerably good decision.\n\nMatters are different in politics. One vote is extraordinarily unlikely to change an election’s outcome.⁵ So suppose an ignorant citizen votes randomly. Except in the freak case where he casts the decisive vote, flipping an otherwise deadlocked election, the marginal effect is zero. If time is money, acquiring political information takes time, and the expected personal benefit of voting is roughly zero, a rational, selfish individual chooses to be ignorant.\n\nThe civics textbook motto, “If everybody thought that way, democracy would produce horrible results,” could well be true. But as an appeal to citizen self-interest, the motto is a bald fallacy of composition. If everyone knows nothing about politics, we are worse off; but it does not follow that if I know nothing about politics, I am worse off. If one person stands up at a concert, that person sees better, but if everyone stands up, no one sees better.\n\nIn the fifties and sixties, economists got used to calling imperfect information a “market failure.”⁶ On reflection, though, the best example of this so-called market failure seemed to be democratic government. As the economics of politics developed, appeals to rational ignorance grew alongside it. Rational ignorance became the root of an intellectual orthodoxy—an orthodoxy I call Classical Public Choice.\n\nRational Ignorance: Evidence and Alleged Consequences\n\nAlthough political scientists classify about one-third of the public as “know-nothings,”⁷ it is hard to find people whose political knowledge is literally nonexistent. There are a handful of facts—like the name of the president—that nearly everyone knows. Incentives are a little more complex than they seem on the surface. Ubiquitous and entertaining facts are easier to absorb than avoid, and recall than forget. Political knowledge also has “off-label” benefits: good grades in impractical subjects still help your career prospects, and your friends or a date might scoff at full-fledged political cluelessness.\n\nSo Classical Public Choice’s stories about rational ignorance prove too much. But not much too much. By any absolute measure, average levels of political knowledge are low.⁸ Less than 40% of American adults know both of their senators’ names.⁹ Slightly fewer know both senators’ parties—a particularly significant finding given its oft-cited informational role.¹⁰ Much of the public has forgotten—or never learned—the elementary and unchanging facts taught in every civics class. About half knows that each state has two senators, and only a quarter knows the length of their terms in office.¹¹ Familiarity with politicians’ voting records and policy positions is predictably close to nil even on high-profile issues, but amazingly good on fun topics irrelevant to policy. As Delli Carpini and Keeter remark:\n\nDuring the 1992 presidential campaign 89 percent of the public knew that Vice President Quayle was feuding with the television character Murphy Brown, but only 19 percent could characterize Bill Clinton’s record on the environment . . . 86 percent of the public knew that the Bushes’ dog was named Millie, yet only 15 percent knew that both presidential candidates supported the death penalty. Judge Wapner (host of the television series “People’s Court”) was identified by more people than were Chief Justices Burger or Rehnquist.¹²\n\nThis is precisely what the logic of rational ignorance would lead one to suspect. When people decide whether to devote mental effort to the dry facts vital for intelligent political choice, or to irrelevant fluff, they choose the latter.¹³\n\nRational ignorance’s intuitive and empirical appeal would have guaranteed it academic airtime. Yet it took an extra selling point to turn rational ignorance into the keystone of Classical Public Choice: its apparent ability to explain the failures of democracy. Imagine that a single voter is sealed in a room for life, cut off from any contact with the world outside his tiny cell. He has a lifetime supply of food and water, but no windows. The cell has a one-way intercom; the voter can tell politicians his preferences, but they are unable to speak to him. Once every four years, the voter gets to voice his support for one of two candidates. The voter knows that he determines the winner, but he has no way to find out what the candidates did in the past or intend to do in the future.\n\nIt would be astonishing if democracy worked in this story, because neither candidate can improve his chance of winning. The voter inside the cell neither sees politicians’ actions nor hears their words. So the winner can do whatever he likes without the slightest fear of losing office as a result of his decisions. This does not mean the officeholder has no worries. He can be voted out of office in the next election. The point is that he is equally likely to be thrown out of office if he follows the voter’s intercom instructions to the letter, or does the opposite.\n\nLittle changes if there are millions of voters in isolation chambers. As long as none know what goes on outside his cell, leaders can ignore the expressed wishes of the majority—even though the majority has complete control over electoral outcomes. If candidate behavior is unobservable, voters cannot condition their votes on candidate behavior. If voters cannot condition their votes on candidate behavior, candidates have no incentive to heed them.\n\nVoters do not live in physical isolation chambers, but they could be comparably ignorant by choice. If they were, the perceived failings of democracy seem easy to explain. Why can special interest groups turn legislatures against majority interests? Voters’ rational ignorance: many fail to realize that tobacco farmers get subsidies, and few know where their representative stands. Why can politicians defy public opinion? Voters’ rational ignorance: few pay attention to politicians’ position on unpopular programs like foreign aid, and fewer remember at the next election. Why are inefficient policies like the minimum wage popular? Voters’ rational ignorance: few bother to learn enough economics to understand the policies’ drawbacks.¹⁴\n\nThe flip side of public ignorance is insider expertise. While the voters sleep, special interests fine-tune their lobbying strategy. Just as voters know little because it doesn’t pay, interest groups know a lot because—for them—it does; hence the mantra of “concentrated benefits, dispersed costs.” As Mancur Olson proclaims, “There is a systematic tendency for exploitation of the great by the small!”¹⁵ The orange tariff costs me, the orange consumer, a few pennies, but it means millions for orange growers.\n\nWhen economists stopped theorizing long enough to peruse the political landscape, special interests seemed to lurk behind practically every government policy. Like an old civics text, the professors grumbled, “If only the voters knew . . .” Unlike the civics text, however, they could not offer the consolation that “one day the electorate is bound to wake up and put the nation’s house in order.” The social harm of rational ignorance does not make it individually advantageous to crusade against it.\n\nIn sum, according to Classical Public Choice, voter ignorance transforms politics from a puzzling anomaly into a textbook example of the explanatory power of information economics. Voter ignorance opens the door to severe government failure. Interest groups—not to mention bureaucrats and politicians themselves—walk straight in.\n\nResisting Irrationality\n\nOrdinary language has many words for disparaging false beliefs and the people who hold them. In spite of subtle shades of meaning, most fall into one of two categories: words that blame the mind of the agent—like “irrational,” “stupid,” “delusional,” and “dogmatic”—and words that blame the information available to the agent—like “ignorant,” “uninformed,” “misled,” and “uneducated.”\n\nThe truth could easily be mixed. But most economists resist mixed accounts of human error that give irrationality any share of the responsibility. You might expect the ones who study politics to be less rigid, but if anything the opposite is true.¹⁶ Downs made rationality a foundation of his analysis, and his successors have been true to his vision. Still, at least Downs defends his decision to ignore irrationality:\n\nOur desire to by-pass political irrationality springs from (1) the complexity of the subject, (2) its incompatibility with our model of purely rational behavior, and (3) the fact that it is an empirical phenomenon which cannot be dealt with by deductive logic alone but also requires actual investigation beyond the scope of this study.¹⁷\n\nIn contrast, the orthodoxy Downs inspired often forgets that an alternative exists. Any popular error, no matter how bizarre, supposedly confirms that voters are rationally ignorant. After perusing the empirical evidence of systematically biased beliefs about economics, many in the tradition of Classical Public Choice interpret it as evidence of rational ignorance. Indeed, the economists most willing to accept the empirics are often least willing to interpret them as the very thing that Downs “bypassed” 50 years ago: political irrationality.\n\nWhy are economists so hostile towards theories rooted in “stupidity” or “irrationality,” and so friendly towards the extreme “ignorance only” take on human error? One defense is tautologous: equating all error with “ignorance,” then equivocating between the standard and catchall definitions. Yet whatever words you prefer, two distinct causes of error remain: Either you lack sufficient data, or you fail to take full advantage of the data you have. A mystery might remain unsolved by a detective because he needs more clues, or because he lacks the desire or wit to piece his clues together.\n\nWhen proponents of the ignorance-only view tire of semantic debate, the next defense is to appeal to the difficulty of empirically distinguishing the two sources of error. Who is to say what is or is not “irrational”?¹⁸ This objection is puzzling because modern economic theorists have a simple and appealing benchmark: “rational expectations,” which essentially equates rationality with the absence of systematic error.¹⁹ The intuition is that mere ignorance produces nothing worse than random mistakes. If you overestimate the level of traffic one morning, and underestimate it the day after, no one impugns your rationality. How are you supposed to know if a car will break down at rush hour and block two lanes? In contrast, if you underestimate the severity of traffic every day, “How was I supposed to know?” is a hollow excuse. There was not enough information to predict perfectly; but that hardly explains why predictions consistently fail the same way.\n\nAs formalizations go, rational expectations makes a lot of sense. Its violation is close to the everyday meaning of “irrationality.” Furthermore, an assumption akin to rational expectations is hard to do without. Who has not said something like “As price goes up, sellers increase their production”? Yet this elementary claim assumes that objective facts and subjective beliefs about price move in the same direction. If sellers systematically mistook rising prices for falling prices, their response would be the reverse of the standard prediction.\n\nIt is not surprising, then, that informal substitutes for rational expectations predate the formal literature. Years before Muth or Lucas, economists routinely affirmed that one can judge the “rationality” of actors’ means. For Downs, “The term rational is never applied to an agent’s ends, but only to his means. This follows from the definition of rational as efficient, i.e., maximizing output for a given input.”²⁰ Like rational expectations, Downs’s benchmark measures agents’ beliefs against objective reality:\n\nIf a theorist knows the ends of some decision-maker, he can predict which actions will be taken to achieve them as follows: (1) he calculates the most reasonable way for the decision-maker to reach his goals, and (2) he assumes this way will actually be chosen because the decision-maker is rational.²¹\n\nThe “rational expectations revolution” is a misnomer. It did triumph quickly as an analytical approach. But—with the exception of Keynesian macroeconomics—the change was usually cosmetic. Rational expectations primarily gave older styles of economics a more definite shape, leaving their spirit intact.\n\nStill, economists often lose their enthusiasm for rational expectations once evidence of systematic errors starts to pour in. If you equate rationality with the absence of systematic errors, hard empirical evidence of their presence is an open-and-shut case for irrationality. Rather than accept this unpalatable conclusion, lots of economists throw the rational expectations benchmark to the wolves.\n\nThen a third defense springs up: a looser definition of rationality that allows for systematic mistakes. Bayesianism is one alternative. As long as people update their beliefs according to Bayes’ Rule, they qualify as “rational,” even if they are grossly in error. However, this weak standard too has been experimentally tested and found wanting.²²\n\nA still weaker definition of rationality equates it with “truth-seeking.”²³ As long as a person sincerely tries to understand the world, he is rational in this sense, no matter what he believes. The only irrational people are those who fail to try; everyone else gets an A for effort.\n\nIt is important to notice that systematic errors like those in the SAEE are constitutive of irrationality in the rational expectations sense of the term, but remain a symptom of irrationality in its weaker senses. The sillier errors get, the more likely it is that the cause is lack of mental discipline, not lack of information.\n\nThe deepest problem with substitutes for rational expectations is that they give only a semantic victory. A lower threshold for “rationality” makes it easier to vouch for an individual’s rationality, but there is a high cost. Most models assume that individuals’ beliefs are unbiased, not merely that they are rational in some sense. So once you lower the threshold of rationality, you can no longer safely build on standard “rational actor” theorems. You have to go back to square one to save a word.\n\nWhat’s Wrong with Rational Ignorance, I\n\nThe phrase “rational ignorance” functions as a disclaimer. Stamping “rationally ignorant” on a person certifies that “the aforementioned ignorance of the subject does not impugn his rationality, which continues to enjoy a full warranty.” When people mention “irrationality,” economists dismiss them with the truism, “There is a difference between irrationality and ignorance.”²⁴ But this cuts both ways: if ignorance can be mistaken for irrationality, irrationality can be mistaken for ignorance. Maybe failing students in introductory econ could excel if they attended class and read the textbook. Then again, maybe not.\n\nStill, I do not want to dismiss the “ignorance only” view too hastily. What is wrong with it? This section and the next ask two critical questions of the “ignorance only” view:\n\nFirst: Is the ignorance-only view consistent with introspection and personal testimony?\n\nSecond: Can the ignorance-only view explain democratic failure?\n\nThe connection between error and lack of information is obvious. But is lack of information the root of all error? Introspection and personal testimony advance another candidate: emotional commitment.²⁵ Holding fast to beloved opinions increases subjective well-being. When the typical person defends the claims of his religion, to take the clearest example, he cares about the answer, and meets pertinent information with hostility if it goes against his convictions. To a large degree, we expect religious discussions to be “dogmatic,” with believers on all sides refusing to give rival sects a fair hearing. Cynics might call this posturing, but it is usually hard to doubt devotees’ sincerity. By and large, people are not pretending to be closed-minded on matters of faith.\n\nIn a secular age, politics and economics have displaced religion itself as the focal point for passionate conviction and dogmatism. As McCloskey says, “The man in the street cherishes his erroneous ideas about free trade. . . . He regards his ideas as part of his character, like his personality or his body type, and takes very unkindly to critical remarks about them.”²⁶ When liberals and conservatives quarrel about the effect of tax cuts, they have emotional investments in the answer. Conservatives like arguments that support tax cuts even if they are factually dubious; liberals dislike arguments that support tax cuts even if they make perfect sense.\n\nUndoubtedly this is partly strategic, but it strains credulity to claim that the confidence of the typical ideologue is “just an act.” Listen to Arthur Koestler describe his conversion to Communism:\n\nTo say that one has “seen the light” is a poor description of the mental rapture which only the convert knows (regardless of what faith he has been converted to). The new light seems to pour from all directions across the skull; the whole universe falls into pattern like the stray pieces of a jigsaw puzzle assembled by magic at one stroke. There is now an answer to every question, doubts and conflicts are a matter of the tortured past—a past already remote, when one had lived in dismal ignorance in the tasteless, colorless world of those who don’t know. Nothing henceforth can disturb the convert’s inner peace and serenity—except the occasional fear of losing faith again, losing thereby what alone makes life worth living, and falling back into the outer darkness, where there is wailing and gnashing of teeth.²⁷\n\nWhittaker Chambers makes the same point more succinctly:\n\nI was willing to accept Communism in whatever terms it presented itself, to follow the logic of its course wherever it might lead me, and to suffer the penalties without which nothing in life can be achieved. For it offered me what nothing else in the dying world had power to offer at the same intensity—faith and a vision, something for which to live and something for which to die.²⁸\n\nThe fanaticism of Koestler or Chambers is obviously rare, but I submit that in politics, disinterested objectivity is just as scarce.\n\nIntrospection also uncovers mixed cognitive motives. Recall the last argument you had on a topic you feel strongly about. You probably made an effort to give the other side a fair hearing. Why was it necessary, though, to make an effort? Because you knew that your emotions might carry you away; you might heatedly proclaim yourself the victor even if the evidence was against you. Whether or not you give in to temptation, there are always many who will. Irrationality is therefore all around us, and not just according to a demanding test like rational expectations. Drop the standard of rationality down to “truth-seeking” if you like. You can grade people for effort, and they still flunk.\n\nIf ignorance were the sole cause of error, sufficiently large doses of information would be a cognitive panacea. You could fix any misconception with enough facts. A few thought experiments show how implausible this is. Imagine trying to convert an audience of creationists to Darwinism. You might change some minds with patient lectures on genetics, fossil evidence, or fruit fly experiments.²⁹ But it would be miraculous if you convinced half. Similarly, envision John Lott addressing the Million Mom March on “more guns, less crime.”³⁰ Even if his empirical work were impeccable, it is hard to see more than a handful of crusaders for gun control exclaiming, “Oops, who would have guessed?” Indeed, few would concede, “This issue is more complicated than I thought; I’ll stop protesting until I get a better grip.” Or consider explaining the benefits of free trade to globalization protestors. A few might gain new insight into comparative advantage and economic development. Yet is anyone naive enough to suppose that he could convince a majority?\n\nMy point is not that real-world evidence is one-sided (though it often is!). Rather, my point is that if the evidence were one-sided, the fraction convinced would not rise to 100% with all the relevant information. Their emotional attachment to their beliefs is too intense: “Don’t confuse me with the facts.”\n\nAlmost every interesting topic in economics fits this description. Think about the SAEE. What would it take to convince everyone that supply-and-demand typically governs price? That excessive foreign aid is not a major problem? That downsizing is good in the long run? That living standards are rising? In each case, emotional commitment to the wrong answer—and hostility to naysayers—is widespread. A good teacher could change some minds, but the best teacher in the world would be lucky to convince half.\n\nAristotle says that “all men by nature desire to know,”³¹ but that is not the whole story. It is also true that all men by nature desire not to know unpleasant facts. Much of the time, both motives are at work. The human mind has mixed motives: people want to learn about the world without sacrificing their worldview.³² Investigating only the first motive yields a distorted picture of the way we use our heads.\n\nWhat’s Wrong with Rational Ignorance, II\n\nMany detractors reject Classical Public Choice on aesthetic grounds.³³ The civics textbook presents a beautiful picture of democracy. Its flaws should never be depicted as more than transient aberrations. Information economics adds insult to injury. It not only unveils deep flaws in democracy; it paints its flaws as inherent. Voters are ignorant due to inborn human selfishness, not an epidemic of apathy induced by “insufficient democracy.” Other critics, however, have substantive objections to Classical Public Choice. Taken together, they seriously undermine it.\n\nThe Miracle of Aggregation and the Irrelevance of Biased Information\n\nChapter 1 already worked through the deepest objection to Classical Public Choice’s account of political failure: Ignorant voters choose randomly, so with a reasonably large electorate they balance each other out, leaving the well informed in the driver’s seat.³⁴ A natural objection to this Miracle of Aggregation is that it takes on a straw man. The problem, one might say, is not that the ignorant vote randomly, but that the ignorant are easily misled by propaganda. The trouble is not the shortage of information, but its bias, which fills the heads of the ignorant with lies.³⁵\n\nWhile this story sounds good, it is theoretically wobbly. Ignorant does not mean impressionable. When you walk onto a used car lot, you may be highly ignorant, but you can still discount or ignore the words of the salesmen who shout, “You won’t get a better deal anywhere else!” As Wittman critically remarks:\n\nI have never met anyone who believes that the defense department does not exaggerate the need for defense procurement. But if everyone knows the defense department will exaggerate the importance of its contribution to human welfare, then, on average, voters will sufficiently discount defense department claims. Even when the ruling class has a virtual domestic monopoly on the instruments of information, as was the case in the former Soviet Union, we observe people discounting the information contained in their papers and trusting foreign sources.³⁶\n\nAt minimum, why wouldn’t highly ignorant voters tune out unreliable sources? They do not have to fact-check political ads, just greet them with blanket skepticism. That is the commonsense response to unverified assertions from sources with questionable motives.\n\nPopular metaphors are partly to blame for the confusion. Writers often compare the ignorant to empty vessels, clean sheets of paper, or blank slates. Mao Zedong thought it fortunate that the Chinese peasantry was “poor and blank” because “a clean sheet of paper has no blotches and so the newest and most beautiful words can be written on it.”³⁷ Such metaphors gloss over the distinction between being ignorant and being receptive to new ideas. One does not follow from the other. A blank slate can be difficult to write upon; an ignorant voter can be hard to persuade. If you hear only cheap talk by rival politicians, the rational course is to stay agnostic.\n\nThus, you can grant that (almost all) voters are morbidly ignorant yet remain optimistic about how well democracy works. There is nothing mystical about the Miracle of Aggregation—it is simple statistics. And as long as ignorance is circumscribed by common sense, the Miracle of Aggregation is sturdy enough to withstand floods of biased information.³⁸\n\nOptimal Punishment and Correlations between Information and Interests. What happens if the more informed have predictably different interests than the less informed—in technical terms, if there is a correlation between information and interests? Political corruption is a clear example. Those who know the most about the corruption—the bribe-taker and the bribe-payer—profit from it; the people who suffer because of corruption do not know who is paying whom to do what.\n\nYou face the same problem if well-informed voters have different interests than the rest of the population. Suppose that 60% of voters are uninformed and poor, 20% are uninformed and rich, 5% are well informed and poor, and 15% are well informed and rich. If people vote their pocketbooks in a two-candidate race, the more prorich politician gets half the uninformed votes but three-quarters of the well-informed votes. The prorich candidate wins with 55% of the vote, though 65% of voters are poor.\n\nCorrelations between information and interests seem like a strong objection to the Miracle of Aggregation. The more informed have the power to manipulate the system, and there is nothing the less informed can do about it. But like biased information, there is less to this problem than meets the eye. One can circumvent its dangers with a little help from the economics of crime.\n\nSuppose a robber has a 50% chance of being caught lifting $1,000 from a cash register. If the punishment is a $1,000 fine, crime pays: Heads, the thief wins; tails, he breaks even. Legal systems cope with this problem by making a convicted criminal much worse off than he would have been if he had obeyed the law. In economic jargon, the law imposes “probability multipliers”—making sentences tougher as the chance of being caught declines.³⁹ As Gary Becker originally put it, the idea is “to keep police and other expenditures relatively low and to compensate by meting out strong punishments to those convicted.”⁴⁰\n\nAn ignorant electorate can use the same strategy to control politicians. Voters do not need to pay much attention to politics; they only need to vow revenge if they catch their leaders misbehaving. You learn that a congressman uses the franking privilege to send personal mail—give him a year of jail. A cabinet member mutters a racial epithet on tape—demand his resignation. A convict on furlough commits a murder—vote against the incumbent governor in the next election. Finally, if a politician pays too much attention to the well informed, declare him an elitist and throw the snob out. What appears to be an “overreaction” is an easy way for the ignorant to elicit good behavior day in, day out.\n\nBig Government: The Neglected Victim of Asymmetric Information. Information is “asymmetric” when more-knowledgeable people interact with less-knowledgeable people. The classic example is the used-car market: the dealer knows details that customers can only guess.⁴¹ Political corruption fits the same description: A politician knows if he has been dishonest, but the public may not.\n\nHarsh punishment is the simplest way for the ignorant to protect their interests. But what if the harshest available punishment is too mild to keep politicians in line? A used-car dealer who gets caught lying to his customers might lose more than their goodwill; he risks a fraud conviction as well. In contrast, after he irreversibly ruins his public reputation, a politician can earn a comfortable living in a law firm. A democratically elected leader can break all his campaign promises without risking a day in jail or a one-dollar lawsuit. Heads he wins, tails he breaks even: a recipe for constant abuse.\n\nTo many, unmitigated asymmetric information provides a clean account of how democracy fails.⁴² It is the alleged mechanism that sustains Big Government, letting politicians, bureaucrats, and lobbyists waste taxpayers’ money on one pointless program and regulation after another. The insiders are the only ones who know what is going on, and if they are caught red-handed, they get a slap on the wrist, not harsh “optimal punishments.”\n\nThis story is plausible but incomplete and easy to misinterpret. To see why, return to the used-car market. Due to their informational disadvantage, as Akerlof explained,⁴³ prospective purchasers of used cars are wary. Salesmen must demonstrate the quality of their product to consumers’ satisfaction. If the demonstration is unconvincing, buyers slash their bids to reflect uncertainty. If their doubts are strong enough, they walk away. Thus, the greater sellers’ informational advantage, the smaller the demand for their product. Asymmetric information is bad for sellers as well as buyers.\n\nThe same principle applies to politics. You do not need to follow politics closely in order to realize that insiders know more than you do. Armed with this epiphany, you have a straightforward countermove: When in doubt, say no.⁴⁴ Voters can assign fewer responsibilities and surrender less money to a government they do not trust by voting for politicians who share their doubts. So contrary to popular stories, asymmetric information leads to less government.⁴⁵\n\nTo see why, suppose that there are 10 proposed government programs. Four of them make the typical voter $100 better off; the other six transfer $100 from the typical voter to an interest group. If voters know which programs are good and which are bad, four of the 10 will enjoy popular support. However, if there is asymmetric information, if voters cannot distinguish good programs from bad, they expect to lose $20 from any given program, and therefore oppose all 10.\n\nIf insiders lobbied harder for the bad programs, the effect of asymmetric information would be even stronger. There could be forty good proposals, and only six bad ones. If voters hear about all of the bad ones, but only 10% of the good ones, asymmetric information leads voters to oppose every new program that crosses their path. The whole barrel can go to waste because of a few bad apples.\n\nYes, in vital areas, voters might prefer corrupt government to none at all. But these are rare compared to the countless marginal functions that voters might assign to government if they knew it would do a good job.⁴⁶ Government transparency is bad for insiders with something to hide, but good for government overall.\n\nInarticulate Knowledge and Cognitive Shortcuts. The preceding arguments are skeptical about the consequences of voter ignorance. None questions its severity. But some critics add that voters’ ignorance is greatly exaggerated. Objective tests show that voters are bad at articulating what they know about politics. Perhaps, however, they hold the same positions they would have adopted after intensive study. How? By falling back on “cognitive shortcuts”—informal or subliminal cues.⁴⁷ Lupia and McCubbins use the example of a motorist crossing a busy intersection:\n\nAdvocates of complete information might argue that successful automotive navigation requires as much information as you can gather about the intentions of other drivers and the speed, acceleration, direction, and mass of their cars. At many intersections, however, there is a simple substitute for all of this information—a traffic signal.⁴⁸\n\nBrand names help shoppers far more than Consumer Reports ever will. Perhaps party labels play an analogous role in politics. Or consider word of mouth. You often buy on a friend’s recommendation. You would look foolish if you were quizzed about the pros and cons of your decision. But it was ultimately well informed. The same could hold for political stances—a person who slavishly follows friends’ advice might flunk a test of political knowledge despite the fact that his decision indirectly draws from a well of careful deliberation. As Lupia and McCubbins wryly observe: “Asserting that limited information precludes reasoned choice is equivalent to requiring that people who want to brush their teeth recall the ingredients of their toothpaste.”⁴⁹\n\nThe leading version of this approach is the theory of retrospective voting.⁵⁰ The intuition: Instead of second-guessing your leader’s decisions, look at the country during his tenure. If it enjoyed prosperity and peace, reelect the incumbent or his anointed successor. If it suffered from depression and war, throw the bum out. This cognitive shortcut rewards smart decisions, and in turn spurs politicians to make smart decisions—even if you have no idea what the smart decision is.\n\nIn my view, appeals to inarticulate knowledge are far less compelling than other objections to Classical Public Choice. Inarticulate knowledge clearly exists, but you would expect articulate and inarticulate knowledge to positively correlate. Knowledge of anatomy does not make one a surgeon, but most trained surgeons can still describe in detail how the human body works. Low objective test scores are not sure proof of incompetence, but they point in that direction.\n\nShoppers rely on brand names and word of mouth, but that is not the limit of their knowledge. They also have a lot of articulate knowledge, without which their cognitive shortcuts would be far less useful. If you do not grasp the difference between orange juice and detergent, brand names will at best help you drink the finest detergent on the market, and wash your dishes with the right amount of pulp. What protects shoppers from making this mistake is their conscious ability to identify and explain the pros and cons, the uses and limitations, of hundreds of products.\n\nIn contrast, a voter unable to describe his representative’s policies, demarcate his areas of authority—or name him—is not out of the ordinary. This puts a serious damper on retrospective voting. If voters do not know term lengths, incumbent politicians will be punished for the sins of their predecessor, and share credit for their achievements with their successors. If voters pay no attention to policy, “prosperity and peace” voting heavily discourages the adoption of policies with long-run gains but short-term costs—such as a preemptive war against a rising menace.\n\nFurthermore, what good does retrospective voting do if voters do not know which branch—or branches—of government are responsible for what?⁵¹ Reelecting incumbent presidents during periods of prosperity is a silly shortcut if economic performance primarily depends on the independent central bank. Correctly assigning credit and blame is especially important under divided government, when retrospective voting could create truly perverse incentives. If voters punish presidents for high unemployment, a Republican Congress could defeat a Democratic president by fighting against recovery.\n\nSomeone unschooled in physics can be a great pool player. Researchers who emphasize inarticulate knowledge correctly point out that tests of articulate knowledge understate functional know-how.⁵² But they do not show that tests of political knowledge understate functional voter know-how to a larger than normal degree, still less that articulate knowledge and voter know-how are unrelated. Indeed, as Althaus observes, research on enlightened preferences shows the opposite. Articulate knowledge usually predicts systematically different policy views:\n\nWhile many respondents may use heuristics, on-line processing, and information shortcuts to arrive at the political opinions they express in surveys, these substitutes for political knowledge do not necessarily help ill-informed people express policy preferences similar to those of well-informed people. If they did, surveyed opinion across the board should closely resemble fully informed opinion.⁵³\n\nWittman’s Fork\n\nThe most compelling objections to Classical Public Choice accommodate rational ignorance. Instead of disputing its theoretical coherence or empirical accuracy, they quarrel with conventional beliefs about its consequences:\n\n• Contrary to Classical Public Choice, the level of voter ignorance has little effect on policy. More careful analysis, guided by the law of large numbers, shows that the influence of well-informed voters is disproportionate to their head count.\n\n• Ignorance does not turn voters into easy marks for propaganda and deceit. Lack of information is not equivalent to folly, and only a fool would take unverified, self-serving political advertising at face value.\n\n• Voter ignorance does not imply corruption and insider manipulation. True, if the severity of formal and informal punishment stays constant, voter inattention implies lower expected penalties for misbehavior. But there is an obvious cure: compensate for lax monitoring with unforgiving punishment.\n\n• Finally, if harsh punishments cannot be imposed, the sensible voter response to insider manipulation is skepticism. They can reject so-called government “solutions” until the day—and that day may never come to pass—when there is solid proof of their efficacy.\n\nThe implications for Classical Public Choice are radical. Rational ignorance, long since convicted by a vast literature of subverting democracy, lacks the means to commit the crime of which is stands accused. The defendant has a solid alibi. Appeals to the self-evidence of the premise or the conclusion are beside the point. The issue is the link, or lack thereof, between rational ignorance and inefficiently large government.\n\nOnce we understand how rational ignorance does and does not matter, there is a temptation to “close the case” against democracy. Yet it would be premature to infer that the conclusions of Classical Public Choice are false. The fact that the prime suspect in a murder investigation is innocent does not mean that the victim died of natural causes. Logic texts are full of examples of invalid arguments from true premises to true conclusions. The premises “Some men are mortal” and “I am a man” are true, and so is the conclusion, “I am mortal.” But “Some men are mortal; I am a man; therefore I am mortal” is not a valid argument. (Consider the logically parallel “Some men have red hair; I am a man; therefore I have red hair.”) The failure of rational ignorance implies that democracy’s critics must find an alternative mechanism.\n\nThis is not as easy as it sounds. The maverick economist Donald Wittman of UC Santa Cruz persuasively contends that there are essentially three routes: “Behind every model of government failure is an assumption of extreme voter stupidity, serious lack of competition, or excessively high negotiation/transfer costs.”⁵⁴ Wittman adds that economists ordinarily treat all three sorts of explanations as dubious. I call this Wittman’s Fork: there are but three paths to democratic failure (fig. 4.1).⁵⁵\n\nBuilding on the preceding critique of rational ignorance, Wittman deliberately says “extreme stupidity,” not “ignorance.” This is a bit harsh: Wittman might make you wear a dunce cap for having a mediocre grasp of advanced game theory.⁵⁶ His point, though, is that ignorance cannot carry the weight the critics of democracy assign to it. If voters are to blame for the failures of democracy, their flaw has to be deeper than “lack of information.”\n\nWhat about Wittman’s two other options? Despite its focus on the rational ignorance of the electorate, Classical Public Choice leaves room for fully informed political failure. A market monopoly can fleece fully informed consumers; a political monopoly could do the same to fully informed voters. But in recent decades economists have met charges of “monopoly” with suspicion.⁵⁷ How do you become a “monopoly”—business, political, or other—in the first place? Wittman aptly encapsulates modern thinking:\n\n[f0110-01]\n\nFigure 4.1 Wittman’s Fork\n\nIncumbents tend to be reelected for the same reason that the winner of the last footrace is likely to win the next one and the head of a corporation is likely to maintain his position tomorrow. They are the best. That is why they won in the first place and why they are likely to win again.⁵⁸\n\nIf market monopoly worries you, then probably so should political monopoly. But before you rush to “restore competition,” in either arena, reflect on the long-term dangers of penalizing success.\n\nA parallel story holds for the remaining refuge of Classical Public Choice: “excessively high negotiation/transfer costs.” Markets fail to execute some otherwise beneficial trades because of transaction costs. Political logrolling has the same problem.⁵⁹ Yet it is hard to get excited about these missed opportunities. Will it not be the marginal deals, of little consequence, that remain undone? On top of this, democracy is designed to shear the transactions costs of ordinary contract law.⁶⁰ In markets, you need participants’ unanimous consent to strike a bargain; under democracy, majorities often suffice to reach a decision.\n\nIt is tempting to reply that Wittman’s sanguine views on political competition and transactions costs have been empirically refuted during the past decade.⁶¹ Direct democracy yields different outcomes than indirect democracy. Senators from the same state often disagree. Open primaries, redistricting, campaign finance rules, and the degree of party competition affect political outcomes.⁶² Besley and Case feel comfortable stating, “At a general level, the median voter model, the workhorse of so much political economy modeling for more than a generation, receives little empirical support.”⁶³\n\nI suspect, however, that Wittman would be unfazed by these findings. In the grand scheme of things, he would probably say, the reported effects are small. Perhaps Besley and Case are right, for example, that “a ten percentage point increase in the fraction of seats held by Democrats in both the lower and upper houses is associated with an increase in overall state spending per capita of $10 in 1982 dollars.”⁶⁴ Does that refute the claim that government basically gives voters what they want? Even the fact that senators from the same state often disagree is not so troubling. Maybe voters deliberately elect senators from different parties to dilute the effect of ideological shirking.⁶⁵ And if new legislation slightly adjusts a status quo that is close to constituents’ preferences to begin with, senators from the same state only need a little slack in order to vote differently.\n\nWittman would also probably argue that other researchers misinterpret their findings. If lopsided legislatures really pushed policy away from voters’ preferences, they would stop voting for them. A more plausible story, for Wittman, would be that researchers do not correctly measure voter preferences. Voters elect lopsided legislatures in order to get the policies lopsided legislatures typically deliver. Surely, he might inquire, you are not suggesting that people systematically underestimate the effects of giving one party the lion’s share of power?\n\nWittman’s aim is to drive Classical Public Choice in his preferred direction by blocking every option but his own. Then all serious students of politics will have to concede that democracy works well. In fact, however, Wittman leaves the route of “extreme voter stupidity” wide open. He provides little empirical evidence of voters’ mental prowess.⁶⁶ Instead, he tries to dissuade us with scary rhetoric. Faced with a choice between the implausible view that “democracy works well” and the embarrassing view that “voters are extremely stupid,” Wittman bets that democracy’s detractors will endorse the former.\n\nRethinking “Extreme Voter Stupidity”\n\nEfforts to minimize the effect of voter ignorance may have struck you as far-fetched. Errors harmlessly cancel out? The average voter seamlessly adjusts for media bias, and imposes probability multipliers to discipline misbehavior? Government shrinks because voters do not know how well its programs work? To escape such odd conclusions, all you have to do is stop talking about voter ignorance, and start talking about voter irrationality.⁶⁷\n\nTake the Miracle of Aggregation. The mistakes of ignorant voters cancel each other out, leaving informed voters in charge. If you find this conclusion fantastic, relief is at hand. Admit that voters have systematic biases, that they are, in technical terms, somewhat irrational. Then instead of canceling, the electorate’s errors tilt policy in the expected direction.\n\nThe same goes for biased information. Rationally ignorant individuals would not be swayed, but that does not mean no swaying occurs. If individuals fall short of full rationality, they might inadequately adjust for the credibility of the source. They might embrace propaganda because they like the way that a speaker sounds or smiles or dresses—or the movies he starred in. Irrationality does not imply impressionability, but—unlike rational ignorance—it does not rule it out.\n\nIrrational voters’ punishment strategies may be equally inept. Just because they possess the right tools for keeping politicians honest does not mean they will deploy them. Optimal punishment rises as the probability of detection falls and the benefit of breaking the rules goes up. Irrational voters may flout these vital principles. They might bemoan politicians’ dishonesty, then turn around and forgive flagrant promise-breaking. Irrational voters could make the reputational fallout for minor offenses higher than major ones with the same probability of detection. In the real world, which is more likely to incur the public’s wrath: an off-color joke, or a broken campaign promise? A sex scandal, or failure to prevent a terrorist attack?\n\nAlong the same lines, irrational voters may respond to asymmetric information with blind faith rather than cautious skepticism. Rationally ignorant voters employ a “When in doubt, say no” strategy, giving politicians and activists with good ideas a strong incentive to prove their case. But irrational voters might take the naive stance, “If they say we need a program, we must!”—tempting insiders to concoct one scary story after another.⁶⁸\n\nConclusion\n\nUnlike ignorance, irrationality allows a wide range of outcomes. Many see the absence of a unique prediction as a defect, or a sign of intellectual sloth. I do not. As Richard Thaler pointedly asks, “Would you rather be elegant and precisely wrong, or messy and vaguely right?”⁶⁹ Recognizing that objective facts do not nail down political beliefs shows us how to spend our time more wisely. Theories of irrationality need discipline from the empirics of public opinion. We should focus on this vital task—as the last chapter did—instead of making tortured arguments about how voters’ beliefs flow logically from the facts.\n\nUnfortunately, many economists have trouble getting over the conflict between voter irrationality and economic theory. One is tempted to say “So what?” but this is a flippant response. In all fairness, the economic approach to human behavior has been extremely fruitful. Basic economic theory cannot be lightly cast aside.\n\nFortunately, there is no need to do so. With a slight conceptual twist, voter irrationality becomes a natural extension of basic economic theory, not a deviation from it. The next chapter develops and explores a new model of cognition to show how one and the same individual can be both a “rational consumer” and an “irrational voter.” From this standpoint, the evidence of systematic error ceases to be anomalous. Economists should have expected it all along. With this new groundwork laid, a disquieting yet intuitive vision of political economy falls naturally into place.\n\nChapter 5\n\nRATIONAL IRRATIONALITY\n\nFor it seemed to me that I could find much more truth in\nthe reasonings that each person makes concerning matters\nthat are important to him, and whose outcome ought\nto cost him dearly later on if he judged badly, than in\nthose reasonings engaged in by a man of letters in his\nstudy, which touch on speculations that produce no effect\nand are of no other consequence to him except perhaps\nthat, the more they are removed from common sense, the\nmore pride he will take in them.\n\n—Rene Descartes, Discourse on Method¹\n\nSUPPOSE you grant that voters are irrational. Can you stop there? Voters are people. If they are highly irrational on election day, one would expect them to be equally irrational the rest of the year. Do individuals magically transform into a lower form of life when they enter the voting booth, then revert to their normal state upon exit?\n\nThe thesis of global human rationality is internally consistent. So is the opposite thesis that humans are irrational through and through. Is there a coherent intermediate position? Without one, the practical relevance of voters’ folly shrinks or vanishes. If people are rational on Monday and irrational on Tuesday, it is a good idea to shift decision-making to Monday. But if people are irrational twenty-four seven, you just have to live with the fact that all decisions will be worse. By the same reasoning, if people are rational as consumers but irrational as voters, it is a good idea to rely more on markets and less on politics. But if people are irrational across the board, we should expect less of every form of human organization. The relative merits of alternative systems stay roughly the same.²\n\nEven if an intermediate position is coherent, is it consistent with what we already know? One could postulate voter irrationality as an ad hoc exception to the laws of human behavior. But ad hoc exceptions to well-established principles understandably provoke skepticism.³ Is there any way to subsume established patterns and anomalies under a single rule?\n\nThis chapter meets these theoretical challenges. Though initially jarring, it is coherent to assert that people are rational in some areas but not others. Irrational beliefs probably play a role in all human activities, but politics makes the “short list” of areas where irrationality is exceptionally pronounced. Furthermore, basic economic theory—properly interpreted—helps define the boundaries of rationality. Political irrationality is not an ad hoc anomaly, but a predictable response to unusual incentives.\n\nPreferences over Beliefs\n\n“I ca’n’t believe that!” said Alice.\n“Ca’n’t you?” the Queen said in a pitying tone.\n“Try again: draw a long breath, and shut your eyes.”\nAlice laughed. “There’s no use trying,” she said.\n“One ca’n’t believe impossible things.”\n“I dare say you haven’t had much practice,” said\nthe Queen. “When I was your age, I always did it for half-\nan-hour a day. Why, sometimes I’ve believed as many as\nsix impossible things before breakfast.”\n\n—Lewis Carroll, Through the Looking-Glass⁴\n\nThe desire for truth can clash with other motives. Material self-interest is the leading suspect. We distrust salesmen because they make more money if they shade the truth. In markets for ideas, similarly, people often accuse their opponents of being “bought,” their judgment corrupted by a flow of income that would dry up if they changed their minds. Dasgupta and Stiglitz deride the free-market critique of antitrust policy as “well-funded” but “not well-founded.”⁵ Some accept funding from interested parties, then bluntly speak their minds anyway. The temptation, however, is to balance being right and being rich.\n\nSocial pressure for conformity is another force that conflicts with truth-seeking.⁶ Espousing unpopular views often transforms you into an unpopular person. Few want to be pariahs, so they self-censor. If pariahs are less likely to be hired, conformity blends into conflict of interest. However, even bereft of financial consequences, who wants to be hated? The temptation is to balance being right and being liked.\n\nBut greed and conformism are not the only forces at war with truth. Human beings also have mixed cognitive motives.⁷ One of our goals is to reach correct answers in order to take appropriate action, but that is not the only goal of our thought. On many topics, one position is more comforting, flattering, or exciting, raising the danger that our judgment will be corrupted not by money or social approval, but by our own passions.\n\nEven on a desert isle, some beliefs make us feel better about ourselves. Gustave Le Bon refers to “that portion of hope and illusion without which [men] cannot live.”⁸ Religion is the most obvious example.⁹ Since it is often considered rude to call attention to the fact, let Gaetano Mosca make the point for me:\n\nThe Christian must be enabled to think with complacency that everybody not of the Christian faith will be damned. The Brahman must be given grounds for rejoicing that he alone is descended from the head of Brahma and has the exalted honor of reading the sacred books. The Buddhist must be taught highly to prize the privilege he has of attaining Nirvana soonest. The Mohammedan must recall with satisfaction that he alone is a true believer, and that all others are infidel dogs in this life and tormented dogs in the next. The radical socialist must be convinced that all who do not think as he does are either selfish, money-spoiled bourgeois or ignorant and servile simpletons. These are all examples of arguments that provide for one’s need of esteeming one’s self and one’s own religion or convictions and at the same time for the need of despising and hating others.¹⁰\n\nWorldviews are more a mental security blanket than a serious effort to understand the world: “Illusions endure because illusion is a need for almost all men, a need they feel no less strongly than their material needs.”¹¹ Modern empirical work suggests that Mosca was on to something: The religious consistently enjoy greater life satisfaction.¹² No wonder human beings shield their beliefs from criticism, and cling to them if counterevidence seeps through their defenses.\n\nMost people find the existence of mixed cognitive motives so obvious that “proof” is superfluous. Jost and his coauthors casually remark in the Psychological Bulletin that “Nearly everyone is aware of the possibility that people are capable of believing what they want to believe, at least within certain limits.”¹³ But my fellow economists are unlikely to sign off so easily. If one economist tells another, “Your economics is just a religion,” the allegedly religious economist normally takes the distinction between “emotional ideologue” and “dispassionate scholar” for granted, and paints himself as the latter. But when I assert the generic existence of preferences over beliefs, many economists challenge the whole category. How do I know preferences over beliefs exist? Some eminent economists imply that this is impossible to know because preferences are unobservable.¹⁴\n\nThey are mistaken. I observe one person’s preferences every day—mine. Within its sphere I trust my introspection more than I could ever trust the work of another economist.¹⁵ Introspection tells me that I am getting hungry, and would be happy to pay a dollar for an ice cream bar. If anything qualifies as “raw data,” this does. Indeed, it is harder to doubt than “raw data” that economists routinely accept—like self-reported earnings.\n\nOne thing my introspection tells me is that some beliefs are more emotionally appealing than their opposites. For example, I like to believe that I am right. It is worse to admit error, or lose money because of error, but error is disturbing all by itself. Having these feelings does not imply that I indulge them—no more than accepting money from a source with an agenda implies that my writings are insincere. But the temptation is there.\n\nIntrospection is a fine way to learn about your own preferences. But what about the preferences of others? Perhaps you are so abnormal that it is utterly misleading to extrapolate from yourself to the rest of humanity. The simplest way to check is to listen to what other people say about their preferences.\n\nI was once at a dinner with Gary Becker where he scoffed at this idea. His position, roughly, was, “You can’t believe what people say,” though he still paid attention when the waiter named the house specialties. Yes, there is a sound core to Becker’s position. People fail to reflect carefully. People deceive.¹⁶ But contrary to Becker, these are not reasons to ignore their words. We should put less weight on testimony when people speak in haste, or have an incentive to lie. But listening remains more informative than plugging your ears. After all, human beings can detect lies as well as tell them. Experimental psychology documents that liars sometimes gives themselves away with demeanor or inconsistencies in their stories.¹⁷\n\nOnce we take the testimony of mankind seriously, evidence of preferences over beliefs abounds. People can’t shut up about them. Consider the words of philosopher George Berkeley:\n\nI can easily overlook any present momentary sorrow when I reflect that it is in my power to be happy a thousand years hence. If it were not for this thought I had rather be an oyster than a man.¹⁸\n\nPaul Samuelson himself revels in the Keynesian revelation, approvingly quoting Wordsworth to capture the joy of the General Theory:\n\nBliss was it in that dawn to be alive,\n\nBut to be young was very heaven\\!¹⁹\n\nMany autobiographies describe the pain of abandoning the ideas that once gave meaning to the author’s life. As Whittaker Chambers puts it:\n\nSo great an effort, quite apart from its physical and practical hazards, cannot occur without a profound upheaval of the spirit. No man lightly reverses the faith of an adult lifetime, held implacably to the point of criminality. He reverses it only with a violence greater than the faith he is repudiating.²⁰\n\nNo wonder that—in his own words—Chambers broke with Communism “slowly, reluctantly, in agony.”²¹ For Arthur Koestler, deconversion was “emotional harakiri.” He adds, “Those who have been caught by the great illusion of our time, and have lived through its moral and intellectual debauch, either give themselves up to a new addiction of the opposite type, or are condemned to pay with a lifelong hangover.” Richard Wright laments, “I knew in my heart that I should never be able to feel with that simple sharpness about life, should never again express such passionate hope, should never again make so total a commitment of faith.”²²\n\nThe desire for “hope and illusion” plays a role even in mental illness.²³ According to his biographer, Nobel Prize winner and paranoid schizophrenic John Nash often preferred his fantasy world—where he was a “Messianic godlike figure”²⁴ —to harsh reality:\n\nFor Nash, the recovery of everyday thought processes produced a sense of diminution and loss. . . . He refers to his remissions not as joyful returns to a healthy state, but as “interludes, as it were, of enforced rationality.”²⁵\n\nHistorians of thought also frequently document enthusiastic support for dubious dogmas. Listen to Böhm-Bawerk trace the psychological appeal of Marxian exploitation theory:\n\nIt drew up the line of battle on a field where the heart, as well as the head is wont to speak. What people wish to believe, they believe very readily. . . . When the implications of a theory point toward raising the claims of the poor and lowering those of the rich, many a man who finds himself faced with that theory will be biased from the outset. And so he will in large measure neglect to apply that critical acuity which he ordinarily would devote to an examination of scientific justification. Naturally it goes without saying that the great masses will become devotees of such doctrines. Critical deliberation is of course no concern of theirs, nor can it be; they simply follow the bent of their wishes. They believe in the exploitation theory because of its conformity to their preferences, and despite its fallaciousness. And they would still believe in it, if its scientific foundations were even less stable than they actually are.²⁶\n\nIf neither way of verifying the existence of preferences over beliefs appeals to you, a final one remains. Reverse the direction of reasoning. Smoke usually means fire. The more bizarre a mistake is, the harder it is to attribute to lack of information. Suppose your friend thinks he is Napoleon. It is conceivable that he got an improbable coincidence of misleading signals sufficient to convince any of us. But it is awfully suspicious that he embraces the pleasant view that he is a world-historic figure, rather than, say, Napoleon’s dishwasher. Similarly, suppose an adult sees trade as a zero-sum game. Since he experiences the opposite every day, it is hard to blame his mistake on “lack of information.” More plausibly, like blaming your team’s defeat on cheaters, seeing trade as disguised exploitation soothes those who dislike the market’s outcome.\n\nThe Material Costs of Error\n\nThe human being . . . very rarely fails to keep two great aspirations\nbefore his eyes, two sentiments that ennoble, uplift,\nand purify him. He seeks the truth, he loves justice;\nand sometimes he is able to sacrifice to those two ideals\nsome part of the satisfaction he would otherwise give to\nhis passions and his material interests.\n\n—Gaetano Mosca, The Ruling Class²⁷\n\nIn extreme cases, mistaken beliefs are fatal. A baby-proofed house illustrates many errors that adults cannot afford to make. It is dangerous to think that poisonous substances are candy. It is dangerous to reject the theory of gravity at the top of the stairs. It is dangerous to hold that sticking forks in electrical sockets is harmless fun.\n\nBut false beliefs do not have to be deadly to be costly. If the price of oranges is 50 cents each, but you mistakenly believe it is a dollar, you buy too few oranges. If bottled water is, contrary to your impression, neither healthier nor better-tasting than tap water, you may throw hundreds of dollars down the drain. If your chance of getting an academic job is lower than you guess, you could waste your twenties in a dead-end Ph.D. program.\n\n[f0120-01]\n\nFigure 5.1 The Material Costs of Error\n\nMore fancifully, suppose you think the world ends tomorrow. You would probably decide you had more important tasks than going to work. Maybe you would loudly quit your job, then spend all the money in your bank account. If you awake the next morning to find that reports of the earth’s demise were exaggerated, you will be happy to be alive but chagrined to realize that you are unemployed and broke.\n\nIt is amusing when the deluded triumph because of dumb luck: “I started with wrong directions, but I took a wrong turn, so I got to the right place on time.” The story works because it cuts against our expectations. Ordinarily, false beliefs lead individuals to take actions that would be optimal if the world were different. For example, figure 5.1 contrasts the number of oranges a person buys with the number he would buy conditional on correctly perceiving the market price. The larger his misperception, the larger the triangle representing the dollar cost of the error.\n\nThe cost of error varies with the belief and the believer’s situation. For some people, the belief that the American Civil War came before the American Revolution would be a costly mistake. A history student might fail his exam, a history professor ruin his professional reputation, a Civil War reenactor lose his friends’ respect, a public figure face damaging ridicule.\n\nNormally, however, a firewall stands between this mistake and “real life.” Historical errors are rarely an obstacle to wealth, happiness, descendants, or any standard metric of success. The same goes for philosophy, religion, astronomy, geology, and other “impractical” subjects. The point is not that there is no objectively true answer in these fields. The Revolution really did precede the Civil War. But your optimal course of action if the Revolution came first is identical to your optimal course if the Revolution came second.\n\nTo take another example: Think about your average day. What would you do differently if you believed that the earth began in 4004 B.C., as Bishop Ussher infamously maintained?²⁸ You would still get out of bed, drive to work, eat lunch, go home, have dinner, watch TV, and go to sleep. Ussher’s mistake is cheap.\n\nVirtually the only way that mistakes on these questions injure you is via their social consequences. A lone man on a desert island could maintain practically any historical view with perfect safety. When another person washes up, however, there is a small chance that odd historical views will reduce his respect for his fellow islander, impeding cooperation. Notice, however, that the danger is deviance, not error. If everyone else has sensible historical views, and you do not, your status may fall. But the same holds if everyone else has bizarre historical views and they catch you scoffing.²⁹\n\nMistakes on more practical questions also often fail to ricochet back with dire consequences. Some errors are costly for the person who commits them only under special circumstances that hardly ever arise. The belief that you can outrun a cheetah would prove fatal at the wrong place and the wrong time. But given the chance of cheetah encounters, it is usually a safe mistake. More interestingly, errors with drastic real-world repercussions can be cheap for the individual who makes them. How? When most or all of the cost of the mistake falls upon strangers. One person messes up, but other people live with the aftermath.\n\nTo use economic jargon, the private cost of an action can be negligible, though its social cost is high.³⁰ Air pollution is the textbook example. When you drive, you make the air you breathe worse. But the effect is barely perceptible. Your willingness to pay to eliminate your own emissions might be a tenth of a cent. That is the private cost of your pollution. But suppose that you had the same impact on the air of 999,999 strangers. Each disvalues your emissions by a tenth of a cent too. The social cost of your activity—the harm to everyone including yourself—is $1,000, a million times the private cost.\n\nNotice that in the pollution story, you are not—selfishly speaking—making a mistake. But the distinction between social and private costs also applies to erroneous beliefs. A mad scientist, convinced he is too brilliant to fail, might unleash a virus on the world. If he is immune—and if no one catches him—the private cost of his inflated ego is zero, even though millions pay with their lives.\n\nStories with a lone polluter or a mad scientist are an unthreatening way to illustrate the distinction between private and social costs. In the real world, the roles of hero and villain are seldom so discrete. Practically everyone is a victim and a perpetrator; most of the people who breathe my auto emissions are drivers themselves. Returning to the pollution example, suppose that all of the million people drive and pollute, bringing the total social cost of pollution to a billion dollars.³¹ Commonsense morality brands anyone who complains as a hypocrite, but the pollution level is still inefficiently high.\n\nGulfs between the private and social costs of error permeate group decision-making. Take a hiring committee. Its members deliberate between candidates A and B. The committee as a group has absolute power over the decision, and all members are worse off if the committee makes the inferior choice. Nevertheless, the most that any member can do is slightly tilt the scales, implying a gap between the private and social costs of mistaken beliefs about A and B.³² When I tilt the scales the wrong way, I hurt everyone on the committee, not myself alone.\n\nRational Irrationality\n\nThus the typical citizen drops down to a lower level of\nmental performance as soon as he enters the political\nfield. He argues and analyzes in a way which he would\nreadily recognize as infantile within the sphere of his real\ninterests. He becomes a primitive again.\n\n—Joseph Schumpeter, Capitalism,\nSocialism, and Democracy³³\n\nTwo forces lie at the heart of economic models of choice: preferences and prices. A consumer’s preferences determine the shape of his demand curve for oranges; the market price he faces determines where along that demand curve he resides. What makes this insight deep is its generality. Economists use it to analyze everything from having babies to robbing banks.\n\nIrrationality is a glaring exception. Recognizing irrationality is typically equated with rejecting economics.³⁴ A “logic of the irrational” sounds self-contradictory. This chapter’s central message is that this reaction is premature. Economics can handle irrationality the same way it handles everything: preferences and prices. As I have already pointed out:\n\n• People have preferences over beliefs: A nationalist enjoys the belief that foreign-made products are overpriced junk; a surgeon takes pride in the belief that he operates well while drunk.\n\n[f0123-01]\n\nFigure 5.2 The Demand for Irrationality\n\n• False beliefs range in material cost from free to enormous: Acting on his beliefs would lead the nationalist to overpay for inferior domestic goods, and the surgeon to destroy his career.\n\nSnapping these two building blocks together leads to a simple model of irrational conviction. If agents care about both material wealth and irrational beliefs, then as the price of casting reason aside rises, agents consume less irrationality.³⁵ I might like to hold comforting beliefs across the board, but it costs too much. Living in a Pollyanna dreamworld would stop me from coping with my problems, like that dead tree in my backyard that looks like it is going to fall on my house.\n\nI refer to this approach as rational irrationality to emphasize both its kinship with and divergence from, rational ignorance.³⁶ Both treat cognitive inadequacy as a choice, responsive to incentives. The difference is that rational ignorance assumes that people tire of the search for truth, while rational irrationality says that people actively avoid the truth.³⁷\n\nRational irrationality implies that people have “demand for irrationality” curves (fig. 5.2). As usual, quantity is on the x-axis and price on the y-axis, but with an interpretive twist. The “quantity” is a degree of irrationality—the magnitude of the agent’s departure from the unbiased, rational belief. To consume zero irrationality is to be fully rational. The “price of irrationality” is the amount of wealth an agent implicitly sacrifices by consuming another unit of irrationality.³⁸\n\nEconomic theory says little about the shape of demand curves.³⁹ As the price of irrationality falls, quantity demanded rises. But demand for irrationality (fig. 5.3) could be relatively flat—like D₁—with a small increase in price leading to a large reduction in quantity, or relatively steep—like D₂—requiring large price increases to curtail consumption. Demand could in fact be a vertical line overlapping the y-axis, indicating an agent who has no desire to be irrational at any price. I call this a neoclassical demand-for-irrationality curve because it is the assumption that most economists adopt by default (fig. 5.4).\n\n[f0124-01]\n\nFigure 5.3 Varying Price-Sensitivity of Demand for Irrationality\n\nOne interesting prediction of rational irrationality is that fluctuating incentives make people bounce between contradictory viewpoints.⁴⁰ As a consumer, for instance, the protectionist usually casts bad economic theory aside. Suddenly, products’ price and quality become more important, and national origin is lucky to have any influence. Similarly, most people reject the view that pushing up wages increases unemployment. When I teach intro econ, linking unemployment and excessive wages frequently elicits not only students’ disbelief, but anger: How could I be so callous? But irrationality about labor demand is selective. What happens when my outraged students reach the “Salary Requirements” line on job applications? They could ask for a million dollars a year, but they don’t. When their future rides on it, students honor the economic truism that labor demand slopes down.\n\nThe cynical explanation is that my students understood labor demand curves all along. But why would you get angry at a professor for saying what you believe yourself? They are more likely in denial. When they fill out the application, though, their standby rationality kicks in, telling them: “This is no time to get angry.” It does not take an A student to reflect: “I do not want to lowball it, but I am an entry-level worker, and the only way I am going to land a job is by asking for an entry-level salary. The more I ask for, the less likely they are to hire me.”\n\n[f0125-01]\n\nFigure 5.4 Neoclassical Demand for Irrationality\n\nPsychological Plausibility\n\nThe bulk of available evidence suggests that people in all\nsocieties tend to be relatively rational when it comes to\nthe beliefs and practices that directly involve their\nsubsistence. . . . The more remote these beliefs and\npractices are from subsistence activities, the more likely\nthey are to involve nonrational characteristics.\n\n—Robert Edgerton, Sick Societies⁴¹\n\nArguably the main reason why economists have not long since adopted an approach like mine is that it seems psychologically implausible.⁴² Rational irrationality appears to map an odd route to delusion:\n\nStep 1: Figure out the truth to the best of your ability.\n\nStep 2: Weigh the psychological benefits of rejecting the truth against its material costs.\n\nStep 3: If the psychological benefits outweigh the material costs, purge the truth from your mind and embrace error.\n\nThe psychological plausibility of this stilted story is underrated. It coheres well with George Orwell’s chilling account of “doublethink” in 1984:\n\nDoublethink means the power of holding two contradictory beliefs in one’s mind simultaneously, and accepting both of them. The Party intellectual knows in which direction his memories must be altered; he therefore knows he is playing tricks with reality; but by the exercise of doublethink he also satisfies himself that reality is not violated. The process has to be conscious, or it would not be carried out with sufficient precision, but it also has to be unconscious, or it would bring with it a feeling of falsity and hence of guilt. . . . Even in using the word doublethink it is necessary to exercise doublethink. For by using the word one admits that one is tampering with reality; by a fresh act of doublethink one erases this knowledge; and so on indefinitely, with the lie always one step ahead of the truth.⁴³\n\nBut rational irrationality does not require Orwellian underpinnings. The psychological interpretation can be seriously toned down without changing the model. Above all, the steps should be conceived as tacit. To get in your car and drive away entails a long series of steps—take out your keys, unlock and open the door, sit down, put the key in the ignition, and so on. The thought processes behind these steps are rarely explicit. Yet we know the steps on some level, because when we observe a would-be driver who fails to take one—by, say, trying to open a locked door without using his key—it is easy to state which step he skipped.\n\nOnce we recognize that cognitive “steps” are usually tacit, we can enhance the introspective credibility of the steps themselves. The process of irrationality can be recast:\n\nStep 1: Be rational on topics where you have no emotional attachment to a particular answer.\n\nStep 2: On topics where you have an emotional attachment to a particular answer, keep a “lookout” for questions where false beliefs imply a substantial material cost for you.\n\nStep 3: If you pay no substantial material costs of error, go with the flow; believe whatever makes you feel best.\n\nStep 4: If there are substantial material costs of error, raise your level of intellectual self-discipline in order to become more objective. Step 5: Balance the emotional trauma of heightened objectivity—the progressive shattering of your comforting illusions—against the material costs of error.\n\nThere is no need to posit that people start with a clear perception of the truth, then throw it away. The only requirement is that rationality remain on “standby,” ready to engage when error is dangerous.\n\nWhat does this mean in practice? To help convince readers of the psychological plausibility of rational irrationality, this section illustrates my thesis using case studies from a wide variety of fields. Obviously, a series of examples will not prove me correct. The point, rather, is to get readers to look at different fact patterns, and see what the lens of rational irrationality brings into focus.\n\nNudity and the Jains. John Noss’s comparative religion textbook, Man’s Religions, summarizes an amusing doctrinal dispute between two branches of the Jain religion:\n\nEarly in the history of the faith the Jains divided on the question of wearing clothes. The Shvetambaras or the “white-clad” were the liberals who took their stand on wearing at least one garment, whereas the stricter and more conservative Digambaras got their name from their insistence on going about, whenever religious duty demanded it, “clad in atmosphere.” Mahavira [the last of the founding prophets of Jainism] did not wear clothes, they pointed out, so why, when there is a religious reason for not wearing clothes, should they? The Shvetambaras were in the north and yielded a bit both to the cold winds and to the social and cultural influences of the Ganges River plain. The Digambaras, not looked at askance by the Dravidian residents of their southland, have more easily maintained the earlier, sterner attitudes down the years.⁴⁴\n\nHow could these suspiciously convenient doctrinal differences emerge? A plausible story: The default of members of both branches is to accept the teachings of their religion. But their beliefs about permissible clothing affect their bodily comfort—especially in colder climates. So northern Jains apply stricter intellectual scrutiny to their doctrines than southern Jains: “How do we really know that Mahavira wanted it this way?” The northerners are therefore less likely to accept their religion’s more extreme teachings.\n\nMosca and Jihad. In the Jain example, stubborn belief leads to discomfort. Gaetano Mosca presents a case where stubborn belief leads to death.\n\nMohammed, for instance, promises paradise to all who fall in a holy war. Now if every believer were to guide his conduct by that assurance in the Koran, every time a Mohammedan army found itself faced by unbelievers it ought either to conquer or to fall to the last man. It cannot be denied that a certain number of individuals do live up to the letter of the Prophet’s word, but as between defeat and death followed by eternal bliss, the majority of Mohammedans normally elect defeat.⁴⁵\n\nEconomists’ knee-jerk reading is that Mosca describes a Prisoners’ Dilemma. Soldiers who run away improve their own chances of survival at the expense of their compatriots; though widespread desertion ensures defeat of the group, deserters act in their individual interest. But this misses the heart of Mosca’s story. If a soldier believes that death in battle sends him to paradise, running away is imprudent, not cowardly. He is literally better off dead. As danger approaches, then, the Muslim warrior does not act more selfishly; he revises his beliefs about how to pursue his self-interest.\n\nRational irrationality makes sense of Mosca’s example. Muslim soldiers’ “default belief” is that their religion’s teachings are true. As long as they are at peace or militarily have the upper hand, the belief that Allah brings the fallen to paradise gives psychological comfort with little risk. When they are losing, however, soldiers’ “standby” rationality kicks in. The devil on their shoulders whispers: “What makes you think that paradise even exists?” Some would rather die than doubt. But, confronting the choice between fidelity and death, most quietly put on their thinking caps and abandon their fatal belief.\n\nThe reader may be tempted to throw the World Trade Center suicide attacks in his face, but Mosca does not forget heterogeneity. He presciently adds that “a certain number of individuals do live up to the letter of the Prophet’s word.” A handful of people climb Mount Everest in spite of risks that scare off the rest of the human race. A few Muslims sacrifice their lives for their faith, but a billion do not.⁴⁶\n\nSati. On some interpretations of Hinduism, a widow must join her deceased husband on his funeral pyre, a practice known as sati. Fulfilling this duty supposedly has great rewards in the afterlife. On the surface, sati looks like a clear case of persistent irrationality despite deadly incentives. But the reality, explains anthropologist Robert Edgerton, is different. Few Hindu widows ever complied with their putative duty: “Even in Bengal where sati was most common, only a small minority of widows—less than 10 percent—chose sati although the prospect of widowhood was dismal at best.”⁴⁷ Some of these were frankly murdered by their husband’s relatives. When the widow refused the pyre, she was not allowed to resume a normal life. She could not remarry, and had to spend the rest of her years in fasting and prayer. Overall, one of the world’s most shocking religious practices coheres well with rational irrationality:\n\nDespite the wretched conditions of widowhood, the promised rewards of sati, and the often relentless pressure exerted by the deceased husband’s relatives on the widow to choose their supreme act of devotion, the great majority of widows preferred to live.⁴⁸\n\nGenetics, relativity, and Stalin. Marxist philosophers have dogmatic objections to modern biology and physics. Genetics is “a bourgeois fabrication designed to undermine the true materialist theory of biological development,” and relativity theory and quantum mechanics are “idealist positions” that “contravene[d] the materialism espoused by Lenin in Materialism and Empirio-Criticism.”⁴⁹ But Marxist regimes—and Stalin in particular—treated biology and physics asymmetrically.\n\nIn biology, Stalin and other prominent Marxist leaders elevated the views of the quack antigeneticist Trofim Lysenko to state-supported orthodoxy, leading to the dismissal of thousands of geneticists and plant biologists.⁵⁰ Lysenkoism hurt Soviet agriculture, and helped trigger the deadliest famine in human history during China’s Great Leap Forward.⁵¹\n\nIn physics, on the other hand, leading scientists enjoyed more intellectual autonomy than any other segment of Soviet society. Internationally respected physicists ran the Soviet atomic project, not Marxist ideologues. When their rivals tried to copy Lysenko’s tactics, Stalin balked. A conference intended to start a witch hunt in Soviet physics was abruptly canceled, a decision that had to originate with Stalin. Holloway recounts a telling conversation between Beria, the political leader of the Soviet atomic project, and Kurchatov, its scientific leader:\n\nBeria asked Kurchatov whether it was true that quantum mechanics and relativity theory were idealist, in the sense of antimaterialist. Kurchatov replied that if relativity theory and quantum mechanics were rejected, the bomb would have to be rejected too. Beria was worried by this reply, and may have asked Stalin to call off the conference.⁵²\n\nThe “Lysenkoization” of Soviet physics never came.\n\nThe best explanation for the difference is that modern physics had a practical payoff that Stalin and other Communist leaders highly valued: nuclear weapons. “The Soviet Union wanted the bomb as soon as possible, and was prepared to pay virtually any price to obtain it.”⁵³ Lysenkoist biology, in contrast, injured the low-priority agricultural sector. Stalin had already presided over decades of hunger, and knew that it posed little threat to the Soviet state.\n\nMost of Stalin’s biographers view him as power-hungry but fairly sincere.⁵⁴ His default was to embrace the secular religion of Marxism-Leninism, but he retained a good helping of “standby” rationality. When he sensed that strict adherence to Leninist dogma put his power at risk, he set ideology aside:\n\nStalin was not so concerned about the condition of agriculture—he tolerated, after all, a desperate famine in the Ukraine in 1947—and so it may not have mattered very much to him whether Lysenko was a charlatan or not. The nuclear project was more important, however, than the lives of Soviet citizens, so it was crucial to be sure that the scientists in the nuclear project were not frauds.⁵⁵\n\nIndeed, not only did Stalin squelch philosophical attacks on modern physics; he also embraced other commonsensical “bourgeois” heresies to accelerate his atomic program. Soviet economic failures were routinely blamed not on inadequate resources, but on “Trotskyite wrecking” and other bizarre conspiracies. For the atomic project, though, Stalin recognized the realities of scarcity: “He told Kurchatov that ‘it was not worth engaging in small-scale work, but necessary to conduct the work broadly, with Russian scope, that in that connection the broadest all-round help would be provided. Comrade Stalin said it was not necessary to seek cheaper paths.’ ”⁵⁶\n\nSimilarly, in many other areas of the Soviet economy, Marxism fostered reluctance to motivate workers with material rewards for success. In the atomic project, however, Stalin dumped Marxist dogma in favor of bourgeois horse sense:\n\nStalin said also that he was anxious to improve the scientists’ living conditions, and to provide prizes for major achievements—“for example, for the solution of our problem,” Kurchatov wrote. Stalin “said that our scientists were very modest and they sometimes did not notice that they live poorly . . . our state has suffered very much, yet it is surely possible to ensure that several thousand people can live very well, and several thousand people better than very well, with their own dachas, so that they can relax, and with their own cars.”⁵⁷\n\nHe kept his promises, tripling the science budget, giving scientists large pay raises in 1946, and dachas and cars to the leading nuclear scientists after the successful nuclear test in 1949.⁵⁸\n\nMaybe Stalin covertly scoffed at the inanities of Marxism, but a more plausible interpretation is that he was rationally irrational. Marxism-Leninism was important to his sense of identity, but his preference was not absolute. As the price of illusion went up, he chose to be less fanatical and more objective.\n\nWant to bet? We encounter the price-sensitivity of irrationality whenever someone unexpectedly offers us a bet based on our professed beliefs.⁵⁹ Suppose you insist that poverty in the Third World is sure to get worse in the next decade. A challenger immediately retorts, “Want to bet? If you’re really ‘sure,’ you won’t mind giving me ten-to-one odds.” Why are you are unlikely to accept this wager? Perhaps you never believed your own words; your statements were poetry—or lies. But it is implausible to tar all reluctance to bet with insincerity. People often believe that their assertions are true until you make them “put up or shut up.” A bet moderates their views—that is, changes their minds—whether or not they retract their words.⁶⁰\n\nHow does this process work? Your default is to believe what makes you feel best. But an offer to bet triggers standby rationality. Two facts then come into focus. First, being wrong endangers your net worth. Second, your belief received little scrutiny before it was adopted. Now you have to ask yourself which is worse: Financial loss in a bet, or psychological loss of self-worth? A few prefer financial loss, but most covertly rethink their views. Almost no one “bets the farm” even if—pre-wager—he felt sure.\n\nRational Irrationality and Politics\n\nMerchants eagerly grasp all philosophic generalizations\npresented to them without looking closely into them, and\nthe same is true about politics, science, and the arts. But\nonly after examination will they accept those concerning\ntrade, and even then they do so with reserve.\n\n—Alexis de Tocqueville, Democracy in America⁶¹\n\nSuppose a referendum determines whether we have policy A or policy B. A is $10,000 better for you. What is the material cost of believing the opposite and voting accordingly? The naive answer of $10,000 is wrong unless your vote is “decisive”; that is, if it reverses or flips the electoral outcome. This is possible only if the choices of all other voters exactly balance. Thus, in elections with millions of voters, the probability that your erroneous policy beliefs cause unwanted policies is approximately zero.⁶² The infamous Florida recounts of 2000 do not undermine this analysis.⁶³ Losing by a few hundred votes is a far cry from losing by one vote.\n\nCritics of polling say it hurts democracy. The leading complaint is that polls provide no incentive to seriously weigh policy consequences.⁶⁴ Unlike elections, polls do not change policy, right? Wrong. Politicians frequently take action based on polls, and your response might push them over the edge. Survey respondents have about as much—or as little—incentive to think seriously as voters do. Indeed, elections are surveys. Responses to both are cheap talk bundled with a remote chance of swaying policy.\n\nIf you listen to your fellow citizens, you get the impression that they disagree. How many times have you heard, “Every vote matters”? But people are less credulous than they sound. The infamous poll tax—which restricted the vote to those willing to pay for it—provides a clean illustration. If individuals acted on the belief that one vote makes a big difference, they would be willing to pay a lot to participate. Few are. Historically, poll taxes significantly reduced turnout.⁶⁵ There is little reason to think that matters are different today. Imagine setting a poll tax to reduce presidential turnout from 50% to 5%. How high would it have to be? A couple hundred dollars? What makes the poll tax alarming is that most of us subconsciously know that most of us subconsciously know that one vote does not count.\n\nCitizens often talk as if they personally have power over electoral outcomes. They deliberate about their options as if they were ordering dinner. But their actions tell a different tale: They expect to be served the same meal no matter what they “order.”\n\nWhat does this imply about the material price a voter pays for political irrationality? Let D be the difference between a voter’s willingness to pay for policy A instead of policy B. Then the expected cost of voting the wrong way is not D, but the probability of decisiveness p times D. If p = 0, pD = 0 as well. Intuitively, if one vote cannot change policy outcomes, the price of irrationality is zero.\n\nThis zero makes rational irrationality a politically pregnant idea. The institutional structure of democracy makes political irrationality a free good for its ultimate decision-makers, the electorate.⁶⁶ So we should expect voters to be on their worst cognitive behavior; in the words of Le Bon, to “display in particular but slight aptitude for reasoning, the absence of the critical spirit, irritability, credulity, and simplicity.”⁶⁷\n\nA diner at an all-you-can-eat buffet stuffs himself until he cannot bear another bite. In economic jargon, he consumes up to his “satiation point,” where his demand curve and the x-axis intersect (fig. 5.5). Voter irrationality works the same way. Since delusional political beliefs are free, the voter consumes until he reaches his “satiation point,” believing whatever makes him feel best. When a person puts on his voting hat, he does not have to give up practical efficacy in exchange for self-image, because he has no practical efficacy to give up in the first place.\n\nConsider how the typical person forms beliefs about the deterrent effect of the death penalty. Ordinary intellectual self-discipline requires you to look at the evidence before you form a strong opinion. In practice, though, most people with definite views on the effectiveness of the death penalty never feel the need to examine the extensive empirical literature. Instead, they start with strong emotions about the death penalty, and heatedly “infer” its effect.⁶⁸\n\n[f0133-01]\n\nFigure 5.5 Voters’ Demand for Irrationality\n\nThe death penalty is an unusually emotional issue, but its template fits most politically relevant beliefs. How many people can take sides in a military conflict and still have the detachment of George Orwell?\n\nI have little direct evidence about the atrocities in the Spanish civil war. I know that some were committed by the Republicans, and far more (they are still continuing) by the Fascists. But what impressed me then, and has impressed me ever since, is that atrocities are believed in or disbelieved in solely on grounds of political predilection. Everyone believes in the atrocities of the enemy and disbelieves in those of his own side, without ever bothering to examine the evidence.⁶⁹\n\nThe same people who practice intellectual self-discipline when they figure out how to commute to work, repair a car, buy a house, or land a job “let themselves go” when they contemplate the effects of protectionism, gun control, or pharmaceutical regulation. Who ever made an enemy by contradicting someone’s belief about what is wrong with his car? For practical questions, standard procedure is to acquire evidence before you form a strong opinion, match your confidence to the quality and quantity of your evidence, and remain open to criticism. For political questions, we routinely override these procedural safeguards.\n\nThe contrast between markets and politics is sharpest when voters have what I call near-neoclassical demand for irrationality.⁷⁰ Under normal market conditions, an agent with these preferences appears fully rational. He is willing and able to live without irrationality. Under normal political conditions, however, he pulls off the mask of objectivity. His reasonableness in one sphere fails to carry over to the other; or to be more precise, he chooses not to carry it over because the market has a “user fee” for irrationality, and democracy does not.\n\n[f0134-01]\n\nFigure 5.6 Near-Neoclassical Demand for Irrationality\n\nWhen Joseph Schumpeter compares rationality in politics and the market, he seems to have near-neoclassical demand for irrationality in mind.⁷¹ Alongside his famous complaints about voters’ illogic in Capitalism, Socialism, and Democracy, Schumpeter affirms that “Neither the intention to act as rationally as possible nor the steady pressure toward rationality can seriously be called into question at whatever level of industrial or commercial activity we choose to look.”⁷² He adds:\n\nAnd so it is with most of the decisions of daily life that lie within the little field which the individual citizen’s mind encompasses with a full sense of its reality. Roughly, it consists of the things that directly concern himself, his family, his business dealings, his hobbies, his friends and enemies, his township or ward, his class, church, trade union or any other social group of which he is an active member—the things under his personal observation, the things which are familiar to him independently of what his newspaper tells him, which he can directly influence or manage and for which he develops the kind of responsibility that is induced by a direct relation to the favorable or unfavorable effects of a course of action.⁷³\n\nBastiat similarly states that make-work bias has zero effect on private action:\n\nNo one has ever seen, and no one will ever see, any person who works, whether he be farmer, manufacturer, merchant, artisan, soldier, writer, or scholar, who does not devote all the powers of his mind to working better, more quickly, and more economically—in short, to doing more with less.⁷⁴\n\nWhether or not Schumpeter and Bastiat are right, the near-neoclassical demand curve is analytically useful. It is a microscopic departure from standard economic assumptions, so economists would have to be awfully dogmatic to rule it out.⁷⁵\n\nRational Irrationality and Experimental Evidence\n\nRational irrationality is a modest refinement of existing models of human behavior. Assuming that all people are fully rational all the time is bad economics. It makes more sense to assume that people tailor their degree of rationality to the costs of error.⁷⁶\n\nResearchers at the intersection of psychology and economics often take a more radical position: Not only are people irrational, but their irrationality stays the same or increases as its cost rises. The eminent Richard Thaler said so at the 2004 American Economic Association Meetings.⁷⁷ The abstract of a well-known survey article by Colin Camerer and Robin Hogarth on the experimental effects of financial incentives seems to back Thaler up:\n\nWe review 74 experiments with no, low, or high performance-based financial incentives. The modal result is no effect on mean performance (though variance is usually reduced by higher payment). . . . We also note that no replicated study has made rationality violations disappear purely by raising incentives.⁷⁸\n\nOn closer reading, however, Camerer and Hogarth reach a nuanced conclusion. First, they emphasize that experimental findings are heterogeneous. Incentives often improve performance on tasks of judgment and decision. People “spend” hypothetical money more freely than actual money; they are much more likely to say they will buy something than to actually do so.⁷⁹ Incentives also lead subjects away from “favorable self-presentation behavior toward more realistic choices.”⁸⁰ Furthermore, a recent paper finds that people get less overconfident when they have to bet real money on their beliefs.⁸¹\n\nSecond, and more importantly, Camerer and Hogarth recognize experiments’ limitations:\n\nOur view is that experiments measure only short-run effects, essentially holding capital fixed. The fact that incentives often do not induce different (or better) performance in the lab may understate the effect of incentives in natural settings, particularly if agents faced with incentive changes have a chance to build up capital—take classes, seek advice, or practice.⁸²\n\nThink about any skilled worker. Would he have his specialized knowledge if there were no market demand for what he does? To answer no is to admit that incentives massively improve human judgment in the real world. It just takes time for incentives to work their magic. Camerer and Hogarth concur: “Useful cognitive capital probably builds up slowly, over days of mental fermentation or years of education rather than in the short-run of an experiment (1–3 hours) . . . [I]ncentives surely do play an important role in inducing long-run capital formation.”⁸³ This claim is consistent with the growing literature on field experiments: Economic actors in their “natural habitat” look considerably more rational than they do in the lab.⁸⁴\n\nCamerer and Hogarth also admit that experiments slight the power of incentives by relying on volunteers, whose “intrinsic motivation”—desire to do well for its own sake—is unusually high.⁸⁵ Money cannot spur greater effort in those who are already trying their best. A related point that Camerer and Hogarth do not make is that most experiments avoid touchy subjects like religion and politics, where participants have “intrinsic motivation” to reach incorrect answers. Once there is a trade-off between psychological and material well-being, incentives have more room to operate.\n\nA common summary of the experimental literature is that incentives improve performance on easy problems but hurt performance on hard problems.⁸⁶ As Einhorn and Hogarth argue:\n\nPerformance . . . depends on both cognition and motivation. Thus, if incentive size can be thought of as analogous to the speed with which one travels in a given direction, cognition determines the direction. Therefore, if incentives are high but cognition is faulty, one gets to the wrong place faster.⁸⁷\n\nWhat Camerer and Hogarth highlight, however, is that the difficulty of a problem falls if you have more time and flexibility to solve it. Hard problems naturally decay into easier problems. Once they are easy enough, incentives work like they are “supposed to.”\n\nThe moral is that we should take experimental evidence seriously, but not be intimidated when experimentalists announce that “there is little or no experimental evidence that stronger incentives make people more rational.” As Camerer and Hogarth observe, few experiments on human beings last more than a few hours. It would be too expensive to continue for days or years. If rationality gradually responds to incentives, existing experiments will not detect it.\n\nFortunately, experiments are not our only information. Everyday experience is relevant. The typical person faces both practical questions—doing his job, buying groceries, or driving—and impractical ones—like politics and religion. It is hard to deny that both intellectual effort and accuracy are much higher for practical questions. How many people believe they can catch bullets in their teeth—or fly without mechanical assistance? Furthermore, when previously impractical questions suddenly become practical—perhaps due to a change in occupation—intellectual effort plainly rises, and accuracy eventually along with it. In a world without water, there would be no demand for ships, so few would know how to design and build them. To me, these are ubiquitous facts; I leave it to readers to judge whether they agree.\n\nEven if we trust only experimental evidence, rational irrationality is a credible explanation for the public’s biased beliefs about economics. Experimentalists admit that incentives help for relatively easy questions. Antimarket, antiforeign, make-work, and pessimistic bias all qualify. These are not subtle errors, but knee-jerk reactions. In non-political contexts, people routinely overcome them. How many refrain from buying appliances because it “destroys jobs”? Experimentalists also emphasize that incentives help less when there is intrinsic motivation to get things right. In economics, there is intrinsic motivation to get things wrong. If you think the right answer, you feel insensitive and unpatriotic; if you say the right answer, you feel like a pariah. There is about as much intrinsic motivation to understand economics as there is to take out the garbage.\n\nRational Irrationality and Expressive Voting\n\nMy work owes a great deal to Geoffrey Brennan and Loren Lomasky’s expressive voting model, best articulated in their Democracy and Decision: The Pure Theory of Electoral Preference.⁸⁸ Though complementary, our accounts differ in several key respects.\n\nSince the work of Brennan and Lomasky has enjoyed less attention than it deserves, let me begin with a summary. Nearly all economists assume that people vote instrumentally; that is, they vote to get the policies they prefer. What else would they do?\n\nBrennan and Lomasky point to the expressive function of voting. Fans at a football game cheer not to help the home team win, but to express their loyalty. Similarly, citizens might vote not to help policies win, but to express their patriotism, their compassion, or their devotion to the environment. This is not hair-splitting. One implication is that inefficient policies like tariffs or the minimum wage might win because expressing support for them makes people feel good about themselves.\n\nThe same holds to some degree for consumer products. Even if generic perfume smelled as good as Calvin Klein, some shoppers would pay extra for the glamorous image of the name brand. In politics, though, Brennan and Lomasky point out that voters’ low probability of decisiveness drastically distorts the trade-off. If your vote does not change the outcome, you can safely vote for “feel good” policies even if you know they will be disastrous in practice.\n\nCase in point: When economists analyze discrimination, they emphasize the financial burden of being a bigot.⁸⁹ In politics, the social cost of prejudice remains, but the private cost vanishes due to voters’ low probability of decisiveness:\n\nThe bigot who refuses to serve blacks in his shop foregoes the profit he might have made from their custom; the anti-Semite who will not work with Jews is constrained in his choice of jobs and may well have to knock back one she would otherwise have accepted. To express such antipathy at the ballot box involves neither threat of retaliation nor any significant personal cost.⁹⁰\n\nBrennan and Lomasky do not merely draw the moderate conclusion that political decisions, like market decisions, depend on expressive as well as instrumental concerns. Their conclusion is instead the radical one that—unlike market decisions—political decisions depend primarily on expressive concerns:\n\nPrivate interests in the electoral context will be heavily muted and the purely expressive or symbolic greatly magnified. This is simply a matter of relative prices. We should, moreover, emphasize that the relative price change at stake is of an order of magnitude that is enormous in comparison with those with which economists normally deal.⁹¹\n\nThe parallels with rational irrationality are clear. Both views focus on the psychological benefits voters enjoy, not their microscopic effect on policy. Both argue that voters’ low probability of decisiveness bifurcates economic and political behavior; as Brennan and Lomasky put it, “Considerations dormant in market behavior become significant in the polling booth.”⁹² Both explain how ineffective and counterproductive policies can be politically popular.\n\nThe key difference is the mechanism. In expressive voting theory, voters know that feel-good policies are ineffective. Expressive voters do not embrace dubious or absurd beliefs about the world. They simply care more about how policies sound than how they work. The expressive protectionist thinks: “Sure, protectionism makes Americans poorer. But who cares, as long as I can wave the flag and chant ‘U.S.A.! U.S.A.!’ ” In contrast, rationally irrational voters believe that feel-good policies work. The rationally irrational protectionist genuinely holds that protectionism makes Americans richer. If he must deny comparative advantage, so be it.\n\nTo repeat, expressive voting and rational irrationality are not mutually exclusive. A person might simultaneously think, “Protectionism leads to prosperity” and, “I do not care if protectionism leads to prosperity.” But in most cases, the rational irrationality account is more credible. False descriptive views usually accompany support for feel good policies. Few protectionists see their policies as economically harmful.⁹³ If they realistically assessed the effect of this “feel-good” policy, supporting the policy would no longer make its friends feel good.\n\nThe best way to illustrate the contrast between the two approaches is with one of Brennan and Lomasky’s own examples. Suppose an electorate chooses between a cataclysmic war with honor, or peace and prosperity with dishonor. The majority pragmatically prefers the latter: “Just as individuals, in situations of interpersonal strain, will often swallow their pride, shrug their shoulders, and stroll off rather than commit to an all-out fight (particularly one that might imply someone’s death), so the interests of most voters would be better served by drawing back from the belligerent course.”⁹⁴ But by the logic of expressive voting, a war referendum could easily prevail. “Individual voters may, each of them, be entirely rational in voting for war—even where no one of them would, if decisive, take that course.”⁹⁵\n\nBrennan and Lomasky’s story is logically possible. But unless we relax the rationality assumption, it comes off as odd. How many vocal hawks would admit to themselves that war leads to devastation and appeasement to prosperity? They would more likely insist, against all evidence, “The boys will be out of the trenches by Christmas”—and add that no matter how bad war looks, appeasement is the true threat to our well-being. And most of the people who took this position would sincerely believe it! Consider this famous scene from Gone with the Wind:⁹⁶\n\nMR. O’HARA: The situation is very simple. The Yankees can’t fight and we can.\n\nCHORUS: You’re right!\n\nMAN: There won’t even be a battle, that’s what I think! They’ll just turn and run every time.\n\nMAN: One Southerner can lick twenty Yankees.\n\nMAN: We’ll finish them in one battle. Gentlemen can always fight better than rabble.\n\nRhett Butler enrages the crowd by taking the contrary position:\n\nRHETT BUTLER: I think it’s hard winning a war with words, gentlemen.\n\nCHARLES: What do you mean, sir?\n\nRHETT: I mean, Mr. Hamilton, there’s not a cannon factory in the whole South.\n\nMAN: What difference does that make, sir, to a gentleman?\n\nRHETT: I’m afraid it’s going to make a great deal of difference to a great many gentlemen, sir.\n\nCHARLES: Are you hinting, Mr. Butler, that the Yankees can lick us?\n\nRHETT: No, I’m not hinting. I’m saying very plainly that the Yankees are better equipped than we. They’ve got factories, shipyards, coal mines . . . and a fleet to bottle up our harbors and starve us to death. All we’ve got is cotton, and slaves and . . . arrogance.\n\nMAN: That’s treacherous!\n\nCHARLES: I refuse to listen to any renegade talk!\n\nRHETT: I’m sorry if the truth offends you.\n\nThe Southerners are not pretending to overestimate their military strength. They really do overestimate it. If they had as accurate an assessment of their side’s military prospects as Rhett Butler, their war fervor would be hard to sustain. The lesson: Support for counterproductive policies and mistaken beliefs about how the world works normally come as a package. Rational irrationality emphasizes this link; expressive voting theory—despite its strengths—neglects it.\n\nConclusion\n\nRational irrationality does not imply that political views are invariably senseless. You will not gorge on all-you-can-eat pizza if you hate Italian food. But rational irrationality does put political beliefs under suspicion—and yes, that includes mine.\n\nDemocracy asks voters to make choices, but gives each only an infinitesimal influence. From the standpoint of the lone voter, what happens is independent of his choice. Practically every economist admits this. But after their admission, most economists minimize the broader implications.⁹⁷\n\nI take the opposite approach: Voters’ lack of decisiveness changes everything. Voting is not a slight variation on shopping. Shoppers have incentives to be rational. Voters do not. The naive view of democracy, which paints it as a public forum for solving social problems, ignores more than a few frictions. It overlooks the big story inches beneath the surface. When voters talk about solving social problems, they primary aim is to boost their self-worth by casting off the workaday shackles of objectivity.\n\nMany escape my conclusion by redefining the word rational. If silly beliefs make you feel better, maybe the stickler for objectivity is the real fool. But this is why the term rational irrationality is apt: Beliefs that are irrational from the standpoint of truth-seeking are rational from the standpoint of individual utility maximization. More importantly—whatever words you prefer—a world where voters are happily foolish is unlike one where they are calmly logical. We shall soon see how.\n\nPolitical behavior seems weird because the incentives that voters face are weird. Economists have often been criticized for evading the differences between political and market behavior.⁹⁸ But this is a failure of economists rather than a failure of economics. Economists should never have expected political behavior to parallel market behavior in the first place. Irrationality in politics is not a puzzle. It is precisely what an economic theory of irrationality predicts.\n\nChapter 6\n\nFROM IRRATIONALITY TO POLICY\n\nA jaded old statehouse reporter noticed my astonishment\nand offered some perspective on the unruly behavior of\nthe elected representatives. “If you think these guys are\nbad,” he said, “you should see their constituents.”\n\n—William Greider, Who Will Tell the People?¹\n\nIRRATIONAL VOTERS open up novel ways for democracy to fail—counterintuitive to economists, but perhaps common sense to others. For example:\n\n• People might blame all their troubles on harmless scapegoats, and rally to politicians who persecute them.²\n\n• Irrational voters could “kill the messenger” of bad news, giving politicians an incentive to paper over problems instead of facing them. Histories of the savings-and-loan bailouts often appeal to this mechanism.³\n\n• Citizens of a wealthy, well-fed nation may vote for a candidate who warns of imminent starvation unless the Fatherland acquires more Lebensraum.⁴\n\nThere are parallels with a classic philosophical paradox.⁵ Recall the story of Oedipus. Oedipus wanted to marry Jocasta. Jocasta was Oedipus’ mother. But Oedipus did not want to marry his mother: He put out his own eyes when he found he had. Similarly: The median voter wants protection. Protection makes the median voter worse off. But the median voter does not want to be worse off. The efforts of both Oedipus and the median voter backfire due to their false beliefs. For Oedipus, the false belief is that Jocasta is not his mother; for the median voter, the false belief is that protectionism is good for the economy.\n\nEconomists have spent more time criticizing the public’s misconceptions than precisely explaining how they cause bad policies. They take the connection largely for granted. For Böhm-Bawerk, bad policies virtually imply public confusion: “The legal prohibitions of interest may, of course, be taken as evidence of a strong and widespread conviction that the taking of interest was, as a practical thing, to be condemned. . . .”⁶ Donald Wittman himself casually grants that\n\nA model that assumes that voters or consumers are constantly fooled and there are no entrepreneurs to clear up their confusion will, not surprisingly, predict that the decision-making process will lead to inefficient results.⁷\n\nBöhm-Bawerk and Wittman are too hasty. In theory, it is conceivable that the public’s biases spin the wheels of democracy with little effect on policy.⁸ In a variant of the Miracle of Aggregation, different delusions might mutually annihilate. Maybe each voter who overestimates the social benefits of protectionism also overestimates his ability to thrive under free trade. With selfish voters, free trade would still prevail, enriching a population convinced that “free trade hurts everyone but me.”\n\nThe purpose of this chapter is to move from the microfoundations of individual voter irrationality to the macro outcome of democratic policy. I proceed in the economist’s usual way: Start with a simple case, then gradually complicate it. The method is pedantic, but works better than any other. As Paul Krugman amusingly begins his essay “The Accidental Theorist”:\n\nImagine an economy that produces only two things: hot dogs and buns. Consumers in this economy insist that every hot dog come with a bun, and vice versa. And labor is the only input to production.\n\nOK, time out. Before we go any further, I need to ask what you think of an essay that begins this way. Does it sound silly to you?⁹\n\nKrugman retorts:\n\nOne of the points of this essay is to illustrate a paradox: You can’t do serious economics unless you are willing to be playful. Economics . . . is a menagerie of thought experiments—parables, if you like—that are intended to capture the logic of economic processes in a simplified way. In the end, of course, ideas must be tested against the facts. But even to know what facts are relevant, you must play with those ideas in hypothetical settings.¹⁰\n\nBecause the real world is tricky, I start with a thought experiment that has a transparent link between irrational beliefs and inefficient policy outcomes. I then progressively add empirically relevant complications, which usually leave the connection between irrational public opinion and inefficient public policy intact.¹¹ Finally, in answer to the question, “Given public opinion, why isn’t democracy far worse than it is?,” I discuss forces that dilute the policy fallout of voter irrationality.\n\nThought Experiment 1: Irrationality with Identical Voters\n\nDemocracy aggregates preferences. Members of a group want things done. Democracy combines their wants and stirs to get a group decision. This process is terribly confusing because humans almost never completely agree. So what happens? On every issue, democracy must either impose a compromise, or favor one side over its rivals—which is another way of answering, “Who knows?”\n\nIn order to demystify democracy, we need to start small. Since ubiquitous disagreement makes the waters of democracy murky, let us temporarily forget about it. For the sake of argument, ask yourself: How would democracy work in the absence of disagreement?¹² How would democracy respond to a unanimous public demand? To be more precise, assume the following:\n\n1. All voters have the same preferences and endowments.¹³\n\n2. Two politicians compete for voter support by taking positions on a single issue.\n\n3. People vote for the politician whose position is closer to their own. If both politicians take the same position, they flip a coin.\n\n4. Politicians care only about winning, not how they play the game.\n\n5. The politician with more votes wins the election and implements his promised position.\n\nWhat happens? The politician closer to everyone’s first choice captures 100% of the votes. Since both politicians want to prevail but only one can, they race to match the electorate’s preferences, until both adopt the voters’ most-preferred position. Voters get their first choice, and politicians settle for a fifty-fifty shot of holding office.\n\nThis democracy seems above reproach. Every voter gets his first choice. How many political decisions in the real world can claim half as much? It is easy to fault the outcome, however, if voters share a taste for a relevant form of irrationality.\n\nSuppose the tariff rate is at issue. The conceivable positions range from complete free trade—0% tariff—to absolute embargo—infinite tariff. Since voters are identical, class conflict cannot be a motive for protectionism. If each member of the electorate votes for the policy most in his material self-interest, indifferent to the fate of everyone else, the Law of Comparative Advantage tells us that the unanimous first pick is a 0% tariff.¹⁴\n\n[f0145-01]\n\nFigure 6.1 Electoral Impact of Irrationality with Identical Voters\nTop: Distribution of Beliefs on Welfare-Maximizing Level of Protection\nBottom: Most-Preferred Degree of Protectionism\n\nBut what if one of the voters’ shared preferences is a mild fondness for antiforeign bias? To be more concrete, what happens if voters want to believe that the best tariff rate for people like themselves (i.e., everyone!) is not 0%, but 100%?\n\nAn inkling of this desire turns the election upside down. A 100% tariff could reduce per capita income by $10,000 a person, and each person could put a $1 value on fealty to antiforeign bias. As long as the probability of voter decisiveness is under 1 in 10,000, each voter sticks to his belief in the glory of the 100% tariff.¹⁵ Voters unanimously prefer a protectionist over a free-trader, so rival politicians scramble to endorse the public’s ideal. The 100% tariff wins hands down, inflicting a net loss of $9,999 per capita. A mild taste for psychological well-being precipitates a massive reduction in material well-being.\n\nIf individuals get a sense of meaning and identity from their worldview, cost-benefit analysis counts it a benefit. Nevertheless, because voters are not decisive, the social cost of irrationality exceeds its benefit. Think of it this way. Irrationality makes society as a whole better off as long as the psychological benefits minus the material costs are positive:\n\nPsychological Benefits – Material Costs > 0.\n\nIrrationality makes the individual better off under far less stringent conditions:\n\nPsychological Benefits – p \\* Material Costs > 0.\n\nwhere p is the probability of casting the decisive vote. If p = 0, irrationality is utility-maximizing as long as there are any psychological benefits:\n\nPsychological Benefits > 0\n\nThe implications are especially stark if voters have what the last chapter dubbed near-neoclassical demand for irrationality (fig. 5.6). Under this assumption, Psychological Benefits—the area under the demand for irrationality curve—are negligible. Unless the Material Costs of acting on irrational beliefs are negligible, too, heeding irrational beliefs always makes society worse off. Yet everyone chooses to be irrational, because the private benefits ever so slightly exceed zero. With identical preferences, lots of voters, and near-neoclassical demand for irrationality, acting on irrational beliefs is invariably a bad idea for society, but society obeys these irrational beliefs without fail—indeed, without dissent.\n\nWith identical voters, most of the biases from the SAEE readily map into foolish policies. Antimarket bias boosts price controls and shortsighted redistribution. Antiforeign bias pushes for protectionism and immigration restrictions, and against trade agreements. Make-work bias recommends labor market regulation to “save jobs.” The policy ramifications of pessimistic bias are less clear, but it is a catalyst for all sorts of ill-conceived crusades and scapegoating.¹⁶ In this simple thought experiment, fallacies remain harmless primarily if they are irrelevant.\n\nWhat about relevant errors that mutually cancel, leaving no net effect of irrationality on policy? Even if voters are identical, this cannot be completely ruled out. Especially on issues that engage the emotions, however, it seems more common for errors to compound, not cancel. When you dislike someone, you tend to see all his actions through a negative filter. When you dislike imports, similarly, it is only natural to overestimate the economic harm of imports, their quantity, the number of jobs they destroy, and the “unfairness” of other countries’ trade policies.\n\nAs beliefs rise to higher levels of generality, the link between error and poor policy tightens. If voters underestimate the benefit of trade with Japan, maybe this is balanced by their overestimate of the benefit of trade with Great Britain, leaving the tariff rate at its optimal level.¹⁷ But if voters underestimate the benefit of foreign trade in general—as they empirically do—what countervailing beliefs are left to undo the damage?\n\nThought Experiment 2: Irrationality with Belief Heterogeneity\n\nIn the real world, unanimity is an unmistakable sign of dictatorship, not democracy. An empirically relevant model of democracy must allow for disagreement. To get it takes only a small twist on the first thought experiment. Keep assumptions 2–5, but change assumption 1 to assumption 1′:\n\n1′. All voters have the same endowments. All voters have the same preferences with one exception: their preference over beliefs.\n\nSince endowments remain the same, there is still no room for class conflict. The disagreement that emerges is ideological. The near-clones have diverse tastes over beliefs, and therefore choose to see the political world differently.\n\nReturning to the trade policy example: voters no longer unanimously prefer to believe that a tariff of 100% is optimal. Some feel the right tax rate is 110%, or 200%. Others say 0%. What are politicians to do? Whatever stance they adopt, they make enemies. Fortunately, all the winner needs is a majority.\n\nSince citizens vote for the politician closer to them, and both politicians want to win, thought experiment 2 has a simple outcome: Both politicians adopt the position of the median voter, offering a tariff rate that half the electorate sees as too high, and half sees as too low.¹⁸ The only novelty: Since conflicting beliefs are the source of voter disagreement, executing the wishes of the median voter is equivalent to acting as if the median belief about the optimal tariff were true.\n\nIf the Miracle of Aggregation holds, then the median belief is true. There is no cause for alarm. Democracy listens to those who are “in the know” and ignores the deluded fanatics. Unfortunately, the Miracle of Aggregation is a hoax. It is both theoretically possible and empirically typical for the median voter to be one of the deluded fanatics, albeit a relatively moderate one.\n\n[f0148-01]\n\nFigure 6.2 Electoral Impact of Irrationality of Otherwise Identical Voters with Heterogeneous Beliefs\nTop: Distribution of Beliefs on Welfare-Maximizing Level of Protection\nBottom: Distribution of Most-Preferred Platforms on Protection\n\nA Necessary Digression on the Self-Interested Voter Hypothesis\n\nThe link between irrationality and policy is plain in highly stylized thought experiments. But relaxing more assumptions seems to make matters painfully intractable. If voters have different endowments, then many may objectively benefit from socially harmful policies. Inequality of wealth is the simplest reason: Even if redistribution is an awfully leaky bucket, it might still enrich the majority.¹⁹ But inequality is only the beginning. The owner of a textile mill may be just as rich as the owner of a clothing store, but tariffs affect their interests oppositely. With so much complexity, perhaps the people who overestimate the social benefits of protection really do lose out because of foreign competition. They might be protectionists because they correctly judge its effect on their personal well-being, not because they overestimate its effect on national well-being.\n\nIf people vote in a narrowly selfish way, there is no easy way to untangle the effect of misconceptions on policy. The problem seems insoluble. Fortunately, the problem does not need to be solved, because, contrary to both economists and the man in the street, voters are not selfishly motivated.²⁰ The self-interested voter hypothesis—or SIVH—is false. In the political arena, voters focus primarily on national well-being, not personal well-being. That makes it straightforward to move from systematic errors about the causes of national well-being to policies that are—from the standpoint of national well-being—counter-productive.\n\nThe SIVH is so embedded in both economics and popular culture that it has to be debunked before I can go on. Many economists find it peculiar even to speak of self-interested voting as a “hypothesis” in need of empirical support.²¹ Political cynicism drives the general public to the same conclusion: If you still haven’t noticed that people vote their pocketbooks, grow up!\n\nSince economists and the public rarely agree on anything of substance, their shared sympathy for the SIVH has long made me uneasy. In graduate school, I rarely came across hard evidence one way or the other. Many economists took the SIVH for granted, but few bothered to defend it.²² After completing my doctorate I read more outside my discipline, and discovered that political scientists have subjected the SIVH to extensive and diverse empirical tests.²³ Their results are impressively uniform: The SIVH fails.\n\nStart with the easiest case: partisan identification.²⁴ Both economists and the public almost automatically accept the view that poor people are liberal Democrats and rich people are conservative Republicans. The data paint a quite different picture. At least in the United States, there is only a flimsy connection between individuals’ incomes and their ideology or party. The sign fits the stereotype: As your income rises, you are more likely to be conservative and Republican. But the effect is small, and shrinks further after controlling for race. A black millionaire is more likely to be a Democrat than a white janitor.²⁵ The Republicans might be the party for the rich, but they are not the party of the rich.\n\nWe see the same pattern for specific policies.²⁶ The elderly are not more in favor of Social Security and Medicare than the rest of the population. Seniors strongly favor these programs, but so do the young.²⁷ Contrary to the SIVH-inspired bumper sticker “If men got pregnant, abortion would be a sacrament,” men appear a little more pro-choice on abortion than women.²⁸ Compared to the overall population, the unemployed are at most a little more in favor of government-guaranteed jobs, and the uninsured at most a little more supportive of national health insurance.²⁹ Measures of self-interest predict little about beliefs about economic policy.³⁰ Even when the stakes are life and death, political self-interest rarely surfaces: Males vulnerable to the draft support it at normal levels, and families and friends of conscripts in Vietnam were in fact more opposed to withdrawal than average.³¹\n\nThe broken clock of the SIVH is right twice a day. It fails for party identification, Social Security, Medicare, abortion, job programs, national health insurance, Vietnam, and the draft. But it works tolerably well for a few scattered issues.³² You might expect to see the exceptions on big questions with a lot of money at stake, but the truth is almost the reverse. The SIVH shines brightest on the banal issue of smoking. Donald Green and Ann Gerken find that smokers and nonsmokers are ideologically and demographically similar, but smokers are a lot more opposed to restrictions and taxes on their favorite vice.³³ Belief in “smokers’ rights” cleanly rises with daily cigarette consumption: fully 61.5% of “heavy” smokers want laxer antismoking policies, but only 13.9% of people who “never smoked” agree. If the SIVH were true, comparable patterns of belief would be everywhere. They are not.\n\nMost voters disown selfish motives. They personally back the policies that are best for the country, ethically right, and consistent with social justice. At the same time, they see other voters—not just their opponents, but often their allies too—as deeply selfish. The typical liberal Democrat says he votes his conscience, and impugns opponents for caring only about the rich. But he often ascribes selfish motives to fellow Democrats too: “Why do lower-income people vote Democratic? In order to better their own condition, of course.” The typical voter’s view of the motivation of the typical voter is schizophrenic: I do not vote selfishly, but most do.\n\nWhen individuals contrast their own faultless motives to the selfishness of others, our natural impulse is to interpret it as self-serving bias. But the empirical evidence suggests that self-descriptions are accurate. People err not in overestimating their own altruism, but in underestimating the altruism of others. Indeed, “underestimate” is an understatement. Individuals are not just less politically selfish than usually thought. As voters, they scarcely appear selfish at all.\n\nI suspect that the real reason most economists embrace the SIVH is not empirical evidence, but basic economic theory. If people are selfish consumers, workers, and investors, how can they fail to be selfish voters? It is tempting to respond, “If the theory fails empirically, so much the worse for the theory.” But we should first verify that the theory has been correctly applied.\n\nConsider. First, altruism and morality generally are consumption goods like any other, so we should expect people to buy more altruism when the price is low.³⁴ Second, due to the low probability of decisiveness, the price of altruism is drastically cheaper in politics than in markets.³⁵ Voting to raise your taxes by a thousand dollars when your probability of decisiveness is 1 in 100,000 has an expected cost of a penny.³⁶\n\n[f0151-01]\n\nFigure 6.3 The Price of Altruism in Markets versus Politics\n\nNow snap the pieces together. If people buy more altruism when the price is low, and altruistic voting is basically free, we should expect voters to consume a lot more altruism. It would cut against basic economics if the SIVH did work.³⁷\n\nHollywood is famous for its leftist millionaires like Tim Robbins and Susan Sarandon. Clinton’s victory over Bush in 1992 probably cost the respective stars of The Shawshank Redemption and The Rocky Horror Picture Show hundreds of thousands in extra taxes. But Robbins’s and Sarandon’s votes were not six-figure philanthropy. All they did was buy a negative lottery ticket: In the astronomically unlikely case that Clinton won because of their actions, they would have lost a large sum. With these incentives, you would expect Hollywood millionaires to “vote their conscience.” If they pay hundreds of dollars for a trendy haircut, it would be odd if they refused to pay a few expectational pennies to enhance their self-image.\n\nThe case of Hollywood leftists is unusually vivid, but entirely typical. The average American has less money riding on the outcome of a presidential election. But given the tiny probability of decisiveness, any psychological benefit is almost sure to outweigh its expected financial cost.³⁸\n\nSumming up: Correctly interpreted, the simple economic model specifically predicts that people will be less selfish as voters than as consumers. Indeed, like diners at an all-you-can-eat buffet, we should expect voters to “stuff themselves” with moral rectitude. Once again, analogies between voting and shopping are deeply misleading.\n\nThought Experiment 3: Irrationality with Unselfish Voters\n\nThe empirical evidence against the SIVH points to our next thought experiment. Replace assumption 1 with 1″:\n\n1″. All voters want to maximize social welfare, but they have different preferences over their beliefs about how to maximize it. Voters can have any endowments.\n\nIf voters’ goal is to maximize social welfare, if their motivation is, as political scientists term it, sociotropic, the complex interaction between policy and individual endowments can be ignored.³⁹ Whether you are rich or poor, a landowner or a stockholder, a creditor or a debtor, does not change the answer to the question “Which policies are best for society overall?” If people have the common goal of maximizing social welfare, the only source of conflict is disagreement about how to maximize it. The only obstacle to maximum social welfare is false beliefs about what policies work best.\n\nSuppose the polity is fighting over tariffs. According to thought experiment 3, this is a symptom of ideological struggle between those who think high tariffs are good for the nation, and those who do not. If the median voter has antiforeign bias, the system performs poorly. Even though everyone wants to maximize social welfare, even though democratic competition gives the people what they want, the outcome paradoxically disappoints.\n\nPolitical theorists often allege that economists’ belief in the SIVH leads them to underestimate democracy. According to Virginia Held, “There are good reasons to believe that a society resting on no more than bargains between self-interested or mutually disinterested individuals will not be able to withstand the forces of egoism and dissolution pulling such societies apart.”⁴⁰ However, once you accept the reality of systematic biases, the SIVH fades in importance as a handicap for democracy. Voters who solemnly put their own interests aside still do a bad job. If voters are rational and selfish, at least the status quo benefits somebody.\n\nUnselfishness expands the range of democratic performance.⁴¹ Good gets better. With rational selfishness, you get socially optimal outcomes if and only if incentives align private interests and the public interest. With rational unselfishness, this alignment is superfluous: People pursue the public interest for its own sake.\n\nBut unselfishness also lets democratic performance fall from bad to worse. Irrational unselfish voters are probably more dangerous than irrational selfish ones. If unselfish voters misunderstand the world, they can easily reach a misguided consensus. Their irrationality points them in the wrong direction; their unselfishness keeps them in marching formation, enabling them to rapidly approach their destination. In contrast, if selfish voters misunderstand the world, dissension persists. They move less cohesively—or not at all.\n\nSuppose voters overestimate the social benefits of price controls on petroleum. If they vote altruistically, then everyone—from owners of gas-guzzling Hummers to oil barons—supports price controls. The response of selfish voters would be less monolithic. Some—like those who own petroleum stock—would want to protect their “right to gouge” despite what they misperceive as its negative effect on society. Their selfishness helps mitigate the effect of antimarket bias on policy.\n\nThe upshot is that the failure of the SIVH makes democracy look worse. Voter irrationality is not tempered by the petty squabbling ordinarily guaranteed by human selfishness. Precisely because people put personal interests aside when they enter the political arena, intellectual errors readily blossom into foolish policies.\n\nMulti-Issue Democracy and the Dimensionality of Public Opinion\n\nAll of the thought experiments so far assume that the public is only concerned about one issue, such as the tariff rate. In reality, there are hundreds or thousands of contentious topics, which means that the tidy results of the Median Voter Theorem cease to hold. The winning platform for N issues decided as a package differs from the winning platform for N issues decided one by one. Strange as it sounds, a “winning platform”—a platform able to defeat any other—may not exist. Theorists often expect democratic policies to “cycle,” and wonder why real-world policies are so stable.⁴²\n\nIn my view, this is another dilemma that can be sidestepped using existing research on public opinion. There are countless issues that people care about, from gun control and abortion to government spending and the environment. But on closer inspection, these superficially disparate topics contain a great deal of structure. If you know a person’s position on one, you can predict his views on the rest to a surprising degree.⁴³\n\nIn formal statistical terms, political opinions look one-dimensional. They boil down roughly to one big opinion, plus random noise. Numerical ratings of “how liberal” or “how conservative” a congressman is often accurately predict his votes.⁴⁴ Higher-powered statistical analyses reach the same conclusion.⁴⁵ The same is true for the general public. Partisan voting is prevalent, suggesting that the public and elites use a similar ideological framework.⁴⁶ Data on specific beliefs confirms this story. For economic beliefs, for instance, self-reported position on the liberal-conservative spectrum predicts a lot about respondents’ specific views, and almost always confirms ideological stereotypes.⁴⁷\n\nOpinion is more clearly one-dimensional at some times and places than others.⁴⁸ Overall, though, one overriding fact about public opinion is that it is far less multi-dimensional than you might guess. The analytically tractable Median Voter Theorem stands on firmer empirical ground than usually thought.\n\nBut suppose you are not convinced by the empirical work on the dimensionality of public opinion.⁴⁹ What follows? It definitely gets harder to specify which policies will win out and stay on top. But that is no reason to expect better policies. Multi-dimensionality might undermine an especially foolish policy that the median voter favors, but it is equally able to sustain policies so silly even the median voter balks. In short, the policy consequences of one-dimensional opinion are more predictable—but not predictably worse—than those of multi-dimensional opinion.\n\nAnother Necessary Digression: What Makes People Think Like Economists?\n\nThe preceding thought experiments put a spotlight on a widely neglected variable: the economic literacy of the median voter. When the median voter suffers from strong systematic biases, foolish policies prevail; if the median voter sees clearly, democracy picks socially optimal policies.\n\nThis suggests a pressing question: What determines the median voter’s economic literacy? Are all segments of the population equally in the dark? Or do some “think more like economists” than others? We know from chapter 3 that education reduces the lay-expert belief gap. But this is only one of several regularities in the data.⁵⁰ All else equal, the following predict greater agreement with economists:\n\n• Education\n\n• Income growth\n\n• Job security\n\n• Male gender\n\nConsistent with the failure of self-serving bias and ideological bias to account for the lay-expert belief gap, income level and ideological conservatism do not make the list.\n\nFigure 6.4 shows how much education, income growth, job security, and gender matter.⁵¹ The top bar is the yardstick. It indicates how much two otherwise average people would disagree if one were a Ph.D. economist and one were not. The lower bars show how much larger or smaller the belief gap gets if the noneconomist is, in one respect, not average. Bars smaller than 100% mean below-average levels of disagreement. Bars larger than 100% mean the opposite.\n\n[f0155-01]\n\nFigure 6.4 The Distribution of Economic Illiteracy\n% Belief Gap Relative to Average\n\nEducation is the strongest predictor of economic literacy. The belief gap of the least educated is 127% as large as average; the belief gap of the most educated is only 81% as large as average. In other words, moving from the highest to the lowest education level expands disagreement by over 50%.\n\nIncome growth is a close runner-up. The SAEE asked respondents whether their income rose, fell, or stayed the same during the last five years, and asked them what they expect will happen to their income during the next five years. Individuals who both had and expect rising income think markedly more like economists than individuals who gave the opposite answers. The risers’ belief gap was 79% of the usual size; the fallers’ 115%. This is almost as large as five steps up the seven-step educational ladder.\n\nJob security and male gender have smaller effects. Moving from being “very concerned” about losing your job to “not at all concerned” matters as much as two steps of education; being male rather than female matters slightly less.\n\nWhat do all these results mean? The role of education is no surprise. Education predicts knowledge on a wide variety of subjects. Economics is no exception. Figuring out why is harder. Does education directly cause greater knowledge of economics in the classroom?⁵² Does it do so indirectly by raising the economic knowledge of the people you socialize with? Or is education just a proxy for other traits—like intelligence or curiosity?⁵³ Given the limits of the data, this is an open question.\n\nThe gender gap is not out of the ordinary either. One gender often knows more about a field than the other. Economics is a field where men happen to have the advantage. Other researchers document similar disparities. Men also have more political knowledge than women, and think more like toxicologists.⁵⁴ There are many possible explanations, but the differences do exist.⁵⁵\n\nThe link between income growth, job security, and economic literacy is the hardest to rationalize. Determined believers in self-serving bias will probably take comfort in these patterns: Upwardly mobile people with secure jobs can safely adopt economists’ callous outlook. But then why does income level conspicuously fail to matter? A more plausible story is that personal and social optimism go together. Maybe some people are just optimists, or perhaps personal experience with progress makes it easier to spot on a larger scale.\n\nSelective Participation\n\nFailure to vote is a major leak in the pipeline between public opinion and public policy. Politicians only need the support of a majority of people who exercise their right to vote. If they can win the affection of one voter by alienating a thousand nonvoters, competition spurs them to do so.\n\nThis would not affect policy if voters and nonvoters had the same distribution of preferences and beliefs, but voters are not a random sample. The most visible difference is that voters are richer than nonvoters. On closer examination, income is largely a proxy for education; education increases both income and the probability of voting. The other big predictor of turnout is age; the old vote more than the young.⁵⁶\n\nMost commentators treat disparate turnout as a grave social evil. If voters are out to promote their own interests, then groups that show up use and abuse groups that stay home.⁵⁷ Many blame the high turnout of the rich for policies that “make the rich richer and the poor poorer,” and the high turnout of the elderly for the expense of Social Security and Medicare.\n\nThe weakness of these complaints is that they take the discredited SIVH for granted. Yes, the rich are more likely to vote. But since the rich are not trying to advance upper-class interests, it does not follow that the interests of the poor suffer. Similarly, just because the old vote in greater numbers, it does not follow that the young lose out. For that fear to be justified, the young would have to be less supportive of old-age programs than their seniors. They are not.⁵⁸\n\nGood intentions are ubiquitous in politics; what is scarce is accurate beliefs. The pertinent question about selective participation is whether voters are more biased than nonvoters, not whether voters take advantage of nonvoters.⁵⁹ Empirically, the opposite holds: The median voter is less biased than the median nonvoter. One of the main predictors of turnout, education, substantially increases economic literacy. The other two—age and income—have little effect on economic beliefs.\n\nThough it sounds naive to count on the affluent to look out for the interests of the needy, that is roughly what the data advise. All kinds of voters hope to make society better off, but the well educated are more likely to get the job done.⁶⁰ Selective turnout widens the gap between what the public gets and what it wants. But it narrows the gap between what the public gets and what it needs.\n\nIn financial and betting markets, there are intrinsic reasons why clearer heads wield disproportionate influence.⁶¹ People who know more can expect to earn higher profits, giving them a stronger to incentive to participate. Furthermore, past winners have more assets to influence the market price. In contrast, the disproportionate electoral influence of the well educated is a lucky surprise. Indeed, since the value of their time is greater, one would expect them to vote less. To be blunt, the problem with democracy is not that clearer heads have surplus influence. The problem is that, compared to financial and betting markets, the surplus is small.\n\nIf education causes better economic understanding, there is an argument for education subsidies—albeit not necessarily higher subsidies than we have now.⁶² If the connection is not causal, however, throwing money at education treats a symptom of economic illiteracy, not the disease. You would get more bang for your buck by defunding efforts to “get out the vote.”⁶³ One intriguing piece of evidence against the causal theory is that educational attainment rose substantially in the postwar era, but political knowledge stayed about the same.⁶⁴\n\nEducation is the only variable that predicts both economic literacy and voter participation. But other predictors of economic literacy—particularly income growth and job security—interact with democratic politics in potentially interesting ways. For example, suppose income growth and job security cause higher economic literacy. Then given a negative economic shock, income growth and job security would decline, reducing the median voter’s economic literacy, increasing the demand for foolish economic policies, which in turn hurts economic performance further. I refer to this downward spiral as “the idea trap.”⁶⁵ Perhaps it can help solve the central puzzle of development economics: Why poor countries stay poor.⁶⁶\n\nBefore studying public opinion, many wonder why democracy does not work better. After one becomes familiar with the public’s systematic biases, however, one is struck by the opposite question: Why does democracy work as well as it does? How do the unpopular policies that sustain the prosperity of the West survive? Selective participation is probably one significant part of the answer. It is easy to criticize the beliefs of the median voter, but at least he is less deluded than the median nonvoter.\n\nThought Experiment 4: Mixed Policy/Outcome Preferences\n\nNow let us see where one last empirically interesting complication leads. Suppose voters have systematically biased beliefs about the effectiveness of economic policies, but perceive the current state of the economy without bias. What happens if they hold politicians accountable for both their policy decisions and the state of the economy?⁶⁷\n\nWith these incentives, politicians who want to retain power need to keep their eyes on two balls, not one. If voters’ beliefs about effective policy were correct, this would be easy, because the two balls would be fused together. But in the real world, politicians face a visual challenge: keeping their eyes on two balls flying in different directions. If leaders ignore the public’s policy preferences, they will be thrown out of office no matter how good economic conditions are. If they fully implement those preferences, though, leaders become scapegoats for poor economic performance.\n\nThis mechanism resembles what political scientists call “retrospective voting.”⁶⁸ Its novel feature is the perverse trade-off between policies and outcomes. In most retrospective voting models, voters are agnostic about policy, and judge politicians purely for their observable success. Leaders’ dominant strategy is therefore to implement the most effective policies.⁶⁹ This is no longer true, however, if voters “know what ain’t so”—if they want specific policies but resent their predictable consequences.\n\nThese incentives interestingly lead politicians to supply better economic policies than the public wants. Take Clinton’s support of NAFTA.⁷⁰ He knew both that NAFTA would raise American living standards, and that a majority of Americans thought the opposite. If Clinton’s sole goal were to maximize his probability of reelection, what should he have done? Both options were unappealing. The first was to defy the public, lose face, and hope that the economic benefits of NAFTA undid the damage before the next election. The second was to go along with the public, retain its trust, and hope it would overlook the lackluster economy. Clinton took the first route, and it may well have been the prudent choice.\n\nIf voters are systematically mistaken about what policies work, there is a striking implication: They will not be satisfied by the politicians they elect. A politician who ignores the public’s policy preferences looks like a corrupt tool of special interests. A politician who implements the public’s policy preferences looks incompetent because of the bad consequences. Empirically, the shoe fits: In the GSS, only 25% agree that “people we elect to Congress try to keep the promises they have made during the election,” and only 20% agree that “most government administrators can be trusted to do what is best for the country.”⁷¹ Why does democratic competition yield so few satisfied customers? Because politicians are damned if they do and damned if they don’t. The public calls them venal for failing to deliver the impossible.\n\nOne problem with outcome-linked voting is that judgments about outcomes may be biased too. “Believing is seeing”—people may wear rose-colored glasses if and only if their preferred policies hold sway.⁷² During the 1990s, employment rates reached peaks not seen in three decades, but opponents of NAFTA announced that its dire consequences were plain for all to see.⁷³\n\nAnother weakness of outcome-linked voting is that voters may punish leaders for problems outside their control.⁷⁴ As Achen and Bartels observe:\n\nIf jobs have been lost in a recession, something is wrong, but is that the president’s fault? If it is not, then voting on the basis of economic results may be no more rational than killing the pharaoh when the Nile does not flood.⁷⁵\n\nThis is especially troublesome under divided government. If the public holds the president accountable for economic turmoil, then Congressmen from the other party might prevent his reelection by doing a bad job. Alternately, Congress might push popular but counterproductive policies, forcing the president to either veto them (and lose votes for being out of sync with public opinion) or sign them (and lose votes for bad economic performance). Costly but popular social legislation sponsored by the Democrats during the 1988–92 Bush presidency has been interpreted this way.⁷⁶\n\nA final reason not to overrate outcome-linked voting is that many people have a low threshold for what counts as a “result.” Social scientists conceive of “results” as things like economic growth, life expectancy, crime rates, or peace. But politicians habitually equate “results” with passing legislation and spending money. How many campaign ads cite “achievements” like a “tough new gun control bill”? It would be odd to call this a “result” if gun control increases the murder rate.\n\nDespite these caveats, mixed policy/outcome preferences remain a plausible explanation for why democracy is not worse. Respondents in the SAEE have biased beliefs about outcomes, not just policies. Yet their outcome judgments are less biased, and their perceptions about the current state of the economy are fairly accurate.⁷⁷ Unless the costs of economic policy are well in the future, politicians have to think twice before caving in to popular misconceptions.\n\nBias beyond Economics: Systematically Biased Beliefs about Toxicology\n\nMost of my examples come from economics, and with good reason: Economics dominates the agenda of modern governments. But my analysis can and should be applied to other politically relevant fields where the general public’s beliefs are systematically mistaken.\n\nTable 6.1\nThe Public versus Toxicologists on Dosage\n\n[t0161-01]\n\nToxicology, with its obvious implications for environmental, health, and safety policy, is a compelling example. The public has numerous prejudices about this apparently dry, technical field.⁷⁸ For instance, Kraus, Malmfors, and Slovic ask people whether they agree with this statement: “For pesticides, it’s not how much of the chemical you are exposed to that should worry you, but whether or not you are exposed at all.”⁷⁹\n\nToxicologists are far more likely to emphasize dosage. Nontoxicologists “tend to view chemicals as either safe or dangerous and they appear to equate even small exposures to toxic or carcinogenic chemicals with almost certain harm.”⁸⁰\n\nAs in economics, laymen reject the basics, not merely details. Toxicologists are vastly more likely than the public to affirm that “use of chemicals has improved our health more than it has harmed it,” to deny that natural chemicals are less harmful than man-made chemicals, and to reject the view that “it can never be too expensive to reduce the risks associated with chemicals.”⁸¹ While critics might like to impugn the toxicologists’ objectivity, it is hard to take such accusations seriously. The public’s views are often patently silly, and toxicologists who work in industry, academia, and regulatory bureaus largely see eye to eye.⁸²\n\nHow would the public’s misconceptions about, say, dosage, affect policy? This chapter’s thought experiments are a useful guide. With identical voters, failure to recognize the importance of dosage leads straight to misguided environmental regulations. Instead of focusing on quantitatively significant risks, the government wastes resources trying to eliminate minute dangers.⁸³ If otherwise identical voters disagree about the importance of dosage, but the median believer doubts the truism that “the poison is in the dosage,” environmental regulations lean in a wasteful direction. Similarly wasteful policies can be expected if voters are not identical but seek to maximize social welfare.\n\nWhy then does environmental policy put as much emphasis on dosage as it does? Selective participation is probably part of the story. Mirroring my results, Kraus, Malmfors, and Slovic (1992) find that education makes people think like toxicologists.⁸⁴ The bulk of the explanation, though, is probably that voters care about economic well-being as well as safety from toxic substances. Moving from low dosage to zero is expensive. It might absorb all of GDP. This puts a democratic leader in a tight spot. If he embraces the public’s doseless worldview and legislates accordingly, it would spark economic disaster. Over 60% of the public agrees that “it can never be too expensive to reduce the risks associated with chemicals,”⁸⁵ but the leader who complied would be a hated scapegoat once the economy fell to pieces. On the other hand, a leader who dismisses every low-dose scare as “unscientific” and “paranoid” would soon be a reviled symbol of pedantic insensitivity. Given their incentives, politicians cannot disregard the public’s misconceptions, but they often drag their feet.\n\nConclusion\n\nThe proposition that irrational beliefs lead to foolish policies is largely correct. Under realistic assumptions, irrational thought leads to foolish action. Recognizing the empirical weakness of the SIVH sweeps away needless complexity. If voters aim to advance the public interest rather than their own, there is no need to build a rickety bridge from the public interest to each individual’s private interests. We can walk straight from misperceptions about the public interest to support for misguided policy.\n\nThe main caveat is that if the public got exactly what it asked for, policy would be a lot worse. The United States is more market-oriented and open to international competition than you would expect after studying the economic beliefs of its inhabitants, whose aspirations seem more in tune with those of Latin American populists like Perón.\n\nOn further consideration, this disparity should be expected. Selective participation, so often maligned as a source of class bias, leaves the median voter more economically literate than the median citizen. More importantly, the public’s ungracious tendency to scapegoat its most faithful agents encourages felicitous hypocrisy. Politicians face an uneasy predicament: “Unabashed populism plays well at first, but once the negative consequences hit, voters will blame me, not themselves.” This hardly implies that it never pays to take the populist route. But leaders have to strike a balance between doing what the public thinks works, and what actually does.\n\nTECHNICAL APPENDIX\n\nWhat Makes People Think Like Economists\n\nQualitatively, there are five main variables in the SAEE that make people “think like economists”: education, male gender, past income growth, expected income growth, and job security.⁸⁶ They frequently push beliefs in the same direction as economic training, and almost never push in the opposite direction. But how strong is the overall link between these variables and the economic way of thinking? My article in the Journal of Law and Economics quantified it using the following technique.⁸⁷\n\nStep 1 is to set up a system of 37 equations, one for each question in the SAEE:\n\n[f0163-01]\n\nand so on for equations (4)–(37). Each of the coefficients inside the brackets—the e’s—has to be the same across all 37 equations. For instance, the coefficient on Education, e(1), has the same value in equation (1), equation (2), equation (3), and so on. Conversely, the constants and the w coefficients freely vary in each equation. The impact of the set of economistic variables in a given equation can thus be positive, negative, or zero, because there is an equation-specific w coefficient in front of the brackets. Intuitively, the e coefficients capture “how economistic” an independent variable is, while the w coefficients capture “how economistic” a dependent variable is.\n\nStep 2 is to estimate the whole system’s coefficients using nonlinear least squares. The results are completely consistent with qualitative appearances. Despite the strong collinearity restrictions, the w coefficients are highly significant in both statistical and economic terms. Economistic variables are significant at the 5% level in 34 out of 37 equations.\n\nFurthermore, all of the e coefficients are positive and overwhelmingly statistically significant, indicating that they really do march in formation with economic training.\n\nTable 6.2\nThe w Coefficients\n\n[t0164-01]\n\nUsing the information in table 6.3, one can express the economic literacy of various subgroups of the general public as a scalar. The estimated belief gap between (a) a noneconomist with average characteristics across the board and (b) the Enlightened Public equals: the coefficient on education, e(1), times 2.46 (the amount by which the Enlightened Public exceeds the average education level) plus 1 (the implicit coefficient on Econ). This comes out to .093 \\* 2.46 + 1 = 1.229. The belief gaps of other segments of the population can then be compared to this benchmark, as shown in Figure 6.4.\n\nTable 6.3\nThe e Coefficients\n\n[t0164-02]\n\nExample 1. The belief gap between the Enlightened Public and otherwise average members of the public with the lowest education level is 6 \\* .093 + 1 = 1.558. In percentage terms, this means that the belief gap of the lowest-educated segment of the population is roughly 1.558/1.229 = 127% as large as the benchmark.\n\nExample 2. The belief gap between the Enlightened Public and otherwise average members of the public with maximal job security is e(5) multiplied by −1.12 (the difference between the average level of job security and the highest possible level) plus 1.229 (the normal gap). This simplifies to 1.163, roughly 95% the size of the benchmark.\n\nChapter 7\n\nIRRATIONALITY AND THE SUPPLY\n\nSIDE OF POLITICS\n\nFirst, even if there were no political groups trying to\ninfluence him, the typical citizen would in political\nmatters tend to yield to extra-rational or irrational\nprejudice and impulse. . . .\n\nSecond, however, the weaker the logical element in the\nprocesses of the public mind and the more complete\nthe absence of rational criticism . . . the greater are the\nopportunities for groups with an ax to grind.\n\n—Joseph Schumpeter, Capitalism, Socialism,\nand Democracy¹\n\nMY JAUNDICED VIEW of the average voter is the most distinctive feature of my political economy, but it is not the only distinctive feature. Competing for the affection of irrational voters calls for different tactics and talents than competing for the affection of rational voters.² Voter irrationality reshapes the whole political landscape, from leadership and delegation to propaganda and lobbying.\n\nThe Rationality of Politicians\n\nThe successful politician instinctively feels what the voters\nfeel, regardless of what facts and logic say. His guiding\nprinciple is neither efficiency nor equity but electability—\nabout which he knows a good deal.\n\n—Alan Blinder, Hard Heads, Soft Hearts³\n\nWhat happens if fully rational politicians compete for the support of irrational voters—specifically, voters with irrational beliefs about the effects of various policies? It is a recipe for mendacity. If politicians understand the benefits of free trade, but the public is dogmatically protectionist, honest politicians do not get far. Every serious contender must not only keep his economic understanding to himself, but “pander”—zealously advocate the protectionist views he knows to be false.\n\nMachiavelli infamously advises his readers to break promises when it enhances their political careers: “A prudent ruler ought not to keep faith when by so doing it would be against his interest. . . . If all men were good, this precept would not be a good one; but as they are bad, and would not observe their faith with you, so you are not bound to keep faith with them.”⁴ Machiavelli is saying that—morally objectionable or not—lying is equilibrium behavior. In a modern democratic milieu, he could as easily written, “A prudent ruler ought not to promote socially beneficial policies when by so doing he would lose votes. . . . If all men were rational, this precept would not be a good one; but as they are irrational, and prone to kill the bearers of bad tidings, so you are not bound to challenge their misconceptions.”\n\nBut are politicians likely to be paragons of rationality, a breed apart? It depends on the topic.⁵ Sometimes politicians—unlike ordinary voters—have strong incentives for rationality. Above all, it pays a politician to understand how his policy positions and other actions change his electoral prospects. Politicians have as strong an incentive to think rationally about their popularity as capitalists have to think rationally about their profits.\n\nFor example, it is valuable for politicians to accurately estimate the effect of political advertising and the “rate of exchange” between campaign contributions and votes. If they overestimate the vote-benefit of money, they allocate too much time to raising money, and make too many damaging compromises. If they underestimate the vote-benefit, they allocate too little time to fund-raising, and are overly squeamish about repaying donors’ favors.\n\nOr consider incentives to think rationally about the media. Politicians often have skeletons in their closet, and face daily temptations to add to their collection. Unbiased beliefs about the probability of getting caught and the severity of the backlash are useful tools of political survival. This does not mean that politicians put zero value on illicit fun, but we should expect them to make intelligent trade-offs. Clinton’s relations with “that woman, Miss Lewinsky” ultimately drew massive media attention, but he took many measures along the way to protect himself.⁶\n\nIn sum, politicians, unlike average voters, make some political choices where their cost of systematic error is high. In these cases, we should expect leaders to be shrewd and clear-eyed. Selection pressure reinforces this point. Politicians who alienate voters soon cease to be politicians at all.⁷\n\nHowever, there is one important area where matters are less clear: Beliefs about policy effectiveness. Does it pay politicians to correctly diagnose how well policies work? If all that voters care about is adherence to their policy preferences, the answer is no. For the vote-maximizing politician, the majority is always right. Thomas Sowell explains:\n\nWhen most voters do not think beyond stage one, many elected officials have no incentive to weigh what the consequences would be in later stages—and considerable incentives to avoid getting beyond what their constituents think and understand, for fear that rival politicians can drive a wedge between them and their constituents by catering to public misconceptions.⁸\n\nIf voters are committed protectionists, politicians do not win their friendship with patient lectures on comparative advantage. Instead of trying to correct popular errors, they indulge them. As Alexander Hamilton put it in The Federalist Papers, they “flatter their prejudices to betray their interests.”⁹\n\nUnusually talented politicians do more than cater to current misconceptions. They steer the grateful public toward the “new and improved” misconceptions of tomorrow. A good politician tells the public what it wants to hear; a better one tells the public what it is going to want to hear. After a sudden rise in oil prices, the public would probably blame the greed of the oil companies on its own initiative, but lack the imagination to propose price controls. A skillful politician capitalizes on the crisis by alerting his constituents to an attractive solution: “Price controls! Why didn’t we think of that?”\n\nLeaders’ incentive to rationally assess the effects of policy might be perverse, not just weak. Machiavelli counsels the prince “to do evil if constrained” but at the same time “take great care that nothing goes out of his mouth which is not full of” “mercy, faith, integrity, humanity and religion.” One can freely play the hypocrite because “everybody sees what you appear to be, few feel what you are, and those few will not dare oppose themselves to the many.”¹⁰ Yet, contra Machiavelli, psychologists have documented humans’ real if modest ability to detect dishonesty from body language, tone of voice, and more.¹¹ George Costanza memorably counseled Jerry Seinfeld, “Just remember, it’s not a lie if you believe it.”¹² The honestly mistaken politician appears more genuine because he is more genuine. This gives leaders who sincerely share their constituents’ policy views a competitive advantage over Machiavellian rivals.¹³\n\nAs discussed in the previous chapter, there is a countervailing force. If voters care about both policies and outcomes, the pandering cynic has a fighting chance against the deluded idealist. The cynic suffers from voters’ uneasy sense that, deep down, he is not one of them. But the cynic is more equipped to avoid disaster than the idealist, because he independently weighs the cost of the public’s favorite policies. The true child of Machiavelli undermines and soft-pedals the public’s worst ideas, paying them lip service the whole time.\n\nTo get ahead in politics, leaders need a blend of naive populism and realistic cynicism. No wonder the modal politician has a law degree. Dye and Zeigler report that “70 percent of the presidents, vice presidents, and cabinet officers of the United States and more than 50 percent of the U.S. senators and House members” have been lawyers.¹⁴ The economic role of government has greatly expanded since the New Deal, but the percentage of congressmen with economic training remains negligible.¹⁵ Economic issues are important to voters, but they do not want politicians with economic expertise—especially not ones who lecture them and point out their confusions.\n\nInstead, the electoral process selects people who are professionally trained to plead cases persuasively and sincerely regardless of their merits.¹⁶ Many politicians keep economists around to advise them. But the masters of rhetoric call the shots because they possess the most valuable political skill: Knowing how to strike the optimal balance between being right and being popular.\n\nThe Political Economy of Faith\n\nLeaders have been known to inspire blind faith. Michels refers to “the belief so frequent among the people that their leaders belong to a higher order of humanity than themselves” evidenced by “the tone of veneration in which the idol’s name is pronounced, the perfect docility with which the least of his signs is obeyed, and the indignation which is aroused by any critical attack on his personality.”¹⁷ Many totalitarian movements insist upon their leaders’ infallibility. “The Duce is always right,” was a popular Fascist slogan.¹⁸ Rudolf Hess waxed poetic about the perfection of Hitler’s judgment:\n\nWith pride we see that one man remains beyond all criticism, that is the Führer. This is because everyone feels and knows: he is always right, and he will always be right. The National Socialism of all of us is anchored in uncritical loyalty, in the surrender to the Führer that does not ask for the why in individual cases, in the silent execution of his orders. We believe that the Führer is obeying a higher call to fashion German history. There can be no criticism of this belief.¹⁹\n\nDemocratically elected leaders rarely claim anything so outrageous. But they seem to enjoy a milder form of unreasoning deference.²⁰ The most charismatic president may not radiate infallibility to anyone, but that does not stop people from choosing to believe that he is honest in the absence of rock solid evidence to the contrary. As an exasperated Paul Krugman writes:\n\nMr. Bush has made an important political discovery. Really big misstatements, it turns out, cannot be effectively challenged, because voters can’t believe that a man who seems so likable would do that sort of thing.²¹\n\nEven a colorless politician might find that his title makes his words credible. It works for the pope. Why not the president?\n\nOne striking instance of unreasoning deference: Shortly after 9/11, polls strangely found that the nation’s citizens suddenly had more faith in their government.²² How often can you “trust the government in Washington to do what is right”? In 2000, only 30% of Americans said “just about always” or “most of the time.” Two weeks after 9/11, that number more than doubled to 64%. It is hard to see consumers trusting GM more after a major accident forces a recall. The public’s reaction is akin to that of religious sects who mispredict the end of the world: “We believe now more than ever.”\n\nA close relative of blind faith is the ability to change men’s minds with mere rhetoric. Think about it: People modify their view of the world because a current or aspiring leader redescribes the facts. With normal faith, the audience says, “I believe because he said it.” Faith inspired by verbal ability is a slight variation: “I believe because he said it so well.” Perhaps the most extreme illustration is the political influence of great poets like Pablo Neruda. Common sense snaps, “What does he know? He’s a poet,” but many would rather listen and be swayed by the beautiful words.\n\nWhat happens to democracy if the public puts a degree of irrational faith in its leaders? The most obvious effect is to give leaders slack or “wiggle room.” Though they have to conform to public opinion, public opinion becomes partly a function of the politicians’ own choices. If doing A gives the public faith in the wisdom of A, and doing B gives the public faith in the wisdom of B, then a politician may safely choose either one. It is arrogant for a leader to snicker that “the people will think what I tell them to think,” but that does not make him wrong.\n\nFaith helps explain politicians’ tendency to dodge pointed questions with vague answers.²³ How can refusing to take a position (or changing the subject) be strategically better than candidly endorsing a moderate position?²⁴ Put yourself in the shoes of a voter who opposes the moderate view but has a degree of faith in a candidate’s good intentions. If the candidate announces his allegiance to the moderate view, faith in him dissolves. But as long as the candidate is silent or vague, it does not tax your faith to maintain, “He’s a decent man, he must agree with me.” From politicians’ point of view, the critical fact is that voters on both sides of the issue can “reason” in the same fashion.\n\nThe downside of quasi-religious faith in the powers-that-be (or want-to-be) is plain. Cushioned by the masses’ credulity, an elected official could shirk to their detriment.²⁵ Recall that the simplest way to keep politicians in line is to harshly punish them when you catch them misbehaving. An electorate with faith in its leaders spares the rod and spoils the child.\n\nMachiavelli notoriously urges leaders to take full advantage of leader worship: “But it is necessary . . . to be a great feigner and dissembler; and men are so simple and so ready to obey present necessities, that one who deceives will always find those who allow themselves to be deceived.”²⁶ A corrupt politician can use faith-based slack to cater to special interests, a ideologue to push his agenda. Regardless of what one thinks about the war on terror, it is hard to deny that George Bush would have enjoyed comparable support if he made fairly different choices. If he decided that invading Iraq was not worth the effort, how many of his supporters would have balked? Since some of Bush’s options were better for his financial backers and cohered more with his ideology, he faced temptations to shirk. The only question is whether he gave into temptation.\n\nStill, one should not ignore the upside of political faith: its ability to neutralize the public’s irrationality. A leader who understands the benefits of free trade might ignore the public’s protectionism if he knows that the public will stand behind whatever decision he makes. Since politicians are well educated, and education makes people think more like economists, there is a reason for hope. Blind faith does not create an incentive to choose wisely, but it can eliminate the disincentive to do so. Whether this outweighs the dangers of political faith is an open question.\n\nThe same goes for faith in experts. It opens up a low road and a high road. The low road is for experts to take advantage of the public, promoting their personal finances or ideology. The high road is for experts to help the public in spite of itself. Suppose the public has faith in the FDA. Its drug policy experts could take the low road, telling the credulous public that it is “in the public interest” to test drugs for efficacy as well as safety, ignoring the lives lost from years of delay.²⁷ But sometimes experts take the high road instead. The public might be sure that Thalidomide should be totally banned, but defer when the FDA approves it as a treatment for leprosy.²⁸\n\nIrrationality and Delegation\n\nPrinces should let the carrying out of unpopular duties\ndevolve on others, and bestow favors themselves.²⁹\n\n—Niccolò Machiavelli, The Prince\n\nIn complex modern political systems, leaders can only make a handful of big decisions. The rest must be left in subordinates’ hands. High-level subordinates face the same dilemma, pushing concrete decisions further down the bureaucratic food chain. This fosters the sense that elected leaders are not in charge. The real power, supposedly, is the “faceless bureaucracy.”\n\nThe economics of principal-agent relations cuts against this inversion.³⁰ When a principal delegates a task to a subordinate, his tacit instruction is, “Do what I myself would have done if I had the time,” not, “Do as you please.” The former does not have to evolve into the latter. Common sense tells a principal to occasionally audit his subordinates to see how well they mimic the decisions he would have made himself.³¹\n\nIt makes little difference if there is one principal and one agent, or one principal at the top of a tall bureaucratic pyramid. The preferences of the apex trickle down to the base. Imagine the pyramid has 26 layers, from A at the top to Z at the bottom. If the Z’s ask, “What is expected of me?” the answer is, “To do what the Y above you would have done.” If the Y’s in turn ask, “What is expected of me?” the answer is, “To do what the X above you would have done.” For any given Z, serving the wishes of the Y above him is equivalent to serving the wishes of the X two levels up. This principle lets us ascend the entire pyramid.\n\nIn a deep sense, the leader of an organization is responsible for everything his organization does. Mistakes happen, but part of the job is keeping an eye on subordinates. That includes keeping an eye on whether they are keeping an eye on their subordinates. If the grocery bagger at a supermarket is rude to you, it is more than a personal failing. It reflects poorly on the entire chain for failing to detect and correct the bagger’s etiquette.\n\nThis argument remains relevant for tenured professors, Supreme Court justices, and others who cannot be fired. When you cannot punish insubordination, rely on reputation instead. Choose candidates with a long history of support for your approach. If a justice undercuts the president who appointed him, a rational electorate can and should blame the president for being a poor judge of character.\n\nSo at first pass, simple models seem able to capture the complexities of modern government. Those who have been a cog in the political machine frequently relay a different impression, but their objections are fairly superficial. The fact that you have some latitude over the cosmetics of a delegated decision hardly shows that you—not your nominal superior—control its substance. The fact that your boss rarely double-checks your work or second-guesses you does not show that he is really working for you. More plausibly, it means that your superior rationally trusts you to make the decision he would have made without being asked. If he thinks you are sound, he leaves you alone, conserving his scrutiny for more questionable underlings.\n\nBy itself, irrationality does not amplify the importance of delegation. If voters believe that protectionism promotes the general welfare, they want more than a leader who promotes protectionism when a chance to do so lands on his desk. They expect their leader to impose his protectionist aims on his underlings—to make it known throughout the hierarchy that all decisions should have a protectionist flavor.\n\nHowever, the right kind of irrationality undermines the standard analysis. Suppose that voters underestimate the ability of politicians to control their subordinates. This creates strange new leeway for politicians. They can take the crowd-pleasing action themselves, but allow or encourage their subordinates to do the opposite.\n\nIn the United States, the president appoints Supreme Court justices and the Senate confirms them. Rationally speaking, a justice’s rulings reflect on the officials who put him on the bench. If a justice defies public opinion by protecting flag-burning, his decision should diminish the popularity of the president who appointed him and the senators who confirmed him.³² This assumes, however, that average voters correctly perceive the chain of responsibility. If they systematically underestimate the strength of its connections, delegation undermines the popular will. Politicians have to denounce flag-burning to win voter approval, but it stays legal as long as the decision is in the hands of subordinates who demur.\n\nThe ability to wash his hands of his underlings’ actions gives a leader extra slack. If he wants something unpopular to happen, he does not have to become unpopular himself. Instead, he publicly stands with the majority, but privately leads his subordinates to undercut him. In its crassest form, he could tell his subordinates, off the record, that his public statements are the opposite of his true wishes. But it is easier to appoint people who want to do the unpopular thing, then look the other way.\n\nWhen the popular view and the reasonable view overlap, systematically biased beliefs about ultimate political responsibility are all for the bad. Corruption and favoritism flower if politicians can wink at their underlings as they denounce “influence-peddling.” On a classic episode of The Simpsons, Bart became famous for excusing his misbehavior with the catchphrase “I didn’t do it.”³³ No one believed Bart. But if the electorate believes politicians who use Bart’s strategy, they have a license to steal. To be more precise, they can sell licenses to steal, hiding behind the fact that they personally stole nothing. Ideologically committed politicians could use the same means for putatively nobler ends: “Funds for the Contras? I didn’t do it.”\n\nBut equating the popular and the reasonable unfairly tilts the scales against political slack. Irrationality about political responsibility has the potential to defuse the effect of irrationality on policy, as Tullock shows in one of his little fables:\n\nConsider a professor of economics and the dullest student in his class. Let us assume that . . . the dull student becomes a king, and . . . the professor of economics becomes his principal advisor. . . . Such a minister has open to him three courses of action: he may resign; he can stop trying to improve the economic conditions of the kingdom and simply implement the king’s stupid ideas on economic matters; or he can try to deceive the king into carrying out the policies that he, the minister, thinks wise while agreeing with the king in council.³⁴\n\nFalse beliefs about who is responsible for what are particularly potent if voters care about both policies and results. Then a leader could win on both metrics. He publicly backs the popular view to show his laudable intentions. Meanwhile, he nudges his underlings to ignore public opinion and shoot for prosperity, proving his competence.\n\nBiased beliefs about political responsibility have arguably greased much of the progress toward free trade. Congress and the president have full authority over trade policy. They can leave the World Trade Organization any time they want. When the WTO overrules protectionist moves by the United States, however, our leaders blame the WTO, conveniently forgetting that it has only the power they gave it.³⁵ Has democracy been undermined by bureaucratic sleight of hand? Yes—and the electorate is better off as a result.\n\nAdmittedly, to say this plays right into the hands of detractors of the economics profession like William Greider:\n\nDisparaging public opinion is, of course, a necessary prelude to ignoring it. The elites’ language of despair over the commonweal is a vital element in their politics, for it creates another screen—a climate that encourages political leaders to be “responsible” by going against the obvious wishes of their constituents.³⁶\n\nBut his lament dodges the hard question: What if public opinion deserves to be disparaged?\n\nAnother oddity that thrives on misconceptions about political responsibility: Leaders often feel public pressure to “do something” about a problem, but the world finds fault with every concrete solution. A way out is to pass legislation that is loudly well intentioned, but vague.³⁷ Practically speaking, this leaves the hard decisions to so-called independent agencies or judges. One might object, “If you created the agency and retain the power to alter or abolish it with a simple majority vote, in what sense is it ‘independent’?” But tough questions are a weak obstacle. Assuming the public falls for their semantic trickery, politicians can rise in popular esteem for “doing something,” but deflect inevitable disappointment onto the shoulders of others.\n\nU.S. antitrust laws are a beautiful example. Try to decipher the meaning of “attempted monopolization” or “restraint of trade” with the help of a dictionary. Am I “attempting to monopolize” the market for books about economics right now? No matter. Though the written law verges on meaningless, sponsors like Senator Sherman and Representative Clayton won credit for “fighting the trusts.” Only after judges and regulators “interpreted” the laws could their effects be seen. From the point of view of the Shermans and Claytons, this makes the deal sweeter still. Someone else makes the tough decisions and risks embarrassment. All it takes to see through this ruse is the common sense to ask, “Who passed the ambiguous law that allowed the bad decisions to happen in the first place?” The ruse works if common sense is not so common.\n\nEconomists of little faith in democracy emphasize how hard it is for constituents to control their “representatives.”³⁸ Defenders of democracy like Donald Wittman downplay the role of political slack. On balance, Wittman gets the better of the theoretical debate: Voters have several easy ways to keep leaders on a short leash. But both sides tend to misjudge the broader implications of their stance on slack. Given everything else we know about democracy, agency “problems” may be agency solutions.\n\nWhen a master does not know his own best interests, a disobedient servant can be a blessing. The more misguided the electorate is, the less desirable it is for politicians to unquestioningly grant its wishes. If voters want price controls, a politician with slack can ignore them for their own good. Or he might take money from Big Oil to oppose controls, proverbially turning a private vice into a public virtue. The lesson is that agency “problems” temper majoritarian extremes. Good outcomes become less good, because corrupt politicians stand in the way of the public’s grand design. Bad outcomes become less bad, because politicians have the wiggle room to tone them down.\n\nStrangely, then, if Wittman is right about agency problems, democracy arguably looks worse. As explained in the previous chapter, unselfish motivation amplifies the risks of irrational cognition. So when the electorate is irrational and unselfish, perhaps you should hope for agency “problems” to open up a livable gap between what voters want and what voters get. If politicians have no choice but to carry out constituents’ wishes, democracy loses one of its main safety valves.\n\nIrrationality and Propaganda\n\nI believe that voter preferences are frequently not\na crucial independent force in political behavior. These\n“preferences” can be manipulated and created through\nthe information and misinformation provided by\ninterested pressure groups, who raise their political\ninfluence partly by changing the revealed “preferences”\nof enough voters and politicians.\n\n—Gary Becker, “A Theory of Competition Among\nPressure Groups for Political Influence”³⁹\n\nThe media want to entertain citizens; politicians, to influence their votes. If informing voters achieves these ends, the media and politicians have an incentive to distribute free information. Many social scientists think these giveaways help democracy work, and if voters are rational, they are correct.⁴⁰ But what happens if voters fall short of this ideal?\n\nIrrationality and the Media\n\nPerhaps the most common reaction to evidence of the public’s systematic biases is to blame the media. Conservatives point to liberal bias in the programming. Liberals are more likely to assail the biases of the advertisers. In both cases, the model is persuasion through repetition: If people on TV repeat themselves often enough, viewers eventually believe them.⁴¹ Many successful propagandists subscribe to this model, though few are as blunt as Hitler:\n\nThe receptivity of the great masses is very limited, their intelligence is small, but their power of forgetting is enormous. In consequence of these facts, all effective propaganda must be limited to a very few points and must harp on these in slogans until the last member of the public understands what you want him to understand by your slogan.⁴²\n\nBlaming the media for biased beliefs has gut-level appeal. Journalists routinely endorse economic fallacies. Trade coverage paints imports as a cost. Business news equates jobs with prosperity, and greed with high prices and dishonesty. Blaming the media for pessimistic bias is easiest of all. As Julian Simon argues:\n\nThe only likely explanation is that newspapers and television—the main source of notions about matters which people do not experience directly—are systematically misleading the public, even if unintentionally. There is also a vicious circle here: The media carry stories about environmental scares, people become frightened, polls then reveal their worry, and the worry is then cited as support for policies to initiate action about the supposed scares, which then raise the level of public concern. The media proudly say “We do not create the ‘news.’ We are merely messengers who deliver it.” The data show that the opposite is true in this case.⁴³\n\nBut the “blame the media” hypothesis has serious flaws. First, the writings of the classical economists show that most economic biases were popular before newspapers and periodicals were widely read.⁴⁴ People are plainly able to form foolish beliefs about economics without journalists’ assistance. Second, uninformative content does not sway rational voters. They discount biased information, and do not naively swallow whatever journalists tell them—especially if they flagrantly rely on logical fallacies like “proof by repetition.” The media can therefore be no more than a catalyst for the public’s preexisting cognitive flaws.\n\nFor pseudoinformation to work as intended, voters need to be not only irrational, but irrational in the right way. The simplest of these is overconfidence in the reliability of the media. Imagine an audience puts blind, unconditional faith in Bill O’Reilly. Its gullibility allows O’Reilly to remake his audience in his own image. If he wanted to transform their faith into personal riches, he could “rent out” the support of his drones to the highest bidder.⁴⁵ O’Reilly’s influence naturally falls short of this extreme, but there is a continuum from full rationality to utter fanaticism.\n\nOverconfidence in the media can rationalize complaints about ideological bias. If viewers have faith in journalists, and most journalists are committed liberals, they have slack to pull audiences in their direction. Especially in a competitive news industry, however, the crafty approach is to move along the margins of the audience’s indifference. If there are two equally entertaining stories, but one has a more left-wing flavor, liberal media can emphasize it without hurting ratings. It may well be, moreover, that most of the entertainment value of the news comes from the charisma of the reporter, not the story. If “star power” is unequally distributed across the political spectrum—as Hollywood suggests—we should expect stories to have a liberal slant.\n\nMedia can also shape opinion if the public is overconfident in particular kinds of content, as opposed to media per se. Schumpeter fears that “Information and arguments in political matters will ‘register’ only if they link up with the citizen’s preconceived ideas.”⁴⁶ Paul Rubin makes the more specific claim that systematically biased beliefs about economics are an “innate property of the mind.” We would not originate them in solitary confinement, but they are easy for our minds to digest. Otherwise\n\nit would be relatively easy to unlearn these beliefs. There is no reason to expect that cultural errors should persist for over 200 years (about ten human generations), the time since Adam Smith first pointed out the benefits of a market economy. We have easily learned to adapt to numerous new technologies in much shorter times, when these technologies did not conflict with innate mental modules.⁴⁷\n\nMaybe we are inherently receptive to messages about bad foreigners who want to hurt us. It could be a leftover from our evolutionary past, when intergroup violence made xenophobia a lifesaver.⁴⁸ Similarly, despite his complaints about the media’s scare-mongering, Julian Simon co-indicts the minds of the audience for their pessimistic bias:\n\nWe will always find grounds for worry. Apparently it is a built-in property of our mental systems that no matter how good things become, our aspiration levels ratchet up so that our anxiety levels decline hardly at all, and we focus on ever smaller actual dangers.⁴⁹\n\nIf people are more susceptible to some messages than others, exposure to balanced media can bring out people’s “inner protectionist” or “inner pessimist.” Coverage consistent with our prejudices resonates, so even a neutral stream of messages propels us deeper into error. Left to their own devices, viewers overreact only to evidence that they personally stumble upon. If the media magically vanished, their former audience would have to search harder for reasons to fear foreigners, and might grow less antiforeign out of laziness. The news industry, no matter how balanced, stops this from happening. It ensures that the public gets a steady stream of antiforeign coverage to which it can overreact.⁵⁰ People who lack the initiative or creativity to reach misconceptions under their own steam can relax and let the media tow them there.\n\nBut if this is so, balanced journalism is the last thing to expect. Journalism is a business. If consumers prefer news that fits their prejudices, journalists have an incentive to cater to them.⁵¹ Pessimistic bias is probably the strongest example. No one spontaneously worries about Alar; it takes coverage to launch a panic.⁵² But this does not make the media an independent causal force. The media are not forcing pessimism down the public’s throats; the public lines up to get its daily dose of pessimism.⁵³ The Web offers “good news every day” at PositivePress.com, but this is no match for traditionally negative CNN.com. If the public were not predisposed to pessimism, CNN’s days would be numbered.\n\nIrrationality, Political Advertising, and Special Interests\n\nPerhaps the angriest complaint against modern democracy is the following: Special interests purchase antisocial favors from politicians; then politicians use the money to “buy elections” with abundant advertising, and the worst candidate wins. As Kuttner laments:\n\nLately, money has become newly influential in political life. As campaigns become more expensive, money tends to drive out more civic forms of participation. . . . Money-driven elections feed into a brand of politics that leaves out ordinary voters, except as objects to be manipulated by polling, focus groups, mass mailings, and paid TV spots.⁵⁴\n\nDonald Wittman objects that a rational electorate would stop this perverse process cold.⁵⁵ Rational voters would wonder how a politician raised the money to purchase airtime. If candidates get money solely by selling socially harmful favors to special interests, then advertising would backfire. The populace would reason: The more a politician spends on advertising, the more money he must have; the more money he has, the more illicit favors he must have sold. Lots of ads equal lots of corruption. If the public thought like this, no politician would advertise in the first place. Better not to advertise and be thought corrupt than to advertise and remove all doubt.\n\nWittman’s mechanism has some empirical relevance: Politicians love to “out” their rivals for accepting money from tobacco companies and other reviled donors. Furthermore, most empirical work finds weak effects of money in politics. The typical study reports little or no effect on how politicians vote, and relative to GDP, the total value of donations is small.⁵⁶\n\nStill, the strategy of responding negatively to well-funded campaigns seems artificial. Rational voters would do it, but real voters? All it takes to avoid Wittman’s curious conclusion is the right kind of irrationality. Suppose voters underestimate the strength of the link between advertising and corruption. Then selling favors to special interests to pay for commercials works as long as naive voters who think more of you outnumber sophisticated voters who think less of you.\n\nIt helps to sell the right kind of favors. Like a journalist with an ax to grind, a shrewd politician moves along the margins of voter indifference. The public is protectionist, but rarely has strong opinions about which industries need help. This is a great opportunity for a politician and a struggling industry to make a deal. Steel manufacturers could pay a politician to take (a) a popular stand against foreigners combined with (b) a not unpopular stand for American steel. In maxim form: Do what the public wants when it cares; take bids from interested parties when its doesn’t. Bear in mind, though, that the important thing is not how burdensome a concession is, but how burdensome voters perceive it to be.\n\nConclusion\n\nAfter studying irrationality on the demand side of politics, it is only human to shift hope to the supply side. Unlike voters, individuals on the supply side—whether politicians, civil servants, the media, or lobbyists—are professionals. Are they standing by to clean up the amateurs’ mess? Unfortunately, it is often more rewarding to exacerbate voter irrationality than defuse it.⁵⁷ Political expertise mainly consists in understanding what the public wants—or will want—and handing it to them. Demand for corrective pedantry is minimal. As Paul Krugman puts it, “Voters have a visceral dislike for candidates who seem intellectual, let alone try to make the electorate do arithmetic.”⁵⁸ Neither do they want politicians to tell them that their complaints about downsizing are misplaced, or watch news about the long-run benefits of flexible labor markets.\n\nExperts are not an antidote to voter irrationality. But for better and worse, they loosen the link between public opinion and policy. The electorate’s blind spots open loopholes for politicians, bureaucrats, and the media to exploit. But if the public was working against its own interests in the first place, the welfare effect of “exploitation” is ambiguous.\n\nFaith in leaders is the clearest example. Its dangers are obvious—picture a charismatic sociopath, or a “rally round the flag” effect that reelects an incompetent incumbent. But political faith also allows leaders—if they are so inclined—to circumvent their supporters’ misconceptions. Faith creates slack, and slack in the right hands leads to better outcomes. All you need are leaders who are somewhat well intentioned and less irrational than their followers. Since leaders are well educated, and education dilutes sympathy for popular misconceptions, at least the second condition is not hard to satisfy.\n\nBureaucracy also has mixed effects. If the public lets them, politicians pass the buck, blaming their mistakes and misdeeds on subordinates. Before we condemn buck-passing, however, we should remember how many good ideas and socially beneficial actions the public classifies as “mistakes” and “misdeeds.”\n\nLast, consider propaganda. We tend to think that causes twist the facts and appeal to emotions when truth is not on their side. Nazism and Communism are obvious examples. But in theory, propaganda can be used to fight error as well. If a person clings to his mistakes despite the evidence, irrational persuasion is his only hope.\n\nOn balance, most economists underestimate the dangers of the supply side of politics, but orthodox provoter critics of democracy overestimate them. Economists correctly reason that as long as the general public is rational, the best servants of voter interests win elections. This makes economists reluctant to recognize political phenomena like blind faith, buck-passing, or propaganda. “Blind faith” becomes “reputation,” “buck-passing” becomes “agency costs,” and “propaganda” becomes “information.” If voters fall short of full rationality, however, these concerns can no longer be dismissed.\n\nNoneconomists, in contrast, are too quick to pin democracies’ failings on suppliers. Supply-side problems usually need voter irrationality to get off the ground, and if you acknowledge voters’ irrationality, you weaken the presumption against thwarting their will. If a principal does not know his own interests, his agent’s shirking may benefit principal and agent alike. Supply-side chicanery is only unambiguously harmful given conditions—full voter rationality—under which it does not arise.\n\nChapter 8\n\n“MARKET FUNDAMENTALISM” VERSUS THE\n\nRELIGION OF DEMOCRACY\n\nThe trouble with the world is that the stupid are cocksure\nand the intelligent are full of doubt.\n\n—Bertrand Russell¹\n\nECONOMISTS perennially debate each other about how well the free market works. They have to step outside their profession to remember how much—underneath it all—they agree.² For economists, greedy intentions establish no presumption of social harm. Indeed, their rule of thumb is to figure out who could get rich by solving a problem—and start worrying if no one comes to mind. Most noneconomists find this whole approach distasteful, even offensive. Disputes between economists are quibbles by comparison.\n\nOut of all their contrarian views, nothing about economists aggravates other intellectuals more than their sympathy for markets. As Melvin Reder aptly states, comprehension of mainstream economics “tends to generate appreciation of the merits of laissez-faire even when that appreciation does not extend to acceptance.”³ Left to their own devices, “normal” intellectuals could spend their careers cataloging human greed and the evils that flow from it. But economists stand in their midst, a fifth column, using their mental gifts to defend the enemy.\n\nThe hostility that economists provoke is evident from all the name-calling. Karl Marx, the classic poison pen, accused Ricardo and his fellow classical economists of “miserable sophistry,” of suffering from “the obsession that bourgeois production is production as such, just like a man who believes in a particular religion and sees it as the religion, and everything outside of it only as false religions.” For Marx, economists are apologists for the bourgeoisie, who “set up that single, unconscionable freedom—Free Trade” and replaced the feudal era’s “exploitation veiled by religious and political illusions” with “naked, shameless, direct, brutal exploitation.”⁴ Rosa Luxemburg, in her essay “What is Economics?” proclaims with disgust that\n\nThe bourgeois professors serve up a tasteless stew made from the leftovers of a hodge-podge of scientific notions and intentional circumlocutions—not intending to explore the real tendencies of capitalism, at all. On the contrary, they try only to send up a smoke screen for the purpose of defending capitalism as the best of all possible orders, and the only possible one.⁵\n\nModern detractors continue to oscillate between calling economists hired intellectual guns of the rich and a coven of conservative ideologues. But the more sophisticated critics protest that they object to certain brands of economics, not the whole field. For instance, Robert Kuttner’s “quarrel is with a utopian—really, a dystopian—view of markets, not with economists as a breed.”⁶ But he takes back with one hand what he gives with the other, accusing “self-described liberal” economists of “dismantling much of the case for a mixed economy.” If liberal Democratic economists are beyond the pale, who is not?\n\nThe Charge of Market Fundamentalism\n\n“Market fundamentalism” is probably the most popular insult against economics these days. The world listened when billionaire George Soros declared that “Market fundamentalism . . . has rendered the global capitalist system unsound and unsustainable.”⁷ Robert Kuttner has a handy summary of what market fundamentalism amounts to:\n\nThere is at the core of the celebration of markets a relentless tautology. If we begin, by assumption, with the premise that nearly everything can be understood as a market and that markets optimize outcomes, then everything comes back to the same conclusion—marketize! If, in the event, a particular market doesn’t optimize, there is only one possible inference: it must be insufficiently marketlike.⁸\n\nHe insists, moreover, that this fault is not limited to a right-wing fringe: “Today, the only difference between the utopian version and the mainstream version is degree.” Indeed, “As economics has become more fundamentalist, the most extreme version of the market model has carried the greatest political, intellectual, and professional weight.”⁹ Even worse, economists’ fundamentalism overflows into the policy arena:\n\nAmerican liberals and European social democrats often seem unable to offer more than a milder version of the conservative program—deregulation, privatization, globalization, fiscal discipline, but at a less zealous extreme. Few have been willing to challenge the premise that nearly everything should revert to a market.¹⁰\n\nJoseph Stiglitz joins the chorus against market fundamentalism, happily discarding the guarded professorial prose of his Nobel prize-winning research:\n\nThe discontent with globalization arises not just from economics seeming to be pushed over everything else, but because a particular view of economics—market fundamentalism—is pushed over all other views. Opposition to globalization in many parts of the world is not to globalization per se . . . but to the particular set of doctrines, the Washington Consensus policies that the international financial institutions have imposed.¹¹\n\nMarket fundamentalism is a harsh accusation. Christian fundamentalists are notorious for their strict biblical literalism, their unlimited willingness to ignore or twist the facts of geology and biology to match their prejudices. For the analogy to be apt, the typical economist would have to believe in the superiority of markets virtually without exception, regardless of the evidence, and dissenters would have to fear excommunication.\n\nFrom this standpoint, the charge of “market fundamentalism” is silly, failing even as a caricature. If you ask the typical economist to name areas where markets work poorly, he gives you a list on the spot: Public goods, externalities, monopoly, imperfect information, and so on. More importantly, almost everything on the list can be traced back to other economists. Market failure is not a concept that has been forced upon a reluctant economics profession from the outside. It is an internal outgrowth of economists’ self-criticism. After stating that markets usually work well, economists feel an urge to identify important counterexamples. Far from facing excommunication for sin against the sanctity of the market, discoverers of novel market failures reap professional rewards. Flip through the leading journals. A high fraction of their articles present theoretical or empirical evidence of market failure.\n\nTrue market fundamentalists in the economics profession are few and far between. Not only are they absent from the center of the profession; they are rare at the “right-wing” extreme. Milton Friedman, a legendary libertarian, makes numerous exceptions, on everything from money to welfare to antitrust:\n\nOur principles offer no hard and fast line how far it is appropriate to use government to accomplish jointly what is difficult or impossible for us to accomplish separately through strictly voluntary exchange. In any particular case of proposed intervention, we must make up a balance sheet, listing separately the advantages and disadvantages.¹²\n\nWhen Friedman prefers laissez-faire, he often openly acknowledges its defects. He has no quasi-religious need to defend the impeccability of the free market. For example, his discussion of natural monopoly states:\n\n[T]here are only three alternatives that seem available: private monopoly, public monopoly, or public regulation. All three are bad so we must choose among evils. . . . I reluctantly conclude that, if tolerable, private monopoly may be the least of the evils.¹³\n\nFriedman is far more market-friendly than the average economist. But a “market fundamentalist”? Hardly. He recognizes numerous cases where market performance is poor, and does not excommunicate less promarket colleagues for heresy.\n\nIf neither the typical economist nor Milton Friedman himself qualifies as a market fundamentalist, who does? The only plausible candidates are the followers of Ludwig von Mises and especially his student Murray Rothbard. The latter does seem to categorically reject the notion of suboptimal market performance:\n\nSuch a view completely misconceives the way in which economic science asserts that free-market action is ever optimal. It is optimal, not from the personal ethical views of an economist, but from the standpoint of the free, voluntary actions of all participants and in satisfying the freely expressed needs of the consumers. Government interference, therefore, will necessarily and always move away from such an optimum.¹⁴\n\nBoth Mises and Rothbard have passed away, but their outlook—including Ph.D.s who subscribe to it—lives on in the Ludwig von Mises Institute. But groups like these have basically given up on mainstream economics; members mostly talk to each other and publish in their own journals. The closest thing to market fundamentalists are not merely outside the mainstream of the economics profession. They are way outside.\n\nPopular accusations of market fundamentalism are plain wrong. Yes, economists think that the market works better than other people admit. But they acknowledge exceptions to the rule. The range of these exceptions changes as new evidence comes in. And it is usually economists themselves who discover the exceptions in the first place.\n\nDemocratic Fundamentalism\n\nIn wide areas of life majorities are entitled to rule, if\nthey wish, simply because they are majorities.\n\n—Robert Bork, The Tempting of America¹⁵\n\nThe disparity between economists’ open-mindedness and the charge of market fundamentalism is so vast that it is hard not to speculate about the motives behind it. I sense a strong element of projection: accusing others of the cognitive misdeeds one commits oneself. Take “creation scientists.” Faculty and researchers of the Institute for Creation Research follow a party line: “The scriptures, both Old and New Testaments, are inerrant in relation to any subject with which they deal, and are to be accepted in their normal and intended sense.”¹⁶ You can hardly get less scientific. Yet a standard debating tactic of creation scientists is to insist that “evolutionary theory, along with its bedfellow, secular humanism, is really a religion.”¹⁷ Creationists’ attacks on the objectivity of mainstream evolutionists seem to stem from their sense of scientific inferiority to their opponents.\n\nSimilarly, the most vocal opponents of “market fundamentalism” are themselves often believers in what can accurately be called “democratic fundamentalism.” Its purest expression is the cliché, attributed to failed 1928 presidential candidate Al Smith, that “All the ills of democracy can be cured by more democracy.”¹⁸ In other words, no matter what happens, the case for democracy remains untouched. Victor Kamber has a book called Giving Up on Democracy.¹⁹ The title’s rhetorical power stems from the widespread belief that democracy has to be the answer. You can complain about democracy, but you cannot “give up” on it. Indeed, many admire its flaws. As Adam Michnik exclaims, “Democracy is gray,” but “Gray is beautiful!”²⁰\n\nA person who said, “All the ills of markets can be cured by more markets” would be lampooned as the worst sort of market fundamentalist. Why the double standard? Because unlike market fundamentalism, democratic fundamentalism is widespread. In polite company, you can make fun of the worshippers of Zeus, but not Christians or Jews. Similarly, it is socially acceptable to make fun of market fundamentalism, but not democratic fundamentalism, because market fundamentalists are scarce, and democratic fundamentalists are all around us.\n\nEveryone from journalists and politicians to empirical social scientists and academic philosophers is willing to publicly profess his democratic fundamentalism without embarrassment. At the end of a book cataloging his decades of disappointment with American politics, William Greider still cheerfully writes:\n\nAfter thirty years of working as a reporter, I am steeped in disappointing facts about self-government. Having observed politics from the small-town courthouse to the loftiest reaches of the federal establishment, I know quite a lot about duplicitous politicians and feckless bureaucracies, about gullible voters and citizens who are mean-spirited cranks. These experiences, strangely enough, have not undermined my childhood faith in democratic possibilities, but rather tended to confirm it.²¹\n\nWhat—if anything—would undermine Greider’s “childhood faith”? The post-1992 political direction was probably not a dramatic improvement in his eyes. But you can bet that his faith is as vibrant as ever. If an economist waxed poetic about his childhood faith in the free market, he would be tagged a market fundamentalist, and his credibility would plummet.\n\nPerhaps we should expect no better of journalists, however talented their writing. But one would hope that empirical social scientists would strive harder for objectivity, or at least feel social pressure to keep their faith to themselves. Yet democratic fundamentalism is not hard to find there either. To take only one example, Pranab Bardhan rigorously analyzes the causal relationship between democracy and development.²² But before he gets down to business, Bardhan not only virtually admits to democratic fundamentalism, but presumes his readers to be democratic fundamentalists too! “Most of us, ardent democrats all, would like to believe that democracy is not merely good in itself, it is also valuable in enhancing the process of development.” Unfortunately, the empirical literature testing this claim is “rather unhelpful and unpersuasive. It is unhelpful because it usually does not confirm a causal process and the results go every which way.” Despite the shortage of empirical support, Bardhan gratuitously ends with an affirmation of faith: “I remain an incorrigible optimist for the long-run healing powers of democracy.”²³ How many scholars would survey an expansive literature on market performance, admit that the evidence is too mixed to draw any conclusion, then speak of the “long-run healing powers of capitalism”? They would be too embarrassed—and should be.\n\nDemocratic fundamentalism is also evident in analytic philosophy, legendary for its guarded skepticism. Normative political theorist Ian Shapiro is a prime example. He objects to the notion of “some ‘bird’s-eye’ standpoint, existing previously to and independently of democratic procedures, by reference to which we can evaluate the outcomes they produce.”²⁴ In plain language, democracy is right by definition, for there is no extra-democratic standard of right and wrong.\n\nThis is an admittedly uncharitable reading. Like most philosophers, Shapiro quickly qualifies his position, affirming that political principles must be defended on “consequentialist grounds.” But he then qualifies his qualification, leaving his democratic fundamentalism intact. “The difficulty then becomes that the desirability of the consequences in question is debatable, suggesting that they should have to vie for support with other values and policies. Like it or not, democracy rears its head in the very definition of justice.”²⁵ This is one of the baldest rigged juries in the history of philosophy: Democracy must be judged by its consequences, but the only way to judge its consequences is by a vote!\n\nLest someone dare to assert that the consequences of a policy are not “debatable,” Shapiro elsewhere rules out the possibility. Highly technical matters might be beyond debate, but not questions of substantive democratic interest:\n\nIn certain (though not all) circumstances one can reasonably act on the advice of an airplane pilot, an auto mechanic, an architect, or a physician without understanding its rationale or even being interested in it. But the idea that there is an analogous political expertise reasonably prompts suspicion.²⁶\n\nWhy?\n\nMost minimally, the suggestion that there is political expertise is suspect because there are few reasons to believe that there is in fact much of it. What is typically billed as knowledge about the world of politics seems so meager, and is so regularly undermined by events, that people who set themselves up as political experts often give off the whiff of snake oil.²⁷\n\nBy now, sweeping rejections of expert opinion should be painfully familiar, but it is still odd for a noted political expert to belittle the idea of political expertise. If Shapiro does not consider himself an expert, why does he bother writing books? Anyone who grades final exams in political science courses has seen for himself that disparities in political knowledge are real and large. If that is not good enough, there is plenty of empirical evidence about political knowledge, none of which Shapiro bothers to challenge.²⁸\n\nBut isn’t he right about the experts being “regularly undermined by events”? It depends on how strictly you grade them. If the “experts” are less than impressive, try comparing them to laymen. Moreover, much of the experts’ bad press can be explained by selection: Sensible experts and questions with well-established answers get less coverage than cranks and controversy.\n\nShapiro is slightly more hesitant to make a sweeping dismissal of economics. But democratic fundamentalism triumphs in the end:\n\nIt would be foolish not to recognize that economists, for instance, often have esoteric knowledge (perhaps less than they think they have) about the workings of the economy that is relevant to democratic deliberation about it. But because decisions about the limits of the market sphere and the structure of its governance are linked to the controversial exercise of power, they are inescapably political; thus economic policy making should never be ceded to professional economists. They must persuade lay representatives, in nontechnical terms, if we are to be bound by their advice.²⁹\n\nPerversely, then, the more irrational the electorate is, the less of a say economists have. If a lay audience will listen to reason, economists wield some influence. But a stubbornly wrongheaded lay audience is entitled to do whatever it likes: “Economic policy making should never be ceded to professional economists.”³⁰ If this is not democratic fundamentalism, what is?\n\nIn his research on “sacred values,” psychologist Philip Tetlock observes that “people often insist with apparently great conviction that certain relationships and commitments are sacred and that even to contemplate trade-offs with the secular values of money or convenience is anathema.”³¹ In the modern world, democracy is one of the best examples; the faithful equate minor deviations with total apostasy, and condemn sinful thoughts as harshly as wicked deeds.\n\nA standard rhetorical tactic is to equate modest reductions in the role of government with the elimination of government regulation altogether. Robert Kuttner tells us that “in the emblematic case of airline regulation, what began under President Carter as ‘regulatory reform’ quickly evolved into a drive for complete deregulation.”³² Apparently, the Federal Aviation Administration’s continuing regulation of safety does not count. A similar ploy is to equate mere talk of cutting government with doing it. Richard Leone of the Twentieth Century Fund alleges that “faith in idealized market structures also has spawned a political jihad intent upon stripping away the community and government safeguards against market abuses and imperfections. . . . Democrats and moderate Republicans are stumbling all over each other to prove their conversion to the one true faith of laissez-faire economics.”³³ Strangely, the laissez-faire jihad failed to push federal spending as a percentage of GDP below 18%—and most of the decline during the 1990s clearly stemmed from the end of the Cold War.³⁴\n\nIn the end, apologists for democracy often fall back on Winston Churchill’s slogan, “Democracy is the worst form of government, except all those other forms that have been tried from time to time.”³⁵ On the surface, this sounds like mature realism, not democratic fundamentalism. But Churchill’s maxim is an all-or-nothing rhetorical trick. Imagine if an economist dismissed complaints about the free market by snapping: “The free market is the worst form of economic organization, except all the others.” This is a fine objection to communism, but only a market fundamentalist would buy it as an argument against moderate government intervention. Churchill’s slogan is every bit as weak. Just because dictatorship is disastrous, it hardly follows that democracy must have free rein. Like markets, democracy can be limited, regulated, or overruled. Contramajoritarian procedures like judicial review can operate alongside democratic ones. Supermajority rules allow minorities to thwart the will of the majority. Twisting a marginal trade-off into a binary choice is fundamentalism trying to sound reasonable.\n\nWill the Real Fundamentalism Please Stand Up? The Case of the Policy Analysis Market\n\nA major story broke on July 28, 2003.³⁶ Senators Ron Wyden and Byron Dorgan demanded that the Department of Defense end funding for an obscure program, the Policy Analysis Market (henceforth PAM). Still in its preliminary stage, the program’s aim was to create online betting markets for questions of national security. PAM traders could profit by—among other things—correctly predicting the number of Western terror casualties. Critics quickly labeled it the “Terror Market” scheme. Wyden and Dorgan condemned it without reservation:\n\nSpending taxpayer dollars to create terrorism betting parlors is as wasteful as it is repugnant. The American people want the Federal government to use its resources enhancing our security, not gambling on it.³⁷\n\nTelevision and newspaper coverage was almost entirely unfavorable—and so was public opinion. Could the PAM’s backers be too blind to see that it gave a financial incentive for terrorism? Was there any more egregious case of market fundamentalism? The Secretary of Defense killed the program on July 29—one day after the publicity began. John Poindexter, head of the Information Awareness Office, had to offer his resignation the next day. After two months, all funding for the office was terminated. So much for bureaucratic inertia.\n\nThen a funny thing happened. Other media—published less frequently and aimed at more sophisticated audiences—followed up on the Terror Market story. They delved into the rationale of the project, and talked to its creators about possible flaws in its design. Several lessons emerged.³⁸\n\nFirst, there is a large body of empirical evidence on the predictive accuracy of speculative markets, on everything from horse-racing to elections to invasions. “Put your money where your mouth is” turns out to be a great way to get the well informed to reveal what they know, and the poorly informed to quiet down. No system is perfect, but betting markets outperform other methods of prediction in a wide variety of circumstances. The PAM was inspired not by ivory tower theorizing, but by the proven success of betting markets in other areas.\n\nSecond, the amount of money on the PAM table was very small. Individual bets were limited to a few tens of dollars. The idea that these paltry sums would motivate additional terrorism is ludicrous. Terrorists who wanted to profit from their attacks could make a lot more money by manipulating normal financial markets—shorting airline stocks and such. Incidentally, the 9/11 Commission found that did not happen either.³⁹\n\nThird, the program was shut down so quickly that there was no time to verify the accusations. According to Robin Hanson, my colleague and one of the brains behind the PAM, “During that crucial day, no one from the government asked the PAM team if the accusations were correct, or if the more offending aspects could be cut from the project.”⁴⁰ The creators had anticipated and already addressed the obvious objections, but opponents were too inflamed to listen. Constructive criticism was in short supply, to say the least; the goal was to kill the program, not improve it.\n\nLast, the PAM experience raised a dilemma for those who embrace the “wisdom of crowds.” Surowiecki forcefully defends the merits of decision markets like the PAM. But he also affirms that “there’s no reason to believe that crowds would be wise in most situations but suddenly become doltish in the political arena.” As long as there is a right answer, “Democracy’s chances of adopting good policies are high.”⁴¹ How then can Surowiecki account for the public’s extreme hostility to the PAM? If decision markets and democracy both work well, the PAM should be popular.⁴²\n\nIf the critics studied the PAM more thoroughly, they would have been angrier still. A key feature was the ability to make conditional bets. You could wager, for example, on the number of Western terrorist casualties if the United States invades Iraq, and the number if it does not. Comparing the price of those two bets would reveal whether the market thinks an invasion will make us more or less safe from terrorist attacks. In short, betting markets could second-guess not only political leaders, but public opinion itself. This is bound to rub democratic fundamentalists the wrong way.\n\nOverall, the creators of the PAM were far from market fundamentalists. They built on a solid body of evidence, thought carefully about potential problems, and were open to criticism. Their plan was to test the program out on a small scale, work out the bugs, and gradually expand it.\n\nAlmost the opposite holds for opponents. They did not question the track record of predictive betting markets. Apparently, they knew nothing about it and did not care to learn. Despite the obvious failures of traditional intelligence in recent years, they were convinced that the best policy was more of the same. Listen to Wyden and Dorgan:\n\nThe example that you provide in your report would let participants gamble on the question, “Will terrorists attack Israel with bioweapons in the next year?” Surely, such a threat should be met with intelligence gathering of the highest quality—not by putting the question to individuals betting on an Internet website.⁴³\n\nSurely? How do they know? At minimum, the PAM would have raced betting markets against old-fashioned intelligence gathering. But democratic fundamentalists did not want to put their antimarket dogma to the test.\n\nPrivate Choice as an Alternative to Democracy and Dictatorship\n\nUndemocratic politics is not the only alternative to democratic politics. Many areas of life stand outside the realm of politics, of “collective choice.” When the law is silent, decisions are “up to the individual” or “left to the market.” If the term were not preempted, private choice could be called “the Third Way,” the alternative to both democracy and dictatorship.\n\nFor most of human history, religion was a state responsibility. The idea that government could have no established religion was inconceivable. All that has changed; now individuals decide which religion, if any, to practice. Verbal gymnastics notwithstanding, this depoliticization is undemocratic. The majority now has as little say about my religion as it would under a dictatorship; in both cases, the law ignores public opinion. Before the 1930s, similarly, many areas of U.S. economic life were undemocratically shielded from federal and state regulation.⁴⁴ The market periodically trumped democracy, on everything from the minimum wage to the National Recovery Administration. And unless you are a democratic fundamentalist, you have to be open to the possibility that this was all for the good.\n\nFervent partisans of democracy often grant that democracy and the market are substitutes. As Kuttner puts it, “The democratic state remains the prime counterweight to the market.”⁴⁵ Their complaint is that the public has less and less say over its destiny because corporations have more and more say over theirs. To “save democracy,” the people must reassert its authority.\n\nFair enough. Though their opponents greatly overstate the extent of privatization and deregulation, these policies take decisions out of the hands of majorities and put them into the hands of business owners. But the critics rarely wonder if this transfer might be desirable. They treat less reliance on democracy as automatically objectionable.\n\nThis is another symptom of democratic fundamentalism. If all that an economist had to say against a government program were, “That’s government intervention. Government is supplanting markets!” he would be pigeonholed, then marginalized, as a market fundamentalist. But when an equally simplistic cry goes up in the name of democracy, there is a sympathetic audience. It is logically possible that clear-eyed business greed makes better decisions than confused voter altruism. Why not at least compare their performance, instead of prejudging?\n\nThe complaint that we are “losing democracy” is especially weak when we bear in mind that this is not a binary choice between unlimited democracy and pure laissez-faire. Just because some democracy is beneficial or necessary, it scarcely follows that we should not have less. Consider deregulation of the television and radio spectrum. Democratic fundamentalists find the idea offensive because it ends democratic oversight.⁴⁶ But it is hard to see the value of democracy in the entertainment industry. Premium networks like HBO demonstrate that the profit motive, uninhibited by majority preferences, is a recipe for high-quality, creative programming. Democratic fundamentalism holds back the rest of the industry.\n\nMost democratic enthusiasts recognize that free markets are a substitute—albeit a self-evidently undesirable one—for democracy. A few take the more extreme position that the notion of depoliticized choice is incoherent.⁴⁷ This position is best expressed in the work of Ian Shapiro, who criticizes the “implausible notion that a scheme of collective action is an alternative to a scheme of private action.”⁴⁸ “Were it possible somehow for society to ‘not undertake’ collective action,” defects in collective decision-making “might amount to a prima facie argument against all collective action.”⁴⁹ But in fact, private action is “parasitic” on collective action:\n\nThe institutions of private property, contract, and public monopoly of coercive force . . . were created and are sustained by the state, partly financed by implicit taxes on those who would prefer an alternative system. The real question, for democrats, is not “whether or not collective action?” but whether or not democratic modes of managing it are superior to the going alternatives.⁵⁰\n\nThis argument is seriously flawed.\n\nFirst, even if private action presupposes the existence of collective action, it remains feasible to eschew collective action in some or most areas. Just because a doctor’s treatment keeps you alive hardly shows that you have to grant him absolute authority over your whole life. You can heed his advice if your survival depends on it, and otherwise do as you please. Similarly, suppose we grant that private action is a parasite on the body of government. It does not follow that the host must have final say across the board. Indeed, a presumption against collective action is compatible with the view that private action depends upon government: What better reason could there be to overrule the presumption than that private action could not otherwise survive?\n\nSecond, Shapiro’s argument can be readily reversed. Collective decision-making is “parasitic” on the wealth created by the market economy. It would be hard to have an orderly vote if businesses had not fed, clothed, housed, and transported the electorate and candidates. Does this reveal an internal contradiction in every regulation? Hardly.\n\nLast, it is not true that private action is inherently parasitic or dependent upon collective action. The existence of the black market proves that property rights and contracts are possible without government approval. That is why one drug dealer can meaningfully tell another, “You stole my crack” or, “We had a deal.” Indeed, the black market shows not only that property and contract can persist without the government’s support, but that they can survive in the face of its determined resistance.\n\nContrary to naysayers, there is no conceptual flaw in prescriptions to rely more on private choice and less on collective choice. The proposal is quite intelligible. In fact, the counterarguments are so weak that their popularity seems to be another symptom of democratic fundamentalism. People want to rule alternatives to democracy out of court, to avoid putting their faith to the test.\n\nVoter Irrationality, Markets, and Democracy\n\nCritics of the economics profession are right about one thing. Economists really do subscribe to a long list of views that are unpopular, even offensive. Perhaps most offensive is economists’ judgment that markets work considerably better than the general public thinks. That judgment is the foundation of economists’ promarket outlook, the so-called Washington Consensus.\n\nWhile this book has debunked the main efforts to undermine the objectivity of the economics profession, it adds little to the debate on the virtues of markets. My book weighs on the other side of the scales. The optimal mix between markets and government depends not on the absolute virtues of markets, but on their virtues compared to those of government. No matter how well you think markets work, it makes sense to rely on markets more when you grow more pessimistic about democracy. If you use two car mechanics and discover that mechanic A drinks on the job, the natural response is to shift some of your business over to mechanic B, whatever your preexisting complaints about B.\n\nShould my book push you toward democratic pessimism? Yes. Above all, I emphasize that voters are irrational. But I also accept two views common among democratic enthusiasts: That voters are largely unselfish, and politicians usually comply with public opinion. Counterintuitively, this threefold combination—irrational cognition, selfless motivation, and modest slack—is “as bad as it gets.”⁵¹\n\nIf public opinion is sensible, selfishness and slack prevent democracy from fulfilling its full promise. But if public opinion is senseless, selfishness and slack prevent democracy from carrying out its full threat. Selfishness and slack are like water rather than poison. They are not intrinsically injurious; they dilute the properties of the systems they affect. Thus, when the public systematically misunderstands how to maximize social welfare—as it often does—it ignites a quick-burning fuse attached to correspondingly misguided policies. This should make almost anyone more pessimistic about democracy.\n\nThe striking implication is that even economists, widely charged with market fundamentalism, should be more promarket than they already are. What economists currently see as the optimal balance between markets and government rests upon an overestimate of the virtues of democracy. In many cases, economists should embrace the free market in spite of its defects, because it still outshines the democratic alternative.\n\nConsider the insurance market failure known as “adverse selection.” If people who want insurance know their own riskiness, but insurers only know average riskiness, the market tends to shrink. Low-risk people drop out, which raises consumers’ average riskiness, which raises prices, which leads more low-risk customers to drop out.⁵² In the worst-case scenario, the market “unravels.” Prices get so high that no one buys insurance, and consumers get so risky that firms cannot afford to sell for less.\n\nEconomists often take the presence of adverse selection as a solid reason to deviate from their laissez-faire presumption.⁵³ But given the way that democracy really works, the shift in presumption is premature. Given public opinion, what kind of regulation is democracy likely to implement? The essence of the adverse selection problem is that insurers do not know enough to charge the riskiest consumers the highest premiums. But how would a person with antimarket bias see things? The last thought on his mind would be, “If only insurance companies could identify the riskiest consumers and charge them accordingly.” Reflected in the fun-house mirror of antimarket bias, the “obvious” problem to fix is higher rates for riskier people, not the imperfect match between risks and rates.\n\nThe fact that regulation could help correct the adverse selection problem—for example, by making everyone buy insurance—is therefore a weak argument for regulation. Given the public’s antimarket bias, democracy will probably force companies to charge high-risk clients the same as everyone else. The basic economics of insurance tells us that this makes the adverse selection problem worse by encouraging low-risk consumers to opt out. But basic economics is what the public refuses to accept. It does not take a market fundamentalist to recognize that it may be prudent to muddle through with the imperfections of the free market, instead of asking the electorate for its opinion.\n\nEven among economists, market-oriented policy prescriptions are often seen as too dogmatic, too unwilling to take the flaws of the free market into account.⁵⁴ Many prefer a more “sophisticated” position: Since we have already belabored the advantages of markets, let us not forget to emphasize the benefits of government intervention. I claim that the qualification needs qualification: Before we emphasize the benefits of government intervention, let us distinguish intervention designed by a well-intentioned economist from intervention that appeals to noneconomists, and reflect that the latter predominate. You do not have to be dogmatic to take a staunchly promarket position. You just have to notice that the “sophisticated” emphasis on the benefits of intervention mistakes theoretical possibility for empirical likelihood.\n\nIn the 1970s, the Chicago school became notorious for its “markets good, government bad” outlook. One could interpret my work as an attempt to revive that tradition. Many of its arguments were flawed, even contradictory. If people were as uniformly rational as Chicago economists assumed, government policy could not stay bad for long. George Stigler eventually pulled the rug out from under Milton Friedman by saying so.⁵⁵ But flawed arguments can still lead to a true conclusion; Stigler was a better logician, but Friedman had greater insight. Placed on a foundation of rational irrationality, perhaps the Chicago research program that Friedman inspired can live again.\n\nCorrecting Democracy?\n\nThe main upshot of my analysis of democracy is that it is a good idea to rely more on private choice and the free market. But what—if anything—can be done to improve outcomes, taking the supremacy of democracy over the market as fixed? The answer depends on how flexibly you define “democracy.” Would we still have a “democracy” if you needed to pass a test of economic literacy to vote? If you needed a college degree? Both of these measures raise the economic understanding of the median voter, leading to more sensible policies. Franchise restrictions were historically used for discriminatory ends, but that hardly implies that they should never be used again for any reason. A test of voter competence is no more objectionable than a driving test. Both bad driving and bad voting are dangerous not merely to the individual who practices them, but to innocent bystanders. As Frédéric Bastiat argues, “The right to suffrage rests on the presumption of capacity”:\n\nAnd why is incapacity a cause of exclusion? Because it is not the voter alone who must bear the consequences of his vote; because each vote involves and affects the whole community; because the community clearly has the right to require some guarantee as to the acts on which its welfare and existence depend.⁵⁶\n\nA more palatable way to raise the economic literacy of the median voter is by giving extra votes to individuals or groups with greater economic literacy. Remarkably, until the passage of the Representation of the People Act of 1949, Britain retained plural voting for graduates of elite universities and business owners. As Speck explains, “Graduates had been able to vote for candidates in twelve universities in addition to those in their own constituencies, and businessmen with premises in a constituency other than their own domicile could vote in both.”⁵⁷ Since more educated voters think more like economists, there is much to be said for such weighting schemes. I leave it to the reader to decide whether 1948 Britain counts as a democracy.\n\nA moderate reform suggested by my analysis is to reduce or eliminate efforts to increase voter turnout. Education and age are the two best predictors of turnout. Since the former is the strongest predictor of economic literacy, and the latter has little connection with it, the median voter’s economic literacy exceeds the median citizen’s. If “get out the vote” campaigns led to 100% participation, politicians would have to compete for the affection of noticeably more biased voters than they do today.⁵⁸\n\nMost worries about de jure or de facto changes in participation take the empirically discredited self-interested voter hypothesis for granted.⁵⁹ If voters’ goal were to promote their individual interests, nonvoters would be sitting ducks. People entitled to vote would intelligently select policies to help themselves, ignoring the interests of everyone else. There is so much evidence against the SIVH, however, that these fears can be discounted. The voters who know the most do not want to expropriate their less clear-headed countrymen. Like other voters, their goal is, by and large, to maximize social welfare. They just happen to know more about how to do it.\n\nSince well-educated people are better voters, another tempting way to improve democracy is to give voters more education. Maybe it would work. But it would be expensive, and as mentioned in the previous chapter, education may be a proxy for intelligence or curiosity. A cheaper strategy, and one where a causal effect is more credible, is changing the curriculum. Steven Pinker argues that schools should try to “provide students with the cognitive skills that are most important for grasping the modern world and that are most unlike the cognitive tools they are born with,” by emphasizing “economics, evolutionary biology, and probability and statistics.”⁶⁰ Pinker essentially wants to give schools a new mission: rooting out the biased beliefs that students arrive with, especially beliefs that impinge on government policy.⁶¹ What should be cut to make room for the new material?\n\nThere are only twenty-four hours in a day, and a decision to teach one subject is also a decision not to teach another one. The question is not whether trigonometry is important, but whether it is more important than statistics; not whether an educated person should know the classics, but whether it is more important for an educated person to know the classics than elementary economics.⁶²\n\nLast but not least on the list of ways to make democracy work better is for economically literate individuals who enjoy some political slack to take advantage of it to improve policy.⁶³ If you work at a regulatory bureau, draft legislation, advise politicians, or hold office, figure out how much latitude you possess, and use it to make policy better. Subvert bad ideas, and lend a helping hand to good ones. As Ronald Coase says, “An economist who, by his efforts, is able to postpone by a week a government program which wastes $100 million a year . . . has, by his action, earned his salary for the whole of his life.”⁶⁴ As Bastiat emphasizes, the voter who acts on his biased judgments is not just hurting himself. If you employ your political wiggle room to improve policy, you are doing your part to tame a public nuisance.\n\nEconomics: What Is It Good For?\n\nOur primary mission should be to vaccinate the minds of\nour undergraduates against the misconceptions that are\nso predominant in what passes for educated discussion\nabout international trade.\n\n—Paul Krugman, “What Do Undergrads\nNeed to Know About Trade?”⁶⁵\n\nMost of the preceding remedies suffer from a catch-22. Once you use up your political slack, the only way to curtail the political influence of the economically illiterate to is convince them it is a good idea. However, if you were persuasive enough to do that, you could “cut out the middleman” and directly convince them to start voting more sensibly. Persuasive resources are scarce. Is there anything that can be done, holding constant the persuasive resources of the economics discipline and “allied forces”?⁶⁶ Is there any way to make better use of their time? I believe there is.\n\nEconomists have a reputation for being unwilling to give definite answers and unable to reach a consensus. Harry Truman famously longed for a “one-handed economist,” who could not say “on the one hand, on the other hand.” Paul Samuelson added, “According to legend, economists are supposed never to agree among themselves. If Parliament were to ask six economists for an opinion, seven answers would come back—two, no doubt, from the volatile Mr. Keynes!”⁶⁷\n\nBoth economists and their detractors know these stereotypes are dead wrong. But for once, however, economists themselves are largely to blame for the misunderstanding. When economists choose between communicating (a) nothing, or (b) simplified but roughly accurate conclusions, they strangely seem to prefer (a). When you have an entire semester with a group of students, they forget all but the main points. If you fail to hammer a few fundamental principles into your students, odds are they will take away nothing at all. Yet in the dozens of economics courses I have taken, the professors rarely took their constraint seriously. Many preferred to dwell on the details of national income accounting, or mathematical subtleties, or the latest academic fad.\n\nI know from experience that professors have an enormous amount of slack. They can drastically change the content and style of their courses at low cost. So to the question, “How can teachers of economics make better use of their time?” I answer that they should strive to channel the spirit of the original one-handed economist, Frédéric Bastiat.\n\nIt makes no difference if “teacher of economics” is your official job description. Everyone who knows some economics—professors, policy wonks, journalists, students, and concerned citizens—has opportunities to teach. Each of us should begin, like Bastiat, by contrasting the popular view of a topic with the economic view. Make it obvious that economists think one thing and noneconomists think something else. Select a few conclusions with profound policy implications—like comparative advantage, the effect of price controls, and the long-run benefits of labor-saving innovation—and exhaust them. As Bastiat advises, “We must . . . present our conclusions in so clear a light that truth and error will show themselves plainly; so that once and for all victory will go either to protectionism or free trade.”⁶⁸\n\nEconomists who follow Bastiat’s advice help their colleagues as well. A stereotype—that they fail to offer definite conclusions—handicaps economists. Being counterstereotypical not only makes you more persuasive and influential as an individual. It also undermines the stereotype, making economists more persuasive and influential as a profession.\n\nAt first, many feel uncomfortable being a one-handed economist. But anyone can do it. Spend less time qualifying general principles. Except at the best schools, introductory classes should be almost qualification free—there is too much nonsense to unlearn to waste time on rare conditions where standard conclusions fail. Most of the exceptions taught in introductory classes can be profitably deferred to intermediate courses; most of the exceptions taught in intermediate courses can be profitably deferred to graduate school. The best students will understand if you tell them, “Those questions will be addressed in more advanced courses.” For the rest, you must respect the Laffer Curve of learning: They retain less if you try to teach them more.\n\nTo take an example that is likely to be controversial, economists do a bad job teaching students about competition.⁶⁹ Textbooks usually say, “Competition works as long as . . .” and then list the many strong assumptions of perfect competition. Many texts are wrong on technical grounds: Perfectly competitive assumptions are sufficient conditions of efficiency, not necessary ones.⁷⁰ But they also deserve censure for failing to emphasize that even imperfect competition defies the cliché that “businesses charge whatever they like.” Indeed, students’ casual equation of greedy motives and bad outcomes is overstated for monopolies. Like competitive firms, monopolies have an incentive to reduce costs, cut their prices when costs fall, and look over their shoulder for potential competition. It is more important for students to understand that self-interest often encourages socially beneficial behavior, than to understand that this mechanism falls short of perfection. Antimarket bias almost ensures that they will not forget the market’s shortcomings.\n\nAt this point, a fair challenge to pose is: If people’s views about economics are so irrational, how is persuasion possible? My answer is that irrationality is not a barrier to persuasion, but an invitation to alternative rhetorical techniques. Think of it this way: If beliefs are, in part, “consumed” for their direct psychological benefits, then to compete in the marketplace of ideas, you need to bundle them with the right emotional content. There is more than one way to make economics “cool,” but I like to package it with an undertone of rebellious discovery, of brash common sense. Who does not side with the child in the Hans Christian Andersen fable who exclaims, “The Emperor is naked!”? You might be afraid of alienating your audience, but it depends on how you frame it. “I’m right, you’re wrong,” falls flat, but “I’m right, the people outside this classroom are wrong, and you don’t want to be like them, do you?” is, in my experience, fairly effective.\n\nYes, these techniques can be used to inculcate fallacies as well as insight. But there is no intrinsic conflict with truth. You can actually get students excited about thinking for themselves on topics where society disapproves, as Ralph Waldo Emerson does in his essay “Self-Reliance.” He paints truth-seeking as not merely responsible, but heroic:\n\nThe nonchalance of boys who are sure of a dinner, and would disdain as much as a lord to do or say aught to conciliate one, is the healthy attitude of human nature. How is a boy the master of society; independent, irresponsible, looking out from his corner on such people and facts as pass by, he tries and sentences them on their merits, in the swift, summary way of boys, as good, bad, interesting, silly, eloquent, troublesome. He cumbers himself never about consequences, about interests; he gives an independent, genuine verdict.⁷¹\n\nBastiat, similarly, makes logic and common sense appealing by ridiculing those who lack them. Take his famous Candlemakers’ Petition:\n\nWe are suffering from the ruinous competition of a foreign rival who apparently works under conditions so far superior to our own for the production of light that he is flooding the domestic market with it at an incredibly low price; for the moment he appears, our sales cease, all the consumers turn to him. . . . This rival . . . is none other than the sun.\n\n[I]f you shut off as much as possible all access to natural light, and thereby create a need for artificial light, what industry in France will not ultimately be encouraged?⁷²\n\nThe petition does more than teach economics. It turns protectionism into a joke. In the process, Bastiat depicts economists not as pedants, but as the life of the intellectual party. Without compromising his intellectual integrity, Bastiat makes readers’ desire to think well of themselves work in his favor.\n\nIf you do not have a full semester to enlighten your audience, my advice becomes more relevant still. The less time you have, the more important it is to (1) highlight the contrast between the popular view and basic economics in stark terms; (2) explain why the latter is true and the former is false; and (3) make it fun.\n\nWhen the media spotlight gives other experts a few seconds to speak their mind, they usually strive to forcefully communicate one or two simplified conclusions. They know that is the best they can do with the time allotted to them. But economists are reluctant to use this strategy. Though the forum demands it, they think it unseemly to express a definite judgment. This is a recipe for being utterly ignored.⁷³ If you are one voice in a sea of self-promotion, you had better speak up clearly when you finally get your chance to talk.\n\nAdmittedly, economists have less latitude on television than in class. If a reporter interviews you about the trade deficit, but you keep changing the subject to comparative advantage, the interview might not be aired, and you reduce your chance of being interviewed again. But it is worth testing the limits of the media’s tolerance. It is not so off-putting to preface any mention of the trade deficit with a short disclaimer: “Trade deficits, contrary to popular opinion, are not a bad thing. Whenever the trade deficit goes up, people always want to ‘do something’ about it, but they’re wrong—like all trade, international trade is mutually beneficial, whether or not there is a trade deficit.” Maybe you could tack on an amusing example too: “I run a huge trade deficit with Wegmans Supermarket—I buy thousands of dollars of its groceries, but Wegmans buys nothing from me—and it is nothing to worry about.” If you cannot steer the conversation away from the latest numbers, at least steal a little time to put the numbers in perspective.\n\nOutlets like newspaper columns and blogs lie somewhere between television sound bites and semester-long courses. You have more slack in print or online than on TV. But you still have to heavily simplify. I know one economist who intentionally writes columns with fewer words than the editor requests. That way, he explains, it is hard for newspapers to cut his favorite parts—which he evidently suspects copy editors are likeliest to hate.\n\nThere is much to learn from Bastiat’s approach to economic education. But that is only the beginning.⁷⁴ Bastiat puts economic education in a broader context. Economists study the world, but are also a part of it. Where do they fit in? Bastiat’s answer is “the refutation of commonplace prejudices.” To use modern terminology, economists supply the public good of correcting systematically biased beliefs. Their main task: “clearing the way for truth . . . preparing men’s minds to understand it . . . correcting public opinion . . . breaking dangerous weapons in the hands of those who misuse them.”⁷⁵\n\nEconomists already do some of this by instinct. It is hard to be sure, but in the absence of generations of economic education, changes like falling tariffs and privatization would probably have happened on a smaller scale, or not at all.⁷⁶ But economists are in a peculiar situation: They correct public opinion not because market forces drive them to, but because market forces grant them the wiggle room to perform this function, if they are so inclined. This means that a great deal depends on the profession’s morale—how enthusiastically it accepts its responsibility.\n\nOne of the main factors that has undermined the profession’s morale in recent decades is the marginalization of the idea of systematically biased beliefs about economics. If it really is the case that voters on average correctly understand economics before they hear word one, who needs economists? What social function do they serve?\n\nThis is not an impossible question to answer. Professional economists could devote themselves to reducing the variance of public opinion, to narrowing dispersion due to random errors. In so doing, they would attain Keynes’s ambition: for economists to become “humble, competent people on a level with dentists.”⁷⁷\n\nSuch professional humility is dangerous. Economists who compare themselves to dentists will basically accept their society as it is. This would be fine if reducing variance were the only task for economists to perform. But in the real world, economists are the main defense against the systematic errors that are the foundation for numerous bad policies. If they look the other way, these mistakes go largely unchecked. Nothing is more likely to make economists desert their posts, to deter them from performing their vital function, than a misguided humility.\n\nEconomists should not forget that they have made mistakes in the past, and will again. We should all admit our limitations. But there are two kinds of errors to avoid. Hubris is one; self-abasement is the other. The first leads experts to overreach themselves; the second leads experts to stand idly by while error reigns.\n\nConclusion\n\nAlong with market fundamentalism, economists are often accused of arrogance. In a way, then, I am playing into the critics’ hands. I advocate neither market fundamentalism nor arrogance, but we should quit trying so hard to avoid the impression of either. There is no reason to be defensive. Economists have created and popularized many of the most socially beneficial ideas in human history, and combated many of the most virulent. If they were self-conscious of their role in the world, they could do much more.\n\nConclusion\n\nIN PRAISE OF THE STUDY OF FOLLY\n\nIt is hard . . . to claim that the same individuals act in\na rational and forward-looking way as economic agents\nbut become fools when casting their vote.\n\n—Torsten Persson and Guido Tabellini,\nPolitical Economics¹\n\nDEMOCRACIES have a lot of apparently counterproductive policies. Economists emphasize the folly of protection and price controls. Experts in other fields have their own bones to pick. How are these policies possible? There are three basic responses.\n\nResponse 1: Defend the accused policies on their merits.\n\nResponse 2: Argue that politicians and special interests have subverted democracy.\n\nResponse 3: Explain how policies can be both popular and counterproductive.\n\nResponse 1 is rarely convincing. We would laugh if a professor spent hours poring over a failing exam scrawled in crayon, searching for its elusive wisdom. Why should we take the effort to rationalize misguided policies any more seriously? Their typical proponent has no subtle counterarguments. Most cannot state the experts’ main objections, much less answer them.\n\nResponse 2 is more intellectually satisfying.² A policy with negative overall effects can still have big benefits for a small minority. But in spite of the academic attention this explanation has accumulated in recent decades, it suffers from two great flaws. First: Theoretically, there are many ways for the majority to cheaply reassert its dominance.³ Second: Empirical public opinion research shows that the status quo—including and perhaps especially its counterproductive policies—enjoys broad popular support, and that politicians respond to changes in public opinion.⁴\n\nThese facts have led me to response 3. Yes, it seems paradoxical for policies to be popular yet counterproductive. Common sense tells us that people like the policies that work the best.⁵ Economic training reinforces this presumption by analogizing democratic participation to market consumption: If the policy is so bad, why do voters keep putting it in their shopping cart?\n\nBut on closer examination, the paradox fades away. The analogy between voting and shopping is false: Democracy is a commons, not a market. Individual voters do not “buy” policies with votes. Rather they toss their vote into a big common pool. The social outcome depends on the pool’s average content.\n\nIn common-pool situations, economists usually fear the worst. Heedless of the aggregate effect, people will foul the waters. The main reason that they are complacent about democracy, I suspect, is that the pollution is hard to visualize. It is not run-of-the-mill physical pollution. Democracy suffers from a more abstract externality: the mental pollution of systematically biased beliefs.\n\nWhile economists rarely discuss the consumption value of beliefs, the idea is intuitively plausible and theoretically unobjectionable. Anything can be a “good,” as far as economic theory is concerned. Daily experience tells us that one of the goods people care about is their worldview. Few of us relish finding out that our religious or political convictions are in error.\n\nOnce you grant this point, you only need to combine it with elementary consumer theory to get my model of rational irrationality. The quantity of irrationality demanded, like the quantity of pears demanded, decreases as its material price goes up. As is often the case in economics, however, this mundane assumption raises uncomfortable questions. In daily life, reality gives us material incentives to restrain our irrationality. But what incentive do we have to think rationally about politics?\n\nAlmost none. To threaten, “You will get bad policies unless you are rational” is a fallacy of composition. Democracy lets the individual enjoy the psychological benefits of irrational beliefs at no cost to himself. This of course does not deny the value of psychological benefits. But the trade-off is not socially optimal; democracy overemphasizes citizens’ psychological payoffs at the expense of their material standard of living.\n\nMigration patterns provide a nice illustration. Citizens of poor countries are often eager to emigrate to rich countries. But they rarely vote for parties that pledge to copy the policies of the rich countries. If an Indian desperately wants to move to the United States but is unable to get a visa, voting to make India more like the United States seems like the next best thing. But there is a crucial difference between the two actions. A migrant who leaves his homeland gives up psychological benefits, such as the belief that his nation is the best in the world, in exchange for a big jump in his material well-being. A voter who turns his back on his nation’s political tradition gives up psychological benefits but—since policy is beyond his control—is not a penny richer.\n\nChanging Course\n\nThe Western economics profession has been spoiled\nrotten by rational expectations thinking, by diverting\nour attention away from the profound misunderstandings\nthat are part of every deep crisis.\n\n—Jeffrey Sachs, “Life in the Economic Emergency Room”⁶\n\nI am certainly not the first social scientist to disconnect policies’ popularity from their effects. A diverse list of thinkers has done the same: Economists like Adam Smith, Frédéric Bastiat, Simon Newcomb, Ludwig von Mises, Frank Knight, Joseph Schumpeter, Charles Schultze, Thomas Sowell, Alan Blinder, and Paul Krugman; political theorists like Niccolò Machiavelli, Gustave Le Bon, Robert Michels, Gaetano Mosca, and Eric Hoffer; even novelists—like George Orwell and Ayn Rand. But my position cuts against the grain of modern social science. If I am right, then a great deal of published research is wrong.\n\nThis is primarily true for formal political theory, as practiced in both economics and political science. Models that assume that the average voter understands how the political-economic system works have some value as foils. But there is little point building ever more complicated variations on the theme of rational voting.⁷ All models simplify, but that is a poor reason to habitually assume the opposite of what we know.\n\nTheorists’ unwillingness to relax the rational expectations assumption has forced them to fashion awfully convoluted models.⁸ Fernandez and Rodrik’s well-known article “Resistance to Reform” is a fine example.⁹ Economic reform in developing countries is often unpopular. The simplest and best explanation, in my view, is that most people underestimate the benefits of economic reform.¹⁰ But Rodrik deplores this explanation on methodological grounds: You can’t say that.¹¹ Instead, Fernandez and Rodrik show that a special kind of uncertainty could lead a majority to oppose policies that would benefit a majority. Example: Suppose 40% of voters know that reform will make them $1,000 richer; remaining voters have a 25% chance to gain $1,000, and a 75% chance to lose $1,000. (40% + .25 \\* 60%) = 55% of the electorate will therefore gain $1,000. But 60% of the electorate expects to lose $500, and therefore votes against reform before it happens.\n\nLike most formal political models, Fernandez-Rodrik is internally consistent.¹² The conclusion—a majority of rational voters may oppose the adoption of reforms that will definitely make a majority better off—follows rigorously from the premises. But it is hard to see this as the reason why real people oppose reform. In the absence of professional scruples against voter irrationality, Fernandez and Rodrik would not have bothered with their model. Why wrack your brain to explain why rational voters would do something that appears irrational, when you already know that voter irrationality is common?\n\nConsidering how many rational voting models are with us, their marginal scientific value has fallen close to zero. Theorists can now teach us far more by exploring the effects of different forms of irrationality. One outstanding example is Timur Kuran and Cass Sunstein’s model of “availability cascades.”¹³ Kuran and Sunstein begin with micro-level evidence that human beings overestimate the probability of memorable events. So what happens, they ask, if the media come across an isolated, vivid, scary anecdote? They lunge for the ratings. Their coverage helps the public remember the anecdote, which amplifies its estimate of the risk, which increases demand for similar stories. Once the scare is widespread, politicians vow to solve the problem, which raises its profile once again. Kuran and Sunstein argue that their mechanism underlies a string of unjustified panics like Love Canal, Alar, and TWA Flight 800. It also helps explain why hysterias vary so much from country to country. A few scary stories about nuclear power snowballed into mass hysteria in the United States, without much impact in Europe; the opposite holds for genetically modified food. Even if Kuran and Sunstein turn out to be wrong, theirs is serious effort to model politics using realistic assumptions about how people think.\n\nIf formal political theory is as flawed as I claim, what about empirical work? A great deal of it is immune to my critique. Public opinion research, for example, has rarely succumbed to the strictures of rational choice theory. Not only have experts in this area continued to publish results that formal theorists have trouble accepting; scholars like David Sears have also exposed important holes in rational choice theory—most notably, the assumption of voter selfishness. Furthermore, if voters are half as irrational as I say, we should be open to evidence that politicians have some slack and take advantage of it.¹⁴\n\nBut not all empirical work escapes unscathed. Some investigations limit themselves to “racing” rational choice explanations against each other. If a coefficient is positive, it supports Rational Choice Theory A; if it is negative, it supports Rational Choice Theory B. If higher income predicts support for free trade, that “shows” that it helps the rich at the expense of the poor; if lower income predicts support for free trade, that “shows” the opposite.\n\nThis whole brand of theory-driven empirical research is questionable. Despite the pretense of openness to evidence, the answer always supports the rational choice approach. Of course, if this approach had withstood extensive testing against alternatives, there would be no problem. But not only has the rational choice approach not endured this kind of scrutiny; when critically examined, it has fared poorly.\n\nStill, even theory-driven empirics can be partly salvaged. Rational choice theory affects the questions that people ask, and skews their interpretation. But as long as the research honestly reports its findings, we can still learn from it. In the rational choice framework one almost automatically treats the fact that higher-income people are less protectionist as proof that protectionism benefits the poor more than the rich. But we can buy the fact without prejudging the explanation. Maybe the rich are less protectionist because they are more rational; or perhaps income is a proxy for education or intelligence, and these make people more rational. Many empirical findings are likely to point in new directions after being liberated from their sterile theoretical milieu.\n\nIt is tempting to say that social scientists have wasted so much effort because economics has spread beyond its appropriate domain. But the real problem is that economics, a vital box of analytical tools, has been misused. Markets are the first thing that economists study, but they have plenty of other ways of looking at human behavior. Once a few pioneers analogized politics to markets, however, there was an unfortunate bandwagon effect. It is time to jump off the bandwagon.\n\nAuthors often close with a call for further research, and so shall I. There is much to learn about politics, and much to unlearn. Social science has pursued many blind alleys—and ignored many promising ones—out of misguided insistence that every model be a “story without fools,” even in areas like politics where folly is central. A proverb tells us that “a wise man learns more from a fool than a fool learns from a wise man.” By closing their eyes to fools and folly, the wise men of social science have artificially hobbled the advance of their own learning.\n\nNOTES\n\nINTRODUCTION: THE PARADOX OF DEMOCRACY\n\n1. Simon (2000).\n\n2. On the economics and politics of dictatorship, see Wintrobe (1998).\n\n3. For some discussions of harmful but democratically adopted policies, see Friedman (2002), Krugman (1998), Olson (1996), and Blinder (1987). Irwin (1996) provides a comprehensive history of economists’ views on protectionism.\n\n4. Grossman and Helpman (2001, 1996, 1994), Rowley, Tollison, and Tullock (1988), Becker (1983), and Brennan and Buchanan (1980) all question the ability of the majority to control its representatives. Somin (2004), Magee, Brock, and Young (1989), Weingast, Shepsle, and Johnson (1981), and Downs (1957) explore the connection between voters’ ignorance and politicians’ ability to act against the public interest.\n\n5. See especially Wittman (1995, 1989), and Stigler (1986).\n\n6. For economists and cognitive psychologists on information processing, see Sheffrin (1996), Kahneman, Slovic, and Tversky (1982), and Nisbett and Ross (1980). The main difference between the two disciplines is that cognitive psychologists are much more likely than economists to conclude that people’s best information processing is not very good.\n\n7. For interesting experimental evidence of this, see Tetlock (2003).\n\n8. See e.g. Applebaum (2003), Courtois et al. (1999), Becker (1996), Payne (1995), Drèze and Sen (1990), and Conquest (1986).\n\n9. For a wide-ranging survey of critiques of democracy, see Dahl (1989).\n\n10. Eigen and Siegel (1993: 109).\n\n11. See e.g. MacEwan (1999), Soros (1998), Kuttner (1997, 1991, 1984), and Greider (1997, 1992).\n\n12. See e.g. Caplan (2002a), Alston, Kearl, and Vaughn (1992), Blinder (1987), and Schultze (1977).\n\nCHAPTER 1: BEYOND THE MIRACLE OF AGGREGATION\n\n1. Mencken (1995: 375).\n\n2. Olson (1971) and Downs (1957) elegantly explain why ignorant voters are acting in the selfishly optimal way. On voters’ low probability of decisiveness, see Edlin, Gelman, and Kaplan (forthcoming), Gelman, Katz, and Bafumi (2004), Fedderson (2004), Mulligan and Hunter (2003), Gelman, King, and Boscardin (1998), and Meehl (1977).\n\n3. Kuttner (1996: xi).\n\n4. See e.g. Kelman (1988), and Rhoads (1985).\n\n5. These terms are near-synonyms, though they carry slightly different connotations. Economists in the tradition of James Buchanan and Gordon Tullock prefer the label public choice. Economists less attached to this tradition substitute political economy or positive political economy. Rational choice theory is more popular among political scientists (Green and Shapiro 1994).\n\n6. See Quirk (1990, 1988).\n\n7. Surowiecki (2004: 11).\n\n8. On random versus systematic error, see e.g. Surowiecki (2004), Austen-Smith and Banks (1996), Wittman (1995, 1989), Page and Shapiro (1993, 1992), Levy (1989), and Muth (1961).\n\n9. For an insightful discussion, see Hoffman (1998).\n\n10. Page and Shapiro (1993: 41).\n\n11. Converse (1990: 383).\n\n12. Brainy Quote (2005b).\n\n13. Surowiecki (2004).\n\n14. Surowiecki (2004: xi–xiii, 3–4, 7–11, 11–15, 17–22).\n\n15. Page and Shapiro (1993: 41).\n\n16. For good overviews, see Somin (2004, 2000, 1999, 1998), Delli Carpini and Keeter (1996), Dye and Zeigler (1996), Bennett (1996), Smith (1989), Neuman (1986), and Converse (1964).\n\n17. Delli Carpini and Keeter (1996: 117).\n\n18. Dye and Zeigler (1992: 206).\n\n19. Delli Carpini and Keeter (1996: 116–22; 89–92).\n\n20. Lecky (1981: 22).\n\n21. For important exceptions, see Althaus (2003, 1998, 1996), Bartels (2004, 1996), Gilens (2001), Wolfers (2001), and Delli Carpini and Keeter (1996).\n\n22. For good overviews, see Rabin (1998) Thaler (1992), Quattrone and Tversky (1988, 1984), Simon (1985), Kahneman, Slovic, and Tversky (1982), and Nisbett and Ross (1980).\n\n23. See e.g. Smith (2003, 1991), Cosmides and Tooby (1996), Barkow, Cosmides, and Tooby (1992), and Cosmides (1989).\n\n24. For my earlier research on this point, see Caplan (2002a, 2002b, 2001d).\n\n25. For simplicity, assume symmetric voter preferences so that the median preference is also the most efficient outcome (Cooter 2000: 32–35).\n\n26. For more examples, see Sowell (2004a, 2004b).\n\n27. See e.g. Krugman (1998) and Siebert (1997).\n\n28. McCloskey (1985: 5).\n\n29. See e.g. Sheffrin (1996).\n\n30. Newcomb (1893: 375).\n\n31. Smith (1981: 488–89).\n\n32. Smith (1981: 493).\n\n33. Smith (1981: 796).\n\n34. Bastiat (1964a: 123).\n\n35. Knight (1960: 19).\n\n36. See e.g. Drazen (2000), Persson and Tabellini (2000), and Rodrik (1996). For an important recent exception, see Romer (2003).\n\n37. Stigler (1986: 309). Earlier in his career, however, Stigler seemed to hold this “singularly obfuscatory” view (Stigler 1959).\n\n38. Skousen (1997: 150).\n\n39. Krugman (1996: 5).\n\n40. For further discussion, see Caplan (2003b).\n\n41. Shermer (2002: 82).\n\n42. Economists who have made this point include Caplan (2001a), Akerlof (1989), and Akerlof and Dickens (1982).\n\n43. Rand (1957: 944).\n\n44. Locke (1977: 570; emphasis added).\n\n45. Locke (1977: 571).\n\n46. Locke (1977: 571).\n\n47. Nietzsche (1954: 635).\n\n48. Le Bon (1960: 73).\n\n49. Hoffer (1951: 26).\n\n50. One fact that makes both religious and political believers less touchy is that many only loosely understand their own doctrines (Converse 1964).\n\n51. For classic treatments of totalitarianism, see Arendt (1973), and Friedrich and Brzezinski (1965).\n\n52. Hoffer (1951: 27).\n\n53. Crossman (1949: 203).\n\n54. Orwell (1983).\n\n55. Hoffer usefully distinguishes between the revolutionary or “active” and institution-building or “consolidation” stages in the life cycle of mass movements. The extreme irrationality of the early phase decays into a more dilute irrationality in the later phase. “The conservatism of a religion—its orthodoxy—is the inert coagulum of a once highly reactive sap” (Hoffer 1951: 14).\n\n56. Shermer (2002) discusses the leading examples.\n\n57. Bastiat (1964a: 84).\n\n58. See Spence (1977) for a formal analysis.\n\n59. Akerlof (1989) is, to the best of my knowledge, the first economist to clearly make this point.\n\n60. Andrews (1993: 229).\n\n61. For overviews of the empirical evidence on the self-interested voter hypothesis, see Mansbridge (1990), Sears and Funk (1990), Citrin and Green (1990), and Sears et al. (1980). On income and party identification, see Gelman et al. (2005), Luttbeg and Martinez (1990), and Kamieniecki (1985). On age and policy preferences, see Ponza et al. (1988). On gender and public opinion about abortion, see Shapiro and Mahajan (1986).\n\n62. Blinder (1987: 89).\n\n63. Le Bon (1960: 110).\n\n64. See e.g. Jacobs and Shapiro (2000) and Bender and Lott (1996).\n\n65. Merriam-Webster’s Collegiate Dictionary (2003: 330).\n\n66. See Fremling and Lott (1996, 1989).\n\n67. For analysis of, and evidence on, elections as imperfect disciplinary devices, see Matsusaka (2005), Persson and Tabellini (2004, 2000), Gerber and Lewis (2004), Besley and Case (2003), Persson (2002), Besley and Coate (2000), and Levitt (1996).\n\n68. Admittedly, it is possible that voters are indifferent over a vast range of policies, giving politicians an enormous degree of slack. For this point I am indebted to Ilya Somin.\n\n69. Sutter (2006) is an excellent economic critique of popular misconceptions about the media. For a standard rational choice analysis of the informational role of the media, see Wittman (2005b); for a more skeptical view, see Mullainathan and Shleifer (2005).\n\n70. Abramson, Aldrich, and Rohde (2002: 131).\n\nCHAPTER 2: SYSTEMATICALLY BIASED BELIEFS ABOUT ECONOMICS\n\n1. Le Bon (1960: 114).\n\n2. Becker (1976b: 246).\n\n3. For example, Austen-Smith (1991) “outs” Magee et al. (1989).\n\n4. Coate and Morris (1995: 1212).\n\n5. Rodrik (1996: 38).\n\n6. See e.g. Kruger and Dunning (1999), Camerer (1995), Taylor (1989), Hogarth and Reder (1987), Gigerenzer and Murray (1987), Kahneman, Slovic, and Tversky (1982), Tversky and Kahneman (1982a), Lichtenstein, Fischhoff, and Phillips (1982), and Nisbett and Ross (1980).\n\n7. On overestimation of vivid, memorable events, see Tversky and Kahneman (1982b) and Slovic, Fischhoff, and Lichtenstein (1980). On people’s tendency to overestimate themselves, see Kruger and Dunning (1999), and Gilovich (1991).\n\n8. See e.g. Sunstein (2000), Rabin (1998), Babcock and Loewenstein (1997), and Thaler (1992).\n\n9. See Harrison and List (2004) and List (2003).\n\n10. Thus, I am more impressed by the fact that reverse mortgages are unpopular in the real world than I am by experiments showing that “mental budgeting” affects behavior in the lab (Thaler 1992: 107–21).\n\n11. See e.g. Smith (2003), Goldstein and Gigerenzer (2002), Gigerenzer (2001, 2000), and Cosmides and Tooby (1996).\n\n12. See Bartels (2004, 1996), Althaus (2003, 1998, 1996), Gilens (2001), Duch, Palmer, and Anderson (2000), Kuklinski et al. (2000), Krause and Granato (1998), Krause (1997), and Delli Carpini and Keeter (1996). For a somewhat contrary result, see Lau and Redlawsk (1997).\n\n13. Kaiser Family Foundation and Harvard University School of Public Health (1995).\n\n14. Economics and Statistics Administration (2004).\n\n15. Leading examples of the enlightened preference approach include Bartels (2004, 1996), Althaus (2003, 1998, 1996), Gilens (2001), and Delli Carpini and Keeter (1996).\n\n16. Althaus (2003: 60).\n\n17. Althaus (2003: 128–30).\n\n18. Althaus (2003: 130).\n\n19. Althaus (2003: 131, 111).\n\n20. Althaus (2003: 115, 109).\n\n21. Krugman (1996: 118). Krugman is specifically referring to misconceptions about international trade.\n\n22. On modern economists’ indifference to history of thought, see Blaug (2001).\n\n23. Of course, this typology is not exhaustive, and some beliefs could sit in more than one category.\n\n24. Herman (1997: 48).\n\n25. See e.g. Sowell (2004a, 2004b), Caplan and Cowen (2004), Mueller (1999), Klein (1999), Shleifer (1998), Cowen (1998), Mises (1998, 1996, 1966), Shiller (1997), Sachs and Warner (1995), Blinder (1987), Henderson (1986), Rhoads (1985), Smith (1981), and Schultze (1977).\n\n26. Schumpeter (1950: 144).\n\n27. Schumpeter (1954: 234).\n\n28. Rubin (2003) elaborates on this theme.\n\n29. Schultze (1977: 18, 47).\n\n30. Mises (1981a: 325).\n\n31. Mises (1966: 854).\n\n32. This is roughly equivalent to what Thomas Sowell (2004a: 4–13) calls “one-stage thinking”—considering only the immediate and obvious effects of a policy, and ignoring the indirect and less obvious effects.\n\n33. Smith (1981: 454).\n\n34. Smith (1981: 456).\n\n35. On usury, see Houkes (2004), and Böhm-Bawerk (1959).\n\n36. Böhm-Bawerk (1959: 10).\n\n37. Kuran (2004: 39).\n\n38. Kuran (2004: 57).\n\n39. Blinder (1987: 136–59).\n\n40. Blinder (1987: 137); for the original study, see Kelman (1981: 98–99).\n\n41. See e.g. Knight (1960: 98–99).\n\n42. See e.g. Scherer and Ross (1990: 208–20).\n\n43. Bastiat (1964b: 19–20).\n\n44. See e.g. Stiglitz (2002b).\n\n45. “We rarely hear, it has been said, of the combinations of masters, though frequently of those of workmen. But whoever imagines, upon this account, that masters rarely combine, is as ignorant of the world as of the subject. Masters are always and everywhere in a sort of tacit, but constant and uniform combination, not to raise the wages of labour above their actual rate. To violate this combination is everywhere a most unpopular action, and a sort of reproach to a master among his neighbours and equals” (Smith 1981: 84).\n\n46. See e.g. Krugman (1998). This is not to deny, of course, that their low productivity may be largely due to poor domestic policies, not to mention the First World’s limits on immigration.\n\n47. Mueller (1999: 5).\n\n48. Boublil, Kretzmer, and Natel (1990: 36).\n\n49. Mueller (1999).\n\n50. Krugman (2003); Stiglitz (2003).\n\n51. Greider (1992: 395).\n\n52. See e.g. Hainmueller and Hiscox (forthcoming, 2005a), Poole (2004), Bhagwati (2002), Roberts (2001), Krugman (1996), Irwin (1996), Phelps (1993), Blinder (1987), Henderson (1986), and Taussig (1905).\n\n53. Newcomb (1893: 379).\n\n54. Blinder (1987: 111).\n\n55. Smith (1981: 457).\n\n56. Newcomb (1893: 377).\n\n57. See Irwin (1996).\n\n58. Krugman (1996: 124–25).\n\n59. Landsburg (1993: 197). Landsburg (1993: 197) attributes this argument to David Friedman. Krugman (1996: 119–20) attributes a similar argument to James Ingram’s (1983) textbook.\n\n60. See e.g. Bhagwati (2002) and Irwin (1996).\n\n61. Smith (1981: 429).\n\n62. Antidumping laws are another interesting expression of our suspicion of foreigners. It is much easier to prosecute foreign firms for “selling below cost” (“dumping”) than it is to prosecute domestic firms for the same offense. As Joseph Stiglitz (2002a: 173–174) explains: “The U.S. estimates costs of production under a peculiar methodology, which, if applied to American firms, would probably conclude that most American firms were dumping as well.”\n\n63. See William J. Clinton Foundation (2005).\n\n64. GSS variable identifiers JAPAN, ENGLAND, and CANADA.\n\n65. U.S. Census Bureau (2005a, 2005b). I would like to thank Ilya Somin for alerting me to this fact.\n\n66. Bureau of Economic Analysis (2005).\n\n67. Krugman (1996: 84).\n\n68. Bastiat (1964a: 26–27).\n\n69. See e.g. Cox and Alm (1999), Krugman (1998), Davis, Haltiwanger, and Schuh (1996), Henderson (1986), and Bastiat (1964a, 1964b).\n\n70. Blinder (1987: 17).\n\n71. Bastiat (1964a: 20).\n\n72. Bastiat (1964a: 20).\n\n73. Newcomb (1893: 380).\n\n74. Schlesinger (1957: 462).\n\n75. Cox and Alm (1999: 116). Note the similarity to Schumpeter’s (1950: 81–86) notion of “creative destruction.”\n\n76. Cox and Alm (1999: 128).\n\n77. Blinder (1987: 124). Blinder is referring to workers displaced by international competition, but his arguments easily extend to workers displaced by technological progress.\n\n78. Cox and Alm (1999: 133).\n\n79. Cox and Alm (1999: 111).\n\n80. Bastiat (1964a: 10).\n\n81. Bastiat (1964a: 10).\n\n82. Herman (1997: 173).\n\n83. See e.g. Kling (2004), Easterbrook (2003), Lomborg (2001), Cox and Alm (1999), Mueller (1999), Whitman (1998), Simon (1996, 1995b), Samuelson (1995), and McCloskey (1993).\n\n84. Rae (1965: 343).\n\n85. See e.g. Krugman (1998, 1996) and Blinder (1987).\n\n86. See e.g. Johnson (2000), Fogel (1999), and Lucas (1993).\n\n87. Herman (1997: 13).\n\n88. Lovejoy and Boas (1965: 7). In Lovejoy and Boas’s historical survey, it is striking that the reasons for pessimism have varied much more than pessimism itself. While modern pessimists insist that material prosperity is slipping through our fingers, most of the pessimists of antiquity focused not on the impermanence of riches, but its negative effect on virtue and community.\n\n89. Smith (1981: 343; emphasis added).\n\n90. Smith (1981: 343–44; emphasis added).\n\n91. Hume (1987: 464).\n\n92. Hume (1987: 73–74).\n\n93. See e.g. Mises (1981b).\n\n94. Herman (1997: 65).\n\n95. Spencer (1981: 3).\n\n96. Spencer (1981: 6).\n\n97. Herman (1997: 297, 1).\n\n98. For some attempts to answer this question, see Easterbrook (2003), Cox and Alm (1999), Mueller (1999), and Whitman (1998).\n\n99. See e.g. Pew Research Center (1997).\n\n100. Easterbrook (2003: 119).\n\n101. Cox and Alm (1999: 200, 44).\n\n102. Cox and Alm (1999: 197).\n\n103. Krugman (1996: 48).\n\n104. See Krueger and Solow (2001).\n\n105. Krugman (1996: 214).\n\n106. See e.g. Starke (2004). It is also widely believed that life is getting worse because of declining cultural/social quality. For a critique of cultural/social pessimism, see Whitman (1998).\n\n107. Erlich (1968).\n\n108. Simon (1996, 1995a).\n\n109. Simon (1995a: 642–43).\n\n110. See e.g. Dasgupta et al. (2002), Freeman (2002), Lomborg (2001), and Johnson (2000). Even Cole (2003), a critical review of Lomborg, emphasizes exceptions to environmental optimism, but accepts the reality of many positive trends.\n\n111. Kremer (1993). Jared Diamond’s (1997) prize-winning Guns, Germs, and Steel links population and innovation in essentially the same way, albeit with little fanfare.\n\n112. There is a distinction between (a) pessimists who believe that scary scenarios are likely, and (b) pessimists who believe that scary scenarios are unlikely, but still worth worrying about. Few economists, myself included, have any strong objection to the second sort of pessimism. It would be foolish to dismiss concerns about nuclear proliferation on the grounds that nuclear weapons will probably never be used. I would like to thank Andrew Gelman for pointing out this distinction.\n\n113. Kirchgässner (2005).\n\nCHAPTER 3: EVIDENCE FROM THE SURVEY OF AMERICANS AND ECONOMISTS ON THE ECONOMY\n\n1. Krugman (1996: 78).\n\n2. Some of the more important: Blinder and Krueger (2004), Chicago Council on Foreign Relations (2004), Fuller and Geide-Stevenson (2003), Chicago Council on Foreign Relations and the German Marshall Fund of the United States (2002a, 2002b, 2002c), Walstad and Rebeck (2002), Scheve and Slaughter (2001a, 2001b), Fuchs, Krueger, and Poterba (1998), Walstad (1997, 1992), Frey and Eichenberger (1993, 1992), Walstad and Larsen (1992), Alston, Kearl, and Vaughn (1992), Ricketts and Shoesmith (1990), Conover, Feldman, and Knight (1987), Conover and Feldman (1986), Rhoads (1985), Pommerehne et al. (1984), McClosky and Zaller (1984), Chong, McClosky, and Zaller (1983), and Kearl et al. (1979).\n\n3. See Fuller and Geide-Stevenson (2003), Alston, Kearl, and Vaughn (1992), and Kearl et al. (1979).\n\n4. Chicago Council on Foreign Relations and the German Marshall Fund of the United States (2002a).\n\n5. Question 765.\n\n6. Question 575/8. Chicago Council on Foreign Relations and the German Marshall Fund of the United States (2002b) heavily emphasizes the Worldviews 2002 finding that large majorities embrace free trade if combined with assistance for displaced workers. On question 770, respondents’ three choices are free trade with worker assistance, free trade without worker assistance, and “I do not favor free trade.” Almost 75 percent select the first answer. In my judgment, this is in large part a question-wording effect. The binary choice between “free trade” and “no free trade” probably masks the public’s preference for an intermediate policy. Furthermore, the alternative to free trade should have been more positively labeled as e.g. “fair trade.” Finally, the last option should have been split into “no free trade + worker assistance” and “no free trade + no worker assistance.” It is also noteworthy that Americans overwhelmingly (93%) want trade agreements to regulate minimum working conditions—a deal-breaker for most low-income nations (question 775). For evidence of strong question-wording effects in surveys of trade attitudes, see Hiscox (2006).\n\n7. Kearl et al. (1979: 30).\n\n8. GSS variable identifier PRICECON (General Social Survey 1998).\n\n9. Washington Post, Kaiser Family Foundation, and Harvard University Survey Project (1996).\n\n10. For more information on the SAEE, see Blendon et al. (1997).\n\n11. The total number of questions actually exceeds 37. Some questions were not presented to both groups of respondents, precluding comparison. I omit handful of other questions due to redundancy.\n\n12. Kahneman and Tversky (1982: 493).\n\n13. Greider (1992: 36).\n\n14. Andrews (1993: 262).\n\n15. Kelman (1981: 7).\n\n16. Kelman (1981: 7).\n\n17. See e.g. Dahl and Ransom (1999) and Babcock and Loewenstein (1997).\n\n18. Marx (1965: 609).\n\n19. Mises (1962: 86).\n\n20. Brossard and Pearlstein (1996: A6).\n\n21. See e.g. Soros (1998), Kuttner (1997, 1991, 1984), Greider (1997, 1992), and Lazonick (1991).\n\n22. Kuttner (1997: 3–4).\n\n23. For further discussion, see this chapter’s technical appendix.\n\n24. Expert bias is not the only reason why adding statistical controls might make belief gaps disappear. If a correct understanding of the economy allows people to prosper, rich noneconomists would agree with professional economists in the absence of self-serving bias. Or suppose that understanding economics causes people to become more conservative. Conservatives would tend to agree with economists in the absence of ideological bias.\n\n25. This is a question where my knee-jerk reaction is to side with the public. But my implicit assumption is that lower taxes would be balanced by privatization and cuts in popular programs like Social Security and Medicare. If cuts are limited to “waste” and unpopular programs, my fellow economists are probably right.\n\n26. See e.g. Easterly (2001).\n\n27. Kaiser Family Foundation and Harvard University School of Public Health (1995).\n\n28. On immigration and population growth, see Borjas (1994: 1668); on immigration and wages, see Borjas (1994: 1695–1700); on immigration’s net effect on the budget, see Lee and Miller (2000) and Simon (1999: 112–52, 313–21).\n\n29. See e.g. Gruber (2005: 509–10).\n\n30. Given how rarely economists and the public agree, I suspect that their meeting of the minds is a sign that economists have failed to free themselves from popular prejudice. Even if education had significant externalities, why are existing subsidies too small to fully correct for them? More fundamentally, the benefits of education for worker productivity seem almost entirely internalized—you acquire more skills, you earn more money. So there is not much of an efficiency case for any subsidies, much less higher subsidies. In fact, decades of experience in the education industry have convinced me that education is often mere jumping through hoops—or in technical terms, “signaling” (Weiss 1995). Subsidizing hoop-jumping is wasteful, for it just increases the number of hoops you are expected to jump.\n\n31. Kaiser Family Foundation and Harvard University School of Public Health (1995).\n\n32. Equal Employment Opportunity Commission (2005).\n\n33. Another possibility is that economists read this as a question about “shirking,” or, in broader terms, “social capital.” The more workers cheat their boss when he looks the other way, the more resources have to be wasted on monitoring. A strong work ethic—an internalized norm against shirking—would therefore have a positive externality.\n\n34. This is another question where I find myself at odds with the professional consensus. There is a plausible economic case for regulation of third-party effects like air pollution. But a great deal of regulation is designed merely to force consumers to buy more safety, health, and other politically correct products than they want. Safety is not free. A worker could always offer to work for a reduced salary in exchange for more on-the-job safety. Safety regulations therefore make workers safer than they want to be. Given standard estimates of the cost of regulation in the hundreds of billions of dollars annually (Office of Management and Budget 1997), and the theoretical conclusion that much of this regulation is not worth its cost, I deem excessive regulation a major problem indeed.\n\n35. For public opinion about the minimum wage, see Gallup Organization (2005); for farm subsidies, see PIPA–Knowledge Networks Poll (2004); for drug testing, see Kaiser Family Foundation (2005).\n\n36. GSS variable identifier SETPRICE.\n\n37. See e.g. Rowley, Tollison, and Tullock (1988), and Weiss and Klass (1986).\n\n38. Washington Post, Kaiser Family Foundation, and Harvard University Survey Project (1996: 4); Walstad and Larsen (1992: 48).\n\n39. Economists would be much more open to the possibility that executive compensation is insufficiently tied to performance. High pay is not a problem, but indiscriminately high pay is.\n\n40. See e.g. Krugman (1998, 1996) and Blinder (1987).\n\n41. See e.g. Cox and Alm (1999).\n\n42. See e.g. Krugman (1998: 62–65), and Kaiser Family Foundation and Harvard University School of Public Health (1995).\n\n43. See Kull (2000) on the public’s lukewarm support for trade agreements, and Bhagwati (2002) on economists’ support for unilateral free trade.\n\n44. Schumpeter (1950: 145).\n\n45. In my view, economists who predict job gains go too far. Trade liberalization increases foreign demand but curtails domestic demand. So there is little reason to expect a short-term employment benefit. A long-term effect is even less likely; macroeconomists doubt that demand shocks have long-run effects on employment (Blinder 1987: 106–7). The theoretically sound response is to focus on living standards, not employment.\n\n46. Note: The response “both” was coded as a 1; “neither” was coded as a 0.\n\n47. See e.g. Blinder (1987).\n\n48. See e.g. Cox and Alm (1999: 139–56).\n\n49. See Gottschalk (1997).\n\n50. See e.g. Cox and Alm (1999: 17–22).\n\n51. See e.g. Fogel (1999), Lucas (1993), and Lebergott (1993).\n\n52. This result further undermines semantic views of lay-expert disagreement. If economists and noneconomists defined “good economic performance” differently, you would expect them to disagree about the current state of the economy, not just the past and the future.\n\n53. Kaiser Family Foundation and Harvard University School of Public Health (1995). Similar results appear in the Survey of Americans’ Knowledge and Attitudes about Entitlements (Washington Post, Kaiser Family Foundation, and Harvard University Survey Project, 1997).\n\n54. Anticipating the ambiguity of the “welfare” category, Kaiser asked respondents which programs they define as “welfare.” A majority of Americans so categorize Medicaid, food stamps, AFDC, child nutrition programs, and public housing assistance. In the 1993 federal budget, these items add up to 10.2% (Kaiser Family Foundation and Harvard University School of Public Health 1995).\n\n55. A natural defense of the public’s good sense is to appeal to definitional ambiguity. In fact, the numbers in table 3.1’s last column are deliberately slanted in the public’s favor. Table 3.1 defines foreign aid broadly to include all spending on international affairs; a narrower definition would pull the budget share down from 1.2% to a mere .4%. Similarly, the measure of health spending omits Medicaid (which is already counted under the “welfare” heading). If you counted Medicaid, health spending would rise to 16.3% of the budget (Office of Management and Budget 2005: 56, 308).\n\n56. Tullock (1987: 28).\n\n57. Blendon et al. (1997: 112–13).\n\n58. Admittedly, the procedure for selecting questions was informal, relying on the judgment calls of the authors. If they wanted to boost their credibility, they could have done a two-stage survey, one to select questions, another to collect answers. But this doubt is hardly strong enough to require us to suspend judgment until someone performs the exercise.\n\n59. Cerf and Navasky (1998).\n\n60. See Klein and Stern (forthcoming).\n\n61. Perhaps the myth of ideological bias originated because people who embrace one extremely right-wing viewpoint usually embrace a lot of them. So when economists summarily dismiss popular complaints about markets, people infer that they subscribe to the whole conservative package.\n\n62. Caplan (2001d: 417).\n\n63. Delli Carpini, and Keeter (1996).\n\n64. Kraus, Malmfors, and Slovic (1992).\n\n65. Caplan (2002a, 2001d).\n\n66. Strictly speaking, our hypothetical student would also be older after completing his Ph.D., so this change in age would have to be factored into the results.\n\nCHAPTER 4: CLASSICAL PUBLIC CHOICE AND THE FAILURE OF RATIONAL IGNORANCE\n\n1. Edgerton (1992: 197).\n\n2. Tullock (1967: 102). I would like to thank Charles Rowley for guiding me to this reference.\n\n3. Downs (1957: 259).\n\n4. For the classic discussion, see Stigler (1961).\n\n5. See e.g. Edlin, Gelman, and Kaplan (forthcoming), Gelman, Katz, and Bafumi (2004), Fedderson (2004), Mulligan and Hunter (2003), Gelman, King, and Boscardin (1998), Brennan and Lomasky (1993, 1989), and Meehl (1977).\n\n6. See e.g. Stiglitz (2002b).\n\n7. See e.g. Somin (2004), and Bennett (1996).\n\n8. For overviews, see Somin (2004, 2000, 1999, 1998), Delli Carpini and Keeter (1996), Dye and Zeigler (1996), Bennett (1996), Smith (1989), and Neuman (1986).\n\n9. Delli Carpini and Keeter (1996: 117).\n\n10. Dye and Zeigler (1992: 206). Actually, matters are worse: People who can state their senators’ name presumably know the answer, but random guessing about political parties yields two correct answers a quarter of the time.\n\n11. Delli Carpini and Keeter (1996: 70–71).\n\n12. Delli Carpini and Keeter (1996: 101).\n\n13. See Olson (1982).\n\n14. For analyses of political failure that rely on rational ignorance, see e.g. Coursey and Roberts (1991), Magee, Brock, and Young (1989), Rowley, Tollison, and Tullock (1988), Blinder (1987), Rhoads (1985), Buchanan and Tollison (1984), Weingast, Shepsle, and Johnson (1981), and Olson (1971).\n\n15. Olson (1971: 29).\n\n16. See e.g. Persson and Tabellini (2000), Drazen (2000), Rodrik (1996), Wittman (1995, 1989), Coate and Morris (1999, 1995), Austen-Smith (1991), and Coursey and Roberts (1991).\n\n17. Downs (1957: 10).\n\n18. See e.g. Wittman (2005c).\n\n19. See e.g. Sheffrin (1996), Pesaran (1987), Maddock and Carter (1982), Lucas (1973), and Muth (1961).\n\n20. Downs (1957: 5).\n\n21. Downs (1957: 4).\n\n22. On economists’ diverse interpretations of rationality, see Cowen (2001). On Bayesianism, see e.g. Hanson (2002), Howson and Urbach (1989), and Aumann (1976). For experimental evidence against Bayesianism, see Rabin (1998) and Kahneman and Tversky (1982).\n\n23. See Cowen and Hanson (forthcoming), Cowen (2003), and Mele (2001, 1987).\n\n24. The second-most-popular response is to appeal to the distinction between individual and social irrationality. Defection in the Prisoners’ Dilemma is said to be “socially” irrational because all players could be better off if they cooperated, but still individually rational because each player is doing as well as possible given the others’ behavior.\n\n25. Errors driven by emotional commitments are also known as “motivational biases.” An enormous literature on “cognitive biases” further shows that ignorance is not the sole cause of error even on questions where people lack emotional precommitments (Gilovich 1991; Nisbett and Ross 1980). I focus on motivational biases because they are both more intuitive and more clearly politically relevant.\n\n26. McCloskey (1985: 177).\n\n27. Crossman (1949: 23).\n\n28. Chambers (1952: 196).\n\n29. For a survey of experimental evidence on evolution, see Bell (1997).\n\n30. Lott (2000).\n\n31. Aristotle (1941: 689).\n\n32. See e.g. Murphy and Shleifer (2004), Mele (2004, 2001, 1987), Tetlock (2003), Redlawsk (2002), Shermer (2002), Taylor (1989), Locke (1977), Hoffer (1951), and Mosca (1939).\n\n33. See e.g. MacEwan (1999), Kuttner (1997, 1996), Hauptmann (1996), and Greider (1992).\n\n34. See e.g. Surowiecki (2004), Wittman (1995), and Page and Shapiro (1992).\n\n35. See e.g. Sutter (2006), Frank (2004), Herman and Chomsky (2002), Murray, Schwartz, and Lichter (2001), Lichter and Rothman (1999), Simon (1996), Kuran (1995), Page and Shapiro (1992), and Geddes and Zaller (1989).\n\n36. Wittman (1995: 15).\n\n37. Johnson (1991: 550).\n\n38. See e.g. Eichenberger and Serna (1996), Wittman (1995, 1989), and Gilovich (1991).\n\n39. See e.g. Posner (2002) and Becker (1968).\n\n40. Becker (1976a: 54).\n\n41. On asymmetric information, see Stiglitz (2003, 2002b) and Akerlof (1970).\n\n42. See e.g. Banks and Weingast (1992), Crew and Twight (1990), Magee, Brock, and Young (1989), and Rowley, Tollison, and Tullock (1988).\n\n43. Akerlof (1970).\n\n44. Some object to the analogy between used cars and government programs because “you do not have to buy a car, but you always get a leader.” The point, however, if that voters can respond to asymmetric information by saying no to candidates who advocate programs of dubious value.\n\n45. See Caplan (2001c), Wittman (1995: 107), and Breton and Wintrobe (1982).\n\n46. For further discussion, see Lee (1989).\n\n47. On cognitive shortcuts, see e.g. Somin (2004), Cutler (2002), Kuklinski et al. (2001), Lau and Redlawsk (2001), Lupia and McCubbins (1998), Hoffman (1998), Lupia (1994), and Popkin (1991).\n\n48. Lupia and McCubbins (1998: 7).\n\n49. Lupia and McCubbins (1998: 37).\n\n50. See e.g. Nadeau and Lewis-Beck (2001), Lanoue (1994), Lockerbie (1991), and Fiorina (1981).\n\n51. See Achen and Bartels (2004), Somin (2004), and Rudolph (2003).\n\n52. However, some empirical studies also find that shortcuts can lead unsophisticated voters to make worse decisions (Lau and Redlawsk 2001).\n\n53. Althaus (2003: 143).\n\n54. Wittman (1989: 1421).\n\n55. For critiques of Wittman, see Lott (1997), Rowley (1997), and Boudreaux (1996), as well as the exchange in Econ Journal Watch (Caplan 2005a, 2005b; Wittman 2005c, 2005d).\n\n56. In seminars, I have seen professional game theorists struggle to match what Wittman called the lowest level of cognitive ability voters could possibly have!\n\n57. On monopoly power in politics, see e.g. Fiorina (1989), Anderson and Tollison (1988), Brennan and Buchanan (1980), Crain (1977), and Tullock (1965). Posner (2002: 295–347) discusses many reasons to be skeptical about the dangers of “monopoly.”\n\n58. Wittman (1995: 25).\n\n59. See e.g. Holcombe (1985) and Shepsle and Weingast (1981).\n\n60. Wittman (1995, 1989).\n\n61. See e.g. Matsusaka (2005), Persson and Tabellini (2004, 2000), Gerber and Lewis (2004), Besley and Case (2003), Persson (2002), Besley and Coate (2000), and Levitt (1996).\n\n62. On direct versus indirect democracy, see Matsusaka (2005). On senators’ disagreement, see Levitt (1996). On the effects of open primaries, redistricting, campaign finance rules, and party competition, see Besley and Case (2003).\n\n63. Besley and Case (2003: 68).\n\n64. Besley and Case (2003: 40).\n\n65. See e.g. Alesina and Rosenthal (1994).\n\n66. Wittman (1995: 10–15).\n\n67. For further discussion, see Caplan (2003b, 2001a, 2001c).\n\n68. For an excellent discussion of people’s failure to fully adjust for sources’ reliability, see Gilovich (1991).\n\n69. Thaler (1992: 198).\n\nCHAPTER 5: RATIONAL IRRATIONALITY\n\n1. Descartes (1999: 6).\n\n2. Wittman (1995: 16–17) argues that irrational minorities fare better under democracy than in markets. Markets let an irrational fringe pursue its self-destructive preferences, but democracy protects the minority from itself. “False political advertising may fool a minority, yet it will have no harmful effect since votes for the minority will not be translated into political power. In contrast, a business does not have to persuade a majority of consumers, only a few, to have any sales.” In other words, democracy works better than markets if the median voter is more rational than the mean voter. The opposite holds if—as chapter 3 suggests—the median voter is less rational than the mean voter.\n\n3. See Austen-Smith (1991) and Coursey and Roberts (1991).\n\n4. Carroll (1999: 184).\n\n5. Dasgupta and Stiglitz (1988: 570).\n\n6. See e.g. Kuran (1995) and Gilovich (1991).\n\n7. See e.g. Caplan (2001a) and Ainslie (1992).\n\n8. Le Bon (1960: 109).\n\n9. Admittedly, social pressure is also usually at work; coreligionists—who usually include family—rarely approve of those who abandon their religion (Iannaccone 1998). Still, many people cling to their religious views despite social pressure to change them, suggesting that they value the beliefs directly.\n\n10. Mosca (1939: 176–77).\n\n11. Mosca (1939: 175).\n\n12. Helliwell (2003); Donovan and Halpern (2002).\n\n13. Jost et al. (2003: 340).\n\n14. See Stigler and Becker (1977) and Friedman (1953). For a critique, see Caplan (2003a).\n\n15. For a more detailed discussion, see Caplan (2001e, 1999).\n\n16. See e.g. Bertrand and Mullainathan (2001), Kahneman, Ritov, and Schkade (1999), Boulier and Goldfarb (1998), Harrison and Kriström (1995), and LaPiere (1934).\n\n17. See e.g. Vrij (2000) and Frank (1988).\n\n18. Shermer (2002: 82).\n\n19. Samuelson (1946: 187).\n\n20. Chambers (1952: 444).\n\n21. Chambers (1952: 15).\n\n22. Crossman (1949: 23, 56, 162).\n\n23. Caplan (forthcoming a).\n\n24. Nasar (1998: 335).\n\n25. Nasar (1998: 295).\n\n26. Böhm-Bawerk (1959: 320; emphasis added).\n\n27. Mosca (1939: 166).\n\n28. Knox (1967).\n\n29. Thus, social pressure can make the cost of irrationality negative. A negative cost is also possible if false beliefs (e.g. overconfidence or overoptimism) increase your abilities by, for example, reducing stress (Compte and Postlewait 2004; Caplan 2000; Taylor 1989). For a rational model of overconfidence, see Van Den Steen (2004).\n\n30. See e.g. Landsburg (1993) and Olson (1971).\n\n31. Of course, the disutility of pollution need not be linear.\n\n32. Notice that the divergence of the private and social costs of false beliefs is conceptually distinct from a more familiar pitfall of group decision-making: conflicting preferences. The mechanism I describe works even if people want the same outcome.\n\n33. Schumpeter (1950: 262).\n\n34. For important exceptions, see Rabin (1998), Thaler (1992), Akerlof (1989), and Akerlof and Dickens (1984, 1982).\n\n35. See e.g. Glaeser (2003) and Caplan (2001a, 2000).\n\n36. Caplan (2000) coined this term, but Mele (2004) and Tirole (2002) have recently used the same label somewhat differently. Mele (2004) in part argues, consistently with my thesis, that it would not be rational to make large material sacrifices in order to become fully rational. Tirole (2002) shows that given imperfect self-knowledge, imperfect willpower, and/or imperfect recall, apparent irrationality can make people more successful. Another approach that is superficially similar to mine is Schelling (1980), which emphasizes that it may be rational to have a reputation for irrationality in order to improve your bargaining power.\n\n37. Rational irrationality is interestingly compatible with Lupia and McCubbins’s (1998: 23) equation of rationality with “all human behavior that is directed toward the pursuit of pleasure and the avoidance of pain.” On my account, though, voters focus on the pleasure and pain of accepting different beliefs, not the pleasure and pain of living under different policies. If you want more pleasure and less pain, it is far more effective to direct your behavior to something you can control—your beliefs—instead of something you cannot control—policy.\n\n38. Note that to draw a horizontal price line, we must assume that the material cost of irrationality is proportional to the quantity consumed.\n\n39. Indeed, under rarely if ever observed circumstances, demand curves can slope upwards.\n\n40. Still, one would still expect that—like other elasticities—the responsiveness of rationality to incentives would be greater in the long run than the short run.\n\n41. Edgerton (1992: 196).\n\n42. See e.g. Mele (2001, 1987).\n\n43. Orwell (1983: 177).\n\n44. Noss (1974: 114–16).\n\n45. Mosca (1939: 181–82).\n\n46. Caplan (forthcoming c) elaborates on this point.\n\n47. Edgerton (1992: 137).\n\n48. Edgerton (1992: 137).\n\n49. Holloway (1994: 208, 209, 22).\n\n50. Holloway (1994: 208).\n\n51. Becker (1996).\n\n52. Holloway (1994: 211).\n\n53. Holloway (1994: 140).\n\n54. See e.g. Conquest (1991), Bullock (1991), and Tucker (1990, 1973).\n\n55. Holloway (1994: 211–12).\n\n56. Holloway (1994: 148).\n\n57. Holloway (1994: 148).\n\n58. Holloway (1994: 149, 218).\n\n59. Hanson (1995).\n\n60. Hoelzl and Rustichini (2005).\n\n61. Tocqueville (1969: 442).\n\n62. See e.g. Mulligan and Hunter (2003), Brennan and Lomasky (1993: 54–61), and Meehl (1977). If voters choose people instead of policy, the probability of decisiveness shrinks further. Even if an election were freakishly decided by one vote, the winning politician could still break the promises that swung the election in his favor.\n\n63. On the recounts, see Ceaser and Busch (2001).\n\n64. See e.g. Weissberg (2002).\n\n65. Highton (2004); Lott and Kenny (1999); Filer, Kenny, and Morton (1993).\n\n66. Furthermore, in contrast to financial and betting markets, democracy lets people with severe biases continue to participate at no extra charge.\n\n67. Le Bon (1960: 175).\n\n68. Tyler and Weber (1982); Lord, Ross, and Lepper (1979).\n\n69. Orwell (1968: 252).\n\n70. For a more detailed discussion, see Caplan (2000).\n\n71. See Prisching (1995).\n\n72. Schumpeter (1950: 258).\n\n73. Schumpeter (1950: 258–59).\n\n74. Bastiat (1964a: 21).\n\n75. See e.g. Kirchgässner and Pommerehne (1993), Kirchgässner (1992), and Akerlof and Yellen (1985).\n\n76. See e.g. Smith and Walker (1993).\n\n77. We were both on the Andrei Shleifer–organized panel on competition. To illustrate his thesis, Thaler summarized his research (Cronqvist and Thaler 2004) on Swedish social-security privatization. Given a choice, most Swedes switched from the default fund (designed by government employees) to plainly inferior funds (higher management fees, higher risk, and lower returns) offered by the private sector. Government made a better decision than the typical market participant did, despite his strong financial incentive to be rational.\n\nAn important problem with this study is that it compares private choice to one of the best-designed government pension programs in the world, instead of comparing private choice to average government pension programs.\n\n78. Camerer and Hogarth (1999: 7).\n\n79. Harrison and Rutström (forthcoming).\n\n80. Camerer and Hogarth (1999: 34).\n\n81. Hoelzl and Rustichini (2005).\n\n82. Camerer and Hogarth (1999: 35).\n\n83. Camerer and Hogarth (1999: 10).\n\n84. Harrison and List (2004); List (2003).\n\n85. Camerer and Hogarth (1999: 23).\n\n86. Glucksberg (1962). Another common caveat is that high incentives can reduce performance by “stressing people out.” Here again, common sense insists on a short-run/long-run distinction. At a given instant, high incentives may reduce performance by increasing stress. But over a longer horizon, people who foresee a high-incentive/high-stress situation work harder to prepare for the challenge.\n\n87. Einhorn and Hogarth (1987: 63). The experimental findings Einhorn and Hogarth are discussing concern utility theory rather than rational expectations; elsewhere in the same volume Hogarth and Reder (1987: 12) apply this point more generally.\n\n88. Brennan and Lomasky (1993, 1989). For another take on expressive voting, see Schuessler (2000a, 2000b). For an experimental test that contradicts Brennan and Lomasky, see Tyran (2004).\n\n89. See e.g. Sowell (2004b), Landsburg (1997), and Becker (1971).\n\n90. Brennan and Lomasky (1993: 48).\n\n91. Brennan and Lomasky (1993: 25; emphasis added).\n\n92. Brennan and Lomasky (1993: 16).\n\n93. This is not to deny that wishful thinking and indifference never coexist: The same people often simultaneously held that “Clinton did not have sexual relations with Lewinsky” and “I don’t care if Clinton had sexual relations with Lewinsky.” (Posner 1999)\n\n94. Brennan and Lomasky (1993: 50).\n\n95. Brennan and Lomasky (1993: 51; emphasis added). Brennan and Lomasky go on to admit that in their war example, “the assumption of voter ‘rationality’ seems particularly strained.” But this is a reservation about whether their model of expressive voting model fits the example; it does not question the conceptual distinction between expressive preferences and irrational beliefs.\n\n96. Fleming (1939).\n\n97. Brennan and Lomasky (1993: 35–36).\n\n98. See e.g. Barber (1993) and Mansbridge (1990).\n\nCHAPTER 6: FROM IRRATIONALITY TO POLICY\n\n1. Greider (1992: 16).\n\n2. See e.g. Glaeser (2005).\n\n3. See e.g. Greider (1992).\n\n4. Hitler (1943). The depression clearly increased support for Hitler, but Germany was still one of the richest countries in the world.\n\n5. On the Oedipus/Jocasta paradox, see Searle (1983).\n\n6. Böhm-Bawerk (1959: 10).\n\n7. Wittman (1989: 1402).\n\n8. See e.g. Camerer (1987).\n\n9. Krugman (1998: 18).\n\n10. Krugman (1998: 19).\n\n11. For further discussion, see Caplan (2003b).\n\n12. See Becker (1958) for an early answer to this question.\n\n13. Endowments include not only current and expected possessions, but current and expected marketable skills.\n\n14. If the costs of tax collection on imports were the same or lower than on domestic products, they might want a positive tariff. Still, no one would see reducing imports as a benefit.\n\n15. See Brennan and Lomasky (1993).\n\n16. See e.g. Easterbrook (2003), Lichter and Rothman (1999), Whitman (1998), Keeter (1996), and Simon (1996).\n\n17. One reason to be pessimistic even in this scenario is that there could be different tariff rates for different countries.\n\n18. See e.g. Cooter (2000).\n\n19. See e.g. Meltzer and Richard (1981).\n\n20. To be more precise, they are not selfish in the conventional sense of trying to maximize their wealth or income. My analysis does assume that people choose their political beliefs based on psychological benefits to themselves, ignoring the costs to society. Thus, my thesis is that voters are selfish in an unusual but non-tautologous sense of the word. I would like to thank philosopher Michael Huemer for highlighting this ambiguity.\n\n21. See Caplan (2001b).\n\n22. For important exceptions, see Peltzman (1990, 1985, 1984).\n\n23. See e.g. Funk (2000), Miller (1999), Funk and García-Monet (1997), Mutz and Mondak (1997), Holbrook and Garand (1996), Mutz (1993, 1992), Mansbridge (1990), Sears and Funk (1990), Citrin and Green (1990), Sears and Lau (1983), Feldman (1982), Sears et al. (1980), Sears, Hensler, and Speer (1979), and Sears et al. (1978).\n\n24. See e.g. Gelman et al. (2005), Manza and Brooks (1999), Luttbeg and Martinez (1990), and Kamieniecki (1985).\n\n25. Caplan (2001b).\n\n26. There is also a large literature finding that voters care a lot more about the nation’s economic success than their own (Funk and García-Monet 1997; Markus 1988; Conover, Feldman, and Knight, 1987; Kinder and Kiewiet 1981, 1979). But some (e.g. Kramer 1983) counter that this is perfectly consistent with self-interested voting because personal economic success, unlike national economic success, has a large random component. (Kinder and Kiewiet [1981: 132] recognized this possibility as well.) This leads voters to selfishly prefer candidates who were good for the country as a whole. Due to this ambiguity, I focus on voters’ preferences for specific policies, where the divergence between personal and national benefits is clearer.\n\n27. See e.g. Huddy, Jones, and Chard (2001), Rhodebeck (1993), Sears and Funk (1990), and Ponza et al. (1988).\n\n28. See e.g. Sears and Huddy (1990), and Shapiro and Mahajan (1986).\n\n29. Sears et al. (1980).\n\n30. See e.g. Blinder and Krueger (2004).\n\n31. Sears and Funk (1990); Lau, Brown, and Sears (1978).\n\n32. See e.g. Wolpert and Gimpel (1998), and Sears and Citrin (1985).\n\n33. Green and Gerken (1989).\n\n34. Does this contradict the self-interest assumption? Only if we interpret “People are selfish” to literally mean that “all people are one hundred percent selfish.” The contradiction disappears if we interpret “People are selfish” to mean “Most people are highly selfish” (Caplan 2001b).\n\n35. Another reason to expect more altruism in democracy is that charitable giving suffers from a Prisoners’ Dilemma (Wittman 2005a).\n\n36. Brennan and Lomasky (1993); Tullock (1981a, 1971).\n\n37. See e.g. Kliemt (1986).\n\n38. My colleague Tyler Cowen poses an interesting challenge to my view that people vote contrary to their self-interest due to low decisiveness: What about the Academy Awards? Would anything change if a voter were decisive? Yes. If a member of the Academy were decisive, he would be far more likely to vote for projects where he or his friends have a large financial stake. Questions of artistic merit would fade into the background.\n\n39. For more discussion, see Caplan (2002b). Some proponents of the sociotropic voting model distinguish it from altruistic voting (Kinder and Kiewiet 1981), but most of the political science literature now equates the two. I follow the current usage.\n\n40. Held (1990: 303).\n\n41. For more discussion, see Caplan (2002b).\n\n42. On the breakdown of the Median Voter Theorem, see e.g. McLean (2002) and Riker (1988). On the puzzle of stability, see Tullock (1981b).\n\n43. See e.g. Lakoff (2002), Hinich and Munger (1994), Jennings (1992), and Feldman (1988).\n\n44. See e.g. Levitt (1996), Kalt and Zupan (1990, 1984), and Kau and Rubin (1979).\n\n45. See e.g. the seminal work by Poole and Rosenthal (1997, 1991).\n\n46. On partisan voting, see e.g. Bartels (2000) and Miller (1991).\n\n47. See e.g. Blinder and Krueger (2004) and Caplan (2002a). However, it could definitely be argued that the SAEE shows that economic beliefs are two-dimensional. The variables that “make people think like economists” are predictively powerful yet orthogonal to left-right ideology (Caplan 2001d).\n\n48. See Poole and Rosenthal (1997: 86–114).\n\n49. For evidence on multidimensionality, see Besley and Coate (2000) and Koford (1994).\n\n50. For more discussion, see Caplan (2001d).\n\n51. For details on the derivation of figure 6.4, see this chapter’s technical appendix.\n\n52. See e.g. Hainmueller and Hiscox (forthcoming, 2005b), Walstad and Rebeck (2002), and Walstad (1997).\n\n53. See e.g. Benjamin and Shapiro (2005) and Frey, Pommerehne, and Gygi (1993). For preliminary work on the effect of intelligence on economic beliefs, see Caplan and Miller (2006).\n\n54. On politics, see Delli Carpini and Keeter (1996: 203–9); on toxicology, see Kraus, Malmfors, and Slovic (1992).\n\n55. For a particularly interesting theory of the gender gap on free trade versus protection, see Burgoon and Hiscox (2006).\n\n56. Verba et al. (1993); Leighley and Nagler (1992a, 1992b).\n\n57. See e.g. Meltzer and Richard (1981).\n\n58. Sears and Funk (1990).\n\n59. Mueller and Stratmann (2003) argue that there is empirical evidence in favor of both hypotheses.\n\n60. Is it legitimate to extrapolate from the economic beliefs of the SAEE’s random sample to the economic beliefs of self-selected voters? Yes; controlling for voter registration leaves the results virtually unchanged (Caplan 2002b: 429).\n\n61. Surowiecki (2004).\n\n62. Dee (2004).\n\n63. This plausibly assumes that get-out-the-vote campaigns disproportionately influence less-educated voters. Independents are less knowledgeable than partisans (Delli Carpini and Keeter 1996: 172–73), and a field experiment by Gerber and Green (2000) finds that a modest get-out-the-vote campaign has no effect on the turnout of registered Democrats and Republicans, but increases the turnout of independents by 7%.\n\n64. Delli Carpini and Keeter (1996: 199).\n\n65. Caplan (2003c).\n\n66. See e.g. Olson (1996).\n\n67. For further discussion, see Zaller (2003).\n\n68. See e.g. Achen and Bartels (2004), Francis et al. (1994), MacKuen, Erikson, and Stimson (1992), and Fiorina (1981).\n\n69. There is also no trade-off if voters understand which policies work. In fact, if voters know which policies work, they can distinguish bad results caused by bad luck from bad results caused by bad leaders. This allows voters to severely punish incompetence and venality without scaring off qualified candidates (Wolfers 2001).\n\n70. For further discussion, see Caplan (2003b).\n\n71. GSS variable identifiers POLLEFF16 and POLLEFF17.\n\n72. See e.g. Duch, Palmer, and Anderson (2000).\n\n73. See e.g. Buchanan (1998).\n\n74. Conversely, voters may fail to punish leaders for problems within their control. Most obviously, they may reward politicians for short-run benefits, but fail to blame them for long-run costs (Achen and Bartels 2004).\n\n75. Achen and Bartels (2004: 6).\n\n76. For a different view, see Groseclose and McCarty (2001).\n\n77. Caplan (forthcoming b; 2002a).\n\n78. See e.g. Gold et al. (2002), Lichter and Rothman (1999), and Kraus, Malmfors, and Slovic (1992).\n\n79. Kraus, Malmfors, and Slovic (1992).\n\n80. Kraus, Malmfors, and Slovic (1992: 228).\n\n81. Kraus, Malmfors, and Slovic (1992: 220–21).\n\n82. Lichter and Rothman (1999) similarly document that cancer researchers’ ideology has little effect on their scientific judgment. Liberal cancer researchers who do not work in the private sector still embrace their profession’s contrarian views. “As a group, the experts—whether conservative or liberal, Democratic or Republican—viewed cancer risks along roughly the same lines. Thus, their perspectives on this topic do not appear to be ‘contaminated’ by either narrow self-interest or broader ideological commitments” (1999: 116).\n\n83. See e.g. Viscusi (1996).\n\n84. Kraus, Malmfors, and Slovic (1992).\n\n85. Kraus, Malmfors, and Slovic (1992: 221).\n\n86. Caplan (2001d).\n\n87. Caplan (2001d). I remain indebted to Robin Hanson for suggesting this approach.\n\nCHAPTER 7: IRRATIONALITY AND THE SUPPLY SIDE OF POLITICS\n\n1. Schumpeter (1950: 262–63).\n\n2. See Frey and Eichenberger (1991, 1989).\n\n3. Blinder (1987: 196).\n\n4. Machiavelli (1952: 92–93).\n\n5. For further discussion, see Caplan (2003b).\n\n6. Posner (1999).\n\n7. See e.g. Lee, Moretti, and Butler (2004).\n\n8. Sowell (2004a: 1–2).\n\n9. Madison, Hamilton, and Jay (1966: 432).\n\n10. Machiavelli (1952: 93–94).\n\n11. For an overview, see Vrij (2000).\n\n12. IMDB (2005).\n\n13. See e.g. Klein (1994).\n\n14. Dye and Zeigler (1996: 295).\n\n15. See Amer (1998).\n\n16. The main difference between legal pleading and political pleading is that a lawyer can get rich by defending unpopular clients, but few politicians can succeed by standing up for unpopular causes.\n\n17. Michels (1962: 93).\n\n18. Gregor (1969: 120).\n\n19. Modern History Project (2005).\n\n20. See Zaller (1992).\n\n21. Krugman (2003: 196).\n\n22. Langer (2002).\n\n23. For a rational model of this and related phenomenon, see Alesina and Cukierman (1990).\n\n24. It is logically possible—though implausible—that voters are simply risk-preferrers. They would rather have a gamble whose expected value is the moderate position than receive the moderate position with certainty. Howitt and Wintrobe (1995) use the opposite assumption to explain why politicians avoid raising issues in the first place: they prefer the status quo with certainty to the gamble of a fresh political contest.\n\n25. See e.g. Burstein (2003), Bender and Lott (1996), and Bernstein (1989).\n\n26. Machiavelli (1952: 93).\n\n27. See e.g. Klein and Tabarrok (2001) and Tabarrok (2000).\n\n28. Food and Drug Administration (1997).\n\n29. Machiavelli (1952: 98).\n\n30. For an overview, see Sappington (1991).\n\n31. For a rational model in which voters actually prefer unresponsive politicians, see Maskin and Tirole (2004).\n\n32. The fact that Supreme Court justices usually stay on the bench long after the president who appointed them leaves office only marginally changes matters. Think about it this way. After the president retires, it is hard to punish him for unpopular rulings by his appointees. But if the public realizes this, it should be hypersensitive about any unpopular rulings that happen while the appointing president is still in office. Unpopular decisions now—when the president responsible for the appointments can still be punished—tell people to expect a series of unpopular decisions in the future—when it will be too late for the public to express its displeasure. It is only common sense to conclude: “No time like the present!” Now is the time to punish the president not only for the current unpopular choices of his subordinates, but for the whole series of unpopular choices you expect his subordinates to make during their tenure.\n\n33. Richmond (1997: 133).\n\n34. Tullock (1987: 74).\n\n35. Siprut (2004).\n\n36. Greider (1992: 89).\n\n37. For further discussion, see Caplan (2001c).\n\n38. See e.g. Matsusaka (2005), Besley and Case (2003), Besley and Coate (2000), Levitt (1996), Rowley, Tollison, and Tullock (1988), Buchanan and Tollison (1984), Brennan and Buchanan (1980), Olson (1971), Tullock (1967), and Downs (1957). For more optimistic views, see Burstein (2003), Cannes-Wrone, Brady, and Cogan (2002), and Jacobs and Shapiro (2000).\n\n39. Becker (1983: 392).\n\n40. Wittman (1995, 1989).\n\n41. On the economics of media bias, see Sutter (2006). On persuasion by repetition, see DeMarzo, Vayanos, and Zweibel (2003).\n\n42. Hitler (1943: 180–81).\n\n43. Simon (1996: 220).\n\n44. On the history of publishing, see Encyclopedia Britannica (2005).\n\n45. Murphy and Shleifer (2004: 7–8).\n\n46. Schumpeter (1950: 263).\n\n47. Rubin (2003: 164).\n\n48. On the evolutionary basis of xenophobia, see Reynolds, Falger, and Vine (1987). For a wide-ranging attempt to link modern economic biases with our evolutionary heritage, see Rubin (2003).\n\n49. Simon (1995a: 655).\n\n50. For a particularly insightful model of this process, see Kuran and Sunstein (1999).\n\n51. See e.g. Mullainathan and Shleifer (2005) and Glaeser (2003).\n\n52. On the Alar scare, see Kuran and Sunstein (1999).\n\n53. See e.g. Murray, Schwartz, and Lichter (2001).\n\n54. Kuttner (1997: 345).\n\n55. Wittman (2005b).\n\n56. See e.g. Stratmann (2005), and Ansolabehere, de Figueiredo, and Snyder (2002).\n\n57. See e.g. Glaeser (2003).\n\n58. Krugman (2003: 145).\n\nCHAPTER 8: “MARKET FUNDAMENTALISM” VERSUS THE RELIGION OF DEMOCRACY\n\n1. Brainy Quote (2005a).\n\n2. Colander (2005) shows that the degree of consensus perceived by the latest generation of economists has substantially increased.\n\n3. Reder (1999: 236).\n\n4. Tucker (1978: 461; 460; 475).\n\n5. Waters (1970: 249).\n\n6. Kuttner (1997: 37).\n\n7. Soros (1998: 20).\n\n8. Kuttner (1997: 6).\n\n9. Kuttner (1997: 6, 9; emphasis added).\n\n10. Kuttner (1997: 7; emphasis added).\n\n11. Stiglitz (2002a: 221).\n\n12. Friedman (2002: 32).\n\n13. Friedman (2002: 28).\n\n14. Rothbard (1962: 887). Even at the libertarian extreme of the economic profession, however, the charge of “market fundamentalism” does not exactly fit. On closer reading, Rothbard only makes the agnostic claim that the effect of government intervention on social welfare is ambiguous because every act of government hurts at least one person (Caplan 1999: 833–35).\n\n15. Bork (1990: 139).\n\n16. Shermer (2002: 142).\n\n17. Shermer (2002: 143).\n\n18. Eigen and Siegel (1993: 115).\n\n19. Kamber (1995).\n\n20. Bardhan (1999: 109).\n\n21. Greider (1992: 407).\n\n22. Bardhan (1999).\n\n23. Bardhan (1999: 93; 109).\n\n24. Shapiro (1996: 9).\n\n25. Shapiro (1996: 9; emphasis added).\n\n26. Shapiro (1996: 128).\n\n27. Shapiro (1996: 128).\n\n28. See e.g. Somin (2004), Delli Carpini and Keeter (1996), Dye and Zeigler (1996), Bennett (1996), Smith (1989), and Neuman (1986).\n\n29. Shapiro (1996: 129).\n\n30. Robert Bork (1990: 36–58) actually takes a more fundamentalist position than Shapiro. Bork largely accepts the economists’ view of the world. But if economics and the public disagree, he maintains that judges should still side with the public.\n\n31. Tetlock (2003: 320; emphasis added).\n\n32. Kuttner (1997: 37).\n\n33. Kuttner (1997: xi–xii).\n\n34. Council of Economic Advisers (2005: 304).\n\n35. Eigen and Siegel (1993: 109).\n\n36. For an in-depth discussion, see Hanson (2005).\n\n37. Wyden (2003).\n\n38. For further discussion, see Hanson (2006), and Wolfers and Zitzewitz (2004).\n\n39. National Commission on Terrorist Attacks Upon the United States (2004: 171–72, 499).\n\n40. Hanson (2006).\n\n41. Surowiecki (2004: 270).\n\n42. My story is that decision markets are more reliable than public opinion because markets charge for biased beliefs, and democracy does not. Surowiecki interestingly observes that play-money markets are less accurate than real-money markets, but still work fairly well because “status and reputation provided incentive enough to encourage a serious investment of time and energy” (2004: 20). But how well would they work if status and reputation depended on the orthodoxy, not the accuracy, of one’s beliefs?\n\n43. Wyden (2003).\n\n44. See e.g. Gillman (1993).\n\n45. Kuttner (1997: 7).\n\n46. See e.g. McChesney (1999).\n\n47. See e.g. Shapiro and Hacker-Cordón (1999), Shapiro (1999, 1996), and Holmes and Sunstein (1999).\n\n48. Shapiro (1996: 8).\n\n49. Shapiro (1996: 37). Shapiro is responding to Riker and Weingast’s concerns about cyclical and strategic voting, but his objection is clearly much more general.\n\n50. Shapiro and Hacker-Cordón (1999: 6).\n\n51. For further discussion, see Caplan (2002b).\n\n52. Akerlof (1970).\n\n53. For doubts about the role of adverse selection in insurance markets, see Chiappori and Salanie (2000), Cawley and Philipson (1999), and Hemenway (1990).\n\n54. See e.g. Stiglitz (2003, 2002a).\n\n55. Stigler (1986).\n\n56. Bastiat (1964b: 57–58).\n\n57. Speck (1993: 175).\n\n58. By way of comparison, Citrin, Schickler, and Sides (2003) conclude that 100% participation would at most mildly help the Democratic Party.\n\n59. See e.g. Caplan (2001b), Sears and Funk (1990), and Citrin and Green (1990).\n\n60. Pinker (2002: 235).\n\n61. On debiasing, see Fischhoff (1982).\n\n62. Pinker (2002: 236).\n\n63. See Tullock (1999) and Harberger (1993).\n\n64. Coase (1999: 44).\n\n65. Krugman (1996: 118).\n\n66. For further discussion, see Tollison and Wagner (1991).\n\n67. Samuelson (1966: 1628).\n\n68. Bastiat (1964a: 5).\n\n69. For further discussion, see Pashigian (2000).\n\n70. For example, price can be driven down to marginal cost by Bertrand competition and contestable monopoly, not just perfect competition.\n\n71. Emerson (n.d.: 42).\n\n72. Bastiat (1964a: 56–57).\n\n73. For further discussion, see Caplan (2002c).\n\n74. For an appreciation of Bastiat’s political economy, see Caplan and Stringham (2005).\n\n75. Bastiat (1964a: 121).\n\n76. On this question, see Frey (2002, 2000).\n\n77. Keynes (1963: 373).\n\nCONCLUSION: IN PRAISE OF THE STUDY OF FOLLY\n\n1. Persson and Tabellini (2000: 419).\n\n2. For an overview, see Grossman and Helpman (2001).\n\n3. Wittman (1995, 1989).\n\n4. See e.g. Caplan and Stringham (2005), Althaus (2003), Monroe (1998, 1983), Page and Shapiro (1992, 1983), Erikson, Wright, and McIver (1989), and Wright, Erikson, and McIver (1987).\n\n5. For further discussion, see Wintrobe (1987).\n\n6. Sachs (1994: 507).\n\n7. See e.g. Friedman (1996) and Green and Shapiro (1994).\n\n8. For further discussion, see Saint-Paul (2000).\n\n9. Fernandez and Rodrik (1991).\n\n10. See e.g. Caplan (2003c, 2001c) and Sachs and Warner (1995).\n\n11. Rodrik (1996).\n\n12. Fernandez and Rodrik (1991).\n\n13. Kuran and Sunstein (1999).\n\n14. For an overview, see Besley and Case (2003).\n\nREFERENCES\n\nAbramson, Paul, John Aldrich, and David Rohde. 2002. Change and Continuity in the 2000 Elections. Washington, DC: CQ Press.\n\nAchen, Christopher, and Larry Bartels. 2004. “Musical Chairs: Pocketbook Voting and the Limits of Democratic Accountability.” URL http://www.princeton.edu/~bartels/chairs.pdf.\n\nAinslie, George. 1992. Piconomics. Cambridge: Cambridge University Press.\n\nAkerlof, George. 1970. “The Market for ‘Lemons’: Quality Uncertainty and the Market Mechanism.” Quarterly Journal of Economics 84(3): 488–500.\n\n———. 1989. “The Economics of Illusion.” Economics and Politics 1(1): 1–15.\n\nAkerlof, George, and William Dickens. 1982. “The Economic Consequences of Cognitive Dissorance.” American Economic Review 72(3): 307–19.\n\n———. 1984. “The Economic Consequences of Cognitive Dissonance.” In George Akerlof. An Economic Theorist’s Book of Tales. Cambridge: Cambridge University Press: 123–44.\n\nAkerlof, George, and Janet Yellen. 1985. “Can Small Deviations from Rationality Make Significant Differences to Economic Equilibria?” American Economic Review 75(4): 708–20.\n\nAlesina, Alberto, and Alex Cukierman. 1990. “The Politics of Ambiguity.” Quarterly Journal of Economics 105(4): 829–50.\n\nAlesina, Alberto, and Howard Rosenthal. 1994. Partisan Politics, Divided Government, and the Economy. Cambridge: Cambridge University Press.\n\nAlston, Richard, J. R. Kearl, and Michael Vaughan. 1992. “Is There a Consensus Among Economists in the 1990’s?” American Economic Review 82(2): 203–9.\n\nAlthaus, Scott. 1996. “Opinion Polls, Information Effects, and Political Equality: Exploring Ideological Biases in Collective Opinion.” Political Communication 13(1): 3–21.\n\n———. 1998. “Information Effects in Collective Preferences.” American Political Science Review 92(2): 545–58.\n\n———. 2003. Collective Preferences in Democratic Politics: Opinion Surveys and the Will of the People. Cambridge: Cambridge University Press.\n\nAmer, Mildred. 1998. “Membership of the 105th Congress: A Profile.” Government Division. Order No. 97–37 GOV, July 17.\n\nAnderson, Gary, and Robert Tollison. 1988. “Legislative Monopoly and the Size of Government.” Southern Economic Journal 54(3): 529–45.\n\nAndrews, Robert, ed. 1993. The Columbia Dictionary of Quotations. New York: Columbia University Press.\n\nAnsolabehere, Stephen, John de Figueiredo, and James Snyder. 2002. “Why Is There So Little Money in U.S. Politics?” NBER Working Paper No. 9409.\n\nApplebaum, Anne. 2003. Gulag: A History. New York: Doubleday.\n\nArendt, Hannah. 1973. The Origins of Totalitarianism. New York: Harcourt, Brace & World.\n\nAristotle. 1941. The Basics Works of Aristotle. Ed. Richard McKeon. New York: Random House.\n\nAumann, Robert. 1976. “Agreeing to Disagree.” Annals of Statistics 4(6): 1236–39.\n\nAusten-Smith, David. 1991. “Rational Consumers and Irrational Voters: A Review Essay on Black Hole Tariffs and Endogenous Policy Theory.” Economics and Politics 3(1): 73–92.\n\nAusten-Smith, David, and Jeffrey Banks. 1996. “Information Aggregation, Rationality, and the Condorcet Jury Theorem.” American Political Science Review 90(1): 34–45.\n\nBabcock, Linda, and George Loewenstein. 1997. “Explaining Bargaining Impasse: The Role of Self-Serving Biases.” Journal of Economic Perspectives 11(1): 109–26.\n\nBanks, Jeffrey, and Barry Weingast. 1992. “The Political Control of Bureaucracies under Asymmetric Information.” American Journal of Political Science 36(2): 509–24.\n\nBarber, Benjamin. 1993. “Reductionist Political Science and Democracy.” In George Marcus and Russell Hanson, eds., Reconsidering the Democratic Public. University Park: Pennsylvania State University Press: 65–72.\n\nBardhan, Pranab. 1999. “Democracy and Development: A Complex Relationship.” In Ian Shapiro and Casiano Hacker-Cordón, eds., Democracy’s Value. Cambridge: Cambridge University Press: 93–111.\n\nBarkow, Jerome, Leda Cosmides, and John Tooby, eds. 1992. The Adapted Mind. New York: Oxford University Press.\n\nBartels, Larry. 1996. “Uninformed Voters: Information Effects in Presidential Elections.” American Journal of Political Science 40(1): 194–230.\n\n———. 2000. “Partisanship and Voting Behavior, 1952–1996.” American Journal of Political Science 44(1): 35–50.\n\n———. 2004. “Homer Gets a Tax Cut: Inequality and Public Policy in the American Mind.” URL http://www.princeton.edu/~bartels/homer.pdf.\n\nBastiat, Frédéric. 1964a. Economic Sophisms. Irvington-on-Hudson, NY: Foundation for Economic Education.\n\n———. 1964b. Selected Essays on Political Economy. Irvington-on-Hudson, NY: Foundation for Economic Education.\n\nBecker, Gary. 1958. “Competition and Democracy.” Journal of Law and Economics 1: 105–9.\n\n———. 1968. “Crime and Punishment: An Economic Approach.” Journal of Political Economy 76(2): 169–217.\n\n———. 1971. The Economics of Discrimination. Chicago: University of Chicago Press.\n\n———. 1976a. The Economic Approach to Human Behavior. Chicago: University of Chicago Press.\n\n———. 1976b. “Toward a More General Theory of Regulation: Comment.” Journal of Law and Economics 19(2): 245–48.\n\n———. 1983. “A Theory of Competition Among Pressure Groups for Political Influence.” Quarterly Journal of Economics 98(3): 371–400.\n\n———. 1985. “Public Policies, Pressure Groups, and Dead Weight Costs.” Journal of Public Economics 28(3): 329–47.\n\nBecker, Jasper. 1996. Hungry Ghosts: Mao’s Secret Famine. New York: Free Press.\n\nBell, Graham. 1997. Selection: The Mechanism of Evolution. New York: Chapman and Hall.\n\nBender, Bruce, and John Lott. 1996. “Legislator Voting and Shirking: A Critical Review of the Literature.” Public Choice 87(1–2): 67–100.\n\nBenjamin, Daniel, and Jesse Shapiro. 2005. “Does Cognitive Ability Reduce Psychological Bias?” URL http://home.uchicago.edu/~jmshapir/iq022605.pdf.\n\nBennett, Stephen. 1996. “ ‘Know-Nothings’ Revisited Again.” Political Behavior 18(3): 219–33.\n\nBernstein, Robert. 1989. Elections, Representation, and Congressional Voting Behavior: The Myth of Constituency Control. Englewood Cliffs, NJ: Prentice Hall.\n\nBertrand, Marianne, and Sendhil Mullainathan. 2001. “Do People Mean What They Say? Implications for Subjective Survey Data.” American Economic Review 91(2): 67–72.\n\nBesley, Timothy, and Anne Case. 2003. “Political Institutions and Policy Choices: Evidence from the United States.” Journal of Economic Literature 41(1): 7–73.\n\nBesley, Timothy, and Stephen Coate. 2000. “Issue Unbundling Via Citizens’ Initiatives.” NBER Working Paper No. 8036.\n\nBhagwati, Jagdish. 2002. Free Trade Today. Princeton, NJ: Princeton University Press.\n\nBlaug, Mark. 2001. “No History of Ideas, Please, We’re Economists.” Journal of Economic Perspectives 15(1): 145–64.\n\nBlendon, Robert, John Benson, Mollyann Brodie, Richard Morin, Drew Altman, Daniel Gitterman, Mario Brossard, and Matt James. 1997. “Bridging the Gap Between the Public’s and Economists’ Views of the Economy.” Journal of Economic Perspectives 11(3): 105–88.\n\nBlinder, Alan. 1987. Hard Heads, Soft Hearts: Tough-Minded Economics for a Just Society. Reading, MA: Addison-Wesley.\n\nBlinder, Alan, and Alan Krueger. 2004. “What Does the Public Know about Economic Policy, and How Does It Know It?” Brookings Papers on Economic Activity 1: 327–87.\n\nBöhm-Bawerk, Eugen von. 1959. Capital and Interest. South Holland, IL: Libertarian Press.\n\nBorjas, George. 1994. “The Economics of Immigration.” Journal of Economic Literature 32(4): 1667–1717.\n\nBork, Robert. 1990. The Tempting of America. New York: Free Press.\n\nBoublil, Alain, Herbert Kretzmer, and Jean-Marc Natel. 1990. Les Misérables: The Complete Symphonic Recording. N.P.: Alain Boublil Music.\n\nBoudreaux, Donald. 1996. “Was Your High-School Civics Teacher Right After All? Donald Wittman’s The Myth of Democratic Failure.” Independent Review 1(1): 111–28.\n\nBoulier, Bryan, and Robert Goldfarb. 1998. “On the Use and Nonuse of Surveys in Economics.” Journal of Economic Methodology 5(1): 1–21.\n\nBrainy Quote. 2005a. “Bertrand Russell Quotes.” URL http://www.brainyquote.com/quotes/authors/b/bertrand\\_russell.html.\n\n———. 2005b. “H. L. Mencken Quotes.” URL http://www.brainyquote.com/quotes/authors/h/h\\_l\\_mencken.html.\n\nBrennan, Geoffrey, and James Buchanan. 1980. The Power to Tax: Analytical Foundations of a Fiscal Constitution. Cambridge: Cambridge University Press.\n\nBrennan, Geoffrey, and Loren Lomasky. 1989. “Large Numbers, Small Costs: The Uneasy Foundations of Democratic Rule.” In Geoffrey Brennan and Loren Lomasky, eds., Politics and Process: New Essays in Democratic Thought. Cambridge: Cambridge University Press: 42–59.\n\n———. 1993. Democracy and Decision: The Pure Theory of Electoral Preference. Cambridge: Cambridge University Press.\n\nBreton, Albert, and Ronald Wintrobe. 1982. The Logic of Bureaucratic Conduct: An Economic Analysis of Competition, Exchange, and Efficiency in Private and Public Organizations. New York: Cambridge University Press.\n\nBrossard, Mario, and Steven Pearlstein. 1996. “Great Divide: Economists vs. Public: Data and Daily Life Tell Different Stories.” Washington Post, October 15, A1.\n\nBuchanan, James, and Robert Tollison, eds. 1984. The Theory of Public Choice II. Ann Arbor: University of Michigan Press.\n\nBuchanan, Patrick. 1998. The Great Betrayal: How American Sovereignty and Social Justice are Being Sacrificed to the Gods of the Global Economy. Boston: Little, Brown.\n\nBullock, Alan. 1991. Hitler and Stalin: Parallel Lives. New York: Vintage Books.\n\nBureau of Economic Analysis. 2005. “Foreign Direct Investment in the U.S.” URL http://www.bea.doc.gov/bea/di/di1fdibal.htm.\n\nBurgoon, Brian, and Michael Hiscox. 2006. “The Mysterious Case of Female Protectionism: Gender Bias in Attitudes Toward International Trade.” URL http://www.people.fas.harvard.edu/~hiscox/FemaleProtectionism.pdf.\n\nBurstein, Paul. 2003. “The Impact of Public Opinion on Public Policy: A Review and an Agenda.” Political Research Quarterly 56(1): 29–40.\n\nCamerer, Colin. 1987. “Do Biases in Probability Judgment Matter in Markets? Experimental Evidence.” American Economic Review 77(5): 981–97.\n\n———. 1995. “Individual Decision Making.” In John Kagel and Alvin Roth, eds., The Handbook of Experimental Economics. Princeton, NJ: Princeton University Press: 587–703.\n\nCamerer, Colin, and Robin Hogarth. 1999. “The Effects of Financial Incentives in Experiments: A Review and Capital-Labor-Production Framework.” Journal of Risk and Uncertainty 19(1–3): 7–42.\n\nCannes-Wrone, Brandice, David Brady, and John Cogan. 2002. “Out of Step, Out of Office: Electoral Accountability and House Member Voting.” American Political Science Review 96(1): 127–40.\n\nCaplan, Bryan. 1999. “The Austrian Search for Realistic Foundations.” Southern Economic Journal 65(4): 823–38.\n\n———. 2000. “Rational Irrationality: A Framework for the Neoclassical-Behavioral Debate.” Eastern Economic Journal 26(2): 191–211.\n\n———. 2001a. “Rational Ignorance versus Rational Irrationality.” Kyklos 54(1): 3–26.\n\n———. 2001b. “Libertarianism Against Economism: How Economists Misunderstand Voters and Why Libertarians Should Care.” Independent Review 5(4): 539–63.\n\n———. 2001c. “Rational Irrationality and the Microfoundations of Political Failure.” Public Choice 107 (3–4): 311–31.\n\n———. 2001d. “What Makes People Think Like Economists? Evidence on Economic Cognition from the Survey of Americans and Economists on the Economy.” Journal of Law and Economics 44(2): 395–426.\n\n———. 2001e. “Probability, Common Sense, and Realism: A Reply to Hülsmann and Block.” Quarterly Journal of Austrian Economics 4(2): 69–86.\n\n———. 2002a. “Systematically Biased Beliefs About Economics: Robust Evidence of Judgemental Anomalies from the Survey of Americans and Economists on the Economy.” Economic Journal 112(479): 433–58.\n\n———. 2002b. “Sociotropes, Systematic Bias, and Political Failure: Reflections on the Survey of Americans and Economists on the Economy.” Social Science Quarterly, 83(2): 416–35.\n\n———. 2002c. “Economic Illiteracy: A Modest Plea Against Humility.” Royal Economic Society Newsletter 119:9–10.\n\n———. 2003a. “Stigler-Becker versus Myers-Briggs: Why Preference-Based Explanations Are Scientifically Meaningful and Empirically Important.” Journal of Economic Behavior and Organization 50(4): 391–405.\n\n———. 2003b. “The Logic of Collective Belief.” Rationality and Society 15(2): 218–42.\n\n———. 2003c. “The Idea Trap: The Political Economy of Growth Divergence.” European Journal of Political Economy 19(2): 183–203.\n\n———. 2005a. “From Friedman to Wittman: The Transformation of Chicago Political Economy.” Econ Journal Watch 2(1): 1–21.\n\n———. 2005b. “Rejoinder to Wittman: True Myths.” Econ Journal Watch 2(2): 165–85.\n\n———. Forthcoming a. “The Economics of Szasz: Preferences, Constraints, and Mental Illness.” Rationality and Society. URL http://www.gmu.edu/departments/economics/bcaplan/szaszjhe.doc.\n\n———. Forthcoming b. “How Do Voters Form Positive Economic Beliefs? Evidence from the Survey of Americans and Economists on the Economy.” Public Choice. URL http://www.gmu.edu/departments/economics/bcaplan/econbelfin.doc.\n\n———. Forthcoming c. “Terrorism: The Relevance of the Rational Model.” Public Choice. URL http://www.gmu.edu/departments/economics/bcaplan/relevance6.doc.\n\nCaplan, Bryan, and Tyler Cowen. 2004. “Do We Underestimate the Benefits of Cultural Competition?” American Economic Review 94(2): 402–7.\n\nCaplan, Bryan, and Stephen Miller. 2006. “Economic Beliefs, Intelligence, and Ability Bias: Evidence from the General Social Survey.” URL http://www.gmu.edu/departments/economics/bcaplan/iqbeliefej.doc.\n\nCaplan, Bryan, and Edward Stringham. 2005. “Mises, Bastiat, Public Opinion, and Public Choice: What’s Wrong With Democracy.” Review of Political Economy 17(1): 79–105.\n\nCarroll, Lewis. 1999. Alice’s Adventures in Wonderland and Through the Looking-Glass. New York: Barnes and Noble.\n\nCawley, John, and Tomas Philipson, 1999. “An Empirical Examination of Information Barriers to Trade in Insurance.” American Economic Review 89(4): 827–46.\n\nCeaser, James, and Andrew Busch. 2001. The Perfect Tie: The True Story of the 2000 Presidential Election. Lanham, MD: Rowman & Littlefield.\n\nCerf, Christopher, and Victor Navasky. 1998. The Experts Speak: The Definitive Compendium of Authoritative Misinformation. New York: Villard.\n\nChambers, Whittaker. 1952. Witness. New York: Random House.\n\nChiappori, Pierre-Andre, and Bernard Salanie, 2000. “Testing for Asymmetric Information in Insurance Markets.” Journal of Political Economy 108(1): 56–78.\n\nChicago Council on Foreign Relations. 2004. “Global Views 2004.” URL http://www.ccfr.org/globalviews2004/sub/pdf/Global\\_Views\\_2004\\_US.pdf.\n\nChicago Council on Foreign Relations and the German Marshall Fund of the United States. 2002a. “Worldviews: Topline Data from U.S. Public Survey.” URL http://www.worldviews.org/detailreports/usreport/public\\_topline\\_report.pdf.\n\n———. 2002b. “Worldviews: American Public Opinion & Foreign Policy.” URL http://www.worldviews.org/detailreports/usreport.pdf.\n\n———. 2002c. “Worldviews: American and European Public Opinion & Foreign Policy.” URL http://www.worldviews.org/detailreports/compreport.pdf.\n\nChong, Dennis, Herbert McClosky, and John Zaller. 1983. “Patterns of Support for Democratic and Capitalist Values in the United States.” British Journal of Political Science 13(4): 401–40.\n\nCitrin, Jack, and Donald Green. 1990. “The Self-Interest Motive in American Public Opinion.” Research in Micropolitics 3:1–28.\n\nCitrin, Jack, Eric Schickler, and John Sides. 2003. “What If Everyone Voted? Simulating the Impact of Increased Turnout in Senate Elections.” American Journal of Political Science 47(1): 75–90.\n\nCoase, Ronald. 1998. “Comment on Thomas W. Hazlett: Assigning Property Rights to Radio Spectrum Users: Why Did FCC License Auctions Take 67 Years?” Journal of Law and Economics 41(2): 577–80.\n\n———. 1999. “Economists and Public Policy.” In Daniel Klein, ed., What Do Economists Contribute? New York: New York University Press: 33–52.\n\nCoate, Stephen, and Stephen Morris. 1995. “On the Form of Transfers to Special Interests.” Journal of Political Economy 103(6): 1210–35.\n\n———. 1999. “Policy Persistence.” American Economic Review 89(5): 1327–36.\n\nColander, David. 2005. “The Making of an Economist Redux.” Journal of Economic Perspectives 19(1): 175–98.\n\nCole, Matthew. 2003. “Environmental Optimists, Environmental Pessimists and the Real State of the World—An Article Examining The Skeptical Environmentalist: Measuring the Real State of the World by Bjorn Lomborg.” Economic Journal 113(488): F362–F380.\n\nCompte, Olivier, and Andrew Postlewait. 2004. “Confidence-Enhanced Performance.” American Economic Review 94(5): 1536–57.\n\nConover, Pamela, and Stanley Feldman. 1986. “Emotional Reactions to the Economy: I’m Mad as Hell and I’m Not Going to Take It Anymore.” American Journal of Political Science 30(1): 50–78.\n\nConover, Pamela, Stanley Feldman, and Kathleen Knight. 1987. “The Personal and Political Underpinnings of Economic Forecasts.” American Journal of Political Science 31(3): 559–83.\n\nConquest, Robert. 1986. Harvest of Sorrow: Soviet Collectivization and the Terror-Famine. New York: Oxford University Press.\n\n———. 1991. Stalin: Breaker of Nations. New York: Penguin.\n\nConverse, Philip. 1964. “The Nature of Belief Systems in Mass Publics.” In David Apter, ed., Ideology and Discontent. New York: Free Press: 206–61.\n\n———. 1990. “Popular Representation and the Distribution of Information.” In John Ferejohn and James Kuklinski, eds., Information and Democratic Processes. Urbana: University of Illinois Press: 369–88.\n\nCooter, Robert. 2000. The Strategic Constitution. Princeton, NJ: Princeton University Press.\n\nCosmides, Leda. 1989. “The Logic of Social Exchange: Has Natural Selection Shaped How Humans Reason? Studies with the Wason Selection Task.” Cognition 31(3): 187–276.\n\nCosmides, Leda, and John Tooby. 1996. “Are Humans Good Intuitive Statisticians After All? Rethinking Some Conclusions from the Literature on Judgment under Uncertainty.” Cognition 58(1): 1–73.\n\nCouncil of Economic Advisers. 2005. Economic Report of the President. Washington, DC: U.S. Government Printing Office.\n\nCoursey, Don, and Russell Roberts. 1991. “Competition in Political and Economic Markets.” Public Choice 70(1): 83–88.\n\nCourtois, Stéphane, Nicolas Werth, Jean-Louis Panné, Andrzej Paczkowski, Karel Bartošek, and Jean-Louis Margolin. 1999. The Black Book of Communism: Crimes, Terror, Repression. Cambridge: Harvard University Press.\n\nCowen, Tyler. 1998. In Praise of Commercial Culture. Cambridge: Harvard University Press.\n\n———. 2001. “How Do Economists Think About Rationality?” URL http://www.gmu.edu/jbc/Tyler/rationality.pdf.\n\n———. 2003. “Self-Deception as the Root of Political Failure.” URL http://www.gmu.edu/jbc/Tyler/PrideandSelf.pdf.\n\nCowen, Tyler, and Robin Hanson. Forthcoming. “Are Disagreements Honest?” Journal of Economic Methodology. URL http://hanson.gmu.edu/deceive.pdf.\n\nCox, W. Michael, and Richard Alm. 1999. Myths of Rich and Poor. New York: Basic Books.\n\nCrain, W. Mark. 1977. “On the Structure and Stability of Political Markets.” Journal of Political Economy 85(4): 829–42.\n\nCrew, Michael, and Charlotte Twight. 1990. “On the Efficiency of Law: A Public Choice Perspective.” Public Choice 66(1): 15–36.\n\nCronqvist, Henrik, and Richard Thaler. 2004. “Design Choices in Privatized Social-Security Systems: Learning from the Swedish Experience.” American Economic Review 94(2): 424–28.\n\nCrossman, Richard, ed. 1949. The God That Failed. New York: Harper and Brothers.\n\nCutler, Fred. 2002. “The Simplest Shortcut of All: Sociodemographic Characteristics and Electoral Choice.” Journal of Politics 64(2): 466–90.\n\nDahl, Gordon, and Michael Ransom. 1999. “Does Where You Stand Depend on Where You Sit?” American Economic Review 89(4): 703–27.\n\nDahl, Robert. 1989. Democracy and Its Critics. New Haven: Yale University Press.\n\nDasgupta, Partha, and Joseph Stiglitz. 1988. “Potential Competition, Actual Competition, and Economic Welfare.” European Economic Review 32(2–3): 569–77.\n\nDasgupta, Susmita, Benoit Laplante, Hua Wang, and David Wheeler. 2002. “Confronting the Environmental Kuznets Curve.” Journal of Economic Perspectives 16(1): 147–68.\n\nDavis, Steven, John Haltiwanger, and Scott Schuh. Job Creation and Destruction. Cambridge: MIT Press.\n\nDee, Thomas. 2004. “Are There Civic Returns to Education?” Journal of Public Economics 88(9): 1697–1720.\n\nDelli Carpini, Michael, and Scott Keeter. 1996. What Americans Know About Politics and Why It Matters. New Haven: Yale University Press.\n\nDeMarzo, Peter, Dimitri Vayanos, and Jeffrey Zwiebel. 2003. “Persuasion Bias, Social Influence, and Unidimensional Opinions.” Quarterly Journal of Economics 68(3): 909–68.\n\nDescartes, René. 1999. Discourse on Method. Indianapolis: Hackett\n\nDiamond, Jared. 1997. Guns, Germs, and Steel: The Fates of Human Societies. New York: Norton.\n\nDonovan, Nick, and David Halpern. 2002. Life Satisfaction: The State of Knowledge and Implications for Government. London: Cabinet Office/Prime Minister’s Strategy Unit.\n\nDowns, Anthony. 1957. An Economic Theory of Democracy. New York: Harper and Row.\n\nDrazen, Allan. 2000. Political Economy in Macroeconomics. Princeton, NJ: Princeton University Press.\n\nDrèze, Jean, and Amartya Sen, eds. 1990. The Political Economy of Hunger. New York: Oxford University Press.\n\nDuch, Raymond, Harvey Palmer, and Christopher Anderson. 2000. “Heterogeneity in Perceptions of National Economic Conditions.” American Journal of Political Science 44(4): 635–52.\n\nDye, Thomas, and Harmon Zeigler. 1992. The Irony of Democracy: An Uncommon Introduction to American Politics. 7th edition. Monterey, CA: Brooks/Cole.\n\n———. 1996. The Irony of Democracy: An Uncommon Introduction to American Politics. 10th ed. New York: Wadsworth.\n\nEasterbrook, Gregg. 2003. The Progress Paradox: How Life Gets Better While People Feel Worse. New York: Random House.\n\nEasterly, William. 2001. The Elusive Quest for Growth: Economists’ Adventures and Misadventures in the Tropics. Cambridge: MIT Press.\n\nEconomics and Statistics Administration. 2004. Statistical Abstract of the United States, 2003. Washington, DC: U.S. Department of Commerce.\n\nEdgerton, Robert. 1992. Sick Societies: Challenging the Myth of Primitive Harmony. New York: Free Press.\n\nEdlin, Aaron, Andrew Gelman, and Noah Kaplan. Forthcoming. “Voting as a Rational Choice: Why and How People Vote to Improve the Well-Being of Others.” Rationality and Society. URL http://www.stat.columbia.edu/~gelman/research/published/rational\\_final5.pdf.\n\nEhrlich, Paul. 1968. The Population Bomb. New York: Ballantine.\n\nEichenberger, Reiner, and Angel Serna. 1996. “Random Errors, Dirty Information, and Politics.” Public Choice 86(1–2): 137–56.\n\nEigen, Lewis, and Jonathan Siegel, eds. 1993. The Macmillan Dictionary of Political Quotations. New York: Macmillan.\n\nEinhorn, Hillel, and Robin Hogarth. 1987. “Decision Making Under Ambiguity.” In Robin Hogarth and Melvin Reder, eds., Rational Choice: The Contrast Between Economics and Psychology. Chicago: University of Chicago Press: 41–66.\n\nEmerson, Ralph. N.d. Essays. New York: Grosset and Dunlap.\n\nEncyclopedia Britannica. 2005. “Publishing.” 26: 415–49.\n\nEqual Employment Opportunity Commission. 2005. “Statistics.” URL http://www.eeoc.gov/stats.\n\nErikson, Robert, Gerald Wright, and John McIver. 1989. “Political Parties, Public Opinion, and State Policy in the United States.” American Political Science Review 83(3): 729–50.\n\nFedderson, Timothy. 2004. “Rational Choice Theory and the Paradox of Not Voting.” Journal of Economic Perspectives 18(1): 99–112.\n\nFeldman, Stanley. 1982. “Economic Self-Interest and Political Behavior.” American Journal of Political Science 26(3): 446–66.\n\n———. 1988. “Structure and Consistency in Public Opinion: The Role of Core Beliefs and Values.” American Journal of Political Science 32(2): 416–40.\n\nFernandez, Raquel, and Rodrik, Dani. 1991. “Resistance to Reform: Status Quo Bias in the Presence of Individual-Specific Uncertainty.” American Economic Review 81(5): 1146–55.\n\nFiler, John, Lawrence Kenny, and Rebecca Morton. 1993. “Redistribution, Income, and Voting.” American Journal of Political Science 37(1): 63–87.\n\nFiorina, Morris. 1981. Retrospective Voting in American National Elections. New Haven: Yale University Press.\n\nFiorina, Morris. 1989. Congress: Keystone of the Washington Establishment. New Haven: Yale University Press.\n\nFischhoff, Baruch. 1982. “Debiasing.” In Daniel Kahneman, Paul Slovic, and Amos Tversky, eds., Judgment under Uncertainty: Heuristics and Biases. Cambridge: Cambridge University Press: 422–44.\n\nFleming, Victor, dir. 1939. Gone with the Wind. DVD. MGM.\n\nFogel, Robert. 1999. “Catching Up with the Economy.” American Economic Review 89(1): 1–21.\n\nFood and Drug Administration. 1997. “FDA Talk Paper.” URL http://www.fda.gov/bbs/topics/ANSWERS/ANS00819.html.\n\nFrancis, Wayne, Lawrence Kenny, Rebecca Morton, and Amy Schmidt. 1994. “Retrospective Voting and Political Mobility.” American Journal of Political Science 38(4): 999–1024.\n\nFrank, Robert. 1988. Passions Within Reason: The Strategic Role of the Emotions. New York: Norton.\n\nFrank, Thomas. 2004. What’s the Matter with Kansas? How Conservatives Won the Heart of America. New York: Metropolitan Books.\n\nFreeman, A. Myrick, III. 2002. “Environmental Policy since Earth Day I: What Have We Gained?” Journal of Economic Perspectives 16(1): 125–46.\n\nFremling, Gertrud, and John Lott. 1989. “Time Dependent Information Costs, Price Controls, and Successive Government Intervention.” Journal of Law, Economics and Organization 5(2): 293–306.\n\n———. 1996. “The Bias Towards Zero in Aggregate Perceptions: An Explanation Based on Rationally Calculating Individuals.” Economic Inquiry 34(2): 276–95.\n\nFrey, Bruno. 2000. “Does Economics Have an Effect? Toward an Economics of Economics.” Institute for Empirical Research in Economics Working Paper No. 36.\n\n———. 2002. “Do Economists Affect Policy Outcomes?” Working Paper Series, Institute for Empirical Research, University of Zürich.\n\nFrey, Bruno, and Reiner Eichenberger. 1989. “Anomalies and Institutions.” Journal of Economic Behavior and Organization 145(3): 423–37.\n\n———. 1991. “Anomalies in Political Economy.” Public Choice 68(1–3): 71–89.\n\n———. 1992. “Economics and Economists: A European Perspective.” American Economic Review 82(2): 216–20.\n\n———. 1993. “American and European Economics and Economists.” Journal of Economic Perspectives 7(4): 185–93.\n\nFrey, Bruno, Werner Pommerehne, and Beat Gygi. 1993. “Economics Indoctrination or Selection? Some Empirical Results.” Journal of Economic Education 24(3): 271–81.\n\nFriedman, Jeffrey, ed. 1996. The Rational Choice Controversy: Economic Models of Politics Reconsidered. New Haven: Yale University Press.\n\nFriedman, Milton. 1953. “The Methodology of Positive Economics.” In Essays in Positive Economics. Chicago: University of Chicago Press: 3–43.\n\n———. 2002. Capitalism and Freedom. Chicago: University of Chicago Press.\n\nFriedrich, Carl, and Zbigniew Brzezinski. 1965. Totalitarian Dictatorship and Autocracy. New York: Praeger.\n\nFuchs, Victor, Alan Krueger, and James Poterba. 1998. “Economists’ Views about Parameters, Values, and Policies: Survey Results in Labor and Public Economics.” Journal of Economic Literature 36(3): 1387–1425.\n\nFuller, Dan, and Doris Geide-Stevenson. 2003. “Consensus Among Economists: Revisited.” Journal of Economic Education 34(4): 369–87.\n\nFunk, Carolyn. 2000. “The Dual Influence of Self-Interest and Societal Interest in Public Opinion.” Political Research Quarterly 53(1): 37–62.\n\nFunk, Carolyn, and Patricia García-Monet. 1997. “The Relationship between Personal and National Concerns in Public Perceptions about the Economy.” Political Research Quarterly 50(2): 317–42.\n\nGallup Organization. 2005. “Create a Trend: Minimum Wage.” URL http://institution.gallup.com/documents/trendQuestion.aspx?QUESTION=119914&Advanced=0&Search\nConType=1&SearchTypeAll=minimum%20wage.\n\nGeddes, Barbara, and John Zaller. 1989. “Sources of Popular Support for Authoritarian Regimes.” American Journal of Political Science 33(2): 319–47.\n\nGelman, Andrew, Jonathan Katz, and Joseph Bafumi. 2004. “Standard Voting Power Indexes Do Not Work: An Empirical Analysis.” British Journal of Political Science 34(4): 657–74.\n\nGelman, Andrew, Gary King, and W. John Boscardin. 1998. “Estimating the Probability of Events That Have Never Occurred: When Is Your Vote Decisive?” Journal of the American Statistical Association 93(441): 1–9.\n\nGelman, Andrew, Boris Shor, Joseph Bafumi, and David Park. 2005. “Rich State, Poor State, Red State, Blue State: What’s the Matter with Connecticut?” URL http://www.stat.columbia.edu/~gelman/research/unpublished/redblue11.pdf.\n\nGeneral Social Survey. 1998. URL http://www.icpsr.umich.edu/GSS/home.htm.\n\nGerber, Alan, and Donald Green. 2000. “The Effect of a Nonpartisan Get-Out-the-Vote Drive: An Experimental Study of Leafleting.” Journal of Politics 62(3): 846–57.\n\nGerber, Elisabeth, and Jeffrey Lewis. 2004. “Beyond the Median: Voter Preferences, Distinct Heterogeneity, and Political Representation.” Journal of Political Economy 112(6): 1364–83.\n\nGigerenzer, Gerd. 2000. Adaptive Thinking: Rationality in the Real World. New York: Oxford University Press.\n\n———. 2001. “The Adaptive Toolbox: Toward a Darwinian Rationality.” In Jeffrey French, Alan Kamil, Daniel Leger, Richard Dienstbier, and Martin Daly, eds., Evolutionary Psychology and Motivation. Lincoln: University of Nebraska Press: 113–43.\n\nGigerenzer, Gerd, and David Murray. 1987. Cognition as Intuitive Statistics. Hillsdale, NJ: Lawrence Erlbaum Associates.\n\nGilens, Martin. 2001. “Political Ignorance and Collective Policy Preferences.” American Political Science Review 95(2): 379–96.\n\nGillman, Howard. 1993. The Constitution Besieged: The Rise and Demise of Lochner Era Police Powers Jurisprudence. Durham, NC: Duke University Press.\n\nGilovich, Thomas. 1991. How We Know What Isn’t So. New York: Macmillan.\n\nGlaeser, Edward. 2003. “Psychology and the Market.” NBER Working Paper No. 10203.\n\n———. 2005. “The Political Economy of Hatred.” Quarterly Journal of Economics 70(1): 45–86.\n\nGlucksberg, Sam. 1962. “The Influence of Strength and Drive on Functional Fixedness and Perceptual Recognition.” Journal of Experimental Psychology 63: 36–41.\n\nGold, Lois, Thomas Slone, Neela Manley, and Bruce Ames. 2002. Misconceptions About the Causes of Cancer. Vancouver, BC: Fraser Institute.\n\nGoldstein, Daniel, and Gerd Gigerenzer. 2002. “Models of Ecological Rationality: The Recognition Heuristic.” Psychological Review 109(1): 75–90.\n\nGottschalk, Peter. 1997. “Inequality, Income Growth, and Mobility: The Basic Facts.” Journal of Economic Perspectives 11(2): 21–40.\n\nGreen, Donald, and Ann Gerken. 1989. “Self-Interest and Public Opinion Toward Smoking Restrictions and Cigarette Taxes.” Public Opinion Quarterly 53(1): 1–16.\n\nGreen, Donald, and Ian Shapiro. 1994. Pathologies of Rational Choice Theory: A Critique of Applications in Political Science. New Haven: Yale University Press.\n\nGregor, A. James. 1969. The Ideology of Fascism. New York: Free Press.\n\nGreider, William. 1992. Who Will Tell the People? New York: Simon and Schuster.\n\n———. 1997. One World, Ready or Not: The Manic Logic of Global Capitalism. New York: Simon and Schuster.\n\nGroseclose, Tim, and Nolan McCarty. 2001. “The Politics of Blame: Bargaining Before an Audience.” American Journal of Political Science 45(1): 100–19.\n\nGrossman, Gene, and Elhanan Helpman. 1994. “Protection for Sale.” American Economic Review 84(4): 833–50.\n\n———. 1996. “Electoral Competition and Special Interest Politics.” Review of Economic Studies 63(2): 265–88.\n\n———. 2001. Special Interest Politics. Cambridge: MIT Press.\n\nGruber, Jonathan. 2005. Public Finance and Public Policy. New York: Worth.\n\nHainmueller, Jens, and Michael Hiscox. 2005a. “Educated Preferences: Explaining Attitudes Toward Immigration in Europe.” URL http://www.people.fas.harvard.edu/~hiscox/EducatedPreferences.pdf.\n\n———. 2005b. “Learning to Love Globalization? Education and Individual Attitudes Toward International Trade. Supplement II: What Drives the Education Effect? Economic Literacy or Tolerance?” Unpub.\n\n———. Forthcoming. “Learning to Love Globalization: Education and Individual Attitudes Toward International Trade.” International Organization. URL http://www.people.fas.harvard.edu/~hiscox/HainmuellerHiscoxEducation.pdf.\n\nHanson, Robin. 1995. “Could Gambling Save Science? Encouraging an Honest Consensus.” Social Epistemology 9(1): 3–33.\n\n———. 2002. “Disagreement is Unpredictable.” Economics Letters 77(3): 365–69.\n\n———. 2005. “The Policy Analysis Market (and FutureMAP) Archive.” URL http://hanson.gmu.edu/policyanalysismarket.html.\n\n———. 2006. “Decision Markets for Policy Advice.” In Eric Patashnik and Alan Gerber, eds., Promoting the General Welfare: American Democracy and the Political Economy of Government Performance. Washington, DC: Brookings Institution Press, forthcoming.\n\nHarberger, Arnold. 1993. “Secrets of Success: A Handful of Heroes.” American Economic Review 83(2): 343–50.\n\nHarrison, Glenn, and Bengt Kriström. 1995. “On the Interpretation of Responses in Contingent Valuation Surveys.” In Per-Olav Johansson, Bengt Kriström, and Karl-Göran Mäler, eds., Current Issues in Environmental Economics. Manchester: Manchester University Press: 35–57.\n\nHarrison, Glenn, and John List. 2004. “Field Experiments.” Journal of Economic Literature 42(4): 1009–55.\n\nHarrison, Glenn, and E. Elisabet Rutström. Forthcoming. “Experimental Evidence of Hypothetical Bias in Value Elicitation Methods.” In Charles Plott and Vernon Smith, eds., Handbook of Experimental Economics Results.\n\nHauptmann, Emily. 1996. Putting Choice Before Democracy: A Critique of Rational Choice Theory. Albany: State University of New York Press.\n\nHeld, Virginia. 1990. “Mothering versus Contract.” In Jane Mansbridge, ed., Beyond Self-Interest. Chicago: University of Chicago Press: 287–304.\n\nHelliwell, John. 2003. “How’s Life: Combining Individual and National Variables to Explain Subjective Well-Being.” Economic Modelling 20(2): 331–60.\n\nHemenway, David. 1990. “Propitious Selection.” Quarterly Journal of Economics 16(4): 1063–69.\n\nHenderson, David. 1986. Innocence and Design: The Influence of Economic Ideas on Policy. New York: Basil Blackwell.\n\nHerman, Arthur. 1997. The Idea of Decline in Western History. New York: Free Press.\n\nHerman, Edward, and Noam Chomsky. 2002. Manufacturing Consent: The Political Economy of the Mass Media. New York: Pantheon.\n\nHighton, Benjamin. 2004. “Voter Registration and Turnout in the United States.” Perspectives on Politics 2(3): 507–15.\n\nHinich, Melvin, and Michael Munger. 1994. Ideology and the Theory of Political Choice. Ann Arbor: University of Michigan Press.\n\nHiscox, Michael. 2006. “Through a Glass and Darkly: Attitudes Toward International Trade and the Curious Effects of Issue Framing.” International Organization, forthcoming.\n\nHitler, Adolf. 1943. Mein Kampf. Boston: Houghton Mifflin.\n\nHoelzl, Erik, and Aldo Rustichini. 2005. “Overconfident: Do You Put Your Money On It?” Economic Journal 115(503): 305–18.\n\nHoffer, Eric. 1951. The True Believer: Thoughts on the Nature of Mass Movements. New York: New American Library.\n\nHoffman, Tom. 1998. “Rationality Reconceived: The Mass Electorate and Democratic Theory.” Critical Review 12(4): 459–80.\n\nHogarth, Robin, and Melvin Reder. 1987. “Introduction: Perspectives from Economics and Psychology.” In Robin Hogarth and Melvin Reder, eds., Rational Choice: The Contrast Between Economics and Psychology. Chicago: University of Chicago Press: 1–23.\n\nHolbrook, Thomas, and James Garand. 1996. “Homo Economus? Economic Information and Economic Voting.” Political Research Quarterly 49(2): 351–75.\n\nHolcombe, Randall. 1985. An Economic Analysis of Democracy. Carbondale: Southern Illinois University Press.\n\nHolloway, David. 1994. Stalin and the Bomb: The Soviet Union and Atomic Energy, 1939–1956. New Haven: Yale University Press.\n\nHolmes, Stephen, and Cass Sunstein. 1999. The Cost of Rights: Why Liberty Depends on Taxes. New York: Norton.\n\nHoukes, John. 2004. An Annotated Bibliography on the History of Usury and Interest from the Earliest Times Through the Eighteenth Century. Lewiston, NY: E. Mellen Press.\n\nHowitt, Peter, and Ronald Wintrobe. 1995. “The Political Economy of Inaction.” Journal of Public Economics 56(2): 329–53.\n\nHowson, Colin, and Peter Urbach. 1989. Scientific Reasoning: The Bayesian Approach. LaSalle, IL: Open Court.\n\nHuddy, Leonie, Jeffrey Jones, and Richard Chard. 2001. “Compassion v. Self-Interest: Support for Old-Age Programs among the Non-Elderly.” Political Psychology 22(3): 443–72.\n\nHume, David. 1987. Essays: Moral, Political and Literary. Indianapolis: LibertyClassics.\n\nIannaccone, Laurence. 1998. “Introduction to the Economics of Religion.” Journal of Economic Literature 36(3): 1465–95.\n\nIMDB. 2005. “Memorable Quotes from ‘Seinfeld.’ ” URL http://www.imdb.com/title/tt0098904/quotes.\n\nIngram, James. 1983. International Economics. New York: Wiley.\n\nIrwin, Douglas. 1996. Against the Tide: An Intellectual History of Free Trade. Princeton, NJ: Princeton University Press.\n\nJacobs, Lawrence, and Robert Shapiro. 2000. Politicians Don’t Pander: Political Manipulation and the Loss of Democratic Responsiveness. Chicago: University of Chicago Press.\n\nJennings, M. Kent. 1992. “Ideological Thinking Among Mass Publics and Political Elites.” Public Opinion Quarterly 56(4): 419–41.\n\nJohnson, D. Gale. 2000. “Population, Food, and Knowledge.” American Economic Review 90(1): 1–14.\n\nJohnson, Paul. 1991. Modern Times: The World from the Twenties to the Nineties. New York: HarperCollins.\n\nJost, John, Jack Glaser, Arie Kruglanski, and Frank Sulloway. 2003. “Political Conservatism as Motivated Social Cognition.” Psychological Bulletin 129(3): 339–75.\n\nKahneman, Daniel, Ilana Ritov, and David Schkade. 1999. “Economic Preferences or Attitude Expressions? An Analysis of Dollar Responses to Public Issues.” Journal of Risk and Uncertainty 19(1–3): 203–35.\n\nKahneman, Daniel, Paul Slovic, and Amos Tversky, eds. 1982. Judgment under Uncertainty: Heuristics and Biases. Cambridge: Cambridge University Press.\n\nKahneman, Daniel, and Amos Tversky. 1982. “On the Study of Statistical Intuitions.” In Daniel Kahneman, Paul Slovic, and Amos Tversky, eds., Judgment under Uncertainty: Heuristics and Biases. Cambridge: Cambridge University Press: 493–508.\n\nKaiser Family Foundation. 2005. “Views On Prescription Drugs And The Pharmaceutical Industry.” URL http://www.kff.org/healthpollreport/feb\\_2005/index.cfm.\n\nKaiser Family Foundation and Harvard University School of Public Health. 1995. “National Survey of Public Knowledge of Welfare Reform and the Federal Budget.” January 12, #1001. URL http://www.kff.org/kaiserpolls/1001-welftbl.cfm.\n\nKalt, Joseph, and Mark Zupan. 1984. “Capture and Ideology in the Economic Theory of Politics.” American Economic Review 74(3): 279–300.\n\n———. 1990. “The Apparent Ideological Behavior of Legislators: Testing for Principal-Agent Slack in Political Institutions.” Journal of Law and Economics 33(1): 103–31.\n\nKamber, Victor. 1995. Giving Up on Democracy: Why Term Limits Are Bad for America. Washington, DC: Regency.\n\nKamieniecki, Sheldon. 1985. Party Identification, Political Behavior, and the American Electorate. Westport, CT: Greenwood Press.\n\nKau, James, and Paul Rubin. 1979. “Self-Interest, Ideology, and Logrolling in Congressional Voting.” Journal of Law and Economics 22(2): 365–84.\n\nKearl, J. R., Clayne Pope, Gordon Whiting, and Larry Wimmer. 1979. “A Confusion of Economists?” American Economic Review 69(2): 28–37.\n\nKeeter, Scott. 1996. “The Origins of the Disjuncture of Perception and Reality: The Cases of Racial Equality and Environmental Protection.” Unpub.\n\nKelman, Mark. 1988. “On Democracy-Bashing: A Skeptical Look At the Theoretical and ‘Empirical’ Practice of the Public Choice Movement.” University of Virginia Law Review 74(2): 199–273.\n\nKelman, Steven. 1981. What Price Incentives? Economists and the Environment. Boston: Auburn House.\n\nKeynes, John Maynard. 1963. Essays in Persuasion. New York: Norton.\n\nKinder, Donald, and Roderick Kiewiet. 1979. “Economic Discontent and Political Behavior: The Role of Personal Grievances and Collective Economic Judgments in Congressional Voting.” American Journal of Political Science 23(3): 495–527.\n\n———. 1981. “Sociotropic Politics: The American Case.” British Journal of Political Science 11(2): 129–61.\n\nKirchgässner, Gebhard. 1992. “Towards a Theory of Low-Cost Decisions.” European Journal of Political Economy 8(2): 305–20.\n\n———. 2005. “(Why) Are Economists Different?” European Journal of Political Economy 21(3): 543–62.\n\nKirchgässner, Gebhard, and Werner Pommerehne. 1993. “Low-Cost Decisions as a Challenge to Public Choice.” Public Choice 77(1): 107–15.\n\nKlein, Daniel. 1994. “If Government is So Villainous, How Come Government Officials Don’t Seem Like Villains?” Economics and Philosophy 10(1) 91–106.\n\nKlein, Daniel. 1999. What Do Economists Contribute? New York: New York University Press.\n\nKlein, Daniel, and Charlotta Stern. Forthcoming. “How Politically Diverse Are the Social Sciences and Humanities? Survey Evidence from Six Fields.” Academic Questions. URL http://www.ratio.se/pdf/wp/dk\\_ls\\_diverse.pdf.\n\nKlein, Daniel, and Alexander Tabarrok. 2001. “Theory, Evidence and Examples of FDA Harm.” FDA Review. URL http://www.fdareview.org/harm.shtml.\n\nKliemt, Hartmut. 1986. “The Veil of Insignificance.” European Journal of Political Economy 2(3): 333–44.\n\nKling, Arnold. 2004. Learning Economics. Philadelphia: Xlibris Corporation.\n\nKnight, Frank. 1951. “The Role of Principles in Economics and Politics.” American Economic Review 41(1): 1–29.\n\n———. 1960. Intelligence and Democratic Action. Cambridge: Harvard University Press.\n\nKnox, R. Buck. 1967. James Ussher: Archbishop of Armagh. Cardiff: University of Wales Press.\n\nKoford, Kenneth. 1994. “What Can We Learn About Congressional Politics From Dimensional Studies of Roll-Call Voting?” Economics and Politics 6(2): 173–86.\n\nKramer, Gerald. 1983. “The Ecological Fallacy Revisited: Aggregate- versus Individual-level Findings on Economics and Elections, and Sociotropic Voting.” American Political Science Review 77(1): 92–111.\n\nKraus, Nancy, Torbjörn Malmfors, and Paul Slovic. 1992. “Intuitive Toxicology: Expert and Lay Judgments of Chemical Risks.” Risk Analysis 12(2): 215–32.\n\nKrause, George. 1997. “Voters, Information Heterogeneity, and the Dynamics of Aggregate Economic Expectations.” American Journal of Political Science 41(4): 1170–1200.\n\nKrause, George, and Jim Granato. 1998. “Fooling Some of the Public Some of the Time? A Test for Weak Rationality With Heterogeneous Information Levels.” Public Opinion Quarterly 62(2): 135–51.\n\nKremer, Michael. 1993. “Population Growth and Technological Change: One Million B.C. to 1990.” Quarterly Journal of Economics 108(3): 681–716.\n\nKrueger, Alan, and Robert Solow, eds. 2001. The Roaring Nineties: Can Full Employment Be Sustained? New York: Russell Sage Foundation.\n\nKruger, Justin, and David Dunning. 1999. “Unskilled and Unaware of It: How Difficulties in Recognizing One’s Own Incompetence Lead to Inflated Self-Assessments.” Journal of Personality and Social Psychology 77(6): 1121–34.\n\nKrugman, Paul. 1996. Pop Internationalism. Cambridge: MIT Press.\n\n———. 1998. The Accidental Theorist. New York: Norton.\n\n———. 2003. The Great Unraveling. New York: Norton.\n\nKuklinksi, James, Paul Quirk, Jennifer Jerit, and Robert Rich. 2001. “The Political Environment and Citizen Competence.” American Journal of Political Science 45(2): 410–24.\n\nKuklinksi, James, Paul Quirk, Jennifer Jerit, David Schwieder, and Robert Rich. 2000. “Misinformation and the Currency of Democratic Citizenship.” Journal of Politics 62(3): 790–816.\n\nKull, Steven. 2000. “Americans on Globalization: A Study of U.S. Public Attitudes.” Program on International Policy Attitudes. URL http://www.pipa.org/OnlineReports/Globalization/contents.html.\n\nKuran, Timur. 1995. Private Truths, Public Lies: The Social Consequences of Preference Falsification. Cambridge: Harvard University Press.\n\n———. 2004. Islam and Mammon: The Economic Predicaments of Islamism. Princeton, NJ: Princeton University Press.\n\nKuran, Timur, and Cass Sunstein. 1999. “Availability Cascades and Risk Regulation.” Stanford Law Review 51(4): 683–768.\n\nKuttner, Robert. 1984. The Economic Illusion: False Choices Between Prosperity and Social Justice. Philadelphia: University of Pennsylvania Press.\n\n———. 1991. The End of Laissez-Faire: National Purpose and the Global Economy After the Cold War. New York: Knopf.\n\n———. 1997. Everything for Sale: The Virtues and Limits of Markets. New York: Knopf.\n\n———. ed. 1996. Ticking Time Bombs: The New Conservative Assault on Democracy. New York: New Press.\n\nLakoff, George. 2002. Moral Politics: How Liberals and Conservatives Think. Chicago: University of Chicago Press.\n\nLandsburg, Steven. 1993. The Armchair Economist: Economics and Everyday Life. New York: Free Press.\n\n———. 1997. Fair Play. New York: Free Press.\n\nLanger, Gary. 2002. “Trust in Government . . . To Do What?” Public Perspective, July–August, 7–10.\n\nLanoue, David. 1994. “Retrospective and Prospective Voting in Presidential-Year Elections.” Political Research Quarterly 47(1): 193–205.\n\nLaPiere, Richard. 1934. “Attitudes vs. Actions.” Social Forces 13(2): 230–37.\n\nLau, Richard, Thad Brown, and David Sears. 1978. “Self-Interest and Civilians’ Attitudes Toward the Vietnam War.” Public Opinion Quarterly 42(4): 464–83.\n\nLau, Richard, and David Redlawsk. 1997. “Voting Correctly.” American Political Science Review 91(3): 585–98.\n\n———. 2001. “Advantages and Disadvantages of Cognitive Heuristics in Political Decision Making. American Journal of Political Science 45(4): 951–71.\n\nLazonick, William. 1991. Business Organization and the Myth of the Market Economy. Cambridge: Cambridge University Press.\n\nLebergott, Stanley. 1993. Pursuing Happiness: American Consumers in the Twentieth Century. Princeton, NJ: Princeton University Press.\n\nLe Bon, Gustave. 1960. The Crowd: A Study of the Popular Mind. New York: Viking Press.\n\nLecky, William. 1981. Liberty and Democracy. Vol. 1. Indianapolis: Liberty Classics.\n\nLee, David, Enrico Moretti, and Matthew Butler. 2004. “Do Voters Affect or Elect Policies? Evidence from the U.S. House.” Quarterly Journal of Economics 69(3): 807–59.\n\nLee, Dwight. 1989. “The Impossibility of a Desirable Minimal State.” Public Choice 61(3): 277–84.\n\nLee, Ronald, and Timothy Miller. 2000. “Immigration, Social Security, and Broader Fiscal Impacts.” American Economic Review 90(2): 350–54.\n\nLeighley, Jan, and Jonathan Nagler. 1992a. “Socioeconomic Class Bias in Turnout, 1964–1988: The Voters Remain the Same.” American Political Science Review 86(3): 725–36.\n\n———. 1992b. “Individual and Systemic Influences on Turnout: Who Votes? 1984. Journal of Politics 54(3): 718–40.\n\nLevitt, Steven. 1996. “How Do Senators Vote? Disentangling the Role of Voter Preferences, Party Affiliation, and Senator Ideology.” American Economic Review 86(3): 425–41.\n\nLevy, David. 1989. “The Statistical Basis of Athenian-American Constitutional Theory.” Journal of Legal Studies 18(1): 79–103.\n\nLichtenstein, Sarah, Baruch Fischhoff, and Lawrence Phillips. 1982. “Calibration of Probabilities: The State of the Art to 1980.” In Daniel Kahneman, Paul Slovic, and Amos Tversky, eds., Judgment under Uncertainty: Heuristics and Biases. Cambridge: Cambridge University Press: 306–34.\n\nLichter, S. Robert, and Stanley Rothman. 1999. Environmental Cancer—A Political Disease? New Haven: Yale University Press.\n\nList, John. 2003. “Does Market Experience Eliminate Market Anomalies?” Quarterly Journal of Economics 68(1): 41–71.\n\nLocke, John. 1977. “An Essay Concerning Human Understanding.” In Steven Cahn, ed., Classics of Western Philosophy. Indianapolis: Hackett Publishing Company: 479–574.\n\nLockerbie, Brad. 1991. “The Influence of Levels of Information on the Use of Prospective Evaluations.” Political Behavior 13(3): 223–35.\n\nLomborg, Bjorn. 2001. The Skeptical Environmentalist: Measuring the Real State of the World. Cambridge: Cambridge University Press.\n\nLord, Charles, Lee Ross, and Mark Lepper. 1979. “Biased Assimilation and Attitude Polarization: The Effect of Prior Theories on Subsequently Considered Evidence.” Journal of Personality and Social Psychology 37(11): 2098–109.\n\nLott, John. 1997. “Donald Wittman’s The Myth of Democratic Failure.” Public Choice 92(1–2): 1–13.\n\n———. 2000. More Guns, Less Crime: Understanding Crime and Gun-Control Laws. Chicago: University of Chicago Press.\n\nLott, John, and Lawrence Kenny. 1999. “Did Women’s Suffrage Change the Size and Scope of Government?” Journal of Political Economy 107(6): 1163–98.\n\nLovejoy, Arthur, and George Boas. 1965. Primitivism and Related Ideas in Antiquity. New York: Octagon Books.\n\nLucas, Robert. 1973. “Some International Evidence on Output-Inflation Tradeoffs.” American Economic Review 63(3): 326–34.\n\n———. 1993. “Making a Miracle.” Econometrica 61(2): 251–72.\n\nLupia, Arthur. 1994. “Shortcuts Versus Encyclopedias: Information and Voting Behavior in California Insurance Reform Elections.” American Political Science Review 88(1): 63–76.\n\nLupia, Arthur, and Matthew McCubbins. 1998. The Democratic Dilemma: Can Citizens Learn What They Need to Know? Cambridge: Cambridge University Press.\n\nLuttbeg, Norman, and Michael Martinez. 1990. “Demographic Differences in Opinion.” Research in Micropolitics 3:83–118.\n\nMacEwan, Arthur. 1999. Neoliberalism or Democracy? Economic Strategy, Markets, and Alternatives for the 21st Century. New York: St. Martin’s Press.\n\nMachiavelli, Niccolò. 1952. The Prince. New York: NAL Penguin.\n\nMacKuen, Michael, Robert Erikson, and James Stimson. 1992. “Peasants or Bankers? The American Electorate and the U.S. Economy.” American Political Science Review 86(3): 597–611.\n\nMaddock, Rodney, and Michael Carter. 1982. “A Child’s Guide to Rational Expectations.” Journal of Economic Literature 20(1): 39–51.\n\nMadison, James, Alexander Hamilton, and John Jay. 1966. The Federalist Papers. New Rochelle, NY: Arlington House.\n\nMagee, Stephen, William Brock, and Leslie Young. 1989. Black Hole Tariffs and Endogenous Policy Theory: Political Economy in General Equilibrium. Cambridge: Cambridge University Press.\n\nMansbridge, Jane, ed. 1990. Beyond Self-Interest. Chicago: University of Chicago Press.\n\nManza, Jeff, and Clem Brooks. 1999. Social Cleavages and Political Change: Voter Alignments and U.S. Party Coalitions. New York: Oxford University Press.\n\nMarkus, Gregory. 1988. “The Impact of Personal and National Economic Conditions on the Presidential Vote: A Pooled Cross-Sectional Analysis.” American Journal of Political Science 32(1): 137–54.\n\nMarx, Karl. 1965. Capital. Vol. 1. Moscow: Progress Publishers.\n\nMaskin, Eric, and Jean Tirole. 2004. “The Politician and the Judge: Accountability in Government.” American Economic Review 94(4): 1034–54.\n\nMatsusaka, John. 2005. “Direct Democracy Works.” Journal of Economic Perspectives 19(2): 185–206.\n\nMcChesney, Robert. 1999. Rich Media, Poor Democracy: Communication Politics in Dubious Times. Urbana: University of Illinois Press.\n\nMcCloskey, Donald. 1985. The Rhetoric of Economics. Madison: University of Wisconsin Press.\n\n———. 1993. “Competitiveness and the Antieconomics of Decline.” In Donald McCloskey, ed., Second Thoughts: Myths and Morals of U.S. Economic History. New York: Oxford University Press: 167–73.\n\nMcClosky, Herbert, and John Zaller. 1984. The American Ethos: Public Attitudes Towards Capitalism and Democracy. Cambridge: Harvard University Press.\n\nMcLean, Iain. 2002. “William H. Riker and the Invention of Heresthetic(s).” British Journal of Political Science 32(3): 535–58.\n\nMeehl, Paul. 1977. “The Selfish Voting Paradox and the Thrown-Away Vote Argument.” American Political Science Review 71(1): 11–30.\n\nMele, Alfred. 1987. Irrationality: An Essay on Akrasia, Self-Deception, and Self-Control. New York: Oxford University Press.\n\n———. 2001. Self-Deception Unmasked. Princeton, NJ: Princeton University Press.\n\n———. 2004. “Rational Irrationality.” Philosophers’ Magazine 26(2): 31–32.\n\nMeltzer, Allan, and Scott Richard. 1981. “A Rational Theory of the Size of Government.” Journal of Political Economy 89(5): 914–27.\n\nMencken, H. L. 1995. A Second Mencken Chrestomathy. New York: Knopf.\n\nMerriam-Webster’s Collegiate Dictionary. 2003. Springfield, MA: Merriam-Webster.\n\nMichels, Robert. 1962. Political Parties: A Sociological Study of the Oligarchical Tendencies of Modern Democracy. New York: Free Press.\n\nMiller, Dale. 1999. “The Norm of Self-Interest.” American Psychologist 54(12): 1053–60.\n\nMiller, Warren. 1991. “Party Identification, Realignment, and Party Voting: Back to the Basics.” American Political Science Review 85(2): 557–68.\n\nMises, Ludwig von. 1962. Bureaucracy. New Haven: Yale University Press.\n\n———. 1966. Human Action: A Treatise on Economics. Chicago: Contemporary Books.\n\n———. 1981a. “A New Treatise on Economics.” In New Individualist Review. Indianapolis: Liberty Fund: 323–26.\n\n———. 1981b. Socialism. Indianapolis: Liberty Classics.\n\n———. 1996. Liberalism: The Classical Tradition. Irvington-on-Hudson, NY: Foundation for Economic Education.\n\n———. 1998. Interventionism: An Economic Analysis. Irvington-on-Hudson, NY: Foundation for Economic Education.\n\nModern History Project. 2005. “Rudolph Hess.” URL http://modernhistoryproject.org/mhp/EntityDisplay.php?Entity=HessR.\n\nMonroe, Alan. 1983. “American Party Platforms and Public Opinion.” American Journal of Political Science 27(1): 27–42.\n\n———. 1998. “Public Opinion and Public Policy, 1980–1993.” Public Opinion Quarterly 62(1): 6–28.\n\nMosca, Gaetano. 1939. The Ruling Class. New York: McGraw-Hill.\n\nMueller, Dennis, and Thomas Stratmann. 2003. “The Economic Effects of Democratic Participation.” Journal of Public Economics 87(9): 2129–55.\n\nMueller, John. 1999. Capitalism, Democracy, and Ralph’s Pretty Good Grocery. Princeton, NJ: Princeton University Press.\n\nMullainathan, Sendhil, and Andrei Shleifer. 2005. “The Market for News.” American Economic Review 95(4): 1031–53.\n\nMulligan, Casey, and Charles Hunter. 2003. “The Empirical Frequency of a Pivotal Vote.” Public Choice 116(1–2): 31–54.\n\nMurphy, Kevin, and Andrei Shleifer. 2004. “Persuasion in Politics.” NBER Working Paper No. 10248.\n\nMurray, David, Joel Schwartz, and S. Robert Lichter. 2001. It Ain’t Necessarily So: How Media Make and Unmake the Scientific Picture of Reality. Lanham, MD: Rowman and Littlefield.\n\nMuth, John. 1961. “Rational Expectations and the Theory of Price Movements.” Econometrica 29(3): 315–35.\n\nMutz, Diana. 1992. “Mass Media and the Depoliticization of Personal Experience.” American Journal of Political Science 36(2): 483–508.\n\n———. 1993. “Direct and Indirect Routes to Politicizing Personal Experience: Does Knowledge Make a Difference?” Public Opinion Quarterly 57(4): 483–502.\n\nMutz, Diana, and Jeffrey Mondak. 1997. “Dimensions of Sociotropic Behavior: Group-Based Judgements of Fairness and Well-Being.” American Journal of Political Science 41(1): 284–308.\n\nNadeau, Richard, and Michael Lewis-Beck. 2001. “National Economic Voting in U.S. Presidential Elections.” Journal of Politics 63(1): 159–81.\n\nNasar, Sylvia. 1998. A Beautiful Mind. New York: Simon and Schuster.\n\nNational Commission on Terrorist Attacks Upon the United States. 2004. The 9/11 Commission Report. URL http://www.9–11commission.gov/report/911Report.pdf.\n\nNeuman, W. Russell. 1986. The Paradox of Mass Politics: Knowledge and Opinion in the American Electorate. Cambridge: Harvard University Press.\n\nNewcomb, Simon. 1893. “The Problem of Economic Education.” Quarterly Journal of Economics 7(4): 375–99.\n\nNietzsche, Friedrich. 1954. The Portable Nietzsche. Ed. Walter Kaufmann. New York: Viking Press.\n\nNisbett, Richard, and Lee Ross. 1980. Human Inference: Strategies and Shortcomings of Social Judgment. Englewood Cliffs, NJ: Prentice-Hall.\n\nNoss, John. 1974. Man’s Religions. New York: Macmillan.\n\nOffice of Management and Budget. 1997. “Report to Congress on the Costs and Benefits of Federal Regulation.” URL http://www.whitehouse.gov/omb/inforeg/rcongress.html.\n\n———. 2005. Historical Tables: Budget of the United States, F.Y. 2006. Washington, DC: U.S. Government Printing Office.\n\nOlson, Mancur. 1971. The Logic of Collective Action: Public Goods and the Theory of Groups. Cambridge: Harvard University Press.\n\n———. 1982. The Rise and Decline of Nations: Economic Growth, Stagflation and Social Rigidities. New Haven: Yale University Press.\n\n———. 1996. “Big Bills Left on the Sidewalk: Why Some Nations are Rich and Others are Poor.” Journal of Economic Perspectives 10(2): 3–24.\n\nOrwell, George. 1968. “Looking Back on the Spanish War.” In Sonia Orwell and Ian Angus, eds., The Collected Essays, Journalism and Letters of George Orwell. Vol. 2. New York: Harcourt, Brace, and World: 249–67.\n\n———. 1983. 1984. New York: Signet Classic.\n\nPage, Benjamin, and Robert Shapiro. 1983. “Effects of Public Opinion on Policy.” American Political Science Review 77(1): 175–90.\n\n———. 1992. The Rational Public: Fifty Years of Trends in Americans’ Policy Preferences. Chicago: University of Chicago Press.\n\n———. 1993. “The Rational Public and Democracy.” In George Marcus and Russell Hanson, eds., Reconsidering the Democratic Public. University Park: Pennsylvania State University Press: 33–64.\n\nPashigian, B. Peter. 2000. “Teaching Microeconomics in Wonderland.” George J. Stigler Center for the Study of Economy and the State Working Paper No. 161.\n\nPayne, Stanley. 1995. A History of Fascism: 1914–1945. Madison: University of Wisconsin Press.\n\nPeltzman, Sam. 1984. “Constituent Interest and Congressional Voting.” Journal of Law and Economics 27(1): 181–210.\n\n———. 1985. “An Economic Interpretation of the History of Congressional Voting in the Twentieth Century.” American Economic Review 75(4): 656–75.\n\n———. 1990. “How Efficient Is the Voting Market?” Journal of Law and Economics 33(1): 27–63.\n\nPersson, Torsten. 2002. “Do Political Institutions Shape Economic Policy?” Econometrica 70(3): 883–905.\n\nPersson, Torsten, and Guido Tabellini. 2000. Political Economics: Explaining Economic Policy. Cambridge: MIT Press.\n\n———. 2004. “Constitutions and Economic Policy.” Journal of Economic Perspectives 18(1): 75–98.\n\nPesaran, M. Hashem. 1987. The Limits to Rational Expectations. Oxford: Blackwell.\n\nPew Research Center. 1997. “The Optimism Gap Grows.” January 17. URL http://people-press.org/reports/display.php3?ReportID=115.\n\nPhelps, Richard. 1993. “American Public Opinion on Trade, 1950–1990.” Business Economics 28(3): 35–40.\n\nPinker, Steven. 2002. The Blank Slate: The Modern Denial of Human Nature. New York: Viking.\n\nPIPA–Knowledge Networks Poll. 2004. “Americans on Farm Subsidies.” URL http://www.pipa.org/OnlineReports/Economics/FarmQnnaire\\_01\\_04.pdf.\n\nPommerehne, Werner, Friedrich Schneider, Guy Gilbert, and Bruno Frey. 1984. “Concordia Discors: Or: What Do Economists Think?” Theory and Decision 16(3): 251–308.\n\nPonza, Michael, Greg Duncan, Mary Corcoran, and Fred Groskind. 1988. “The Guns of Autumn? Age Differences in Support for Income Transfers to the Young and Old.” Public Opinion Quarterly 52(4): 441–66.\n\nPoole, Keith, and Howard Rosenthal. 1991. “Patterns of Congressional Voting.” American Journal of Political Science 35(1): 228–78.\n\n———. 1997. Congress: A Political-Economic History of Roll Call Voting. New York: Oxford University Press.\n\nPoole, William. 2004. “Free Trade: Why Are Economists and Noneconomists So Far Apart?” Federal Reserve Bank of St. Louis Review 6(5): 1–6.\n\nPopkin, Samuel. 1991. The Reasoning Voter: Communication and Persuasion in Presidential Campaigns. Chicago: University of Chicago Press.\n\nPosner, Richard. 1999. An Affair of State: The Investigation, Impeachment, and Trial of President Clinton. Cambridge: Harvard University Press.\n\n———. 2002. Economic Analysis of Law. New York: Aspen Publishers.\n\nPrisching, Manfred. 1995. “The Limited Rationality of Democracy: Schumpeter as the Founder of Irrational Choice Theory.” Critical Review 9(3): 301–23.\n\nQuattrone, George, and Amos Tversky. 1984. “Causal Versus Diagnostic Contingency: On Self-Deception and on the Voter’s Illusion.” Journal of Personality and Social Psychology 46(2): 237–48.\n\n———. 1988. “Contrasting Rational and Psychological Analysis of Political Choice.” American Political Science Review 82(3): 716–36.\n\nQuirk, Paul. 1988. “In Defense of the Politics of Ideas.” Journal of Politics 50(1): 31–41.\n\n———. 1990. “Deregulation and the Politics of Ideas.” In Jane Mansbridge, ed., Beyond Self-Interest. Chicago: University of Chicago Press: 183–99.\n\nRabin, Matthew. 1998. “Psychology and Economics.” Journal of Economic Literature 36(1): 11–46.\n\nRae, John. 1965. Life of Adam Smith. New York: Augustus M. Kelley.\n\nRand, Ayn. 1957. Atlas Shrugged. New York: Signet.\n\nReder, Melvin. 1999. Economics: The Culture of a Controversial Science. Chicago: University of Chicago Press.\n\nRedlawsk, David. 2002. “Hot Cognition or Cool Consideration? Testing the Effects of Motivated Reasoning on Political Decision Making.” Journal of Politics 64(2): 1021–44.\n\nReynolds, Vernon, Vincent Falger, and Ian Vine. 1987. The Sociobiology of Ethnocentrism: Evolutionary Dimensions of Xenophobia, Discrimination, Racism and Nationalism. Athens: University of Georgia Press.\n\nRhoads, Steven. 1985. The Economist’s View of the World: Government, Markets, and Public Policy. Cambridge: Cambridge University Press.\n\nRhodebeck, Laurie. 1993. “The Politics of Greed? Political Preferences among the Elderly.” Journal of Politics 55(2): 342–64.\n\nRichmond, Ray. 1997. The Simpsons: A Complete Guide to Our Favorite Family. New York: HarperCollins.\n\nRicketts, Martin and Edward Shoesmith. 1990. British Economic Opinion: A Survey of a Thousand Economists. London: Institute of Economic Affairs.\n\nRiker, William. 1988. Liberalism Against Populism: A Confrontation between the Theory of Democracy and the Theory of Social Choice. Prospect Heights, IL: Waveland Press.\n\nRoberts, Russell. 2001. The Choice: A Fable of Free Trade and Protectionism. Upper Saddle River, NJ: Prentice Hall.\n\nRodrik, Dani. 1996. “Understanding Economic Policy Reform.” Journal of Economic Literature 34(1): 9–41.\n\nRomer, David. 2003. “Misconceptions and Political Outcomes.” Economic Journal 113(484): 1–20.\n\nRothbard, Murray. 1962. Man, Economy, and State: A Treatise on Economic Principles. Los Angeles: Nash.\n\nRowley, Charles. 1997. “Donald Wittman’s The Myth of Democratic Failure.” Public Choice 92(1–2): 15–26.\n\nRowley, Charles, Robert Tollison, and Gordon Tullock, eds. 1988. The Political Economy of Rent-Seeking. Boston: Kluwer Academic Publishers.\n\nRubin, Paul. 2003. “Folk Economics.” Southern Economic Journal 70(1): 157–71.\n\nRudolph, Thomas. 2003. “Who’s Responsible for the Economy? The Formation and Consequences of Responsibility Attributions.” American Journal of Political Science 47(4): 697–712.\n\nSachs, Jeffrey. 1994. “Life in the Economic Emergency Room.” In John Williamson, ed., The Political Economy of Policy Reform. Washington, DC: Institute for International Economics: 503–23.\n\nSachs, Jeffrey, and Andrew Warner. 1995. “Economic Reform and the Process of Global Integration.” Brookings Papers on Economic Activity 1:1–118.\n\nSaint-Paul, Gilles. 2000. “The ‘New Political Economy’: Recent Books by Allen Drazen and by Torsten Persson and Guido Tabellini.” Journal of Economic Literature 38(4): 915–25.\n\nSamuelson, Paul. 1946. “Lord Keynes and the General Theory.” Econometrica 14(3): 187–200.\n\n———. 1966. “What Economists Know.” In The Collected Scientific Papers of Paul A. Samuelson. Vol. 1. Cambridge: MIT Press: 1619–49.\n\nSamuelson, Robert. 1995. The Good Life and Its Discontents: The American Dream in the Age of Entitlement. New York: Random House.\n\nSappington, David. 1991. “Incentives in Principal-Agent Relationships.” Journal of Economic Perspectives 5(2): 45–66.\n\nSchelling, Thomas. 1980. The Strategy of Conflict. Cambridge: Harvard University Press.\n\nScherer, F. M., and David Ross. 1990. Industrial Market Structure and Economic Performance. Boston: Houghton Mifflin.\n\nScheve, Kenneth, and Matthew Slaughter. 2001a. Globalization and the Perceptions of American Workers. Washington, DC: Institute for International Economics.\n\n———. 2001b. “What Determines Individual Trade Policy Preferences?” Journal of International Economics 54(2): 267–92.\n\nSchlesinger, Arthur. 1957. The Crisis of the Old Order, 1919–1933. Boston: Houghton Mifflin.\n\nSchuessler, Alexander. 2000a. “Expressive Voting.” Rationality and Society 12(1): 87–119.\n\n———. 2000b. A Logic of Expressive Choice. Princeton, NJ: Princeton University Press.\n\nSchultze, Charles. 1977. The Public Use of Private Interest. Washington, DC: Brookings Institution.\n\nSchumpeter, Joseph. 1950. Capitalism, Socialism, and Democracy. New York: Harper and Brothers.\n\nSchumpeter, Joseph. 1954. History of Economic Analysis. New York: Oxford University Press.\n\nSearle, John. 1983. Intentionality: An Essay in the Philosophy of Mind. Cambridge: Cambridge University Press.\n\nSears, David, and Jack Citrin. 1985. Tax Revolt: Something for Nothing in California. Cambridge: Cambridge University Press.\n\nSears, David, and Carolyn Funk. 1990. “Self-Interest in Americans’ Political Opinions.” In Jane Mansbridge, ed., Beyond Self-Interest. Chicago: University of Chicago Press: 147–70.\n\nSears, David, Carl Hensler, and Leslie Speer. 1979. “Whites’ Opposition to ‘Busing’: Self-Interest or Symbolic Politics?” American Political Science Review 73(2): 369–84.\n\nSears, David, and Leonie Huddy. 1990. “On the Origins of the Political Disunity of Women.” In Patricia Gurin and Louise Tilly, eds., Women, Politics, and Change. New York: Russell Sage Foundation: 249–77.\n\nSears, David, and Richard Lau. 1983. “Inducing Apparently Self-Interested Political Preferences.” American Journal of Political Science 27(2): 223–52.\n\nSears, David, Richard Lau, Tom Tyler, and Harris Allen. 1980. “Self-Interest vs. Symbolic Politics in Policy Attitudes and Presidential Voting.” American Political Science Review 74(3): 670–84.\n\nSears, David, Tom Tyler, Jack Citrin, and Donald Kinder. 1978. “Political System Support and Public Response to the Energy Crisis.” American Journal of Political Science 22(1): 56–82.\n\nShapiro, Ian. 1996. Democracy’s Place. Ithaca, NY: Cornell University Press.\n\n———. 1999. Democratic Justice. New Haven: Yale University Press.\n\nShapiro, Ian, and Casiano Hacker-Cordón. 1999. “Reconsidering Democracy’s Value.” In Ian Shapiro and Casiano Hacker-Cordón, eds., Democracy’s Value. Cambridge: Cambridge University Press: 1–19.\n\nShapiro, Robert, and Harpreet Mahajan. 1986. “Gender Differences in Policy Preferences: A Summary of Trends From the 1960s to the 1980s.” Public Opinion Quarterly 50(1): 42–61.\n\nSheffrin, Steven. 1996. Rational Expectations. Cambridge: Cambridge University Press.\n\nShepsle, Kenneth, and Barry Weingast. 1981. “Political Preferences for the Pork Barrel: A Generalization.” American Journal of Political Science 25(1): 96–111.\n\nShermer, Michael. 2002. Why People Believe Weird Things: Pseudoscience, Superstition, and Other Confusions of Our Time. New York: Henry Holt.\n\nShiller, Robert. 1997. “Public Resistance to Indexation: A Puzzle.” Brookings Papers on Economic Activity 1: 159–228.\n\nShleifer, Andrei. 1998. “State versus Private Ownership.” Journal of Economic Perspectives 12(4): 133–50.\n\nSiebert, Horst. 1997. “Labor Market Rigidities: At the Root of Unemployment in Europe.” Journal of Economic Perspectives 11(3): 37–54.\n\nSimon, Herbert. 1985. “Human Nature in Politics: The Dialogue of Psychology with Political Science.” American Political Science Review 79(2): 293–304.\n\nSimon, Julian. 1995a. “What Does the Future Hold? The Forecast in a Nutshell.” In Julian Simon, ed., The State of Humanity. Cambridge: Blackwell: 642–60.\n\nSimon, Julian, ed. 1995b. The State of Humanity. Cambridge: Blackwell.\n\n———. 1996. The Ultimate Resource 2. Princeton, NJ: Princeton University Press.\n\n———. 1999. The Economic Consequences of Immigration. Ann Arbor: University of Michigan Press.\n\nSimon, Scott. 2000. “Music Cues: Adlai Stevenson.” National Public Radio. URL http://www.npr.org/programs/wesat/000205.stevenson.html.\n\nSiprut, Joseph. 2004. “Rational Irrationality: Why Playing The World Trade Organization as a Scapegoat Reduces the Social Costs of Armchair Economics.” Brooklyn Journal of International Law 29(2): 709–45.\n\nSkousen, Mark. 1997. “The Perseverance of Paul Samuelson’s Economics.” Journal of Economic Perspectives 11(2): 137–52.\n\nSlovic, Paul, Baruch Fischhoff, and Sarah Lichtenstein. 1980. “Facts and Fears: Understanding Perceived Risk.” In Richard Schwing and Walter Albers, eds., Societal Risk Assessment: How Safe is Safe Enough? New York: Plenum Press: 181–216.\n\nSmith, Adam. 1981. An Inquiry Into the Nature and Causes of the Wealth of Nations. Indianapolis: Liberty Classics.\n\nSmith, Eric. 1989. The Unchanging American Voter. Berkeley and Los Angeles: University of California Press.\n\nSmith, Vernon. 1991. “Rational Choice: The Contrast Between Economics and Psychology.” Journal of Political Economy 99(4): 877–97.\n\n———. 2003. “Constructivist and Ecological Rationality in Economics.” American Economic Review 93(3): 465–508.\n\nSmith, Vernon, and James Walker. 1993. “Monetary Rewards and Decision Cost in Experimental Economics.” Economic Inquiry 31(2): 245–61.\n\nSomin, Ilya. 1998. “Voter Ignorance and the Democratic Ideal.” Critical Review 12(4): 99–111.\n\n———. 1999. “Resolving the Democratic Dilemma?” Yale Journal on Regulation 16(2): 401–14.\n\n———. 2000. “Do Politicians Pander?” Critical Review 14(2–3): 147–55.\n\n———. 2004. “Political Ignorance and The Countermajoritarian Difficulty: A New Perspective on the ‘Central Obsession’ of Constitutional Theory.” Iowa Law Review 89(4): 1287–1372.\n\nSoros, George. 1998. The Crisis of Global Capitalism: Open Society Endangered. New York: Public Affairs.\n\nSowell, Thomas. 2004a. Applied Economics: Thinking Beyond Stage One. New York: Basic Books.\n\n———. 2004b. Basic Economics: A Citizen’s Guide to the Economy. New York: Basic Books.\n\nSpeck, W. A. 1993. A Concise History of Britain, 1707–1975. Cambridge: Cambridge University Press.\n\nSpence, Michael. 1977. “Consumer Misperceptions, Product Failure, and Producer Liability.” Review of Economic Studies 44(3): 561–72.\n\nSpencer, Herbert. 1981. “From Freedom to Bondage.” In Thomas Mackay, ed., A Plea for Liberty: An Argument Against Socialism and Socialistic Legislation. Indianapolis: Liberty Fund: 3–34.\n\nStarke, Linda, ed. 2004. State of the World, 2004. New York: Norton.\n\nStigler, George. 1959. “The Politics of Political Economists.” Quarterly Journal of Economics 73(4): 522–32.\n\n———. 1961. “The Economics of Information.” Journal of Political Economy 69(3): 213–25.\n\n———. 1986. “Economics or Ethics?” In Kurt Leube and Thomas Gale Moore, eds., The Essence of Stigler. Stanford, CA: Hoover Institution Press: 303–36.\n\nStigler, George, and Gary Becker. 1977. “De Gustibus Non Est Disputandum.” American Economic Review 67(2): 76–90.\n\nStiglitz, Joseph. 2002a. Globalization and Its Discontents. New York: Norton.\n\n———. 2002b. “Information.” In David Henderson, ed., The Concise Encyclopedia of Economics. URL http://www.econlib.org/library/Enc/Information.html.\n\n———. 2003. The Roaring Nineties. A New History of the World’s Most Prosperous Decade. New York: Norton.\n\nStratmann, Thomas. 2005. “Some Talk: Money in Politics. A (Partial) Review of the Literature.” Public Choice 124(1/2): 135–56.\n\nSunstein, Cass, ed. 2000. Behavioral Law and Economics. New York: Cambridge University Press.\n\nSurowiecki, James. 2004. The Wisdom of Crowds. New York: Doubleday.\n\nSutter, Daniel. 2006. Political Bias in the News: A Critical Examination. Unpub.\n\nTabarrok, Alexander. 2000. “Assessing the FDA Via the Anomaly of Off-Label Drug Prescribing.” Independent Review 5(1): 25–53.\n\nTaussig, Frank. 1905. “The Present Position of the Doctrine of Free Trade.” Publications of the American Economic Association 6(1): 29–65.\n\nTaylor, Shelley. 1989. Positive Illusions: Creative Self-Deception and the Healthy Mind. New York: Basic Books.\n\nTetlock, Philip. 2003. “Thinking the Unthinkable: Sacred Values and Taboo Cognitions.” Trends in Cognitive Science 7(7): 320–24.\n\nThaler, Richard. 1992. The Winner’s Curse: Paradoxes and Anomalies of Economic Life. Princeton, NJ: Princeton University Press.\n\nTirole, Jean. 2002. “Rational Irrationality: Some Economics of Self-Management.” European Economic Review 46(4–5): 633–55.\n\nTocqueville, Alexis de. 1969. Democracy in America. New York: Harper-Perennial.\n\nTollison, Robert, and Richard Wagner. 1991. “Romance, Realism, and Policy Reform.” Kyklos 44(1): 57–70.\n\nTucker, Robert. 1973. Stalin as Revolutionary: 1879–1929. New York: Norton.\n\n———. 1990. Stalin in Power: The Revolution From Above 1928–1941. New York: Norton.\n\n———. ed. 1978. The Marx-Engels Reader. New York: Norton.\n\nTullock, Gordon. 1965. “Entry Barriers in Politics.” American Economic Review 55(1/2): 458–66.\n\n———. 1967. Toward a Mathematics of Politics. Ann Arbor: University of Michigan Press.\n\n———. 1971. “Charity of the Uncharitable.” Western Economic Journal 9(4): 379–92.\n\n———. 1981a. “The Rhetoric and Reality of Redistribution.” Southern Economic Journal 47(4): 895–907.\n\n———. 1981b. “Why So Much Stability?” Public Choice 37(2): 189–202.\n\n———. 1987. The Politics of Bureaucracy. Lanham, MD: University Press of America.\n\n———. 1988. “Further Directions for Rent-Seeking Research.” In Charles Rowley, Robert Tollison, and Gordon Tullock, eds., The Political Economy of Rent-Seeking. Boston: Kluwer Academic Publishers: 465–80.\n\n———. 1999. “How to Do Well While Doing Good!” In Daniel Klein, ed., What Do Economists Contribute? New York: New York University Press: 87–103.\n\nTversky, Amos, and Daniel Kahneman. 1982a. “Judgment under Uncertainty: Heuristics and Biases.” In Daniel Kahneman, Paul Slovic, and Amos Tversky, eds., Judgment under Uncertainty: Heuristics and Biases. Cambridge: Cambridge University Press: 3–20.\n\n———. 1982b. “Availability: A Heuristic for Judging Frequency and Probability.” In Daniel Kahneman, Paul Slovic, and Amos Tversky, eds., Judgment under Uncertainty: Heuristics and Biases. Cambridge: Cambridge University Press: 163–78.\n\nTyler, Tom, and Renee Weber. 1982. “Support for the Death Penalty: Instrumental Response to Crime, or Symbolic Attitude?” Law and Society Review 17(1): 21–46.\n\nTyran, Jean-Robert. 2004. “Voting When Money and Morals Conflict: An Experimental Test of Expressive Voting.” Journal of Public Economics 88(7–8): 1645–64.\n\nU.S. Census Bureau. 2005a. “U.S. Trade Balance With Canada.” URL http://www.census.gov/foreign-trade/balance/c1220.html.\n\n———. 2005b. “U.S. Trade Balance With Mexico.” URL http://www.census.gov/foreign-trade/balance/c2010.html.\n\nVan Den Steen, Eric. 2004. “Rational Overoptimism (and Other Biases).” American Economic Review 94(4): 1141–51.\n\nVerba, Sidney, Kay Schlozman, Henry Brady, and Norman Nie. 1993. “Citizen Activity: Who Participates? What Do They Say?” American Political Science Review 87(2): 303–18.\n\nViscusi, W. Kip. 1996. Rational Risk Policy. New York: New York University Press.\n\nVrij, Aldert. 2000. Detecting Lies and Deceit: The Psychology of Lying and the Implications for Professional Practice. New York: John Wiley and Sons.\n\nWalstad, William. 1992. “Economics Instruction in High Schools.” Journal of Economic Literature 30(4): 2019–51.\n\n———. 1997. “The Effect of Economic Knowledge on Public Opinion of Economic Issues.” Journal of Economic Education 28(3): 195–205.\n\nWalstad, William, and Max Larsen. 1992. A National Survey of American Economic Literacy. Lincoln, NE: Gallup Organization.\n\nWalstad, William, and Ken Rebeck. 2002. “Assessing the Economic Knowledge and Economic Opinions of Adults.” Quarterly Review of Economics and Finance 42(5): 921–34.\n\nWashington Post, Kaiser Family Foundation, and Harvard University Survey Project. 1996. “Survey of Americans and Economists on the Economy.” October 16, #1199. URL http://www.kff.org/kaiserpolls/1199-econgen.cfm.\n\n———. 1997. “Survey of Americans’ Knowledge and Attitudes about Entitlements.” URL http://www.kff.org/medicare/loader.cfm?url=/commonspot/security/getfile.cfm&PageID=14513.\n\nWaters, Mary-Alice, ed. 1970. Rosa Luxemburg Speaks. New York: Pathfinder Press.\n\nWeingast, Barry, Kenneth Shepsle, and Christopher Johnsen. 1981. “The Political Economy of Benefits and Costs: A Neoclassical Approach to Distributive Politics.” Journal of Political Economy 89(4): 642–64.\n\nWeiss, Andrew. 1995. “Human Capital vs. Signalling Explanations of Wages.” Journal of Economic Perspectives 9(4): 133–54.\n\nWeiss, Leonard, and Michael Klass. 1986. Regulatory Reform: What Actually Happened. Boston: Little, Brown.\n\nWeissberg, Robert. 2002. Polling, Policy, and Public Opinion: The Case Against Heeding the “Voice of the People.” NY: Palgrave.\n\nWhitman, David. 1998. The Optimism Gap. New York: Walker.\n\nWilliam J. Clinton Foundation. 2005. “Facts Sheet on NAFTA Notes.” URL http://www.clintonfoundation.org/legacy/101293-fact-sheet-on-nafta-notes.htm.\n\nWintrobe, Ronald. 1987. “The Market for Corporate Control and the Market for Political Control.” Journal of Law, Economics, and Organization 3(2): 435–48.\n\n———. 1998. The Political Economy of Dictatorship. Cambridge: Cambridge University Press.\n\nWittman, Donald. 1989. “Why Democracies Produce Efficient Results.” Journal of Political Economy 97(6): 1395–1424.\n\n———. 1995. The Myth of Democratic Failure: Why Political Institutions Are Efficient. Chicago: University of Chicago Press.\n\n———. 2005a. “Voting on Income Redistribution: How a Little Bit of Altruism Creates Transitivity.” URL http://repositories.cdlib.org/ucscecon/586/\n\n———. 2005b. “Pressure Groups and Political Advertising: How Uninformed Voters Can Use Strategic Rules of Thumb.” URL http://people.ucsc.edu/~wittman/working.papers/ruleofthumb.2005.pdf.\n\n———. 2005c. “Reply to Caplan: On the Methodology of Testing for Voter Irrationality.” Econ Journal Watch 2(1): 22–31.\n\n———. 2005d. “Second Reply to Caplan: The Power and Glory of the Median Voter.” Econ Journal Watch 2(2): 186–95.\n\nWolfers, Justin. 2001. “Are Voters Rational? Evidence from Gubernatorial Elections.” Stanford University Graduate School of Business Working Paper No. 1730.\n\nWolfers, Justin, and Eric Zitzewitz. 2004. “Prediction Markets.” Journal of Economic Perspectives 18(2): 107–26.\n\nWolpert, Robin, and James Gimpel. 1998. “Self-Interest, Symbolic Politics, and Public Attitudes toward Gun Control.” Political Behavior 20(3): 241–62.\n\nWright, Gerald, Robert Erikson, and John McIver. 1987. “Public Opinion and Policy Liberalism in the American States.” American Journal of Political Science 31(4): 980–1001.\n\nWyden, Ron. 2003. “Wyden, Dorgan Call For Immediate Halt to Tax-Funded ‘Terror Market’ Scheme.” URL http://wyden.senate.gov/media/2003/07282003\\_terrormarket.html.\n\nZaller, John. 1992. The Nature and Origins of Mass Opinion. Cambridge: Cambridge University Press.\n\n———. 2003. “Coming to Grips with V.O. Key’s Concept of Latent Opinion.” In MacKuen, Michael, and George Rabinowitz, eds. Electoral Democracy. Ann Arbor: University of Michigan Press: 311–36.\n\nINDEX\n\nabortion, 28, 149\n\nAchen, Christopher, 160\n\nAdams, Henry, 43\n\nadverse selection, 196\n\nadvertising, 176\n\npolitical, 179–80\n\naffirmative action, SAEE question regarding, 61\n\nagency, principal-agent relations, 172\n\naggregation. See Miracle of Aggregation\n\nagriculture, 30, 42, 129\n\nAkerlof, George, 105\n\nAlm, Richard, 42–43, 47\n\nAlthaus, Scott, 27–28, 108\n\naltruism, 35, 195\n\n“unselfish voters” thought experiment, 151–53, 161–62\n\nantiforeign bias, 30, 36–39, 49, 146\n\ndefined, 36\n\nevidence for existence of, 51, 58–59, 66, 68, 70–71\n\nimmigration policy and, 38–39\n\nas matter of degree, 39\n\nrational irrationality and situational abandonment of, 137\n\nsystematic error and, 10\n\nantimarket bias, 30–36, 49, 146, 201\n\ncompetition and, 71–72\n\ndebt market and prejudice against interest, 32–33, 142–43\n\ndefined, 30\n\nevidence for existence of, 51, 59–60, 62–63, 64–65, 72–73\n\nmarket payments conflated with transfers, 32\n\nmonopoly theories of price and, 34\n\nprofit motive and, 30–32\n\nrational irrationality and situational abandonment of, 137\n\nsystematic error and, 10\n\nantitrust laws, 175\n\nAristotle, 102\n\nasymmetric information, 105–6, 112\n\nBardhan, Pranab, 187\n\nBartels, Larry, 160\n\nBastiat, Frédéric, 17, 29, 34, 40, 197, 199\n\nCandlemakers’ Petition, 202\n\neconomic education and, 200, 203\n\non make-work bias, 40, 41, 43, 134–35\n\non sophisms (systematic error), 12\n\nBaudelaire, Charles, 30\n\nBayesianism and Bayes’ Rule, 99\n\nBecker, Gary, 23, 104, 117\n\nbehavioral economics. See psychology and economics\n\nbelief: bad policies linked to false beliefs, 142–43, 146–47, 161–62\n\nbias as predisposition to belief, 20\n\nconsumption value of, 206\n\ndogmatic belief and systematic error, 100–102\n\nfalse beliefs as comforting or consoling, 206–7\n\n“heterogeneity of belief” thought experiment, 147–48, 161\n\nirrationality linked to, 2. See also lay/expert belief gap; preference over beliefs\n\nbeliefs, false beliefs about political responsibility, 31, 51, 174\n\nBentham, Jeremy, 54\n\nBerkeley, George, 117\n\nBesley, Timothy, 111\n\nbetting, 130–31\n\nPolicy Analysis Market (PAM), 190–92\n\nbias: democratic fundamentalism, 186–90, 193\n\nideological, 54–56, 82, 83\n\nmarket fundamentalism among economists, 183–85\n\nin the media, 176\n\nas predisposition to belief, 20\n\nprojection of bias onto opposition, 186\n\nself-serving bias, 54, 55, 56, 63–64, 82, 83\n\nof voters vs. nonvoters, 157. See also antiforeign bias; antimarket bias; bias, systematic; make-work bias; pessimistic bias\n\nbias, systematic: about toxicology, 160–62\n\nbad policy linked to, 146–47\n\ndemocracy as polluted by, 206\n\neconomists and refutation of bias, 203\n\neconomists as biased, 48–49, 183–85\n\neducation’s role in eliminating, 198\n\nof experts, 81–82\n\nas innate, 31, 178\n\nmedia’s role in promulgating foolish beliefs, 177–79\n\nobjections to use of SAEE as measure of, 70–81\n\npreference over beliefs as cause of, 101–2\n\npsychology research and evidence of, 24–30\n\npublic opinion research and evidence of, 24–30\n\nrational irrationality and, 137. See also antiforeign bias; antimarket bias; make-work bias; pessimistic bias\n\n“big government,” 105–6\n\nbigotry, 138\n\nBlinder, Alan, 19, 33–34, 37, 41, 42, 167\n\nblind faith in political leaders, 16, 169–72, 181\n\nBoas, George, 45\n\nBöhm-Bawerk, Eugen von, 32–33, 118–19, 142–43\n\nBork, Robert, 186\n\nBrennan, Geoffrey, 137–39\n\nBush, George W., 170, 171\n\nbusiness productivity, 65\n\nCamerer, Colin, 135–36\n\n“Candlemakers’ Petition” (Bastiat), 202\n\nCapital and Interest (Böhm-Bawerk), 32–33\n\ncapitalism, 30–32, 34–35, 54, 70, 183, 187\n\nCapitalism, Socialism, and Democracy (Schumpeter), 122, 134, 166, 167\n\nCarroll, Lewis, 115\n\nCase, Anne, 111\n\nChambers, Whittaker, 101, 118\n\nChicago school, 197\n\nChurchill, Winston, 3, 190\n\nchurn (Cox and Alm), 42–43\n\nClassical Public Choice, 95–97\n\nasymmetric information and, 105–6\n\ncognitive shortcuts and, 106–8\n\ncorrelation between information and interest, 104–5\n\nfailure of democracy and, 96–97\n\nhigh negotiation or transaction costs and, 110–11\n\ninarticulate knowledge and, 106–8\n\nMiracle of Aggregation and, 103–4\n\nmisinformation and, 103–4\n\nrational ignorance and, 94–96\n\nWittman’s fork and, 108–11\n\nClinton, William J., 151, 159, 167\n\nCoase, Ronald, v, 199\n\nCoate, Stephen, 23\n\n“Comment on Thomas W. Hazlett” (Coase), v\n\nCommunism, 101, 118\n\ncomparative advantage. See Law of Comparative Advantage\n\ncompetition, 176\n\nantimarket bias linked to misunderstanding of, 71–72\n\npolitical, 110–11, 167\n\nfor support of irrational voters, 167\n\nconsensus: among economists, 31, 53, 195, 199\n\nunanimity or “identical voter” thought experiment, 144–47\n\ncorruption, political, 7, 104, 106, 171, 174, 180\n\ncosts: Classical Public Choice and high negotiation or transaction costs, 110–11\n\nof ideological loyalty, 18\n\nmaterial costs of error, 18, 119–23\n\nprivate vs. social costs of error, 121\n\nCox, W. Michael, 42–43, 47\n\ncreationism, 186\n\nThe Crowd (Le Bon), 24\n\ncynicism, 100, 168–69\n\nDasgupta, Partha, 115\n\ndeath penalty, 132–33\n\ndebt market and antimarket bias, 32–33, 142–43\n\ndeceit, 108, 117, 167, 168\n\ngreed and, 35\n\nin survey responses, 80\n\ndecision making: cognitive shortcuts and, 106–7\n\nemotional aspects of, 137–40\n\ngroup decision making and private vs. social costs of error, 122\n\nMiracle of Aggregation and, 6\n\nas private vs. collective action, 192–94\n\ndeference: blind faith in leaders, 16, 169–72, 181\n\nto expertise, 171–72\n\ndeficits: attitudes regarding, 28\n\nSAEE question regarding deficit spending, 57–58\n\ntrade deficits, 37, 39, 202–3\n\ndelegation of responsibility, 172–73, 181\n\nDelli Carpini, Michael, 83, 95\n\ndemagoguery, 19–21\n\ndemand. See supply and demand\n\ndemocracy: as a commons, 206\n\ncorrections for, 197–99\n\ndemocratic fundamentalism, 186–90, 193\n\nexternalities of, 206\n\nlack of empirical objectivity in assessment of, 187\n\nmarkets as alternative to, 3–4, 193–94, 195, 197\n\nas overestimated by economists, 4, 181, 195–96\n\nprivate choice as alternative, 192–94\n\nas “sacred value,” 189\n\nsuffrage and, 9, 197\n\nsystematic bias as pollution in, 206. See also failure of democracy\n\nDemocracy and Decision: The Pure Theory of Electoral Preference (Brennan and Lomasky), 137–39\n\nDemocracy and Liberty (Lecky), v\n\nDemocracy in America (Tocqueville), 131\n\ndemocratic fundamentalism, 186–90, 193\n\nDescartes, Rene, 114\n\ndeveloping nations: economic literacy in, 158\n\nresistance to reforms in, 207–8\n\nThird World as exploited, 34–35\n\ndevelopment economics, 158\n\nDiscourse on Method (Descartes), 114\n\nDorgan, Byron, 190, 192\n\n“doublethink” per Orwell, 16, 125–26\n\nDowns, Anthony, 94, 97–99\n\ndownsizing, 42–43\n\nSAEE questions regarding, 66–67, 70\n\nDye, Thomas, 169\n\nEasterbrook, Gregg, 47\n\necological rationality, 24\n\neconomic literacy: education linked to, 154–56, 158, 197, 199–204\n\ngender linked to, 154\n\nincome growth linked to, 154\n\njob security and, 154\n\nmedia as tool to increase, 202–3\n\nstandard pedagogy and textbooks as too equivocal, 200–201\n\nvariables linked to, 154–56, 163–65\n\nof voters, 197\n\neconomics: Chicago school of, 197\n\ndogmatism about, 100–102\n\neffective teaching of, 199–204\n\nreasons for book’s focus on, 21–22\n\nskepticism about, 189\n\nsocial function of, 199, 203–4\n\nsystematically biased beliefs about, 10–14, 30–48, 81–83\n\nthought experiments and, 143\n\nEconomic Sophisms (Bastiat), 12, 17, 40\n\nAn Economic Theory of Democracy (Downs), 94\n\neconomists: ability to influence policy, 174, 198–99\n\nalleged biases of, 52–56\n\nalleged conservatism of, 82\n\nalleged market fundamentalism of, 3, 183–85\n\nbelief in the SIVH, 148–49\n\nconsensus among, 31, 53, 195, 199\n\ndemocracy overestimated by, 4, 181, 195–96\n\ndifferential agreement with, 154–56\n\ndisagreements between, 42, 182\n\ndistrust of what people say, 116–17\n\nexcessive commitment to rationality assumption, v, 2, 23, 97–100, 112, 207–9\n\nhostility towards, 182–83\n\nlaissez-faire presumption of, 196\n\noral tradition of, 13, 48–49\n\nrejection of popular misconceptions, 11–14, 28–30, 50–52\n\nWittman’s Fork and, 108–11\n\nEdgerton, Robert, 128\n\neducation: classroom teaching of economics, 13–14, 45\n\neconomic literacy linked to, 99–204, 154–56, 158, 197\n\nin economics, 198–204\n\nincome as proxy for, 157\n\nKrugman on economic education, 29, 199\n\nlay/expert belief gap diminished by, 83\n\npessimistic bias neglected in, 45\n\nSAEE questions regarding job training and, 60, 67\n\nstrategic appeal to irrationality in, 201\n\nvoter participation linked to, 158\n\nEhrlich, Paul, 48\n\nEinhorn, Hillel, 136\n\nEmerson, Ralph Waldo, 201\n\nemissions trading, 33\n\nemployment: downsizing and unemployment, 42–43\n\nimmigration and, 59–60\n\njob security linked to economic literacy, 154\n\nSAEE questions regarding, 73–74\n\n“wage conspiracies” on part of employers, 34–35. See also labor; make-work bias\n\n“enlightened preferences,” 25–28, 55\n\nEnlightened Public: beliefs of, 55, 56\n\nmethodology for estimating beliefs of, 84\n\nvariables linked to economic literacy, 154–56, 163–65\n\nenthusiasm, 15\n\nenvironmental economics, 48\n\nenvironmental policy: emissions trading, 33\n\ntoxicology biases and, 160–62\n\nenvironmental quality issues, pessimistic bias regarding, 47–48\n\nerror, systematic: material costs of error, 119–23\n\nprivate vs. social costs of error, 121\n\nerrors, as compounding rather canceling, 16\n\nevolution, 186\n\nexecutive salaries, SAEE question regarding, 64–65\n\nexpectations, rationality and, 98–99, 207\n\nexperts: as biased, 52–54\n\nconsensus among, 53\n\ndeference to, 171–72\n\npolitical expertise, 180, 188–89\n\nrejection of economic expertise in political sphere, 189\n\nexpressive voting, rational irrationality and, 137–40\n\nexternalities, democracy and, 3\n\nfailure of democracy: asymmetric information linked to, 105\n\nClassical Public Choice theory as explanation for, 96–97\n\n“ignorance only” view as explanation for, 100\n\ninformation economics and, 102–3\n\nirrational voters and, 142\n\nWittman’s Fork and paths to, 108–11\n\nfamily income, SAEE questions regarding, 74–75\n\nFernandez, Raquel, 207–8\n\nfiscal conservatism, 27\n\nFischer, Louis, 16\n\nforeign aid spending, 79–80\n\nSAEE questions regarding, 58–59\n\nfree trade, 37, 115\n\nideological opposition to, 182\n\nmarket fundamentalism and, 183–85, 197\n\nNAFTA (North American Free Trade Agreement) and, 159–60\n\nas political issue, 166, 171, 174\n\nsurveys of attitudes, 50–51, 69\n\nas unpopular stance, 12. See also protectionism\n\nFriedman, Milton, 184, 197\n\nfull price, 17\n\ngay rights, 27\n\ngender: economic literacy linked to, 154\n\nSAEE question regarding women in workforce, 68\n\ngeneral interest, perceptions of, 19\n\nGeneral Social Survey (GSS), 51\n\nGerken, Ann, 150\n\n“get out the vote” campaigns, 198\n\nGIGO (garbage in, garbage out), 2\n\nGiving Up on Democracy (Kamber), 186\n\nglobal economics: antiforeign bias and, 36–39, 66\n\ndevelopment economics, 158\n\nKrugman on, 14\n\nThird World as exploited, 34\n\nGone with the Wind (Mitchell), 139–40\n\nGreen, Donald, 150\n\nGreider, William, 36, 53, 142, 174–75, 187\n\ngrowth, economic: long-run growth, 77–78\n\npessimism and growth slowdown, 47\n\nGSS (General Social Survey), 51\n\nHanson, Robin, 14, 191\n\nHard Heads, Soft Hearts (Blinder), 167\n\nHeld, Virginia, 152\n\nHerman, Arthur, 45, 46–47\n\nHess, Rudolf, 169–70\n\n“heterogeneity of belief” thought experiment, 147–48, 161\n\nHitler, Adolf, 169–70, 176–77\n\nHoffer, Eric, 16\n\nHogarth, Robin, 135–36\n\nHume, David, 46\n\nThe Idea of Decline in Western History (Herman), 45\n\n“idea traps,” 158\n\n“identical voters” thought experiment, 144–47, 151, 161\n\nideology: blind faith in, 16\n\nideological bias and economists, 54–56, 82, 83\n\nideological dogmatism and systematic error, 100–101\n\nideological loyalty as cognitive shortcut for voters, 106–7\n\npartisan identification, 17–18, 149, 153–54\n\npreference over beliefs linked to, 16\n\nprice of ideological loyalty, 17–18\n\nignorance: conflated with receptivity to information, 103–4\n\nas distinct from irrationality, 100\n\ninarticulate knowledge mistaken for voter ignorance, 106\n\nvs. stupidity, 109\n\nimmigration, 38–39, 68\n\nantiforeign bias and, 58–59\n\nSAEE question regarding, 58–59\n\nincentives, rational behavior and, 135–37, 167–68\n\nincome: family income related questions in SAEE, 74\n\ngrowth linked to economic literacy, 154\n\nmean vs. median, 74\n\ntwo-income households, 76\n\ninequality, 74, 75–76\n\nSAEE question regarding, 74\n\ninformation: asymmetric information, 105–6, 112\n\nimperfect information as cause of market failure, 95\n\nmisinformation and voter ignorance, 108\n\npropaganda, 176–80, 181\n\nsuppression of, 129–30\n\ninformation economics, 94–95, 102–3\n\ninsider expertise, 97\n\ninsurance, 196\n\ninterest (debt market), 32–33, 142–43\n\n“invisible hand,” 32\n\nirrationality: bad policy linked to false beliefs, 143–44, 146–47, 161–62\n\nblind faith in leaders, 16, 169–72, 181\n\nas comfort or consolation, 118–19, 145–46, 206–7\n\ndelegation and, 172–74\n\n“demand for irrationality” curves, 123–24, 125, 133–35\n\nas distinct from ignorance, 100, 109\n\nemotional aspects of decision-making, 118\n\nfailure of democracy and, 142\n\ngullibility and, 177\n\n“heterogeneity of belief” thought experiment, 147–48\n\n“identical voters” thought experiment, 144–47, 161\n\nmaterial price for political irrationality, 132\n\nthe media and, 176–79\n\nmotives for, 115–16, 118–19\n\nnear-classical demand for, 146\n\nneglected or rejected in economic scholarship, 97–100\n\nas neglected or rejected in scholarship, 23\n\nneutralized by political faith, 171\n\nOr-wellian “doublethink” and, 125–26\n\npreferences and, 122\n\nprice of, 122, 123–24\n\nas profound departure from the rational, 94\n\npsychological benefits of, 145–46\n\nrational expectations and, 98–99\n\nrhetorical appeals to, 201\n\nas selective, 2\n\nspecial interests and, 179–80\n\n“unselfish voters” thought experiment, 151–53\n\nof voters, 142, 167, 195\n\nirrationality and, price, 122, 123–24, 132\n\nIslamic economics, 33\n\nJainism, 127\n\njihad, 127–28\n\njob security, economic literacy and, 154\n\nJost, John, et. al, 116\n\nKahneman, Daniel, 24, 52\n\nKamber, Victor, 186\n\nKearl, J. R., 50–51\n\nKeeter, Scott, 83, 95\n\nKelman, Steven, 53\n\n“kill the messenger” responses, 142\n\nKnight, Frank, v, 12–13\n\nKoestler, Arthur, 101\n\nKraus, Nancy, 83, 161–62\n\nKremer, Michael, 48\n\nKrugman, Paul, 14, 37, 47, 50, 143, 170, 180\n\non economic education, 29, 199\n\non influence of attitude on international economy, 39\n\nKuran, Timur, 33, 208\n\nKuttner, Robert, 5, 54, 183, 189, 193\n\nlabor: conservation of, 40–41, 43\n\n“employer conspiracies” and suppression of wages, 34–35\n\nimmigration policy as labor trade issue, 39\n\nmake-work bias and, 40–43\n\ntrade in, 39\n\nwomen in workforce, 68\n\nwork perceived as economically beneficial, 62\n\nLandsburg, Steven, 38\n\nLaw of Comparative Advantage, 38, 66\n\nas counter-intuitive, 10–11\n\nlaw of large numbers, 8\n\nlay/expert belief gap, 55\n\neconomic literacy linked to, 154–55\n\neducation and diminishment of, 83\n\ninstances of diminishing difference, 77–78, 83, 92–93\n\nSAEE as confirmation of, 52, 81–83\n\ntime horizons as partial explanation for, 70, 71\n\nin toxicology, 160–62\n\nLe Bon, Gustave, 15–16, 19, 23, 116, 132\n\nLecky, William, v, 9\n\nLenin, Vladimir, 129\n\nLeone, Richard, 189\n\nLiberty and Democracy (Lecky), 9\n\n“Life in the Economic Emergency Room” (Sachs), 207\n\nLocke, John, 15\n\nlogrolling, 110\n\nLomasky, Loren, 137–39\n\nlong-run growth, 77–78\n\nLovejoy, Arthur, 45\n\nLucas, Robert, 98–99\n\nLupia, Arthur, 106–7\n\nLysenko, Trofim, 129\n\nMachiavelli, Niccolò, 167, 168, 171, 172\n\nmake-work bias, 30, 40–43, 49, 146\n\ndefined, 40\n\nevidence from SAEE indicating, 65–66, 66–67, 70–71\n\nrational irrationality and situational abandonment of, 137\n\nsystematic error and, 10\n\ntechnology and, 41–42\n\nMalmfors, Torbjörn, 83, 161–62\n\nMao Zedong, 103\n\nmarket fundamentalism, 183–85, 197\n\nmarket payments, conflated with transfers, 32\n\nmarkets, 35–36\n\nas alternative to democracy, 3–4, 193–94, 195, 197\n\nfailures of, 95, 184, 196\n\nfree markets, 27–28, 115, 197\n\nmarket fundamentalism, 183–85, 197\n\npolicy and, 27–28\n\nregulation of, 27–28\n\nsupport among economists for free markets, 31, 182\n\nMarx, Karl, 54\n\nMarxism, 54, 118–19, 129–30\n\nmaterial costs, of false beliefs or error, 119–23\n\nMcCloskey, Donald, 11, 100–101\n\nMcCubbins, Matthew, 106–7\n\nmechanization, 41–42\n\nmedia: as economic literacy tool, 202–3\n\nincentives for rationality on part of politicians, 167\n\nas market driven, 20, 21\n\noverconfidence in, 177–78\n\npolitical advertising, 179–80\n\npromulgation of false beliefs by, 177–79, 208\n\nMedian Voter Theorem, 111, 153, 154\n\nMencken, H. L., 5, 8, 18\n\nMichels, Robert, 169\n\nMichnik, Adam, 186\n\nMiracle of Aggregation, 6–9, 112\n\nClassical Public Choice and, 103–4\n\ncorrelation between information and interests, 104\n\nas illusory or hoax, 81, 147\n\nsystematic error and undermining of, 21–22\n\nMises, Ludwig von, 31, 54, 185\n\nmonopolies, 201\n\nperception of, 72\n\npolitical, 110\n\nmonopoly theories of price, 34\n\nMorris, Stephen, 23\n\nMosca, Gaetano, 116, 119, 127–28\n\nMueller, John, 35\n\n“Music Cues: Adlai Stevenson” (Simon), 1\n\nMuth, John, 98–99\n\nNAFTA (North American Free Trade Agreement), 159–60\n\nNash, John, 118\n\nNational Survey of Public Knowledge of Welfare Reform and the Federal Budget, 25, 79–80\n\nNewcomb, Simon, 36–37, 41, 50\n\non systematic error, 11–12\n\nNietzsche, Friedrich, 15\n\n9/11 (September 11, 2001), 170, 191\n\nNorth American Free Trade Agreement (NAFTA), 159–60\n\nNoss, John, 127\n\nobjectivity, 186–87\n\nof economists, 52–54, 195\n\npessimistic bias and, 47\n\nin politics, 101\n\ntheory-driven research and, 208–9\n\nOedipus/Jocasta paradox, 142\n\noff-shoring, 66\n\nOlson, Mancur, 97\n\nOrwell, George, 16, 125–26, 133\n\nPage, Benjamin, 7\n\nPAM (Policy Analysis Market), 190–92\n\nParadox of Democracy, 1–2\n\nPersson, Torstein, 205\n\npessimistic bias, 30, 43–48, 49, 146, 179\n\ndefined, 44\n\nevidence from SAEE indicating, 57, 60–62, 68, 74–75\n\nas historically ubiquitous, 45–47\n\nmedia and, 179\n\nnostalgia linked to, 45\n\npsychological roots of, 46–47\n\nrational irrationality and situational abandonment of, 137\n\nas relatively neglected in economic education, 45\n\nsystematic error and, 10\n\nPinker, Steven, 198\n\nPoindexter, John, 190\n\npolicies: market-oriented policy prescriptions, 196\n\norigins of bad or counterproductive, 205–6\n\nslack exploited to improve, 198–99\n\nPolicy Analysis Market (PAM), 190–92\n\nPolitical Economics (Persson and Tabellini), 205\n\npolitical economy See rational choice theory\n\n“Political I.Q.”, 25\n\npoliticians: background of, 169\n\nblind faith in, 16, 169–72, 181\n\ncompliance with public opinion by, 195\n\nrationality of, 167–69, 195\n\nshirking by, 19–21, 31, 171\n\npolling, 131–32\n\npollution, environmental, 33, 73, 121\n\nPop Internationalism (Krugman), 14, 50\n\npopulation growth, 43, 48, 59\n\npopulism, 162\n\npreference over beliefs, 14–16, 122\n\nas easily observable phenomenon, 117–18\n\nevidence of, 115–19\n\nideology linked to, 16\n\nrational choice theory and, 21–22\n\nreceptivity to information obstructed by, 103–4\n\nreligion linked to, 15–16, 19, 116–17\n\nsystematic bias resulting from, 101–2\n\npreferences: democracy and aggregation of, 144\n\n“enlightened preferences,” 25–28, 55\n\nirrationality and, 122\n\npolicy preferences, 158–59. See also preference over beliefs\n\npresidential influence over economy, SAEE question regarding, 73\n\nprice: full price, 17\n\nof ideological loyalty, 18\n\nmonopoly theories of, 34\n\nsupport policies, 34–35\n\nwage and, 34–35\n\nprice supports, 30\n\nPrimitivism and Related Ideas In Antiquity (Lovejoy and Boas), 45\n\nThe Prince (Machiavelli), 167, 168, 171, 172\n\nprincipal-agent relations, 172\n\nPrisoner’s Dilemma, 127\n\nprivate choice, 192–94, 197\n\nprivate action in relationship to collective action, 193–94\n\nprivate costs, 121\n\nprobability of decisiveness, 150\n\nproductivity, SAEE question regarding, 65\n\nprofits: antimarket bias and profit motive, 30–32\n\nSAEE question regarding, 63–64\n\nprogress, 45\n\npropaganda, 176–80, 181\n\npolitical advertising, 179–80\n\nprotectionism: Candlemakers’ Petition as critique of, 202\n\nas example of socially harmful policy, 1\n\nas popular, 12, 50–51, 69, 81, 100, 139, 142–43, 178\n\nrational irrationality and, 124, 139\n\npsychology: of conviction or blind faith, 16, 189\n\neconomics and, 24, 135–37\n\nevidence of systematic bias from research in, 24–30\n\nexpressive voting as psychologically rewarding, 138\n\nirrationality as comfort or consolation, 118–19, 145–46, 206–7\n\npessimistic bias as rooted in, 46, 47\n\nplausibility of rational irrationality, 125–31\n\n“sacred values,” 189\n\npsychology and economics, 24, 135–37\n\npublic choice theory. See rational choice theory\n\npublic interest, Paradox of Democracy and, 2, 3\n\npublic opinion: dimensionality of public opinion in multi-issue democracy, 153–54\n\neconomists and influence of, 203\n\nPAM betting market and, 191–92\n\npoliticians general compliance with, 195\n\nresearch, 208\n\nresearch in, 24–30\n\npunishment, 109, 112\n\ndeath penalty, 132–33\n\noptimal, 104–5\n\nof shirking politicians, 171\n\nRand, Ayn, 14–15\n\nrational choice theory, 6, 14, 19\n\nempirical research as theory-driven, 208–9\n\ninability to explain political failure, 102–11\n\npreference over beliefs and, 21–22\n\npublic opinion research and, 208–9. See also Classical Public Choice\n\nrational expectations, 98–99, 207\n\nrational ignorance, 5–6\n\nClassical Public Choice (rational choice theory) and, 94–96\n\nflaws in “ignorance only” as explanation of systematic error, 100–102\n\nas inadequate explanation, 108–9\n\nrational irrationality compared to, 123\n\nrational irrationality, 17–18, 206\n\ncase studies in, 127–31\n\ncognitive shortcuts and, 126\n\nexperimental evidence indicating, 135–37\n\nas explanation for political behavior, 141\n\nexpressive voting and, 137–40, 138–39\n\nfluctuating incentives and, 124\n\npolitics and, 131–35\n\nrational ignorance compared to, 123\n\nsincerity of commitment to beliefs and, 140\n\nsituational examples of, 124–25\n\nsystematic bias and, 137\n\ntacit knowledge and, 126\n\nrationality: Bayesian, 99\n\nends vs. means and, 98–99\n\nincentives for, 206\n\nas truth-seeking, 99, 101\n\nreceptivity to ideas, 201\n\npreference over beliefs as obstruction, 103–4\n\npropaganda and, 177–78\n\nrhetoric as persuasive, 170\n\nregulation: deregulation and privatization, 6, 183, 189, 192–93\n\ninsurance industry and, 196\n\nof markets, 27–28\n\nSAEE questions regarding, 62–63, 73\n\nsupply and demand consequences of, 11\n\nsystematic bias and, 19–21, 31, 62–63, 73, 133, 146, 161\n\nreligion: “creation science” and objectivity, 186\n\ndoctrinal disputes within, 127–28\n\nas political issue, 27\n\npreference over beliefs and, 15–16, 19, 116–17\n\nrational irrationality and religious belief, 127–28\n\nseparation of church and state, 192\n\n“Resistance to Reform” (Fernandez and Rodrik), 207–8\n\nretrospective voting, 107–8, 158–60\n\nThe Rhetoric of Economics (McCloskey), 11\n\nRobbins, Tim, 151\n\nRodrik, Dani, 23, 207–8\n\nThe Role of Principles in Economics and Politics (Knight), v\n\nRothbard, Murray, 185\n\nRubin, Paul, 178\n\nThe Ruling Class (Mosca), 119\n\nRussell, Bertrand, 183\n\nSachs, Jeffrey, 207\n\nSAEE. See Survey of Americans and Economists on the Economy (SAEE)\n\nSamuelson, Paul, 13, 117–18, 199\n\nSarandon, Susan, 151\n\nsati (ritual self-immolation), 128\n\nsavings rate, SAEE question regarding, 63\n\nscapegoating, 142, 146, 159\n\nSchultze, Charles, 30–31, 31\n\nSchumpeter, Joseph, 30–31, 70–71, 134, 166, 178\n\nscience: rejection or suppression of, 129–30, 186. See also technology\n\nScott, Howard, 41–42\n\nSears, David, 208\n\nA Second Mencken Chrestomathy (Mencken), 5\n\nselection pressure, 167–68\n\nselective participation in voting, 156–58, 162\n\nself-censorship, 115\n\nself-interest, 18–19\n\nself-serving bias, 54, 55, 56, 63–64, 82, 83\n\ntruth-seeking as limited by material self-interest, 115. See also altruism\n\nself-interested voter hypothesis (SIVH), 18–19, 148–51, 152, 157, 198\n\nself-serving bias, 54, 55, 56, 63–64\n\neconomists and, 82, 83\n\nShapiro, Ian, 187–89, 193–94\n\nShapiro, Robert, 7\n\nShaw, George Bernard, 53\n\nShermer, Michael, 14\n\nshirking, 19–21, 31, 111, 171\n\nshopping, as analogy for voting, 140–41, 151, 206\n\nShweder, Richard, 94\n\nSick Societies (Edgerton), 125\n\nSimon, Julian, 48, 177\n\nSimon, Scott, 1\n\nsincerity, 100\n\nof politicians, 168–69\n\nrational irrationality and commitment to beliefs, 140\n\nrespondents to SAEE as insincere, 80\n\nslack, 175–76, 195\n\nexploited to improve policy, 198–99\n\nfaith-based, 170–71, 181\n\n“wiggle room” for economists to influence public opinion, 203\n\nSlovic, Paul, 83, 161–62\n\nSmith, Adam, 29, 32, 37, 38\n\non pessimism, 44\n\non progress, 45–46\n\non systematic error, 11–12\n\nSmith, Al, 186\n\nsmoking and smokers’ rights issues, 150\n\nsocial costs, 121\n\nsocial policy, 21–22, 27, 60–61, 79–80\n\nsociotropic motivation, 152\n\nsophisms per Bastiat, 12\n\nSoros, George, 183\n\nspecial interests, 20, 97, 176, 179–80\n\nSpencer, Herbert, 46\n\nStalin, Joseph, 129–30\n\nstandard of living, 74\n\npessimistic bias regarding, 47–48\n\nSAEE question regarding, 77–78\n\nstereotypes: of economists, 69, 199–200\n\nideological, 153–54\n\npartisan political, 149\n\nStevenson, Adlai, 1\n\nStigler, George, 13, 197\n\nStiglitz, Joseph, 115, 184\n\nstupidity, 109, 111–12\n\nSunstein, Cass, 208\n\nsupply and demand: antimarket bias linked to ignorance of, 71–72\n\nconsequences of regulation on, 11\n\nfor irrationality, 123–24, 125, 133–35, 146\n\nSAEE questions regarding, 71–73. See also supply-side politics\n\nsupply-side politics, 180–81\n\nrationality of politicians, 167–69\n\nSurowiecki, James, 6, 8, 9, 191\n\nSurvey of Americans and Economists on the Economy (SAEE), 160\n\nadjustments for bias, 54–55\n\ndescribed, 52\n\ninsincerity of respondents as confounding factor, 80\n\nitem-by-item discussion of, 56–79\n\nlay/expert belief gaps confirmed by, 52, 81–83\n\nmethodology for estimating beliefs of Enlightened Public from data provided, 84\n\nquestion selection bias as confounding factor, 80–81\n\nas robust testing tool, 83–84\n\nsystematic error as revealed in, 99–100\n\ntechnical appendix on data and statistical results, 85–93\n\n“vagueness” as confounding factor, 79–80\n\nsurveys of economic belief, 50–51\n\nNational Survey of Public Knowledge of Welfare Reform and the Federal Budget, 79–80. See also Survey of Americans and Economists on the Economy (SAEE)\n\nsystematic error, 9–11\n\nBayesianism and accommodation of, 99\n\ncauses of, 98\n\nconceptual history of, 11–14\n\nfalse beliefs about political responsibility and, 31, 51, 174\n\n“ignorance only” as inadequate explanation for, 100–102\n\nmedia’s role in promulgating foolish beliefs, 177–79\n\nmethodology for identifying, 52–53\n\nas neglected or rejected in scholarship, 23\n\npolicy formation and, 11\n\nsincere beliefs and, 100\n\nTabellini, Guido, 205\n\ntacit knowledge, 106–7\n\ntaxation, 73\n\nSAEE questions regarding, 57–58, 59–60, 63, 68\n\ntechnology: international trade as, 38\n\nmechanization and make-work bias, 41–42\n\nSAEE questions regarding, 65–66, 69\n\nThe Tempting of America (Bork), 186\n\nterrorism: 9/11 (September 11, 2001), 170, 191\n\nPolicy Analysis Market (PAM), 190–92\n\nTetlock, Philip, 189\n\nThaler, Richard, 113, 135\n\n“A Theory of Competition Among Pressure Groups for Political Influence” (Becker), 176\n\nThrough the Looking-Glass (Carroll), 115\n\ntime, lay/expert belief gap and time horizon differences, 70, 71\n\nTocqueville, Alexis de, 46, 131\n\ntotalitarianism, 16, 129–30, 169–70\n\ntoxicology, 83\n\nas example of systematically biased beliefs, 160–62\n\ntrade, 1\n\nagreements, 69, 70–71\n\ndeficits, 37, 39, 202–3\n\nin labor, 39\n\nLaw of Comparative Advantage and, 38\n\nSAEE questions regarding, 69, 71\n\nas technology, 38. See also free trade\n\ntransfers, 32\n\nTruman, Harry S, 199\n\ntruth-seeking, 99, 101, 115, 141\n\nEmersonon, 201–2\n\nTullock, Gordon, 80, 94, 174\n\nTversky, Amos, 24, 52\n\n“unselfish voters” thought experiment, 151–53, 162\n\nvagueness: as politically strategic, 170–71, 175\n\nof SAEE question items, 79–80\n\nvariables associated with economic literacy, 154–56, 163–65\n\nvoter ignorance, 5–6, 94, 97, 108\n\nempirical evidence for, 8\n\n“extreme voter stupidity,” 111–12\n\n“ignorance only” view, 100–102\n\ninarticulate knowledge mistaken for, 106\n\nvoting: as altruistic or unselfish, 150–53, 161–62, 195\n\ndissatisfaction as norm, 159\n\neconomic literacy of voters, 197\n\nas expressive, 137–40\n\n“get out the vote” campaigns, 198\n\n“heterogeneity of belief” thought experiment, 147–48, 161\n\n“identical voters” thought experiment, 144–47\n\nincentives for rational behavior while, 140–41\n\nas instrumental, 137–40\n\nirrationality of voters, 142, 167, 195, 205, 207, 208\n\n“mixed policy/outcome preferences” thought experiment, 158–60\n\nas outcome-linked, 158–60\n\nplural or weighted voting schemes, 197–98\n\nretrospective voting, 158–60\n\nselective participation in, 156–58, 162\n\nself-interested voter hypothesis (SIVH), 18–20, 148–51, 152, 157, 198\n\nshopping as analogy for, 140–41, 151, 206\n\n“unselfish voters” thought experiment, 151–53, 161–62\n\nwages: “employer conspiracies” and suppression of, 34–35\n\nimmigration and depression of, 59\n\nSAEE question regarding, 73–74, 75–76\n\nWashington Consensus, 195\n\nwealth, 63–64\n\ngap between rich and poor, 74, 157\n\nprejudice against, 37–38\n\nratio of result to effort linked to, 41\n\nWealth of Nations (Smith), 12, 32, 37–38\n\nwelfare, 79–80\n\nSAEE question regarding, 60–61\n\n“What Do Undergrads Need to Know About Trade” (Krugman), 199\n\nWho Will Tell the People (Greider), 36, 142\n\n“wisdom of crowds,” 191\n\nThe Wisdom of Crowds (Surowiecki), 6\n\nWittman, Donald, 103, 109–11, 143, 175–76, 179–80\n\nWittman’s fork, 108–11\n\nwomen in workforce, SAEE question regarding, 68\n\nwork ethic, SAEE question regarding, 61–62\n\nWorld Trade Organization (WTO), 174\n\nWorldviews survey, 51\n\nWyden, Ron, 190, 192\n\nZeigler, Harmon, 169", "date_published": "2007-04-01T00:00:00Z", "authors": ["Bryan Caplan"], "summaries": [], "initial_source": "ebook", "source_filetype": "epub"}
{"id": "fc161b24f7bd72155deb29e8c4af0ea5", "title": "Global Catastrophic Risks", "url": "https://www.goodreads.com/book/show/2659696-global-catastrophic-risks", "source": "special_docs", "source_type": "book", "text": "[]\n\nGlobal Catastrophic Risks\n\nGlobal Catastrophic Risks\n\nEdited by\n\nNick Bostrom\nMilan M. Ćirković\n\n[Image]\n\n[Image]\n\nGreat Clarendon Street, Oxford OX2 6DP\n\nOxford University Press is a department of the University of Oxford.\n\nIt furthers the University’s objective of excellence in research, scholarship,\n\nand education by publishing worldwide in\n\nOxford New York\n\nAuckland Cape Town Dares Salaam Hong Kong Karachi\n\nKuala Lumpur Madrid Melbourne Mexico City Nairobi\n\nNew Delhi Shanghai Taipei Toronto\n\nWith offices in\n\nArgentina Austria Brazil Chile Czech Republic France Greece\n\nGuatemala Hungary Italy Japan Poland Portugal Singapore\n\nSouth Korea Switzerland Thailand Turkey Ukraine Vietnam\n\nOxford is a registered trade mark of Oxford University Press\n\ninthe UK and incertain other countries\n\nPublished in the United States\n\nby Oxford University Press Inc., New York\n\n© Oxford University Press 2008\n\nThe moral rights of the authors have been asserted\n\nDatabase right Oxford University Press (maker)\n\nFirst published 2008\n\nAll rights reserved. No part of this publication may be reproduced,\n\nstored in a retrieval system, or transmitted, in any form or by any means,\n\nwithout the prior permission in writing of Oxford University Press,\n\nor as expressly permitted by law, or under terms agreed with the appropriate\n\nreprographics rights organization. Enquiries concerning reproduction\n\noutside the scope of the above should be sent to the Rights Department,\n\nOxford University Press, at the address above\n\nYou must not circulate this book in any other binding or cover\n\nand you must impose the same condition on any acquirer\n\nBritish Library Cataloguing in Publication Data\n\nData available\n\nLibrary of Congress Cataloging in Publication Data\n\nData available\n\nTypeset by Newgen Imaging Systems (P) Ltd., Chennai, India\n\nPrinted in Great Britain\n\non acid-free paper by\n\nCPI Antony Rowe, Chippenham, Wiltshire\n\n13579108642\n\nAcknowledgements\n\nIt is our pleasure to acknowledge the many people and institutions who have in one way or another contributed to the completion of this book. Our home institutions – the Future of Humanity Institute in the James Martin 21st Century School at Oxford University and the Astronomical Observatory of Belgrade – have offered environments conducive to our cross-disciplinary undertaking. Milan wishes to acknowledge the Oxford Colleges Hospitality Scheme and the Open Society Foundation of Belgrade for a pleasant time in Oxford back in 2004 during which this book project was conceived. Nick wishes to thank especially James Martin and Lou Salkind for their visionary support.\n\nPhysicist and polymath Cosma R. Shalizi gave an entire draft of the book a close, erudite and immensely helpful critical reading. We owe a great debt of gratitude to Alison Jones, Jessica Churchman and Dewi Jackson of Oxford University Press, who took so much interest in the project and helped shepherd it across a range of time scales. We are also appreciative of the scientific assistance by Peter Taylor and Rafaela Hillerbrand and for administrative support by Rachel Woodcock, Miriam Wood and Jo Armitage.\n\nWe thank John Leslie for stimulating our interest in extreme risk many years ago. We thank Mathew Gaverick, Julian Savulescu, Steve Rayner, Irena Diklić, Slobodan Popović, Tanja Berić, KenD. Olum, Istvan Aranyosi, Max Tegmark, Vesna Milošević, Toby Ord, Anders Sandberg, Bill Joy, Maja Bulatović, Alan Robertson, James Hughes, Robert J. Bradbury, Zoran Živković, Michael Vasser, Zoran Knežević, Ivana Dragićević, and Susan Rogers for pleasant and useful discussions of issues relevant to this book. Despairing of producing an exhaustive acknowledgement of even our most direct and immediate intellectual debts – which extend beyond science into the humanities and even music, literature, and art – we humbly apologize to all whom we have egregiously neglected.\n\nFinally, let all the faults and shortcomings of this study be an impetus for others to do better. We thank in advance those who take up this challenge.\n\nForeword\n\nIn 1903, H.G. Wells gave a lecture at the Royal Institution in London, highlighting the risk of global disaster: ‘It is impossible’, proclaimed the young Wells, “’to show why certain things should not utterly destroy and end the human race and story; why night should not presently come down and make all our dreams and efforts vain. … something from space, or pestilence, or some great disease of the atmosphere, some trailing cometary poison, some great emanation of vapour from the interior of the earth, or new animals to prey on us, or some drug or wrecking madness in the mind of man.’ Wells’ pessimism deepened in his later years; he lived long enough to learn about Hiroshima and Nagasaki and died in 1946.\n\nIn that year, some physicists at Chicago started a journal called the Bulletin of Atomic Scientists, aimed at promoting arms control. The ‘logo’ on the Bulletin’s cover is a clock, the closeness of whose hands to midnight indicates the editor’s judgement on how precarious the world situation is. Every few years the minute hand is shifted, either forwards or backwards.\n\nThroughout the decades of the Cold War, the entire Western World was at great hazard. The superpowers could have stumbled towards Armageddon through muddle and miscalculation. We are not very rational in assessing relative risk. In some contexts, we are absurdly risk-averse. We fret about statistically tiny risks; carcinogens in food, a one-in-a-million change of being killed in train crashes, and so forth. But most of us were ‘indenial’ about the far greater risk of death in a nuclear catastrophe.\n\nIn 1989, the Bulletin’s clock was put back to 17 minutes to midnight. There is now far less chance of tens of thousands of bombs devastating our civilization. But there is a growing risk of a few going off in a localized conflict. We are confronted by proliferation of nuclear weapons among more nations – and perhaps even the risk of their use by terrorist groups.\n\nMoreover, the threat of global nuclear catastrophe could be merely in temporary abeyance. During the last century the Soviet Union rose and fell; there were two world wars. In the next hundred years, geopolitical realignments could be just as drastic, leading to a nuclear stand-off between new superpowers, which might be handled less adeptly (or less luckily) than the Cuba crisis, and the other tense moments of the Cold War era. The nuclear threat will always be with us – it is based on fundamental (and public) scientific ideas that date from the 1930s.\n\nDespite the hazards, there are, today, some genuine grounds for being a techno-optimist. For most people in most nations, there has never been a better time to be alive. The innovations that will drive economic advance – information technology, biotechnology and nanotechnology – can boost the developing as well as the developed world. Twenty-first century technologies could offer lifestyles that are environmentally benign – involving lower demands on energy or resources than what we had consider a good life today. And we could readily raise the funds – were there the political will – to lift the world’s two billion most-deprived people from their extreme poverty.\n\nBut, along with these hopes, twenty-first century technology will confront us with new global threats – stemming from bio-, cyber- and environmental-science, as well as from physics – that could be as grave as the bomb. The Bulletin’s clock is now closer to midnight again. These threats may not trigger sudden worldwide catastrophe – the doomsday clock is not such a good metaphor – but they are, in aggregate, disquieting and challenging. The tensions between benign and damaging spin-offs from new technologies, and the threats posed by the Promethean power science, are disquietingly real. Wells’ pessimism might even have deepened further were he writing today.\n\nOne type of threat comes from humanity’s collective actions; we are eroding natural resources, changing the climate, ravaging the biosphere and driving many species to extinction.\n\nClimate change looms as the twenty-first century’s number-one environmental challenge. The most vulnerable people – for instance, in Africa or Bangladesh – are the least able to adapt. Because of the burning of fossil fuels, the CO₂ concentration in the atmosphere is already higher than it has ever been in the last half million years – and it is rising ever faster. The higher CO₂ rises, the greater the warming – and, more important still, the greater will be the chance of triggering something grave and irreversible: rising sea levels due to the melting of Greenland’s icecap and so forth. The global warming induced by the fossil fuels we burn this century could lead to sea level rises that continue for a millennium or more.\n\nThe science of climate change is intricate. But it is simple compared to the economic and political challenge of responding to it. The market failure that leads to global warming poses a unique challenge for two reasons. First, unlike the consequences of more familiar kinds of pollution, the effect is diffuse: the CO₂ emissions from the UK have no more effect here than they do in Australia, and vice versa. That means that any credible framework for mitigation has to be broadly international. Second, the main downsides are not immediate but lie a century or more in the future: inter-generational justice comes into play; how do we rate the rights and interests of future generations compared to our own?\n\nThe solution requires coordinated action by all major nations. It also requires far-sightedness – altruism towards our descendants. History will judge us harshly if we discount too heavily what might happen when our grandchildren grow old. It is deeply worrying that there is no satisfactory fix yet on the horizon that will allow the world to break away from dependence on coal and oil – or else to capture the CO₂ that power stations emit. To quote Al Gore, ‘We must not leap from denial to despair. We can do something and we must.’\n\nThe prognosis is indeed uncertain, but what should weigh most heavily and motivate policy-makers most strongly – is the ‘worst case’ end of the range of predictions: a ‘runaway’ process that would render much of the Earth uninhabitable.\n\nOur global society confronts other ‘threats without enemies’, apart from (although linked with) climate change. High among them is the threat to biological diversity. There have been five great extinctions in the geological past. Humans are now causing a sixth. The extinction rate is 1000 times higher than normal and is increasing. We are destroying the book of life before we have read it. There are probably upwards of 10 million species, most not even recorded – mainly insects, plants and bacteria.\n\nBiodiversity is often proclaimed as a crucial component of human well-being. Manifestly it is: we are clearly harmed if fish stocks dwindle to extinction; there are plants in the rain forest whose gene pool might be useful to us. But for many of us these ‘instrumental’ – and anthropocentric – arguments are not the only compelling ones. Preserving the richness of our biosphere has value in its own right, over and above what it means to us humans.\n\nBut we face another novel set of vulnerabilities. These stem not from our collective impact but from the greater empowerment of individuals or small groups by twenty-first century technology.\n\nThe new techniques of synthetic biology could permit inexpensive synthesis of lethal biological weapons – on purpose, or even by mistake. Not even an organized network would be required: just a fanatic or a weirdo with the mindset of those who now design computer viruses – the mindset of an arsonist. Bio (and cyber) expertise will be accessible to millions. In our networked world, the impact of any runaway disaster could quickly become global.\n\nIndividuals will soon have far greater ‘leverage’ than present-day terrorists possess. Can our interconnected society be safeguarded against error or terror without having to sacrifice its diversity and individualism? This is a stark question, but I think it is a serious one.\n\nWe are kidding ourselves if we think that technical education leads to balanced rationality: it can be combined with fanaticism – not just the traditional fundamentalism that we are so mindful of today, but new age irrationalities too. There are disquieting portents – for instance, the Raelians (who claim to be cloning humans) and the Heavens Gate cult (who committed collective suicide in hopes that a space-ship would take them to a ‘higher sphere’). Such cults claim to be ‘scientific’ but have a precarious foothold in reality. And there are extreme eco-freaks who believe that the world would be better off if it were rid of humans. Can the global village cope with its village idiots – especially when even one could be too many?\n\nThese concerns are not remotely futuristic – we will surely confront them within next 10–20 years. But what of the later decades of this century? It is hard to predict because some technologies could develop with runaway speed. Moreover, human character and physique themselves will soon be malleable, to an extent that is qualitatively new in our history. New drugs (and perhaps even implants into our brains) could change human character; the cyberworld has potential that is both exhilarating and frightening.\n\nWe cannot confidently guess lifestyles, attitudes, social structures or population sizes a century hence. Indeed, it is not even clear how much longer our descendants would remain distinctively ‘human’. Darwin himself noted that ‘not one living species will transmit its unaltered likeness to a distant futurity’. Our own species will surely change and diversify faster than any predecessor – via human-induced modifications (whether intelligently controlled or unintended) not by natural selection alone. The post-human era may be only centuries away. And what about Artificial Intelligence? Superintelligent machine could be the last invention that humans need ever make. We should keep our minds open, or at least ajar, to concepts that seem on the fringe of science fiction.\n\nThese thoughts might seem irrelevant to practical policy – something for speculative academics to discuss in our spare moments. I used to think this. But humans are now, individually and collectively, so greatly empowered by rapidly changing technology that we can – by design or as unintended consequences – engender irreversible global changes. It is surely irresponsible not to ponder what this could mean; and it is real political progress that the challenges stemming from new technologies are higher on the international agenda and that planners seriously address what might happen more than a century hence.\n\nWe cannot reap the benefits of science without accepting some risks – that has always been the case. Every new technology is risky in its pioneering stages. But there is now an important difference from the past. Most of the risks encountered in developing ‘old’ technology were localized: when, in the early days of steam, a boiler exploded, it was horrible, but there was an’upper bound’ to just how horrible. In our evermore interconnected world, however, there are new risks whose consequences could be global. Even a tiny probability of global catastrophe is deeply disquieting.\n\nWe cannot eliminate all threats to our civilization (even to the survival of our entire species). But it is surely incumbent on us to think the unthinkable and study how to apply twenty-first century technology optimally, while minimizing the ‘downsides’. If we apply to catastrophic risks the same prudent analysis that leads us to take everyday safety precautions, and sometimes to buy insurance – multiplying probability by consequences – we had surely conclude that some of the scenarios discussed in this book deserve more attention that they have received.\n\nMy background as a cosmologist, incidentally, offers an extra perspective – an extra motive for concern – with which I will briefly conclude.\n\nThe stupendous time spans of the evolutionary past are now part of common culture – except among some creationists and fundamentalists. But most educated people, even if they are fully aware that our emergence took billions of years, somehow think we humans are the culmination of the evolutionary tree. That is not so. Our Sun is less than half way through its life. It is slowly brightening, but Earth will remain habitable for another billion years. However, even in that cosmic time perspective – extending far into the future as well as into the past – the twenty-first century may be a defining moment. It is the first in our planet’s history where one species – ours – has Earth’s future in its hands and could jeopardise not only itself but also lifes immense potential.\n\nThe decisions that we make, individually and collectively, will determine whether the outcomes of twenty-first century sciences are benign or devastating. We need to contend not only with threats to our environment but also with an entirely novel category of risks – with seemingly low probability, but with such colossal consequences that they merit far more attention than they have hitherto had. That is why we should welcome this fascinating and provocative book. The editors have brought together a distinguished set of authors with formidably wide-ranging expertise. The issues and arguments presented here should attract a wide readership – and deserve special attention from scientists, policy-makers and ethicists.\n\nMartinJ. Rees\n\nContents\n\n    Acknowledgements\n\n    Foreword\n\n    Martin J. Rees\n\n 1 Introduction\n\n    Nick Bostrom and Milan M. Ćirković\n\n    1.1   Why?\n\n    1.2   Taxonomy and organization\n\n    1.3   Part I: Background\n\n    1.4   Part II: Risks from nature\n\n    1.5   Part III: Risks from unintended consequences\n\n    1.6   Part IV: Risks from hostile acts\n\n    1.7   Conclusions and future directions\n\n Part I Background\n\n 2 Long-term astrophysical processes\n\n    Fred C. Adams\n\n    2.1   Introduction: physical eschatology\n\n    2.2   Fate of the Earth\n\n    2.3   Isolation of the local group\n\n    2.4   Collisionwith Andromeda\n\n    2.5   The end of stellar evolution\n\n    2.6   The era of degenerate remnants\n\n    2.7   The era of black holes\n\n    2.8   The Dark Era and beyond\n\n    2.9   Life and information processing\n\n    2.10   Conclusion\n\n           Suggestions for further reading\n\n           References\n\n 3 Evolution theory and the future of humanity\n\n    Christopher Wills\n\n    3.1   Introduction\n\n    3.2   The causes of evolutionary change\n\n    3.3   Environmental changes and evolutionary changes\n\n          3.3.1   Extreme evolutionary changes\n\n          3.3.2   Ongoing evolutionary changes\n\n          3.3.3   Changes in the cultural environment\n\n    3.4   Ongoing human evolution\n\n          3.4.1   Behavioural evolution\n\n          3.4.2   The future of genetic engineering\n\n          3.4.3   The evolution of other species, including those on which we depend\n\n    3.5   Future evolutionary directions\n\n          3.5.1   Drastic and rapid climate change without changes in human behaviour\n\n          3.5.2   Drastic but slower environmental change accompanied by changes in human behaviour\n\n          3.5.3   Colonization of new environments by our species\n\n          Suggestions for further reading\n\n          References\n\n 4 Millennial tendencies in responses to apocalyptic threats\n\n    James J. Hughes\n\n    4.1   Introduction\n\n    4.2   Types of millennialism\n\n          4.2.1   Premillennialism\n\n          4.2.2   Amillennialism\n\n          4.2.3   Post-millennialism\n\n    4.3   Messianism and millenarianism\n\n    4.4   Positive or negative teleologies: utopianism and apocalypticism\n\n    4.5   Contemporary techno-millennialism\n\n          4.5.1   The singularity and techno-millennialism\n\n    4.6   Techno-apocalypticism\n\n    4.7   Symptoms of dysfunctional millennialism in assessing future scenarios\n\n    4.8   Conclusions\n\n          Suggestions for further reading\n\n          References\n\n 5 Cognitive biases potentially affecting judgement of global risks\n\n    Eliezer Yudkowsky\n\n    5.1   Introduction\n\n    5.2   Availability\n\n    5.3   Hindsight bias\n\n    5.4   Black Swans\n\n     5.5   The conjunction fallacy\n\n     5.6   Confirmation bias\n\n     5.7   Anchoring, adjustment, and contamination\n\n     5.8   The affect heuristic\n\n     5.9   Scope neglect\n\n     5.10   Calibration and overconfidence\n\n     5.11   Bystander apathy\n\n     5.12   A final caution\n\n     5.13   Conclusion\n\n          Suggestions for further reading\n\n          References\n\n 6 Observation selection effects and global catastrophic risks\n\n    Milan M. Ćirković\n\n    6.1   Introduction: anthropic reasoning and global risks\n\n    6.2   Past-future asymmetry and risk inferences\n\n          6.2.1   A simplified model\n\n          6.2.2   Anthropic overconfidence bias\n\n          6.2.3   Applicability class of risks\n\n          6.2.4   Additional astrobiological information\n\n    6.3   Doomsday Argument\n\n    6.4   Fermi’s paradox\n\n          6.4.1   Fermi’s paradox and GCRs\n\n          6.4.2   Risks following from the presence of extraterrestrial intelligence\n\n    6.5   The SimulationArgument\n\n    6.6   Making progress in studying observation selection effects\n\n          Suggestions for further reading\n\n          References\n\n 7 Systems-based risk analysis\n\n    Yacov Y. Haimes\n\n    7.1   Introduction\n\n    7.2   Risk to interdependent infrastructure and sectors of the economy\n\n    7.3   Hierarchical holographic modelling and the theory of scenario structuring\n\n          7.3.1   Philosophy and methodology of hierarchical holographic modelling\n\n          7.3.2   The definition of risk\n\n          7.3.3   Historical perspectives\n\n    7.4   Phantom system models for risk management of emergent multi-scale systems\n\n    7.5   Risk of extreme and catastrophic events\n\n          7.5.1   The limitations of the expected value of risk\n\n          7.5.2   The partitioned multi-objective risk method\n\n          7. 5.3 Risk versus reliability analysis\n\n          Suggestions for further reading\n\n          References\n\n 8 Catastrophes and insurance\n\n    Peter Taylor\n\n    8.1   Introduction\n\n    8.2   Catastrophes\n\n    8.3   What the business world thinks\n\n    8.4   Insurance\n\n    8.5   Pricing the risk\n\n    8.6   Catastrophe loss models\n\n    8.7   What is risk?\n\n    8.8   Price and probability\n\n    8.9   The age ofuncertainty\n\n    8.10   New techniques\n\n          8.10.1   Qualitative risk assessment\n\n          8.10.2   Complexity science\n\n          8.10.3   Extreme value statistics\n\n    8.11   Conclusion: against the gods?\n\n           Suggestions for further reading\n\n           References\n\n 9 Public policy towards catastrophe\n\n    Richard A. Posner\n\n    References\n\n Part II Risks from nature\n\n10 Super-volcanism and other geophysical processes of catastrophic import\n\n    Michael R. Rampino\n\n    10.1   Introduction\n\n    10.2   Atmospheric impact of a super-eruption\n\n    10.3   Volcanic winter\n\n    10.4   Possible environmental effects of a super-eruption\n\n    10.5   Super-eruptions and human population\n\n    10.6   Frequency of super-eruptions\n\n    10.7   Effects of a super-eruptions on civilization\n\n    10.8   Super-eruptions and life in the universe\n\n          Suggestions for further reading\n\n          References\n\n11 Hazards from comets and asteroids\n\n    William Napier\n\n    11.1   Something like a huge mountain\n\n    11.2   How oftenare we struck?\n\n        11.2.1   Impact craters\n\n        11.2.2   Near-Earth object searches\n\n        11.2.3   Dynamical analysis\n\n    11.3   The effects of impact\n\n    11.4   The role of dust\n\n    11.5   Ground truth?\n\n    11.6   Uncertainties\n\n          Suggestions for further reading\n\n          References\n\n12 Influence of Supernovae, gamma-ray bursts, solar flares, and cosmic rays on the terrestrial environment\n\n    Arnon Dar\n\n    12.1   Introduction\n\n    12.2   Radiationthreats\n\n        12.2.1   Credible threats\n\n        12.2.2   Solar flares\n\n        12.2.3   Solar activity and global warming\n\n        12.2.4   Solar extinction\n\n        12.2.5   Radiation from supernova explosions\n\n        12.2.6   Gamma-ray bursts\n\n    12.3   Cosmic ray threats\n\n        12.3.1   Earth magnetic field reversals\n\n        12.3.2   Solar activity, cosmic rays, and global warming\n\n        12.3.3   Passage through the Galactic spiral arms\n\n        12.3.4   Cosmic rays from nearby supernovae\n\n        12.3.5   Cosmic rays from gamma-ray bursts\n\n    12.4   Origin of the major mass extinctions\n\n    12.5   The Fermi paradox and mass extinctions\n\n    12.6   Conclusions\n\n          References\n\n Part III Risks from unintended consequences\n\n13 Climate change and global risk\n\n    David Frame and Myles R. Allen\n\n    13.1   Introduction\n\n    13.2   Modelling climate change\n\n    13.3   A simple model of climate change\n\n        13.3.1   Solar forcing\n\n        13.3.2   Volcanic forcing\n\n        13.3.3   Anthropogenic forcing\n\n    13.4   Limits to current knowledge\n\n    13.5   Defining dangerous climate change\n\n    13.6   Regional climate risk under anthropogenic change\n\n    13.7   Climate risk and mitigation policy\n\n    13.8   Discussion and conclusions\n\n          Suggestions for further reading\n\n          References\n\n14 Plagues and pandemics: past, present, and future\n\n    Edwin Dennis Kilbourne\n\n    14.1   Introduction\n\n    14.2   The baseline: the chronic and persisting burden of infectious disease\n\n    14.3   The causation of pandemics\n\n    14.4   The nature and source of the parasites\n\n    14.5   Modes of microbial and viral transmission\n\n    14.6   Nature of the disease impact: high morbidity, high mortality, or both\n\n    14.7   Environmental factors\n\n    14.8   Humanbehaviour\n\n    14.9   Infectious diseases as contributors to other natural catastrophes\n\n    14.10   Past Plagues and pandemics and their impact on history\n\n    14.11   Plagues of historical note\n\n              14.11.1   Bubonic plague: the Black Death\n\n              14.11.2   Cholera\n\n              14.11.3   Malaria\n\n              14.11.4   Smallpox\n\n              14.11.5   Tuberculosis\n\n              14.11.6   Syphilis as a paradigm of sexually transmitted infections\n\n              14.11.7   Influenza\n\n    14.12   Contemporary plagues and pandemics\n\n              14.12.1   HIV/AIDS\n\n              14.12.2   Influenza\n\n              14.12.3   HIV and tuberculosis: the double impact of new and ancient threats\n\n    14.13   Plagues and pandemics of the future\n\n              14.13.1   Microbes that threaten without infection: the microbial toxins\n\n              14.13.2   Iatrogenic diseases\n\n              14.13.3   The homogenization of peoples and cultures\n\n              14.13.4   Man-made viruses\n\n    14.14   Discussion and conclusions\n\n              Suggestions for further reading\n\n              References\n\n15 Artificial Intelligence as a positive and negative factor in global risk\n\n    Eliezer Yudkowsky\n\n    15.1   Introduction\n\n    15.2   Anthropomorphic bias\n\n    15.3   Predictionand design\n\n    15.4   Underestimating the power of intelligence\n\n    15.5   Capability and motive\n\n            15.5.1   Optimization processes\n\n            15.5.2   Aiming at the target\n\n    15.6   Friendly Artificial Intelligence\n\n    15.7   Technical failure and philosophical failure\n\n            15.7.1   An example of philosophical failure\n\n            15.7.2   An example of technical failure\n\n    15.8   Rates of intelligence increase\n\n    15.9   Hardware\n\n    15.10   Threats and promises\n\n    15.11   Local and majoritarian strategies\n\n    15.12   Interactions of Artificial Intelligence with other technologies\n\n    15.13   Making progress on Friendly Artificial Intelligence\n\n    15.14   Conclusion\n\n              References\n\n16 Big troubles, imagined and real\n\n    Frank Wilczek\n\n    16.1   Why look for trouble?\n\n    16.2   Looking before leaping\n\n          16.2.1   Accelerator disasters\n\n          16.2.2   Runaway technologies\n\n    16.3   Preparing to Prepare\n\n    16.4   Wondering\n\n           Suggestions for further reading\n\n           References\n\n17 Catastrophe, social collapse, and human extinction\n\n    Robin Hanson\n\n    17.1   Introduction\n\n    17.2   What is society?\n\n    17.3   Social growth\n\n    17.4   Social collapse\n\n    17.5   The distribution of disaster\n\n    17.6   Existential disasters\n\n    17.7   Disaster policy\n\n    17.8   Conclusion\n\n           References\n\n Part IV Risks from hostile acts\n\n18 The continuing threat of nuclear war\n\n    Joseph Cirincione\n\n    18.1   Introduction\n\n           18.1.1   US nuclear forces\n\n           18.1.2   Russiannuclear forces\n\n    18.2   Calculating Armageddon\n\n           18.2.1   Limited war\n\n           18.2.2   Global war\n\n           18.2.3   Regional war\n\n           18.2.4   Nuclear winter\n\n    18.3   The current nuclear balance\n\n    18.4   The good news about proliferation\n\n    18.5   A comprehensive approach\n\n    18.6   Conclusion\n\n           Suggestions for further reading\n\n19 Catastrophic nuclear terrorism: a preventable peril\n\n    Gary Ackerman and William C. Potter\n\n    19.1   Introduction\n\n    19.2   Historical recognition of the risk of nuclear terrorism\n\n    19.3   Motivations and capabilities for nuclear terrorism\n\n           19.3.1   Motivations: the demand side of nuclear terrorism\n\n           19.3.2   The supply side of nuclear terrorism\n\n    19.4   Probabilities of occurrence\n\n           19.4.1   The demand side: who wants nuclear weapons?\n\n           19.4.2   The supply side: how far have terrorists progressed?\n\n           19.4.3   What is the probability that terrorists will acquire nuclear explosive capabilities in the future?\n\n           19.4.4   Could terrorists precipitate a nuclear holocaust by non-nuclear means?\n\n    19.5   Consequences of nuclear terrorism\n\n           19.5.1   Physical and economic consequences\n\n           19.5.2   Psychological, social, and political consequences\n\n    19.6   Risk assessment and risk reduction\n\n           19.6.1   The risk of global catastrophe\n\n           19.6.2   Risk reduction\n\n    19.7   Recommendations\n\n           19.7.1   Immediate priorities\n\n           19.7.2   Long-term priorities\n\n    19.8   Conclusion\n\n           Suggestions for further reading\n\n           References\n\n20 Biotechnology and biosecurity\n\n    Ali Nouri and Christopher F. Chyba\n\n    20.1   Introduction\n\n    20.2   Biological weapons and risks\n\n    20.3   Biological weapons are distinct from other so-called weapons of mass destruction\n\n    20.4   Benefits come with risks\n\n    20.5   Biotechnology risks go beyond traditional virology, micro- and molecular biology\n\n    20.6   Addressing biotechnology risks\n\n           20.6.1   Oversight of research\n\n           20.6.2   ‘Soft’ oversight\n\n           20.6.3   Multi-stakeholder partnerships for addressing biotechnology risks\n\n           20.6.4   A risk management framework for de novo DNA synthesis technologies\n\n           20.6.5   From voluntary codes of conduct to international regulations\n\n           20.6.6   Biotechnology risks go beyond creating novel pathogens\n\n           20.6.7   Spread of biotechnology may enhance biological security\n\n    20.7   Catastrophic biological attacks\n\n    20.8   Strengthening disease surveillance and response\n\n           20.8.1   Surveillance and detection\n\n           20.8.2   Collaboration and communication are essential for managing outbreaks\n\n           20.8.3   Mobilization of the public health sector\n\n           20.8.4   Containment of the disease outbreak\n\n           20.8.5   Research, vaccines, and drug development are essential components of an effective defence strategy\n\n           20.8.6   Biological security requires fostering collaborations\n\n    20.9   Towards a biologically secure future\n\n           Suggestions for further reading\n\n           References\n\n21 Nanotechnology as global catastrophic risk\n\n    Chris Phoenix and Mike Treder\n\n    21.1   Nanoscale technologies\n\n           21.1.1   Necessary simplicity of products\n\n           21.1.2   Risks associated with nanoscale technologies\n\n    21.2   Molecular manufacturing\n\n           21.2.1   Products of molecular manufacturing\n\n           21.2.2   Nano-built weaponry\n\n           21.2.3   Global catastrophic risks\n\n    21.3   Mitigation of molecular manufacturing risks\n\n    21.4   Discussion and conclusion\n\n           Suggestions for further reading\n\n           References\n\n22 The totalitarian threat\n\n    Bryan Caplan\n\n    22.1   Totalitarianism: what happened and why it (mostly) ended\n\n    22.2   Stable totalitarianism\n\n    22.3   Risk factors for stable totalitarianism\n\n           22.3.1   Technology\n\n           22.3.2   Politics\n\n    22.4   Totalitarian risk management\n\n           22.4.1   Technology\n\n           22.4.2   Politics\n\n    22.5   ‘What’s your p?’\n\n           Suggestions for further reading\n\n           References\n\n Authors’ biographies\n\n Index\n\n• 1 • Introduction\n\nNick Bostrom and Milan M. Ćirković\n\n1.1 Why?\n\nThe term ‘global catastrophic risk’ lacks a sharp definition. We use it to refer, loosely, to a risk that might have the potential to inflict serious damage to human well-being on a global scale. On this definition, an immensely diverse collection of events could constitute global catastrophes: potential candidates range from volcanic eruptions to pandemic infections, nuclear accidents to worldwide tyrannies, out-of-control scientific experiments to climatic changes, and cosmic hazards to economic collapse. With this in mind, one might well ask, what use is a book on global catastrophic risk? The risks under consideration seem to have little in common, so does ‘global catastrophic risk’ even make sense as a topic? Or is the book that you hold in your hands as ill-conceived and unfocused a project as a volume on ‘Gardening, Matrix Algebra, and the History of Byzantium’?\n\nWe are confident that a comprehensive treatment of global catastrophic risk will be at least somewhat more useful and coherent than the above-mentioned imaginary title. We also believe that studying this topic is highly important. Although the risks are of various kinds, they are tied together by many links and commonalities. For example, for many types of destructive events, much of the damage results from second-order impacts on social order; thus the risks of social disruption and collapse are not unrelated to the risks of events such as nuclear terrorism or pandemic disease. Or to take another example, apparently dissimilar events such as large asteroid impacts, volcanic super-eruptions, and nuclear war would all eject massive amounts of soot and aerosols into the atmosphere, with significant effects on global climate. The existence of such causal linkages is one reason why it is can be sensible to study multiple risks together.\n\nAnother commonality is that many methodological, conceptual, and cultural issues crop up across the range of global catastrophic risks. If our interest lies in such issues, it is often illuminating to study how they play out in different contexts. Conversely, some general insights – for example, into the biases of human risk cognition – can be applied to many different risks and used to improve our assessments across the board.\n\nBeyond these theoretical commonalities, there are also pragmatic reasons for addressing global catastrophic risks as a single field. Attention is scarce. Mitigation is costly. To decide how to allocate effort and resources, we must make comparative judgements. If we treat risks singly, and never as part of an overall threat profile, we may become unduly fixated on the one or two dangers that happen to have captured the public or expert imagination of the day, while neglecting other risks that are more severe or more amenable to mitigation. Alternatively, we may fail to see that some precautionary policy, while effective in reducing the particular risk we are focusing on, would at the same time create new hazards and result in an increase in the overall level of risk. A broader view allows us to gain perspective and can thereby help us to set wiser priorities.\n\nThe immediate aim of this book is to offer an introduction to the range of global catastrophic risks facing humanity now or expected in the future, suitable for an educated interdisciplinary readership. There are several constituencies for the knowledge presented. Academics specializing in one of these risk areas will benefit from learning about the other risks. Professionals in insurance, finance, and business – although usually preoccupied with more limited and imminent challenges – will benefit from a wider view. Policy analysts, activists, and laypeople concerned with promoting responsible policies likewise stand to gain from learning about the state of the art in global risk studies. Finally, anyone who is worried or simply curious about what could go wrong in the modern world might find many of the following chapters intriguing. We hope that this volume will serve as a useful introduction to all of these audiences. Each of the chapters ends with some pointers to the literature for those who wish to delve deeper into a particular set of issues.\n\nThis volume also has a wider goal: to stimulate increased research, awareness, and informed public discussion about big risks and mitigation strategies. The existence of an interdisciplinary community of experts and laypeople knowledgeable about global catastrophic risks will, we believe, improve the odds that good solutions will be found and implemented to the great challenges of the twenty-first century.\n\n1.2 Taxonomy and organization\n\nLet us look more closely at what would, and would not, count as a global catastrophic risk. Recall that the damage must be serious, and the scale global. Given this, a catastrophe that caused 10,000 fatalities or 10 billion dollars worth of economic damage (e.g., a major earthquake) would not qualify as a global catastrophe. A catastrophe that caused 10 million fatalities or 10 trillion dollars worth of economic loss (e.g., an influenza pandemic) would count as a global catastrophe, even if some region of the world escaped unscathed. As for disasters falling between these points, the definition is vague. The stipulation of a precise cut-off does not appear needful at this stage.\n\nGlobal catastrophes have occurred many times in history, even if we only count disasters causing more than 10 million deaths. A very partial list of examples might include the An Shi Rebellion (756-763), the Taiping Rebellion (1851-1864), and the famine of the Great Leap Forward in China, the Black Death in Europe, the Spanish flu pandemic, the two world wars, the Nazi genocides, the famines in British India, Stalinist totalitarianism, the decimation of the native American population through smallpox and other diseases following the arrival of European colonizers, probably the Mongol conquests, perhaps Belgian Congo – innumerable others could be added to the list depending on how various misfortunes and chronic conditions are individuated and classified.\n\nWe can roughly characterize the severity of a risk by three variables: its scope (how many people – and other morally relevant beings – would be affected), its intensity (how badly these would be affected), and its probability (how likely the disaster is to occur, according to our best judgement, given currently available evidence). Using the first two of these variables, we can construct a qualitative diagram of different types of risk (Fig. 1.1). (The probability dimension could be displayed along a z-axis were this diagram three-dimensional.)\n\n[Image]\n\nFig. 1.1 Qualitative categories of risk. Global catastrophic risks are in the upper right part of the diagram. Existential risks form an especially severe subset of these.\n\nThe scope of a risk can be personal (affecting only one person), local, global (affecting a large part of the human population), or trans-generational (affecting not only the current world population but all generations that could come to exist in the future). The intensity of a risk can be classified as imperceptible (barely noticeable), endurable (causing significant harm but not destroying quality of life completely), or terminal (causing death or permanently and drastically reducing quality of life). In this taxonomy, global catastrophic risks occupy the four risks classes in the high-severity upper-right corner of the figure: a global catastrophic risk is of either global or trans-generational scope, and of either endurable or terminal intensity. In principle, as suggested in the figure, the axes can be extended to encompass conceptually possible risks that are even more extreme. In particular, trans-generational risks can contain a subclass of risks so destructive that their realization would not only affect or pre-empt future human generations, but would also destroy the potential of our future light cone of the universe to produce intelligent or self-aware beings (labelled ‘Cosmic’). On the other hand, according to many theories of value, there can be states of being that are even worse than non-existence or death (e.g., permanent and extreme forms of slavery or mind control), so it could, in principle, be possible to extend the x-axis to the right as well (see Fig. 1.1 labelled ‘Hellish’).\n\nA subset of global catastrophic risks is existential risks. An existential risk is one that threatens to cause the extinction of Earth-originating intelligent life or to reduce its quality of life (compared to what would otherwise have been possible) permanently and drastically. ¹ Existential risks share a number of features that mark them out as deserving of special consideration. For example, since it is not possible to recover from existential risks, we cannot allow even one existential disaster to happen; there would be no opportunity to learn from experience. Our approach to managing such risks must be proactive. How much worse an existential catastrophe would be than a non-existential global catastrophe depends very sensitively on controversial issues in value theory, in particular how much weight to give to the lives of possible future persons. ² Furthermore, assessing existential risks raises distinctive methodological problems having to do with observation selection effects and the need to avoid anthropic bias. One of the motives for producing this book is to stimulate more serious study of existential risks. Rather than limiting our focus to existential risk, however, we thought it better to lay a broader foundation of systematic thinking about big risks in general.\n\nWe asked our contributors to assess global catastrophic risks not only as they presently exist but also as they might develop over time. The temporal dimension is essential for a full understanding of the nature of the challenges we face. To think about how to tackle the risks from nuclear terrorism and nuclear war, for instance, we must consider not only the probability that something will go wrong within the next year, but also about how the risks will change in the future and the factors – such as the extent of proliferation of relevant technology and fissile materials – that will influence this. Climate change from greenhouse gas emissions poses no significant globally catastrophic risk now or in the immediate future (on the timescale of several decades); the concern is about what effects these accumulating emissions might have over the course of many decades or even centuries. It can also be important to anticipate hypothetical risks which will arise if and when certain possible technological developments take place. The chapters on nanotechnology and artificial intelligence are examples of such prospective risk analysis.\n\nIn some cases, it can be important to study scenarios which are almost certainly physically impossible. The hypothetical risk from particle collider experiments is a case in point. It is very likely that these experiments have no potential, whatever, for causing global disasters. The objective risk is probably zero, as believed by most experts. But just how confident can we be that there is no objective risk? If we are not certain that there is no objective risk, then there is a risk at least in a subjective sense. Such subjective risks can be worthy of serious consideration, and we include them in our definition of global catastrophic risks.\n\nThe distinction between objective and subjective (epistemic) risk is often hard to make out. The possibility of an asteroid colliding with Earth looks like a clear-cut example of objective risk. But suppose that in fact no sizeable asteroid is on collision course with our planet within a certain, sufficiently large interval of time. We might then say that there is no objective risk of an asteroid-caused catastrophe within that interval of time. Of course, we will not know that this is so until we have mapped out the trajectories of all potentially threatening asteroids and are able to calculate all perturbations, often chaotic, of those trajectories. In the meantime, we must recognize a risk from asteroids even though the risk might be purely subjective, merely reflecting our present state of ignorance. An empty cave can be similarly subjectively unsafe if you are unsure about whether a lion resides in it; and it can be rational for you to avoid the cave if you reasonably judge that the expected harm of entry outweighs the expected benefit.\n\nIn the case of the asteroid threat, we have access to plenty of data that can help us quantify the risk. We can estimate the probability of a catastrophic impact from statistics of past impacts (e.g., cratering data) and from observations sampling from the population of non-threatening asteroids. This particular risk, therefore, lends itself to rigorous scientific study, and the probability estimates we derive are fairly strongly constrained by hard evidence. ³\n\nFor many other risks, we lack the data needed for rigorous statistical inference. We may also lack well-corroborated scientific models on which to base probability estimates. For example, there exists no rigorous scientific way of assigning a probability to the risk of a serious terrorist attack employing a biological warfare agent occurring within the next decade. Nor can we firmly establish that the risks of a global totalitarian regime arising before the end of the century are of a certain precise magnitude. It is inevitable that analyses of such risks will rely to a large extent on plausibility arguments, analogies, and subjective judgement.\n\nAlthough more rigorous methods are to be preferred whenever they are available and applicable, it would be misplaced scientism to confine attention to those risks that are amenable to hard approaches. ⁴ Such a strategy would lead to many risks being ignored, including many of the largest risks confronting humanity. It would also create a false dichotomy between two types of risks – the ‘scientific’ ones and the ‘speculative’ ones – where, in reality, there is a continuum of analytic tractability.\n\nWe have, therefore, opted to cast our net widely. Although our topic selection shows some skew towards smaller risks that have been subject to more scientific study, we do have a range of chapters that tackle potentially large but more speculative risks. The page count allocated to a risk should not, of course, be interpreted as a measure of how seriously we believe the risk ought to be regarded. In some cases, we have seen it fit to have a chapter devoted to a risk that turns out to be quite small, because learning that a particular risk is small can be useful, and the procedures used to arrive at the conclusion might serve as a template for future risk research. It goes without saying that the exact composition of a volume like this is also influenced by many contingencies beyond the editors’ control and that perforce it must leave out more than it includes. ⁵\n\nWe have divided the book into four sections:\n\nPart I: Background\n\nPart II: Risks from Nature\n\nPart III: Risks from Unintended Consequences\n\nPart IV: Risks from Hostile Acts\n\nThis subdivision into three categories of risks is for convenience only, and the allocation of a risk to one of these categories is often fairly arbitrary. Take earthquakes which might seem to be paradigmatically a ‘Risk from Nature’. Certainly, an earthquake is a natural event. It would happen even if we were not around. Earthquakes are governed by the forces of plate tectonics over which human beings currently have no control. Nevertheless, the risk posed by an earthquake is, to a very large extent, a matter of human construction. Where we erect our buildings and how we choose to construct them strongly influence what happens when an earthquake of a given magnitude occurs. If we all lived in tents, or in earthquake-proof buildings, or if we placed our cities far from fault lines and sea shores, earthquakes would do little damage. On closer inspection, we thus find that the earthquake risk is very much a joint venture between Nature and Man. Or take a paradigmatically anthropogenic hazard such as nuclear weapons. Again we soon discover that the risk is not as disconnected from uncontrollable forces of nature as might at first appear to be the case. If a nuclear bomb goes off, how much damage it causes will be significantly influenced by the weather. Wind, temperature, and precipitation will affect the fallout pattern and the likelihood that a fire storm will break out: factors that make a big difference to the number of fatalities generated by the blast. In addition, depending on how a risk is defined, it may also over time transition from one category to another. For instance, the risk of starvation might once have been primarily a Risk from Nature, when the main causal factors were draughts or fluctuations in local prey population; yet in the contemporary world, famines tend to be the consequences of market failures, wars, and social breakdowns, whence the risk is now at least as much one of Unintended Consequences or of Hostile Acts.\n\n1.3 Part I: Background\n\nThe objective of this part of the book is to provide general context and methodological guidance for thinking systematically and critically about global catastrophic risks.\n\nWe begin at the end, as it were, with Chapter 2 by Fred Adams discussing the long-term fate of our planet, our galaxy, and the Universe in general. In about 3.5 billion years, the growing luminosity of the sun will essentially have sterilized the Earth’s biosphere, but the end of complex life on Earth is scheduled to come sooner, maybe 0.9-1.5 billon years from now. This is the default fate for life on our planet. One may hope that if humanity and complex technological civilization survives, it will long before then have learned to colonize space.\n\nIf some cataclysmic event were to destroy Homo sapiens and other higher organisms on Earth tomorrow, there does appear to be a window of opportunity of approximately one billion years for another intelligent species to evolve and take over where we left off. For comparison, it took approximately 1.2 billion years from the rise of sexual reproduction and simple multicellular organisms for the biosphere to evolve into its current state, and only a few million years for our species to evolve from its anthropoid ancestors. Of course, there is no guarantee that a rerun of evolution would produce anything like a human or a self-aware successor species.\n\nIf intelligent life does spread into space by harnessing the powers of technology, its lifespan could become extremely long. Yet eventually, the universe will wind down. The last stars will stop shining 100 trillion years from now. Later, matter itself will disintegrate into its basic constituents. By 10¹⁰⁰ years from now even the largest black holes would have evaporated. Our present understanding of what will happen at this time scale and beyond is quite limited. The current best guess – but it is really no more than that – is that it is not just technologically difficult but physically impossible for intelligent information processing to continue beyond some finite time into the future. If so, extinction is not a question of whether, but when.\n\nAfter this peek into the extremely remote future, it is instructive to turn around and take a brief peek at the distant past. Some past cataclysmic events have left traces in the geological record. There have been about 15 mass extinctions in the last 500 million years, and 5 of these eliminated more half of all species then inhabiting the Earth. Of particular note is the Permian – Triassic extinction event, which took place some 251.4 million years ago. This ‘mother of all mass extinctions’ eliminated more than 90% of all species and many entire phylogenetic families. It took upwards of 5 million years for biodiversity to recover.\n\nImpacts from asteroids and comets, as well as massive volcano eruptions, have been implicated in many of the mass extinctions of the past. Other causes, such as variations in the intensity of solar illumination, may in some cases have exacerbated stresses. It appears that all mass extinctions have been mediated by atmospheric effects such as changes the atmosphere’s composition or temperature. It is possible, however, that we owe our existence to mass extinctions. In particular, the comet that hit Earth 65 million years ago, which is believed to have been responsible for the demise of the dinosaurs, might have been a sine qua non for the subsequent rise of Homo sapiens by clearing an ecological niche that could be occupied by large mammals, including our ancestors.\n\nAt least 99.9% of all species that have ever walked, crawled, flown, swum, or otherwise abided on Earth are extinct. Not all of these were eliminated in cataclysmic mass extinction events. Many succumbed in less spectacular doomsdays such as from competition by other species for the same ecological niche. Chapter 3 reviews the mechanisms of evolutionary change. Not so long ago, our own species co-existed with at least one other hominid species, the Neanderthals. It is believed that the lineages of H. sapiens and H. neanderthalensis diverged about 800,000 years ago. The Neanderthals manufactured and used composite tools such as handaxes. They did not reach extinction in Europe until 33,000 to 24,000 years ago, quite likely as a direct result of competition with Homo sapiens. Recently, the remains of what might have been another hominoid species, Homo floresiensis – nicknamed ‘the hobbit’ for its short stature – were discovered on an Indonesian island. H. floresiensis is believed to have survived until as recently as 12,000 years ago, although uncertainty remains about the interpretation of the finds. An important lesson of this chapter is that extinction of intelligent species has already happened on Earth, suggesting that it would be naïve to think it may not happen again.\n\nFrom a naturalistic perspective, there is thus nothing abnormal about global cataclysms including species extinctions, although the characteristic time scales are typically large by human standards. James Hughes in Chapter 4 makes clear, however, the idea of cataclysmic endings often causes a peculiar set of cognitive tendencies to come into play, what he calls ‘the millennial, utopian, or apocalyptic psycho cultural bundle, a characteristic dynamic of eschatological beliefs and behaviours’. The millennial impulse is pancultural. Hughes shows how it can be found in many guises and with many common tropes from Europe to India to China, across the last several thousand years. ‘We may aspire to a purely rational, technocratic analysis’, Hughes writes, ‘calmly balancing the likelihoods of futures without disease, hunger, work or death, on the one hand, against the likelihoods of worlds destroyed by war, plagues or asteroids, but few will be immune to millennial biases, positive or negative, fatalist or messianic’. Although these eschatological tropes can serve legitimate social needs and help to mobilize needed action, they easily become dysfunctional and contribute to social disengagement. Hughes argues that we need historically informed and vigilant self-interrogation to help us keep our focus on constructive efforts to address real challenges.\n\nEven for an honest, truth-seeking, and well-intentioned investigator it is difficult to think and act rationally in regard to global catastrophic risks and existential risks. These are topics on which it seems especially difficult to remain sensible. In Chapter 5, Eliezer Yudkowsky observes as follows:\n\nSubstantially larger numbers, such as 500 million deaths, and especially qualitatively different scenarios such as the extinction of the entire human species, seem to trigger a different mode of thinking – enter into a ‘separate magisterium’. People who would never dream of hurting a child hear of an existential risk, and say, ‘Well, maybe the human species doesn’t really deserve to survive’.\n\nFortunately, if we are ready to contend with our biases, we are not left entirely to our own devices. Over the last few decades, psychologists and economists have developed an extensive empirical literature on many of the common heuristics and biases that can be found in human cognition. Yudkowsky surveys this literature and applies its frequently disturbing findings to the domain of large-scale risks that is the subject matter of this book. His survey reviews the following effects: availability; hindsight bias; black swans; the conjunction fallacy; confirmation bias; anchoring, adjustment, and contamination; the affect heuristic; scope neglect; calibration and overconfidence; and bystander apathy. It behooves any sophisticated contributor in the area of global catastrophic risks and existential risks – whether scientist or policy advisor – to be familiar with each of these effects and we all ought to give some consideration to how they might be distorting our judgements.\n\nAnother kind of reasoning trap to be avoided is anthropic bias. Anthropic bias differs from the general cognitive biases reviewed by Yudkowsky; it is more theoretical in nature and it applies more narrowly to only certain specific kinds of inference. Anthropic bias arises when we overlook relevant observation selection effects. An observation selection effect occurs when our evidence has been ‘filtered’ by the precondition that a suitably positioned observer exists to have the evidence, in such a way that our observations are unrepresentatively sampled from the target domain. Failure to take observation effects into account correctly can result in serious errors in our probabilistic evaluation of some of the relevant hypotheses. Milan Ćirković, in Chapter 6, reviews some applications of observation selection theory that bear on global catastrophic risk and particularly existential risk. Some of these applications are fairly straightforward albeit not always obvious. For example, the tempting inference that certain classes of existential disaster must be highly improbable because they have never occurred in the history of our species or even in the history of life on Earth must be resisted. We are bound to find ourselves in one of those places and belonging to one of those intelligent species which have not yet been destroyed, whether planet or species-destroying disasters are common or rare: for the alternative possibility – that our planet has been destroyed or our species extinguished – is something that is unobservable for us, per definition. Other applications of anthropic reasoning – such as the Carter-Leslie Doomsday argument – are of disputed validity, especially in their generalized forms, but nevertheless worth knowing about. In some applications, such as the simulation argument, surprising constraints are revealed on what we can coherently assume about humanity’s future and our place in the world.\n\nThere are professional communities that deal with risk assessment on a daily basis. The subsequent two chapters present perspectives from the systems engineering discipline and the insurance industry, respectively.\n\nIn Chapter 7, Yacov Haimes outlines some flexible strategies for organizing our thinking about risk variables in complex systems engineering projects. What knowledge is needed to make good risk management decisions? Answering this question, Haimes says, ‘mandates seeking the “truth” about the unknowable complex nature of emergent systems; it requires intellectually bias-free modellers and thinkers who are empowered to experiment with a multitude of modelling and simulation approaches and to collaborate for appropriate solutions’. Haimes argues that organizing the analysis around the measure of the expected value of risk can be too constraining. Decision makers often prefer a more fine-grained decomposition of risk that allows them to consider separately the probability of outcomes in different severity ranges, using what Haimes calls ‘the partitioned multi-objective risk method’.\n\nChapter 8, by Peter Taylor, explores the connections between the insurance industry and global catastrophic risk. Insurance companies help individuals and organizations mitigate the financial consequences of risk, essentially by allowing risks to be traded and shared. Peter Taylor argues that the extent to which global catastrophic risks can be privately insured is severely limited for reasons having to do with both their scope and their type.\n\nAlthough insurance and reinsurance companies have paid relatively scant attention to global catastrophic risks, they have accumulated plenty of experience with smaller risks. Some of the concepts and methods used can be applied to risks at any scale. Taylor highlights the importance of the concept of uncertainty. A particular stochastic model of phenomena in some domain (such as earthquakes) may entail a definite probability distribution over possible outcomes. However, in addition to the chanciness described by the model, we must recognize two further sources of uncertainty. There is usually uncertainty in the values of the parameters that we feed into the model. On top of that, there is uncertainty about whether the model we use does, in fact, correctly describe the phenomena in the target domain. These higher-level uncertainties are often impossible to analyse in a statistically rigorous way. Analysts who strive for objectivity and who are expected to avoid making ‘un-scientific’ assumptions that they cannot justify face a temptation to ignore these subjective uncertainties. But such scientism can lead to disastrous misjudgements. Taylor argues that the distortion is often greatest at the tail end of exceedance probability curves, leading to an underestimation of the risk of extreme events.\n\nTaylor also reports on two recent survey studies of perceived risk. One of these, conducted by Swiss Re in 2005, asked executives of multinationals about which risks to their businesses’ financials were of greatest concern to them. Computer-related risk was rated as the highest priority risk, followed by foreign trade, corporate governance, operational/facility, and liability risk. Natural disasters came in seventh place, and terrorism in tenth place. It appears that, as far as financial threats to individual corporations are concerned, global catastrophic risks take the backseat to more direct and narrowly focused business hazards. A similar exercise, but with broader scope, is carried out annually by the World Economic Forum. Its 2007 Global Risk report classified risks by likelihood and severity based on opinions solicited from business leaders, economists, and academics. Risks were evaluated with a 10-year time frame. Two risks were given a severity rating of ‘more than 1 trillion USD’, namely, asset price collapse (10-20%) and retrenchment from globalization (1–5%). When severity was measured in number of deaths rather than economic losses, the top three risks were pandemics, developing world disease, and interstate and civil war. (Unfortunately, several of the risks in this survey were poorly defined, making it hard to interpret the reported opinions – one moral here being that, if one wishes to assign probabilities to risks or rank them according to severity or likelihood, an essential first step is to present clear definitions of the risks that are to be evaluated. ⁶)\n\nThe Background part of the book ends with a discussion by Richard Posner on some challenges for public policy in Chapter 9. Posner notes that governmental action to reduce global catastrophic risk is often impeded by the short decision horizons of politicians with their limited terms of office and the many competing demands on their attention. Furthermore, mitigation of global catastrophic risks is often costly and can create a free-rider problem. Smaller and poorer nations may drag their heels in the hope of taking a free ride on larger and richer countries. The more resourceful countries, in turn, may hold back because of reluctance to reward the free riders.\n\nPosner also looks at several specific cases, including tsunamis, asteroid impacts, bioterrorism, accelerator experiments, and global warming, and considers some of the implications for public policy posed by these risks. Although rigorous cost-benefit analyses are not always possible, it is nevertheless important to attempt to quantify probabilities, potential harms, and the costs of different possible countermeasures, in order to determine priorities and optimal strategies for mitigation. Posner suggests that when a precise probability of some risk cannot be determined, it can sometimes be informative to consider – as a rough heuristic – the ‘implied probability’ suggested by current expenditures on mitigation efforts compared to the magnitude of harms that would result if a disaster materialized. For example, if we spend one million dollars per year to mitigate a risk which would create 1 billion dollars of damage, we may estimate that current policies implicitly assume that the annual risk of the disaster is of the order of 1/1000. If this implied probability seems too small, it might be a sign that we are not spending enough on mitigation. ⁷ Posner maintains that the world is, indeed, under-investing in mitigation of several global catastrophic risks.\n\n1.4 Part II: Risks from nature\n\nVolcanic eruptions in recent historical times have had measurable effects on global climate, causing global cooling by a few tenths of one degree, the effect lasting perhaps a year. But as Michael Rampino explains in Chapter 10, these eruptions pale in comparison to the largest recorded eruptions. Approximately 75,000 years ago, a volcano erupted in Toba, Indonesia, spewing vast volumes of fine ash and aerosols into the atmosphere, with effects comparable to nuclear-winter scenarios. Land temperatures globally dropped by 5-15°C, and ocean-surface cooling of ≈2-6°C might have extended over several years. The persistence of significant soot in the atmosphere for 1-3 years might have led to a cooling of the climate lasting for decades (because of climate feedbacks such as increased snow cover and sea ice causing more of the sun’s radiation to be reflected back into space). The human population appears to have gone through a bottleneck at this time, according to some estimates dropping as low as ≈500 reproducing females in a world population of approximately 4000 individuals. On the Toba catastrophe theory, the population decline was caused by the super-eruption, and the human species was teetering on the brink of extinction. This is perhaps the worst disaster that has ever befallen the human species, at least if severity is measured by how close to terminal was the outcome.\n\nMore than 20 super-eruption sites for the last 2 million years have been identified. This would suggest that, on average, a super-eruption occurs at least once every 50,000 years. However, there may well have been additional super-eruptions that have not yet been identified in the geological record.\n\nThe global damage from super-volcanism would come chiefly from its climatic effects. The volcanic winter that would follow such an eruption would cause a drop in agricultural productivity which could lead to mass starvation and consequent social upheavals. Rampino’s analysis of the impacts of supervolcanism is also relevant to the risks of nuclear war and asteroid or meteor impacts. Each of these would involve soot and aerosols being injected into the atmosphere, cooling the Earth’s climate.\n\nAlthough we have no way of preventing a super-eruption, there are precautions that we could take to mitigate its impacts. At present, a global stockpile equivalent to a 2-month supply of grain exists. In a super-volcanic catastrophe, growing seasons might be curtailed for several years. A larger stockpile of grain and other foodstuffs, while expensive to maintain, would provide a buffer for a range of catastrophe scenarios involving temporary reductions in world agricultural productivity.\n\nThe hazard from comets and meteors is perhaps the best understood of all global catastrophic risks (which is not to deny that significant uncertainties remain). Chapter 11, by William Napier, explains some of the science behind the impact hazards: where comets and asteroids come from, how frequently impacts occur, and what the effects of an impact would be. To produce a civilization-disrupting event, an impactor would need a diameter of at least 1 or 2 km. A 10-km impactor would, it appears, have a good chance of causing the extinction of the human species. But even sub-kilometre impactors could produce damage reaching the level of global catastrophe, depending on their composition, velocity, angle, and impact site.\n\nNapier estimates that ‘the per capita impact hazard is at the level associated with the hazards of air travel and the like’. However, funding for mitigation is meager compared to funding for air safety. The main effort currently underway to address the impact hazard is the Spaceguard project, which receives about 4 million dollars per annum from NASA besides in-kind and voluntary contributions from others. Spaceguard aims to find 90% of near-Earth asteroids larger than 1 km by the end of 2008. Asteroids constitute the largest portion of the threat from near-Earth objects (and are easier to detect than comets) so when the project is completed, the subjective probability of a large impact will have been reduced considerably – unless, of course, it were discovered that some asteroid has a date with our planet in the near future, in which case the probability would soar.\n\nSome preliminary study has been done of how a potential impactor could be deflected. Given sufficient advance warning, it appears that the space technology needed to divert an asteroid could be developed. The cost of producing an effective asteroid defence would be much greater than the cost of searching for potential impactors. However, if a civilization-destroying wrecking ball were found to be swinging towards the Earth, virtually any expense would be justified to avert it before it struck.\n\nAsteroids and comets are not the only potential global catastrophic threats from space. Other cosmic hazards include global climatic change from fluctuations in solar activity, and very large fluxes from radiation and cosmic rays from supernova explosions or gamma ray bursts. These risks are examined in Chapter 12 by Arnon Dar. The findings on these risks are favourable: the risks appear to be very small. No particular response seems indicated at the present time beyond continuation of basic research. ⁸\n\n1.5 Part III: Risks from unintended consequences\n\nWe have already encountered climate change – in the form of sudden global cooling – as a destructive modality of super-eruptions and large impacts (as well as possible consequence of large-scale nuclear war, to be discussed later). Yet it is the risk of gradual global warming brought about by greenhouse gas emissions that has most strongly captured the public imagination in recent years. Anthropogenic climate change has become the poster child of global threats. Global warming commandeers a disproportionate fraction of the attention given to global risks.\n\nCarbon dioxide and other greenhouse gases are accumulating in the atmosphere, where they are expected to cause a warming of Earth’s climate and a concomitant rise in seawater levels. The most recent report by the United Nations’ Intergovernmental Panel on Climate Change (IPCC), which represents the most authoritative assessment of current scientific opinion, attempts to estimate the increase in global mean temperature that would be expected by the end of this century under the assumption that no efforts at mitigation are made. The final estimate is fraught with uncertainty because of uncertainty about what the default rate of emissions of greenhouse gases will be over the century, uncertainty about the climate sensitivity parameter, and uncertainty about other factors. The IPCC, therefore, expresses its assessment in terms of six different climate scenarios based on different models and different assumptions. The ‘low’ model predicts a mean global warming of +1.8°C (uncertainty range 1.1-2.9°C); the ‘high’ model predicts warming by +4.0°C (2.4-6.4°C). Estimated sea level rise predicted by the two most extreme scenarios of the six considered is 18-38 cm, and 26-59 cm, respectively.\n\nChapter 13, by David Frame and Myles Allen, summarizes some of the basic science behind climate modelling, with particular attention to the lowprobability high-impact scenarios that are most relevant to the focus of this book. It is, arguably, this range of extreme scenarios that gives the greatest cause for concern. Although their likelihood seems very low, considerable uncertainty still pervades our understanding of various possible feedbacks that might be triggered by the expected climate forcing (recalling Peter Taylor’s point, referred to earlier, about the importance of taking parameter and model uncertainty into account). David Frame and Myles Allen also discuss mitigation policy, highlighting the difficulties of setting appropriate mitigation goals given the uncertainties about what levels of cumulative emissions would constitute ‘dangerous anthropogenic interference’ in the climate system.\n\nEdwin Kilbourne reviews some historically important pandemics in Chapter 14, including the distinctive characteristics of their associated pathogens, and discusses the factors that will determine the extent and consequences of future outbreaks.\n\nInfectious disease has exacted an enormous toll of suffering and death on the human species throughout history and continues to do so today. Deaths from infectious disease currently account for approximately 25% of all deaths worldwide. This amounts to approximately 15 million deaths per year. About 75% of these deaths occur in Southeast Asia and sub-Saharan Africa. The top five causes of death due to infectious disease are upper respiratory infection (3.9 million deaths), HIV/AIDS (2.9 million), diarrhoeal disease (1.8 million), tuberculosis (1.7 million), and malaria (1.3 million).\n\nPandemic disease is indisputably one of the biggest global catastrophic risks facing the world today, but it is not always accorded its due recognition. For example, in most people’s mental representation of the world, the influenza pandemic of 1918-1919 is almost completely overshadowed by the concomitant World War I. Yet although the WWI is estimated to have directly caused about 10 million military and 9 million civilian fatalities, the Spanish flu is believed to have killed at least 20-50 million people. The relatively low ‘dread factor’ associated with this pandemic might be partly due to the fact that only approximately 2-3% of those who got sick died from the disease. (The total death count is vast because a large percentage of the world population was infected.)\n\nIn addition to fighting the major infectious diseases currently plaguing the world, it is vital to remain alert to emerging new diseases with pandemic potential, such as SARS, bird flu, and drug-resistant tuberculosis. As the World Health Organization and its network of collaborating laboratories and local governments have demonstrated repeatedly, decisive early action can sometimes nip an emerging pandemic in the bud, possibly saving the lives of millions.\n\nWe have chosen to label pandemics a ‘risk from unintended consequences’ even though most infectious diseases (exempting the potential of genetically engineered bioweapons) in some sense arise from nature. Our rationale is that the evolution as well as the spread of pathogens is highly dependent on human civilization. The worldwide spread of germs became possible only after all the inhabited continents were connected by travel routes. By now, globalization in the form of travel and trade has reached such an extent that a highly contagious disease could spread to virtually all parts of the world within a matter of days or weeks. Kilbourne also draws attention to another aspect of globalization as a factor increasing pandemic risk: homogenization of peoples, practices, and cultures. The more the human population comes to resemble a single homogeneous niche, the greater the potential for a single pathogen to saturate it quickly. Kilbourne mentions the ‘one rotten apple syndrome’, resulting from the mass production of food and behavioural fads:\n\nIf one contaminated item, apple, egg or most recently spinach leaf carries a billion bacteria – not an unreasonable estimate – and it enters a pool of cake mix constituents then packaged and sent to millions of customers nationwide, a bewildering epidemic may ensue.\n\nConversely, cultural as well as genetic diversity reduces the likelihood that any single pattern will be adopted universally before it is discovered to be dangerous – whether the pattern be virus RNA, a dangerous new chemical or material, or a stifling ideology.\n\nBy contrast to pandemics, artificial intelligence (AI) is not an ongoing or imminent global catastrophic risk. Nor is it as uncontroversially a serious cause for concern. However, from a long-term perspective, the development of general artificial intelligence exceeding that of the human brain can be seen as one of the main challenges to the future of humanity (arguably, even as the main challenge). At the same time, the successful deployment of friendly superintelligence could obviate many of the other risks facing humanity. The title of Chapter 15, ‘Artificial intelligence as a positive and negative factor in global risk’, reflects this ambivalent potential.\n\nAs Eliezer Yudkowsky notes, the prospect of superintelligent machines is a difficult topic to analyse and discuss. Appropriately, therefore, he devotes a substantial part of his chapter to clearing common misconceptions and barriers to understanding. Having done so, he proceeds to give an argument for giving serious consideration to the possibility that radical superintelligence could erupt very suddenly – a scenario that is sometimes referred to as the ‘Singularity hypothesis’. Claims about the steepness of the transition must be distinguished from claims about the timing of its onset. One could believe, for example, that it will be a long time before computers are able to match the general reasoning abilities of an average human being, but that once that happens, it will only take a short time for computers to attain radically superhuman levels.\n\nYudkowsky proposes that we conceive of a superintelligence as an enormously powerful optimization process: ‘a system which hits small targets in large search spaces to produce coherent real-world effects’. The superintelligence will be able to manipulate the world (including human beings) in such a way as to achieve its goals, whatever those goals might be. To avert disaster, it would be necessary to ensure that the superintelligence is endowed with a ‘Friendly’ goal system: that is, one that aligns the system’s goals with genuine human values.\n\nGiven this set-up, Yudkowsky identifies two different ways in which we could fail to build Friendliness into our AI: philosophical failure and technical failure. The warning against philosophical failure is basically that we should be careful what we wish for because we might get it. We might designate a target for the AI which at first sight seems like a nice outcome but which in fact is radically misguided or morally worthless. The warning against technical failure is that we might fail to get what we wish for, because of faulty implementation of the goal system or unintended consequences of the way the target representation was specified. Yudkowsky regards both of these possible failure modes as very serious existential risks and concludes that it is imperative that we figure out how to build Friendliness into a superintelligence before we figure out how to build a superintelligence.\n\nChapter 16 discusses the possibility that the experiments that physicists carry out in particle accelerators might pose an existential risk. Concerns about such risks prompted the director of the Brookhaven Relativistic Heavy Ion Collider to commission an official report in 2000. Concerns have since resurfaced with the construction of more powerful accelerators such as CERN’s Large Hadron Collider. Following the Brookhaven report, Frank Wilczek distinguishes three catastrophe scenarios:\n\n1. Formation of tiny black holes that could start accreting surrounding matter, eventually swallowing up the entire planet.\n\n2. Formation of negatively charged stable strangelets which could catalyse the conversion of all the ordinary matter on our planet into strange matter.\n\n3. Initiation of a phase transition of the vacuum state, which would propagate outward in all directions at near light speed and destroy not only our planet but the entire accessible part of the universe.\n\nWilczek argues that these scenarios are exceedingly unlikely on various theoretical grounds. In addition, there is a more general argument that these scenarios are extremely improbable which depends less on arcane theory. Cosmic rays often have energies far greater than those that will be attained in any of the planned accelerators. Such rays have been bombarding the Earth’s atmosphere (and the moon and other astronomical objects) for billions of years without a single catastrophic effect having been observed. Assuming that collisions in particle accelerators do not differ in any unknown relevant respect from those that occur in the wild, we can be very confident in the safety of our accelerators.\n\nBy everyone’s reckoning, it is highly improbable that particle accelerator experiments will cause an existential disaster. The question is how improbable? And what would constitute an ‘acceptable’ probability of an existential disaster? In assessing the probability, we must consider not only how unlikely the outcome seems given our best current models but also the possibility that our best models and calculations might be flawed in some as-yet unrealized way. In doing so we must guard against overconfidence bias (compare Chapter 5 on biases). Unless we ourselves are technically expert, we must also take into account the possibility that the experts on whose judgements we rely might be consciously or unconsciously biased. ⁹ For example, the physicists who possess the expertise needed to assess the risks from particle physics experiments are part of a professional community that has a direct stake in the experiments going forward. A layperson might worry that the incentives faced by the experts could lead them to err on the side of downplaying the risks. ¹⁰ Alternatively, some experts might be tempted by the media attention they could get by playing up the risks. The issue of how much and in which circumstances to trust risk estimates by experts is an important one, and it arises quite generally with regard to many of the risks covered in this book.\n\nChapter 17 (by Robin Hanson) from Part III on Risks from unintended consequences focuses on social collapse as a devastation multiplier of other catastrophes. Hanson writes as follows:\n\nThe main reason to be careful when you walk up a flight of stairs is not that you might slip and have to retrace one step, but rather that the first slip might cause a second slip, and so on until you fall dozens of steps and break your neck. Similarly we are concerned about the sorts of catastrophes explored in this book not only because of their terrible direct effects, but also because they may induce an even more damaging collapse of our economic and social systems.\n\nThis argument does not apply to some of the risks discussed so far, such as those from particle accelerators or the risks from superintelligence as envisaged by Yudkowsky. In those cases, we may be either completely safe or altogether doomed, with little probability of intermediary outcomes. But for many other types of risk – such as windstorms, tornados, earthquakes, floods, forest fires, terrorist attacks, plagues, and wars – a wide range of outcomes are possible, and the potential for social disruption or even social collapse constitutes a major part of the overall hazard. Hanson notes that many of these risks appear to follow a power law distribution. Depending on the characteristic exponent of such a power law distribution, most of the damage expected from a given type of risk may consist either of frequent small disturbances or of rare large catastrophes. Car accidents, for example, have a large exponent, reflecting the fact that most traffic deaths occur in numerous small accidents involving one or two vehicles. Wars and plagues, by contrast, appear to have small exponents, meaning that most of the expected damage occurs in very rare but very large conflicts and pandemics.\n\nAfter giving a thumbnail sketch of economic growth theory, Hanson considers an extreme opposite of economic growth: sudden reduction in productivity brought about by escalating destruction of social capital and coordination. For example, ‘a judge who would not normally consider taking a bribe may do so when his life is at stake, allowing others to expect to get away with theft more easily, which leads still others to avoid making investments that might be stolen, and so on. Also, people may be reluctant to trust bank accounts or even paper money, preventing those institutions from functioning.’ The productivity of the world economy depends both on scale and on many different forms of capital which must be delicately coordinated. We should be concerned that a relatively small disturbance (or combination of disturbances) to some vulnerable part of this system could cause a far-reaching unraveling of the institutions and expectations upon which the global economy depends.\n\nHanson also offers a suggestion for how we might convert some existential risks into non-existential risks. He proposes that we consider the construction of one or more continuously inhabited refuges – located, perhaps, in a deep mineshaft, and well-stocked with supplies – which could preserve a small but sufficient group of people to repopulate a post-apocalyptic world. It would obviously be preferable to prevent altogether catastrophes of a severity that would make humanity’s survival dependent on such modern-day ‘Noah’s arks’; nevertheless, it might be worth exploring whether some variation of this proposal might be a cost-effective way of somewhat decreasing the probability of human extinction from a range of potential causes. ¹¹\n\n1.6 Part IV: Risks from hostile acts\n\nThe spectre of nuclear Armageddon, which so haunted the public imagination during the Cold War era, has apparently entered semi-retirement. The number of nuclear weapons in the world has been reduced to half, from a Cold War high of 65,000 in 1986 to approximately 26,000 in 2007, with approximately 96% of these weapons held by the United States and Russia. Relationships between these two nations are not as bad as they once were. New scares such as environmental problems and terrorism compete effectively for media attention. Changing winds in horror-fashion aside, however, and as Chapter 18 makes it clear, nuclear war remains a very serious threat.\n\nThere are several possibilities. One is that relations between the United States and Russia might again worsen to the point where a crisis could trigger a nuclear war. Future arms races could lead to arsenals even larger than those of the past. The world’s supply of plutonium has been increasing steadily to about 2000 tons – about 10 times as much as remains tied up in warheads – and more could be produced. Some studies suggest that in an all-out war involving most of the weapons in the current US and Russian arsenals, 35-77% of the US population (105-230 million people) and 20-40% of the Russian population (28-56 million people) would be killed. Delayed and indirect effects – such as economic collapse and a possible nuclear winter – could make the final death toll far greater.\n\nAnother possibility is that nuclear war might erupt between nuclear powers other than the old Cold War rivals, a risk that is growing as more nations join the nuclear club, especially nations that are embroiled in volatile regional conflicts, such as India and Pakistan, North Korea, and Israel, perhaps to be joined by Iran or others. One concern is that the more nations get the bomb, the harder it might be to prevent further proliferation. The technology and know-how would become more widely disseminated, lowering the technical barriers, and nations that initially chose to forego nuclear weapons might feel compelled to rethink their decision and to follow suit if they see their neighbours start down the nuclear path.\n\nA third possibility is that global nuclear war could be started by mistake. According to Joseph Cirincione, this almost happened in January 1995:\n\nRussian military officials mistook a Norwegian weather rocket for a US submarine-launched ballistic missile. Boris Yelstin became the first Russian president to ever have the ‘nuclear suitcase’ open in front of him. He had just a few minutes to decide if he should push the button that would launch a barrage of nuclear missiles. Thankfully, he concluded that his radars were in error. The suitcase was closed.\n\nSeveral other incidents have been reported in which the world, allegedly, was teetering on the brink of nuclear holocaust. At one point during the Cuban missile crises, for example, President Kennedy reportedly estimated the probability of a nuclear war between the United States and the USSR to be ‘somewhere between one out of three and even’.\n\nTo reduce the risks, Cirincione argues, we must work to resolve regional conflicts, support and strengthen the Nuclear Non-proliferation Treaty – one of the most successful security pacts in history – and move towards the abolition of nuclear weapons.\n\nWilliam Potter and Gary Ackerman offer a detailed look at the risks of nuclear terrorism in Chapter 19. Such terrorism could take various forms:\n\n• Dispersal of radioactive material by conventional explosives (‘dirty bomb’)\n\n• Sabotage of nuclear facilities\n\n• Acquisition of fissile material leading to the fabrication and detonation of a crude nuclear bomb (’improvised nuclear device’)\n\n• Acquisition and detonation of an intact nuclear weapon\n\n• The use of some means to trick a nuclear state into launching a nuclear strike.\n\nPotter and Ackerman focus on ‘high consequence’ nuclear terrorism, which they construe as those involving the last three alternatives from the above list. The authors analyse the demand and supply side of nuclear terrorism, the consequences of a nuclear terrorist attack, the future shape of the threat, and conclude with policy recommendations.\n\nTo date, no non-state actor is believed to have gained possession of a fission weapon:\n\nThere is no credible evidence that either al Qaeda or Aum Shinrikyo were able to exploit their high motivations, substantial financial resources, demonstrated organizational skills, far-flung network of followers, and relative security in a friendly or tolerant host country to move very far down the path toward acquiring a nuclear weapons capability. As best one can tell from the limited information available in public sources, among the obstacles that proved most difficult for them to overcome was access to the fissile material needed …\n\nDespite this track record, however, many experts remain concerned. Graham Allison, author of one of the most widely cited works on the subject, offers a standing bet of 51 to 49 odds that ‘barring radical new anti-proliferation steps’ there will be a terrorist nuclear strike within the next 10 years. Other experts seem to place the odds much lower, but have apparently not taken up Allison’s offer.\n\nThere is wide recognition of the importance of prevention nuclear terrorism, and in particular of the need to prevent fissile material from falling into the wrong hands. In 2002, the G-8 Global Partnership set a target of 20 billion dollars to be committed over a 10-year period for the purpose of preventing terrorists from acquiring weapons and materials of mass destruction. What Potter and Ackerman consider most lacking, however, is the sustained highlevel leadership needed to transform rhetoric into effective implementation.\n\nIn Chapter 20, Christopher Chyba and Ali Nouri review issues related to biotechnology and biosecurity. While in some ways paralleling nuclear risks – biological as well as nuclear technology can be used to build weapons of mass destruction – there are also important divergences. One difference is that biological weapons can be developed in small, easily concealed facilities and require no unusual raw materials for their manufacture. Another difference is that an infectious biological agent can spread far beyond the site of its original release, potentially across the entire world.\n\nBiosecurity threats fall into several categories, including naturally occurring diseases, illicit state biological weapons programmes, non-state actors and bio-hackers, and laboratory accidents or other inadvertent release of disease agents. It is worth bearing in mind that the number of people who have died in recent years from threats in the first of these categories (naturally occurring diseases) is six or seven orders of magnitudes larger than the number of fatalities from the other three categories combined. Yet biotechnology does contain brewing threats which look set to expand dramatically over the coming years as capabilities advance and proliferate. Consider the following sample of recent developments:\n\n• A group of Australian researchers, looking for ways of controlling the country’s rabbit population, added the gene for interleukin-4 to a mousepox virus, hoping thereby to render the animals sterile. Unexpectedly, the virus inhibited the host’s immune system and all the animals died, including individuals who had previously been vaccinated. Follow-up work by another group produced a version of the virus that was 100% lethal in vaccinated mice despite the antiviral medication given to the animals.\n\n• The polio virus has been synthesized from readily purchased chemical supplies. When this was first done, it required a protracted cutting-edge research project. Since then, the time needed to synthesize a virus genome comparable in size to the polio virus has been reduced to weeks. The virus that caused the Spanish flu pandemic, which was previously extinct, has also been resynthesized and now exists in laboratories in the United States and in Canada.\n\n• The technology to alter the properties of viruses and other microorganisms is advancing at a rapid pace. The recently developed method of RNA interference provides researchers with a ready means of turning off selected genes in humans and other organisms. ‘Synthetic biology’ is being established as new field, whose goal is to enable the creation of small biological devices and ultimately new types of microbes.\n\nReading this list, while bearing in mind that the complete genomes from hundreds of bacteria, fungi, viruses – including Ebola, Marburg, smallpox, and the 1918 Spanish influenza virus – have been sequenced and deposited in a public online database, it is not difficult to concoct in one’s imagination frightening possibilities. The technological barriers to the production of super bugs are being steadily lowered even as the biotechnological know-how and equipment diffuse ever more widely.\n\nThe dual-use nature of the necessary equipment and expertise, and the fact that facilities could be small and easily concealed, pose difficult challenges for would-be regulators. For any regulatory regime to work, it would also have to strike a difficult balance between prevention of abuses and enablement of research needed to develop treatments and diagnostics (or to obtain other medical or economic benefits). Chyba and Nouri discuss several strategies for promoting biosecurity, including automated review of gene sequences submitted for DNA-synthesizing at centralized facilities. It is likely that biosecurity will grow in importance and that a multipronged approach will be needed to address the dangers from designer pathogens.\n\nChris Phoenix and Mike Treder (Chapter 21) discuss nanotechnology as a source of global catastrophic risks. They distinguish between ‘nanoscale technologies’, of which many exist today and many more are in development, and ‘molecular manufacturing’, which remains a hypothetical future technology (often associated with the person who first envisaged it in detail, K. Eric Drexler). Nanoscale technologies, they argue, appear to pose no new global catastrophic risks, although such technologies could in some cases either augment or help mitigate some of the other risks considered in this volume. Phoenix and Treder consequently devote the bulk of their chapter to considering the capabilities and threats from molecular manufacturing. As with superintelligence, the present risk is virtually zero since the technology in question does not yet exist; yet the future risk could be extremely severe.\n\nMolecular nanotechnology would greatly expand control over the structure of matter. Molecular machine systems would enable fast and inexpensive manufacture of microscopic and macroscopic objects built to atomic precision. Such production systems would contain millions of microscopic assembly tools. Working in parallel, these would build objects by adding molecules to a workpiece through positionally controlled chemical reactions. The range of structures that could be built with such technology greatly exceeds that accessible to the biological molecular assemblers (such as ribosome) that exist in nature. Among the things that a nanofactory could build: another nanofactory. A sample of potential applications:\n\n• microscopic nanobots for medical use\n\n• vastly faster computers\n\n• very light and strong diamondoid materials\n\n• new processes for removing pollutants from the environment\n\n• desktop manufacturing plants which can automatically produce a wide range of atomically precise structures from downloadable blueprints\n\n• inexpensive solar collectors\n\n• greatly improved space technology • mass-produced sensors of many kinds\n\n• weapons, both inexpensively mass-produced and improved conventional weapons, and new kinds of weapons that cannot be built without molecular nanotechnology.\n\nA technology this powerful and versatile could be used for an indefinite number of purposes, both benign and malign.\n\nPhoenix and Treder review a number of global catastrophic risks that could arise with such an advanced manufacturing technology, including war, social and economic disruption, destructive forms of global governance, radical intelligence enhancement, environmental degradation, and ‘ecophagy’ (small nanobots replicating uncontrollably in the natural environment, consuming or destroying the Earth’s biosphere). In conclusion, they offer the following rather alarming assessment:\n\nIn the absence of some type of preventive or protective force, the power of molecular manufacturing products could allow a large number of actors of varying types–including individuals, groups, corporations, and nations – to obtain sufficient capability to destroy all unprotected humans. The likelihood of at least one powerful actor being insane is not small. The likelihood that devastating weapons will be built and released accidentally (possibly through overly sensitive automated systems) is also considerable. Finally, the likelihood of a conflict between two [powers capable of unleashing a mutually assured destruction scenario] escalating until one feels compelled to exercise a doomsday option is also non-zero. This indicates that unless adequate defences can be prepared against weapons intended to be ultimately destructive – a point that urgently needs research – the number of actors trying to possess such weapons must be minimized.\n\nThe last chapter of the book, authored by Bryan Caplan, addresses totalitarianism as a global catastrophic risk. The totalitarian governments of Nazi Germany, Soviet Russia, and Maoist China were responsible for tens of millions of deaths in the last century. Compared to a risk like that of asteroid impacts, totalitarianism as a global risk is harder to study in an unbiased manner, and a cross-ideological consensus about how this risk is best to be mitigated is likely to be more elusive. Yet the risks from oppressive forms of government, including totalitarian regimes, must not be ignored. Oppression has been one of the major recurring banes of human development throughout history, it largely remains so today, and it is one to which the humanity remains vulnerable.\n\nAs Caplan notes, in addition to being a misfortune in itself, totalitarianism can also amplify other risks. People in totalitarian regimes are often afraid to publish bad news, and the leadership of such regimes is often insulated from criticism and dissenting views. This can make such regimes more likely to overlook looming dangers and to commit serious policy errors (even as evaluated from the standpoint of the self-interest of the rulers). However, as Caplan notes further, for some types of risk, totalitarian regimes might actually possess an advantage compared to more open and diverse societies. For goals that can be achieved by brute force and massive mobilization of resources, totalitarian methods have often proven effective.\n\nCaplan analyses two factors which he claims have historically limited the durability of totalitarian regimes. The first of these is the problem of succession. A strong leader might maintain a tight grip on power for as long as he lives, but the party faction he represents often stumbles when it comes to appointing a successor that will preserve the status quo, allowing a closet reformer – a sheep in wolf’s clothing – to gain the leadership position after a tyrant’s death. The other factor is the existence of non-totalitarian countries elsewhere in the world. These provide a vivid illustration to the people living under totalitarianism that things could be much better than they are, fuelling dissatisfaction and unrest. To counter this, leaders might curtail contacts with the external world, creating a ‘hermit kingdom’ such as Communist Albania or present-day North Korea. However, some information is bound to leak in. Furthermore, if the isolation is too complete, over a period of time, the country is likely to fall far behind economically and militarily, making itself vulnerable to invasion or externally imposed regime change.\n\nIt is possible that the vulnerability presented by these two Achilles heels of totalitarianism could be reduced by future developments. Technological advances could help solve the problem of succession. Brain scans might one day be used to screen out closet sceptics within the party. Other forms of novel surveillance technologies could also make it easier to control population. New psychiatric drugs might be developed that could increase docility without noticeably reducing productivity. Life-extension medicine might prolong the lifespan of the leader so that the problem of succession comes up less frequently. As for the existence of non-totalitarian outsiders, Caplan worries about the possible emergence of a world government. Such a government, even if it started out democratic, might at some point degenerate into totalitarianism; and a worldwide totalitarian regime could then have great staying power given its lack of external competitors and alien exemplars of the benefits of political freedom.\n\nTo have a productive discussion about matters such as these, it is important to recognize the distinction between two very different stances: ‘here a valid consideration in favour of some position X ’ versus ‘X is all-things-considered the position to be adopted’. For instance, as Caplan notes:\n\nIf people lived forever, stable totalitarianism would be a little more likely to emerge, but it would be madness to force everyone to die of old age in order to avert a small risk of being murdered by the secret police in a thousand years.\n\nLikewise, it is possible to favour the strengthening of certain new forms global governance while also recognizing as a legitimate concern the danger of global totalitarianism to which Caplan draws our attention.\n\n1.7 Conclusions and future directions\n\nThe most likely global catastrophic risks all seem to arise from human activities, especially industrial civilization and advanced technologies. This is not necessarily an indictment of industry or technology, for these factors deserve much of the credit for creating the values that are now at risk – including most of the people living on the planet today, there being perhaps 30 times more of us than could have been sustained with primitive agricultural methods, and hundreds of times more than could have lived as hunter – gatherers. Moreover, although new global catastrophic risks have been created, many smaller-scale risks have been drastically reduced in many parts of the world, thanks to modern technological society. Local and personal disasters – such as starvation, thirst, predation, disease, and small-scale violence – have historically claimed many more lives than have global cataclysms. The reduction of the aggregate of these smaller-scale hazards may outweigh an increase in global catastrophic risks. To the (incomplete) extent that true risk levels are reflected in actuarial statistics, the world is a safer place than it has ever been: world life expectancy is now 64 years, up from 50 in the early twentieth century, 33 in Medieval Britain, and an estimated 18 years during the Bronze Age. Global catastrophic risks are, by definition, the largest in terms of scope but not necessarily in terms of their expected severity (probability × harm). Furthermore, technology and complex social organizations offer many important tools for managing the remaining risks. Nevertheless, it is important to recognize that the biggest global catastrophic risks we face today are not purely external; they are, instead, tightly wound up with the direct and indirect, the foreseen and unforeseen, consequences of our own actions.\n\nOne major current global catastrophic risk is infectious pandemic disease. As noted earlier, infectious disease causes approximately 15 million deaths per year, of which 75% occur in Southeast Asia and sub-Saharan Africa. These dismal statistics pose a challenge to the classification of pandemic disease as a global catastrophic risk. One could argue that infectious disease is not so much a risk as an ongoing global catastrophe. Even on a more fine-grained individuation of the hazard, based on specific infectious agents, at least some of the currently occurring pandemics (such as HIV/AIDS, which causes nearly 3 million deaths annually) would presumably qualify as global catastrophes. By similar reckoning, one could argue that cardiovascular disease (responsible for approximately 30% of world mortality, or 18 million deaths per year) and cancer (8 million deaths) are also ongoing global catastrophes. It would be perverse if the study of possible catastrophes that could occur were to drain attention away form actual catastrophes that are occurring.\n\nIt is also appropriate, at this juncture, to reflect for a moment on the biggest cause of death and disability of all, namely ageing, which accounts for perhaps two-thirds of the 57 million deaths that occur each year, along with an enormous loss of health and human capital. ¹² If ageing were not certain but merely probable, it would immediately shoot to the top of any list of global catastrophic risks. Yet the fact that ageing is not just a possible cause of future death, but a certain cause of present death, should not trick us into trivializing the matter. To the extent that we have a realistic prospect of mitigating the problem – for example, by disseminating information about healthier lifestyles or by investing more heavily in biogerontological research – we may be able to save a much larger expected numbers of lives (or quality-adjusted life-years) by making partial progress on this problem than by completely eliminating some of the global catastrophic risk discussed in this volume.\n\nOther global catastrophic risks which are either already substantial or expected to become substantial within a decade or so include the risks from nuclear war, biotechnology (misused for terrorism or perhaps war), social/economic disruption or collapse scenarios, and maybe nuclear terrorism. Over a somewhat longer time frame, the risks from molecular manufacturing, artificial intelligence, and totalitarianism may rise in prominence, and each of these latter ones is also potentially existential.\n\nThat a particular risk is larger than another does not imply that more resources ought to be devoted to its mitigation. Some risks we might not be able to do anything about. For other risks, the available means of mitigation might be too expensive or too dangerous. Even a small risk can deserve to be tackled as a priority if the solution is sufficiently cheap and easy to implement-one example being the anthropogenic depletion of the ozone layer, a problem now well on its way to being solved. Nevertheless, as a rule of thumb it makes sense to devote most of our attention to the risks that are largest and/or most urgent. A wise person will not spend time installing a burglar alarm when the house is on fire.\n\nGoing forward, we need continuing studies of individual risks, particularly of potentially big but still relatively poorly understood risks, such as those from biotechnology, molecular manufacturing, artificial intelligence, and systemic risks (of which totalitarianism is but one instance). We also need studies to identify and evaluate possible mitigation strategies. For some risks and ongoing disasters, cost-effective countermeasures are already known; in these cases, what is needed is leadership to ensure implementation of the appropriate programmes. In addition, there is a need for studies to clarify methodological problems arising in the study of global catastrophic risks.\n\nThe fruitfulness of further work on global catastrophic risk will, we believe, be enhanced if it gives consideration to the following suggestions:\n\n• In the study of individual risks, focus more on producing actionable information such as early-warning signs, metrics for measuring progress towards risk reduction, and quantitative models for risk assessment.\n\n• Develop and implement better methodologies and institutions for information aggregation and probabilistic forecasting, such as prediction markets.\n\n• Put more effort into developing and evaluating possible mitigation strategies, both because of the direct utility of such research and because a concern with the policy instruments with which a risk can be influenced is likely to enrich our theoretical understanding of the nature of the risk.\n\n• Devote special attention to existential risks and the unique methodological problems they pose.\n\n• Build a stronger interdisciplinary and international risk community, including not only experts from many parts of academia but also professionals and policymakers responsible for implementing risk reduction strategies, in order to break out of disciplinary silos and to reduce the gap between theory and practice.\n\n• Foster a critical discourse aimed at addressing questions of prioritization in a more reflective and analytical manner than is currently done; and consider global catastrophic risks and their mitigation within a broader context of challenges and opportunities for safeguarding and improving the human condition.\n\nOur hopes for this book will have been realized if it adds a brick to the foundation of a way of thinking that enables humanity to approach the global problems of the present era with greater maturity, responsibility, and effectiveness.\n\nPART I Background\n\n• 2 • Long-term astrophysical processes\n\nFred C. Adams\n\n2.1 Introduction: physical eschatology\n\nAs we take a longer-term view of our future, a host of astrophysical processes are waiting to unfold as the Earth, the Sun, the Galaxy, and the Universe grow increasingly older. The basic astronomical parameters that describe our universe have now been measured with compelling precision. Recent observations of the cosmic microwave background radiation show that the spatial geometry of our universe is flat (Spergel et al., 2003). Independent measurements of the red-shift versus distance relation using Type Ia supernovae indicate that the universe is accelerating and apparently contains a substantial component of dark vacuum energy (Garnavich et al., 1998; Perlmutter et al., 1999; Riess et al., 1998). ¹ This newly consolidated cosmologicalmodel represents an important milestone in our understanding of the cosmos. With the cosmological parameters relatively well known, the future evolution of our universe can now be predicted with some degree of confidence (Adams and Laughlin, 1997). Our best astronomical data imply that our universe will expand forever or at least live long enough for a diverse collection of astronomical events to play themselves out.\n\nOther chapters in this book have discussed some sources of cosmic intervention that can affect life on our planet, including asteroid and comet impacts (Chapter 11, this volume) and nearby supernova explosions with their accompanying gamma-rays (Chapter 12, this volume). In the longer-term future, the chances of these types of catastrophic events will increase. In addition, taking an even longer-term view, we find that even more fantastic events could happen in our cosmological future. This chapter outlines some of the astrophysical events that can affect life, on our planet and perhaps elsewhere, over extremely long time scales, including those that vastly exceed the current age of the universe.\n\nThese projections are based on our current understanding of astronomy and the laws of physics, which offer a firm and developing framework for understanding the future of the physical universe (this topic is sometimes called Physical Eschatologgy – see the review of Ćirković, 2003). Notice that as we delve deeper into the future, the uncertainties of our projections must necessarily grow. Notice also that this discussion is based on the assumption that the laws of physics are both known and unchanging; as new physics is discovered, or if the physicalconstants are found to be time dependent, this projection into the future must be revised accordingly.\n\n2.2 Fate of the Earth\n\nOne issue of immediate importance is the fate of Earth’s biosphere and, on even longer time scales, the fate of the planet itself. As the Sun grows older, it burns hydrogen into helium. Compared to hydrogen, helium has a smaller partial pressure for a given temperature, so the central stellar core must grow hotter as the Sun evolves. As a result, the Sun, like all stars, is destined to grow brighter as it ages. When the Sun becomes too bright, it will drive a runaway greenhouse effect through the Earth’s atmosphere (Kasting et al., 1988). This effect is roughly analogous to that of global warming driven by greenhouse gases (see Chapter 13, this volume), a peril that our planet faces in the near future; however, this later-term greenhouse effect will be much more severe. Current estimates indicate that our biosphere will be essentially sterilized in about 3.5 billion years, so this future time marks the end of life on Earth. The end of complex life may come sooner, in 0.9-1.5 billion years owing to the runaway greenhouse effect (e.g., Caldeira and Kasting, 1992).\n\nThe biosphere represents a relatively small surface layer and the planet itself lives comfortably through this time of destruction. Somewhat later in the Sun’s evolution, when its age reaches 11-12 billion years, it eventually depletes its store of hydrogen in the core region and must readjust its structure (Rybicki and Denis, 2001; Sackmann et al., 1993). As it does so, the outer surface of the star becomes somewhat cooler, its colour becomes a brilliant red, and its radius increases. The red giant Sun eventually grows large enough to engulf the radius of the orbit of Mercury, and that innermost planet is swallowed with barely a trace left. The Sun grows further, overtakes the orbit of Venus, and then accretes the second planet as well. As the red giant Sun expands, it loses mass so that surviving planets are held less tightly in their orbits. Earth is able to slip out to an orbit of larger radius and seemingly escape destruction. However, the mass loss from the Sun provides a fluid that the Earth must plough through as it makes its yearly orbit. Current calculations indicate that the frictional forces acting on Earth through its interaction with the solar outflow cause the planet to experience enough orbital decay that it is dragged back into the Sun. Earth is thus evaporated, with its legacy being a small addition to the heavy element supply of the solar photosphere. This point in future history, approximately 7 billion years from now, marks the end of our planet.\n\nGiven that the biosphere has at most only 3.5 billion years left on its schedule, and Earth itself has only 7 billion years, it is interesting to ask what types of ‘planet-saving’ events can take place on comparable time scales. Although the odds are not good, the Earth has some chance of being ‘saved’ by being scattered out of the solar system by a passing star system (most of which are binary stars). These types of scattering interactions pose an interesting problem in solar system dynamics, one that can be addressed with numerical scattering experiments. A large number of such experiments must be run because the systems are chaotic, and hence display sensitive dependence on their initial conditions, and because the available parameter space is large. Nonetheless, after approximately a half million scattering calculations, an answer can be found: the odds of Earth being ejected from the solar system before it is accreted by the red giant Sun is a few parts in 10⁵ (Laughlin and Adams, 2000).\n\nAlthough sending the Earth into exile would save the planet from eventual evaporation, the biosphere would still be destroyed. The oceans would freeze within a few million years and the only pockets of liquid water left would be those deep underground. The Earth contains an internal energy source – the power produced by the radioactive decay of unstable nuclei. This power is about 10,000 times smaller than the power that Earth intercepts from the present-day Sun, so it has little effect on the current operation of the surface biosphere. If Earth were scattered out of the solar system, then this internal power source would be the only one remaining. This power is sufficient to keep the interior of the planet hot enough for water to exist in liquid form, but only at depths 14 km below the surface. This finding, in turn, has implications for present-day astronomy: the most common liquid water environments may be those deep within frozen planets, that is, those that have frozen water on their surfaces and harbour oceans of liquid water below. Such planets may be more common than those that have water on their surface, like Earth, because they can be found in a much wider range of orbits about their central stars (Laughlin and Adams, 2000).\n\nIn addition to saving the Earth by scattering it out of the solar system, passing binaries can also capture the Earth and thereby allow it to orbit about a new star. Since most stars are smaller in mass than our Sun, they live longer and suffer less extreme red giant phases. (In fact, the smallest stars with less than one-fourth of the mass of the Sun will never become red giants – Laughlin et al., 1997.) As a result, a captured Earth would stand a better chance of long-term survival. The odds for this type of planet-saving event taking place while the biosphere remains intact are exceedingly slim – only about one in three million (Laughlin and Adams, 2000), roughly the odds of winning a big state lottery.\n\nFor completeness, we note that in addition to the purely natural processes discussed here, human or other intentional intervention could potentially change the course of Earth’s orbit given enough time and other resources. As a concrete example, one could steer an asteroid into the proper orbit so that gravitational scattering effectively transfers energy into the Earth’s orbit, thereby allowing it to move outward as the Sun grows brighter (Korycansky et al., 2001). In this scenario, the orbit of the asteroid is chosen to encounter both Jupiter and Saturn, and thereby regain the energy and angular momentum that it transfers to Earth. Many other scenarios are possible, but the rest of this chapter will focus on physical phenomena not including intentional actions.\n\n2.3 Isolation of the local group\n\nBecause the expansion rate of the universe is starting to accelerate (Garnavich et al., 1998; Perlmutter et al., 1999; Riess et al., 1998), the formation of galaxies, clusters, and larger cosmic structures is essentially complete. The universe is currently approaching a state of exponential expansion and growing cosmological fluctuations will freeze out on all scales. Existing structures will grow isolated. Numerical simulations illustrate this trend (Fig. 2.1) and show how the universe will break up into a collection of ‘island universes’, each containing one bound cluster or group of galaxies (Busha et al., 2003; Nagamine and Loeb, 2003). In other words, the largest gravitationally bound structures that we see in the universe today are likely to be the largest structures that ever form. Not only must each group of galaxies (eventually) evolve in physical isolation, but the relentless cosmic expansion will stretch existing galaxy clusters out of each others’ view. In the future, one will not even be able to see the light from galaxies living in other clusters. In the case of the Milky Way, only the Local Group of Galaxies will be visible. Current observations and recent numerical studies clearly indicate that the nearest large cluster-Virgo – does not have enough mass for the Local Group to remain bound to it in the future (Busha et al., 2003; Nagamine and Loeb, 2003). This local group consists of the Milky Way, Andromeda, and a couple of dozen dwarf galaxies (irregulars and spheroidals). The rest of the universe will be cloaked behind a cosmological horizon and hence will be inaccessible to future observation.\n\n[Image]\n\nFig. 2.1 Numerical simulation of structure formation in an accelerating universe with dark vacuum energy. The top panel shows a portion of the universe at the present time (cosmic age 14 Gyr). The boxed region in the upper panel expands to become the picture in the central panel at cosmic age 54 Gyr. The box in the central panel then expands to become the picture shown in the bottom panel at cosmic age 92 Gyr. At this future epoch, the galaxy shown in the centre of the bottom panel has grown effectively isolated. (Simulations reprinted with permission from Busha, M.T., Adams, F.C., Evrard, A.E., and Wechsler, R.H. (2003). Future evolution of cosmic structure in an accelerating universe. Astrophys. J., 596, 713.)\n\n2.4 Collision with Andromeda\n\nWithin their clusters, galaxies of ten pass near each other and distort each other’s structure with their strong gravitational fields. Sometimes these interactions lead to galactic collisions and merging. A rather important example of such a collision is coming up: the nearby Andromeda galaxy is headed straight for our Milky Way. Although this date with our sister galaxy will not take place for another 6 billion years or more, our fate is sealed – the two galaxies are a bound pair and will eventually merge into one (Peebles, 1994).\n\nWhen viewed from the outside, galactic collisions are dramatic and result in the destruction of the well-defined spiral structure that characterizes the original galaxies. When viewed from within the galaxy, however, galactic collisions are considerably less spectacular. The spaces between stars are so vast that few, if any, stellar collisions take place. One result is the gradual brightening of the night sky, by roughly a factor of 2. On the other hand, galactic collisions are frequently associated with powerful bursts of star formation. Large clouds of molecular gas within the galaxies merge during such collisions and produce new stars at prodigious rates. The multiple supernovae resulting from the deaths of the most massive stars can have catastrophic consequences and represent a significant risk to any nearby biosphere (see Chapter 12, this volume), provided that life continues to thrive in thin spherical layers on terrestrial planets.\n\n2.5 The end of stellar evolution\n\nWith its current age of 14 billion years, the universe now lives in the midst of a Stelliferous Era, an epoch when stars are actively forming, living, and dying. Most of the energy generated in our universe today arises from nuclear fusion that takes place in the cores of ordinary stars. As the future unfolds, the most common stars in the universe – the low-mass stars known as red dwarfs – play an increasingly important role. Although red dwarf stars have less than half the mass of the Sun, they are so numerous that their combined mass easily dominates the stellar mass budget of the galaxy. These red dwarfs are parsimonious when it comes to fusing their hydrogen into helium. By hoarding their energy resources, they will still be shining trillions of years from now, long after their larger brethren have exhausted their fuel and evolved into white dwarfs or exploded as supernovae. It has been known for a long time that smaller stars live much longer than more massive ones owing to their much smaller luminosities. However, recent calculations show that red dwarfs live even longer than expected. In these small stars, convection currents cycle essentially all of the hydrogen fuel in the star through the stellar core, where it can be used as nuclear fuel. In contrast, our Sun has access to only about 10% of its hydrogen and will burn only 10% of its nuclear fuel while on the main sequence. A small star with 10% of the mass of the Sun thus has nearly the same fuel reserves and will shine for tens of trillions of years (Laughlin et al., 1997). Like all stars, red dwarfs get brighter as they age. Owing to their large population, the brightening of red dwarfs nearly compensates for the loss of larger stars, and the galaxy can maintain a nearly constant luminosity for approximately one trillion years (Adams et al., 2004).\n\nEven small stars cannot live forever, and this bright stellar era comes to a close when the galaxies run out of hydrogen gas, star formation ceases, and the longest-lived red dwarfs slowly fade into oblivion. As mentioned earlier, the smallest stars will shine for trillions of years, so the era of stars would come to an end at a cosmic age of several trillion years if new stars were not being manufactured. In large spiral galaxies like the Milky Way, new stars are being made from hydrogen gas, which represents the basic raw material for the process. Galaxies will continue to make new stars as long as the gas supply holds out. If our Galaxy were to continue forming stars at its current rate, it would run out of gas in ‘only’ 10–20 billion years (Kennicutt et al., 1994), much shorter than the lifetime of the smallest stars. Through conservation practices-the star formation rate decreases as the gas supply grows smaller – galaxies can sustain normal star formation for almost the lifetime of the longest-lived stars (Adams and Laughlin, 1997; Kennicutt et al., 1994). Thus, both stellar evolution and star formation will come to an end at approximately the same time in our cosmic future. The universe will be about 100 trillion (10¹⁴) years old when the stars finally stop shining. Although our Sun will have long since burned out, this time marks an important turning point for any surviving biospheres – the power available is markedly reduced after the stars turn off.\n\n2.6 The era of degenerate remnants\n\nAfter the stars burn out and star formation shuts down, a significant fraction of the ordinary mass will be bound within the degenerate remnants that remain after stellar evolution has run its course. For completeness, however, one should keep in mind that the majority of the baryonic matter will remain in the form of hot gas between galaxies in large clusters (Nagamine and Loeb, 2004). At this future time, the inventory of degenerate objects includes brown dwarfs, white dwarfs, and neutron stars. In this context, degeneracy refers to the state of the high-density material locked up in the stellar remnants. At such enormous densities, the quantum mechanical exclusion principle determines the pressure forces that hold up the stars. For example, when most stars die, their cores shrink to roughly the radial size of Earth. With this size, the density of stellar material is about one million times greater than that of the Sun, and the pressure produced by degenerate electrons holds up the star against further collapse. Such objects are white dwarfs and they will contain most of the mass in stellar bodies at this epoch. Some additional mass is contained in brown dwarfs, which are essentially failed stars that never fuse hydrogen, again owing to the effects of degeneracy pressure. The largest stars, those that begin with masses more than eight times that of the Sun, explode at the end of their lives as supernovae. After the explosion, the stellar cores are compressed to densities about one quadrillion times that of the Sun. The resulting stellar body is a neutron star, which is held up by the degeneracy pressure of its constituent neutrons (at such enormous densities, typically afew x 10¹⁵ g/cm³, electrons and protons combine to form neutrons, which make the star much like a gigantic atomic nucleus). Since only three or four out of every thousand stars are massive enough to produce a supernova explosion, neutron stars will be rare objects.\n\nDuring this Degenerate Era, the universe will look markedly different from the way it appears now. No visible radiation from ordinary stars will light up the skies, warm the planets, or endow the galaxies with the faint glow they have today. The cosmos will be darker, colder, and more desolate. Against this stark backdrop, events of astronomical interest will slowly take place. As dead stars trace through their orbits, close encounters lead to scattering events, which force the galaxy to gradually readjust its structure. Some stellar remnants are ejected beyond the reaches of the galaxy, whereas others fall in toward the centre. Over the next 10²⁰ years, these interactions will enforce the dynamical destruction of the entire galaxy (e.g., Binney and Tremaine, 1987; Dyson, 1979).\n\nIn the meantime, brown dwarfs will collide and merge to create new low-mass stars. Stellar collisions are rare because the galaxy is relentlessly empty. During this future epoch, however, the universe will be old enough so that some collisions will occur, and the merger products will often be massive enough to sustain hydrogen fusion. The resulting low-mass stars will then burn for trillions of years. At any given time, a galaxy the size of our Milky Way will harbour a few stars formed through this unconventional channel (compare this stellar population with the ~100 billion stars in the Galaxy today).\n\nAlong with the brown dwarfs, white dwarfs will also collide at roughly the same rate. Most of the time, such collisions will result in somewhat larger white dwarfs. More rarely, white dwarf collisions produce a merger product with a mass greater than the Chandrasekhar limit. These objects will result in a supernova explosion, which will provide spectacular pyrotechnics against the dark background of the future galaxy.\n\nWhite dwarfs will contain much of the ordinary baryonic matter in this future era. In addition, these white dwarfs will slowly accumulate weakly interacting dark matter particles that orbit the galaxy in an enormous diffuse halo. Once trapped within the interior of a white dwarf, the particles annihilate with each other and provide an important source of energy for the cosmos. Dark matter annihilation will replace conventional nuclear burning in stars as the dominant energy source. The power produced by this process is much lower than that produced by nuclear burning in conventional stars. White dwarfs fuelled by dark matter annihilation produce power ratings measured in quadrillions of Watts, roughly comparable to the total solar power intercepted by Earth (~10¹⁷ Watts). Eventually, however, white dwarfs will be ejected from the galaxy, the supply of the dark matter will get depleted, and this method of energy generation must come to an end.\n\nAlthough the proton lifetime remains uncertain, elementary physical considerations suggest that protons will not live forever. Current experiments show that the proton lifetime is longer than about 10³³ years (Super-Kamiokande Collaboration, 1999), and theoretical arguments (Adams et al., 1998; Ellis et al., 1983; Hawking et al., 1979; Page, 1980; Zeldovich, 1976) suggest that the proton lifetime should be less than about 10⁴⁵ years. Although this allowed range of time scales is rather large, the mass-energy stored within white dwarfs and other degenerate remnants will eventually evaporate when their constituent protons and neutrons decay. As protons decay inside a white dwarf, the star generates power at a rate that depends on the proton lifetime. For a value near the centre of the (large) range of allowed time scales (specifically 10³⁷ years), proton decay within a white dwarf generates approximately 400 Watts of power – enough to run a few light bulbs. An entire galaxy of these stars will appear dimmer than our present-day Sun. The process of proton decay converts the mass energy of the particles into radiation, so the white dwarfs evaporate away. As the proton decay process grinds to completion, perhaps 10⁴⁰ years from now, all of the degenerate stellar remnants disappear from the universe. This milestone marks a definitive end to life as we know it, as no carbon-based life can survive the cosmic catastrophe induced by proton decay. Nonetheless, the universe continues to exist, and astrophysical processes continue beyond this end of known biology.\n\n2.7 The era of black holes\n\nAfter the protons decay, the universe grows even darker and more rarefied. At this late time, roughly when the universe is older than 10⁴⁵ years, the only stellar-like objects remaining are black holes. They are unaffected by proton decay and slide unscathed through the end of the previous era. These objects are often defined to be regions of space-time with such strong gravitational fields that even light cannot escape from their surfaces. But at this late epoch, black holes will be the brightest objects in the sky. Thus, even black holes cannot last forever. They shine ever so faintly by emitting a nearly thermal spectrum of photons, gravitons, and other particles (Hawking, 1974). Through this quantum mechanical process – known as Hawking radiation – black holes convert their mass into radiation and evaporate at a glacial pace (Fig. 2.2). In the far future, black holes will provide the universe with its primary source of power.\n\nAlthough their energy production via Hawking radiation will not become important for a long time, the production of black holes, and hence the black hole inventory of the future, is set by present-day (and past) astrophysical processes. Every large galaxy can produce millions of stellar black holes, which result from the death of the most massive stars. Once formed, these black holes will endure for up to 10⁷⁰ years. In addition, almost every galaxy harbours a super-massive black hole anchoring its centre; these monsters were produced during the process of galaxy formation, when the universe was only a billion years old, or perhaps even younger. They gain additional mass with time and provide the present-day universe with accretion power. As these large black holes evaporate through the Hawking process, they can last up to 10¹⁰⁰ years. But even the largest black holes must ultimately evaporate. This Black Hole Era will be over when the largest black holes have made their explosive exits from our universe.\n\n[Image]\n\nFig. 2.2 This plot shows the long-term evolution of cold degenerate stars in the H-R diagram. After completing the early stages of stellar evolution, white dwarfs and neutron stars cool to an equilibrium temperature determined by proton decay. This figure assumes that proton decay is driven by gravity (microscopic black holes) on a time scale of 10⁴⁵ years. The white dwarf models are plotted at successive twofold decrements in mass. The mean stellar density (in log[p/g]) is indicated by the grey scale shading, and the sizes of the circles are proportional to stellar radius. The relative size of the Earth and its position on the diagram are shown for comparison. The evaporation of a neutron star, starting with one solar mass, is illustrated by the parallel sequence, which shows the apparent radial sizes greatly magnified for clarity. The Hawking radiation sequence for black holes is also plotted. The arrows indicate the direction of time evolution. (Reprinted with permission from Adams, F. C., Laughlin, G., Mbonye, M., and Perry, M.J. (1998). Gravitational demise of cold degenerate stars. Phys. Rev. D, 58, 083003.)\n\n2.8 The Dark Era and beyond\n\nWhen the cosmic age exceeds 10¹⁰⁰ years, the black holes will be gone and the cosmos will be filled with the leftover waste products from previous eras: neutrinos, electrons, positrons, dark matter particles, and photons of incredible wavelength. In this cold and distant Dark Era, physical activity in the universe slows down, almost (but not quite) to a standstill. The available energy is limited and the expanses of time are staggering, but the universe doggedly continues to operate. Chance encounters between electrons and positrons can forge positronium atoms, which are exceedingly rare in an accelerating universe. In addition, such atoms are unstable and eventually decay. Other low-level annihilation events also take place, for example, between any surviving dark matter particles. In the poverty of this distant epoch, the generation of energy and entropy becomes increasingly difficult.\n\nAt this point in the far future, predictions of the physical universe begin to lose focus. If we adopt a greater tolerance for speculation, however, a number of possible events can be considered. One of the most significant potential events is that the vacuum state of the universe could experience a phase transition to a lower energy state. Our present-day universe is observed to be accelerating, and one possible implication of this behaviour is that empty space has a non-zero energy associated with it. In other words, empty space is not really empty, but rather contains a positive value of vacuum energy. If empty space is allowed to have a non-zero energy (allowed by current theories of particle physics), then it remains possible for empty space to have two (or more) different accessible energy levels. In this latter case, the universe could make a transition from its current (high energy) vacuum state to a lower-energy state sometime in the future (the possibility of inducing such a phase transition is discussed in Chapter 16). As the universe grows increasingly older, the probability of a spontaneous transition grows as well. Unfortunately, our current understanding of the vacuum state of the universe is insufficient to make a clear predictions on this issue – the time scale for the transition remains enormously uncertain. Nonetheless, such a phase transition remains an intriguing possibility. If the universe were to experience a vacuum phase transition, it remains possible (but is not guaranteed) that specific aspects of the laws of physics (e.g., the masses of the particles and/or the strengths of the forces) could change, thereby giving the universe a chance for a fresh start.\n\n2.9 Life and information processing\n\nThe discussion in this chapter has focused on physical processes that can take place in the far future. But what about life? How far into the future can living organisms survive? Although this question is of fundamental importance and holds enormous interest, our current understanding of biology is not sufficiently well developed to provide a clear answer. To further complicate matters, protons must eventually decay, as outlined above, so that carbon-based life will come to a definitive end. Nonetheless, some basic principles can be discussed if we are willing to take a generalized view of life, where we consider life to be essentially a matter of information processing. This point of view has been pioneered by Freeman Dyson (1979), who argued that the rate of metabolism or information processing in a generalized life form should be proportional to its operating temperature.\n\nIf our universe is accelerating, as current observations indicate, then the amount of matter and hence energy accessible to a given universe will be finite. If the operating temperature of life remains constant, then this finite free energy would eventually be used up and life would come to an end. The only chance for continued survival is to make the operating temperature of life decrease. More specifically, the temperature must decrease fast enough to allow for an infinite amount of information processing with a finite amount of free energy.\n\nAccording to the Dyson scaling hypothesis, as the temperature decreases, the rate of information processing decreases, and the quality of life decreases accordingly. Various strategies to deal with this problem have been discussed, including the issue of digital versus analogous life, maintaining long-term survival by long dormant periods (hibernation), and the question of classical versus quantum mechanical information processing (e.g., Dyson, 1979; Krauss and Starkman, 2000). Although a definitive conclusion has not been reached, the prospects are rather bleak for the continued (infinite) survival of life. The largest hurdle seems to be continued cosmic acceleration, which acts to limit the supply of free energy. If the current acceleration comes to an end, so that the future universe expands more slowly, then life will have a better chance for long-term survival.\n\n2.10 Conclusion\n\nAs framed by a well-known poem by Robert Frost, the world could end either in fire or in ice. In the astronomical context considered here, Earth has only a small chance of escaping the fiery wrath of the red giant Sun by becoming dislodged from its orbit and thrown out into the icy desolation of deep space. Our particular world is thus likely to end its life in fire. Given that humanity has a few billion years to anticipate this eventuality, one can hope that migration into space could occur, provided that the existential disasters outlined in other chapters of this book can be avoided. One alternative is for a passing star to wander near the inner portion of our solar system. In this unlikely event, the disruptive gravitational effects of the close encounter could force Earth to abandon its orbit and be exiled from the solar system. In this case, our world would avoid a scalding demise, but would face a frozen future.\n\nA similar fate lies in store for the Sun, the Galaxy, and the Universe. At the end of its life as an ordinary star, the Sun is scheduled to become a white dwarf. This stellar remnant will grow increasingly cold and its nuclei will atrophy to lower atomic numbers as the constituent protons decay. In the long run, the Sun will end up as a small block of hydrogen ice. As it faces its demise, our Galaxy will gradually evaporate, scattering its stellar bodies far and wide. The effective temperature of a stellar system is given by the energies of its stellar orbits. In the long term, these energies will fade to zero and the galaxy will end its life in a cold state. For the universe as a whole, the future is equally bleak, but far more drawn out. The currently available astronomical data indicate that the universe will expand forever, or at least for long enough that the timeline outlined above can play itself out. As a result, the cosmos, considered as a whole, is likely to grow ever colder and face an icy death.\n\nIn the beginning, starting roughly 14 billion years ago, the early universe consisted of elementary particles and radiation – essentially because the background was too hot for larger structures to exist. Here we find that the universe of the far future will also consist of elementary particles and radiation – in this case because the cosmos will be too old for larger entities to remain intact. From this grand perspective, the galaxies, stars, and planets that populate the universe today are but transient phenomena, destined to fade into the shifting sands of time. Stellar remnants, including the seemingly resilient black holes, are also scheduled to decay. Even particles as fundamental as protons will not last forever. Ashes to ashes, dust to dust, particles to particles-such is the ultimate fate of our universe.\n\nSuggestions for further reading\n\nAdams, F.C. and Laughlin, G. (1997). A dying universe: the long term fate and evolution of astrophysical objects. Rev. Mod. Phys., 69, pp. 337–372. This review outlines the physics of the long-term future of the universe and its constituent astrophysical objects (advanced level).\n\nAdams, F.C. and Laughlin, G. (1999). Five Ages of the Universe: Inside the Physics of Eternity (New York: The Free Press). Provides a popular level account of the future history of the universe.\n\nĆirković, M.M. (2003). Resource letter Pes-1: physical eschatology. Am. J. Phys., 71, pp. 122–133. This paper provides a comprehensive overview of the scientific literature concerning the future of the universe (as of 2003). The treatment is broad and also includes books, popular treatments, and philosophical accounts (advanced level).\n\nDyson, F.J. (1979). Time without end: physics and biology in an open universe. Rev. Mod. Phys., 51, pp. 447–460. This review represents one of the first comprehensive treatments of the future of the universe and includes discussion of the future of both communication and biology (advanced level).\n\nIslam, J.N. (1983). The Ultimate Fate of the Universe (Cambridge: Cambridge University Press). This book provides one of the first popular level accounts of the future of the universe and raises for the first time many subsequently discussed questions.\n\nRees, M. (1997). Before the Beginning: Our Universe and Others (Reading, MA: Addision-Wesley). This book provides a popular level treatment of the birth of the universe and hence the starting point for discussions of our cosmic future.\n\nReferences\n\nAdams, F.C. and Laughlin, G. (1997). A dying universe: the long term fate and evolution of astrophysical objects. Rev. Mod. Phys., 69, 337–372.\n\nAdams, F.C., Laughlin, G., and Graves, G.J.M. (2004). Red dwarfs and the end of the main sequence. Rev. Mexican Astron. Astrophys., 22, 46–49.\n\nAdams, F.C., Laughlin, G., Mbonye, M., and Perry, M.J. (1998). Gravitational demise of cold degenerate stars. Phys. Rev. D, 58, 083003 (7 pages).\n\nBinney, J. and Tremaine, S. (1987). Galactic Dynamics (Princeton, NJ: Princeton University Press).\n\nBusha, M.T., Adams, F.C., Evrard, A.E., and Wechsler, R.H. (2003). Future evolution of cosmic structure in an accelerating universe. Astrophys. J., 596, 713–724.\n\nCaldeira, K. and Kasting, J.F. (1992). The life span of the biosphere revisited. Nature, 360, 721–723.\n\nĆirković, M.M. (2003). Resource letter PEs-1: physical eschatology. Am. J. Phys., 71, 122–133.\n\nDyson, F.J. (1979). Time without end: physics and biology in an open universe. Rev. Mod. Phys., 51, 447–460.\n\nEllis, J., Hagelin, J.S., Nanopoulos, D.V., and Tamvakis, K. (1983). Observable gravitationally induced baryon decay. Phys. Lett., B124, 484–490.\n\nGarnavich, P.M., Jha, S., Challis, P., Clocchiatti, A., Diercks, A., Filippenko, A.V., Gilliland, R.L., Hogan, C.J., Kirshner, R.P., Leibundgut, B., Phillips, M.M., Reiss, D., Riess, A.G., Schmidt, B.P., Schommer, R.A., Smith, R.C., Spyromilio, J., Stubbs, C., Suntzeff, N.B., Tonry, J., and Carroll, S.M. (1998). Supernova limits on the cosmic equation of state. Astrophys. J., 509, 74–79.\n\nHawking, S.W. (1974). Black hole explosions? Nature, 248, 30–31.\n\nHawking, S.W., Page, D.N., and Pope, C.N. (1979). The propagation of particles in space-time foam. Phys. Lett., B86, 175–178.\n\nKasting, J.F. (1988). Runaway and moist greenhouse atmospheres and the evolution of Earth and Venus. Icarus, 74, 472–494.\n\nKennicutt, R.C., Tamblyn, P., and Congdon, C.W. (1994). Past and future star formation in disk galaxies. Astrophys. J., 435, 22–36.\n\nKorycansky, D.G., Laughlin, G., and Adams, F.C. (2001). Astronomical Engineering: a strategy for modifying planetary orbits. Astrophys. Space Sci., 275, 349–366.\n\nKrauss, L.M. and Starkman, G.D. (2000). Life, the Universe, and Nothing: life and death in an ever-expanding universe. Astrophys. J., 531, 22–30.\n\nLaughlin, G. and Adams, F.C. (2000). The frozen Earth: binary scattering events and the fate of the Solar System. Icarus, 145, 614–627.\n\nLaughlin, G., Bodenheiemr, P., and Adams, F.C. (1997). The end of the main sequence. Astrophys. J., 482, 420.\n\nNagamine, K. and Loeb, A. (2003). Future evolution of nearby large-scale structures in a universe dominated by a cosmological constant. New Astron., 8, 439–448.\n\nNagamine, K. and Loeb, A. (2004). Future evolution of the intergalactic medium in a universe dominated by a cosmological constant. New Astron., 9, 573–583.\n\nPage, D. N. (1980). Particle transmutations in quantum gravity. Phys. Lett., B95, 244–246.\n\nPeebles, P.J.E. (1994). Orbits of the nearby galaxies. Astrophys. J., 429, 43–65.\n\nPerlmutter, S., Aldering, G., Goldhaber, G., Knop, R.A., Nugent, P., Castro, P.G., Deustua, S., Fabbro, S., Goobar, A., Groom, D.E., Hook, I.M., Kim, A.G., Kim, M.Y., Lee, J.C., Nunes, N.J., Pain, R., Pennypacker, C.R., Quimby, R., Lidman, C., Ellis, R.S., Irwin, M., McMahon, R.G., Ruiz-Lapuente, P., Walton, N., Schaefer, B., Boyle, B.J., Filippenko, A.V., Matheson, T., Fruchter, A.S., Panagia, N., Newberg, H.J.M., and Couch, W.J. (1999). Measurements of Q and A from 42 high-redshift supernovae. Astrophys. J., 517, 565–586.\n\nRiess, A.G., Filippenko, A.V., Challis, P., Clocchiatti, A., Diercks, A., Garnavich, P.M., Gilliland, R.L., Hogan, C.J., Jha, S., Kirshner, R.P., Leibundgut, B., Phillips, M.M., Reiss, D., Schmidt, B.P., Schommer, R.A., Smith, R.C., Spyromilio, J., Stubbs, C., Suntzeff, N.B., and Tonry, J. (1998). Observational evidence from supernovae for an accelerating universe and a cosmological constant. Astron. J., 116, 1009–1038.\n\nRybicki, K.R. and Denis, C. (2001). On the final destiny of the Earth and the Solar System. Icarus, 151, 130–137.\n\nSackmann, I.J., Boothroyd, A.I., and Kramer, K.E. (1993). Our Sun III: present and future. Astrophys. J., 418, 457–468.\n\nSpergel, D.N., Verde, L., Peiris, H.V., Komatsu, E., Nolta, M.R., Bennett, C.L., Halpern, M., Hinshaw, G., Jarosik, N., Kogut, A., Limon, M., Meyer, S.S., Page, L., Tucker, G.S., Weiland, J.L., Wollack, E., and Wright, E.L. (2003). First-year Wilkinson microwave anisotropy probe (WMAP) Observations: determination of cosmological parameters. Astrophys. J. Suppl., 148, 175–194.\n\nSuper-Kamiokande Collaboration. (1999). Search for proton decay through p → vK⁺ in a large water Cherenkov detector. Phys. Rev. Lett., 83, 1529–1533.\n\nZeldovich, Ya.B. (1976). A new type of radioactive decay: gravitational annihilation of baryons. Phys. Lett., A59, 254.\n\n• 3 • Evolution theory and the future of humanity\n\nChristopher Wills\n\n3.1 Introduction\n\nNo field of science has cast more light on both the past and the future of our species than evolutionary biology. Recently, the pace of new discoveries about how we have evolved has increased (Culotta and Pennisi, 2005).\n\nIt is now clear that we are less unique than we used to think. Genetic and palaeontological evidence is now accumulating that hominids with a high level of intelligence, tool-making ability, and probably communication skills have evolved independently more than once. They evolved in Africa (our own ancestors), in Europe (the ancestors of the Neanderthals) and in Southeast Asia (the remarkable ‘hobbits’, who may be miniaturized and highly acculturated Homo erectus).\n\nIt is also becoming clear that the genes that contribute to the characteristics of our species can be found and that the histories of these genes can be understood. Comparisons of entire genomes have shown that genes involved in brain function have evolved more quickly in hominids than in more distantly related primates.\n\nThe genetic differences among human groups can now be investigated. Characters that we tend to think of as extremely important markers enabling us to distinguish among different human groups now turn out to be understandable at the genetic level, and their genetic history can be traced. Recently a single allelic difference between Europeans and Africans has been found (Lamason et al., 2005). This functional allelic difference accounts for about a third of the differences in skin pigmentation in these groups. Skin colour differences, in spite of the great importance they have assumed in human societies, are the result of natural selection acting on a small number of genes that are likely to have no effects beyond their influence on skin colour itself.\n\nHow do these and other recent findings from fields ranging from palaeontology to molecular biology fit into present-day evolution theory, and what light do they cast on how our species is likely to evolve in the future?\n\nI will introduce this question by examining briefly how evolutionary change takes place. I will then turn to the role of environmental changes that have resulted in evolutionary changes in the past and extrapolate from those past changes to the changes that we can expect in the short-term and long-term future. These changes will be placed in the context of what we currently know about the evolution of our species. I will group these changes into physical changes and changes that stem from alterations of our own intellectual abilities. I will show that the latter have played and will continue to play a large role in our evolution and in the evolution of other animal and plant species with which we interact. Finally, I will turn to a specific examination of the probable course of future evolution of our species and of the other species on which we depend.\n\n3.2 The causes of evolutionary change\n\nEvolutionary changes in populations, of humans and all other organisms, depend on five factors.\n\nThe first and perhaps the most essential is mutation. Evolution depends on the fact that genetic material does not replicate precisely, and that errors are inevitably introduced as genes are passed from one generation to the next. In the absence of mutation, evolutionary change would slow and eventually stop.\n\nThe effects of mutations are not necessarily correlated with the sizes of the mutational changes themselves. Single changes in the base sequence of DNA can have no effect or profound effects on the phenotype – the allelic differences that affect skin colour, as discussed in Section 3.1, can be traced to a single alteration in a base from G to A, changing one amino acid in the protein from an alanine to a threonine. At the other end of the spectrum, entire doublings of chromosome number, which take place commonly in plants and less often in animals, can disturb development dramatically – human babies who have twice the normal number of chromosomes die soon after birth. But such doubling can sometimes have little effect on the organism.\n\nA fascinating source of mutation-like changes has recently been discovered. Viruses and other pieces of DNA can transfer genes from one animal, plant, or bacterial species to another, a process known as horizontal gene transfer. Such transfers appear to have played little part in our own recent history, but they have been involved in the acquisition of important, new capabilities in the past: the origin of our adaptive immune system is one remarkable example (Agrawal et al., 1998).\n\nThe most important mechanism that decides which of these mutational changes are preserved and which are lost is natural selection. We normally think of natural selection as taking place when the environment changes. But environmental change is not essential to evolution. Darwin realized that natural selection is taking place all the time. In each generation, even if the environment is unchanged, the fittest organisms are the most likely to survive and produce offspring. New mutations will continue to arise, a few of which will enable their carriers to take greater advantage of their environment even if it is not changing.\n\nIt is now realized that natural selection often acts to preserve genetic variation in populations. This type of selection, called balancing selection, results from a balance of selective pressures acting on genetic variation. It comes in many forms (Garrigan and Hedrick, 2003). Heterozygote advantage preserves the harmful sickle cell allele in human populations because people who are heterozygous for the allele are better able to resist the effects of malaria. A more prevalent type of balancing selection is frequency-dependent selection, in which a mutant allele may be beneficial when it is rare but loses that benefit as it rises in frequency. Such selection has the capability of maintaining many alleles at a genetic locus in a population. It also has the intriguing property that as alleles move to their internal equilibrium frequencies, the cost of maintaining the polymorphism goes down. This evolutionary “freebie” means that many frequency-dependent polymorphisms can be maintained in a population simultaneously.\n\nThree other factors play important but usually subordinate roles in evolutionary change: genetic recombination, the chance effects caused by genetic drift, and gene flow between populations.\n\nArguments have been made that these evolutionary processes are having little effect on our species at the present time (Jones, 1991). If so, this is simply because our species is experiencing a rare halcyon period in its history. During the evolutionary eye blink of the last 10,000 years, since the invention of agriculture and the rise of technology, our population has expanded dramatically. The result has been that large numbers of individuals who would otherwise have died have been able to survive and reproduce. I have argued elsewhere (Wills, 1998) and will explore later in this chapter the thesis that even this halcyon period may be largely an illusion. Powerful psychological pressures and new environmental factors (Spira and Multigner, 1998) are currently playing a major role in determining who among us reproduces.\n\nIt seems likely that this halcyon period (if it really qualifies as one) will soon come to an end. This book examines many possible scenarios for such resurgence in the strength of natural selection, and in this chapter I will examine how these scenarios might affect our future evolution.\n\n3.3 Environmental changes and evolutionary changes\n\nAs I pointed out earlier, evolutionary change can continue even in the absence of environmental change, but its pace is likely to be slow because it primarily ‘fine-tunes’ the adaptation of organisms that are already well adapted to their environment. When environmental changes occur, they can spur the pace of evolutionary change and can also provide advantages to new adaptations that would not have been selected for in an unchanging environment. What will be the evolutionary effects of environmental change that we can expect in the future? To gauge the likelihood of such effects, we must begin by examining the evolutionary consequences of changes that took place in the past. Let us look first at completed evolutionary changes, in which the evolutionary consequences of an environmental change have been fully realized, and then examine ongoing evolutionary changes in which the evolutionary changes that result from environmental change have only begun to take place.\n\n3.3.1 Extreme evolutionary changes\n\nThroughout the history of life environmental changes, and the evolutionary changes that result, have sometimes been so extreme as to cause massive extinctions. Nonetheless, given enough time, our planet’s biosphere can recover and regain its former diversity. Consider the disaster that hit the Earth 65 million years ago. A brief description of what happened can hardly begin to convey its severity.\n\nOne day approximately 65 ± 1 million years ago, without warning, a 10 km wide asteroid plunged into the atmosphere above the Yucatan peninsula at a steep angle from the southeast. It traversed the distance from upper stratosphere to the shallow sea in 20 seconds, heating the air ahead of it to a blue-hot plasma. The asteroid hit the ocean and penetrated through the sea bottom, first into the Earth’s crust and then into the molten mantle beneath the crust. As it did so, it heated up and exploded. The energy released, equivalent to 100 million megatons of TNT, was at least a million times as great as the largest hydrogen bomb that we humans have ever exploded. The atmospheric shock wave moved at several times the speed of sound across North America, incinerating all the forests in its path. Crust and mantle material erupted towards the sky, cooling and forming an immense choking cloud as it spread outwards. Shock waves raced through the crust, triggering force-10 earthquakes around the planet, and a 300 m high tsunami spread further destruction across a wide swath of all the Earth’s coastal regions. Volcanoes erupted along the planet’s great fault lines, adding their own noxious gases and dust to the witches’ brew that was accumulating in the atmosphere.\n\nMost of the animals and plants that lived in southern North America and the northern part of South America were killed by the direct effects of the impact. As the great cloud of dust blanketed the Earth over the next six months, blocking out the sun, many more animals and plants succumbed. There was no safe place on land or sea. Carbon dioxide released from the bolide impact caused a spike in temperatures worldwide (Beerling et al., 2002). All the dinosaurs perished, along with all the large flying and ocean-going reptiles and all the abundant nautilus-like ammonites that had swum in the oceans; of the mammals and birds, only a few survived.\n\nWhen the dust eventually began to clear the landscape was ghastly and moonlike, with only a few timid ferns poking out of cracks in the seared rocks and soil, and a few tiny mammals and tattered birds surviving on the last of their stores of seeds. It took the better part of a million years for the planet to recover its former verdant exuberance. And it took another 4 million years before new species of mammals filled all the ecological niches that had been vacated by the ruling reptiles.\n\nAsteroid impacts as large as the one that drove the dinosaurs to extinction are rare and have probably happened no more than two or three times during the last half billion years. But at least seven smaller impacts, each sufficiently severe to result in a wave of extinction, have occurred during that period. Each was followed by a recovery period, ranging up to a few million years (though many of the recovery times may have been less than that (Alroy et al., 2001)). During these recovery periods further extinctions took place and new clades of animals and plants appeared.\n\nAsteroids are not the only source of environmental devastation. Massive volcanic eruptions that took place some 251 million years ago were the probable cause of the most massive wave of extinctions our planet has ever seen, the Permian-Triassic extinction (e.g., Benton, 2003). As befits the violence of that extinction event, the resulting alterations in the biosphere were profound. The event set in motion a wave of evolutionary change leading to mammal-like therapsid reptiles (although therapsids with some mammalian characteristics had already appeared before the extinction event). It also gave the ancestors of the dinosaurs an opportunity to expand into vacant ecological niches, though the earliest dinosaurs of which we have records did not appear in the fossil record until 20 million years after the extinction event (Flynn et al., 1999).\n\nCompleted catastrophic events are characterized by both mass extinctions and sufficient time for recovery to take place. In general, the more severe the extinction event, more are the differences found to separate the pre-event world from the recovered world. The characteristics of the recovered world are largely shaped by the types of organisms that survived the catastrophe, but complex and unexpected subsequent interactions can take place. Therapsids with some mammalian characteristics survived the Permian-Triassic extinction, and the descendants of these surviving therapsids dominated the world for much of the Triassic period. Nonetheless, halfway through the Triassic, the therapsids began to lose ground. Dinosaurs began to dominate the niches for large land animals, with the result that the mammalian lineages that survived this conflict were primarily small herbivores and insectivores. However, some mammals were able to occupy specialized niches and grow quite large (Ji et al., 2006), and others were sufficiently big and fierce that they were able to prey on small dinosaurs (Hu et al., 2005). The later Cretaceous-Tertiary extinction provided the mammals with the opportunity to take over from the dinosaurs once more.\n\n3.3.2 Ongoing evolutionary changes\n\nA massive extinction event such as the Cretaceous-Tertiary event has a low likelihood of occurrence during any given short time period, and that probability has fluctuated only moderately over the course of the history of Earth, at least before the advent of humanity. But even though such major events are unlikely in the near future, less dramatic environmental changes, many driven by our own activities, are taking place at the present time. Glaciers have advanced and retreated at least 11 times during the last 2.4 million years. The diversity of vertebrates before the onset of this series of ice ages was far greater than the diversity of vertebrates of the present time (Barnosky et al., 2004; Zink and Slowinski, 1995). More recently, human hunting resulted in a wave of large mammal and bird extinctions in the late Pleistocene (Surovell et al., 2005).\n\nThere has been a relatively mild decrease in the number of species of mammal, compared with their great abundance in the early Ploicene. This decrease has resulted from Pliocene and Pleistocene climate change and from the human Pleistocene overkill. Now, the decrease is likely to become substantially steeper. It is clear that the relatively mild decrease in the number of species resulting from Pliocene and Pleistocene climate change and from the human Pleistocene overkill is likely to become substantially steeper. Some of the often discussed worst-case scenarios for the future range from the onset of an ice age of such severity that the planet freezes from poles to equator, to a series of nuclear wars or volcanic eruptions that irreversibly poison the atmosphere and oceans. If such terrifying scenarios do not transpire, however, we have a good chance of coming to terms with our environment and slowing the rate of extinction.\n\nAs we have seen, alterations in the environment can open up opportunities for evolutionary change as well as close them off through extinction. Even small environmental changes can sometimes have dramatic evolutionary consequences over short spans of time. Three species of diploid flowering plant (Tragopogon, Asteraceae) were introduced into western Washington State, North America, from Europe about 100 years ago. Two tetraploid species, arising from different combinations of these diploids, arose without human intervention soon afterwards and have thrived in this area (though such tetraploids have not appeared in their native Europe). DNA studies have shown that a variety of genetic modifications have occurred in these two tetraploids over a few decades (Cook et al., 1998). This report and many similar such stories of rapid recent evolution in both animals and plants indicate that evolutionary changes can take place within the span of a human lifetime.\n\nNew analyses of the fossil record suggest that recovery to former diversity levels from even severe environmental disasters may be more rapid than had previously been thought (Alroy et al., 2001). Present-day ecosystem diversity may also be regained rapidly after minor disturbances. We have recently shown that the diversity levels of tropical forest ecosystems are resilient, and that while these forests may not recover easily from severe environmental disasters there is an advantage to diversity that can lead to rapid recovery after limited damage (Wills et al., 2006).\n\nNothing illustrates the potential for rapid evolutionary response to environmental change in our own species more vividly than the discovery in 2004 of a previously unknown group of hominids, the ‘hobbits’. These tiny people, one metre tall, lived on the island of Flores and probably on other islands of what is now Indonesia, as recently as 12,000 years ago (Brown et al., 2004). They have been given the formal name Homo floresiensis, but I suspect that it is the name hobbit that will stick. Sophisticated tools found near their remains provide strong evidence that these people, who had brains no larger than those of chimpanzees, were nonetheless expert tool users and hunters. Stone points and blades, including small blades that showed signs of being hafted, were found in the same stratum as the skeletal remains (Brown et al., 2004). Using these tools the hobbits might have been able to kill (and perhaps even help to drive extinct!) the pygmy mastodons with which they shared the islands.\n\nIt is probable that the hobbits had physically larger ancestors and that the hobbits themselves were selected for reduced stature when their ancestors reached islands such as Flores, where food was limited. It was this relatively minor change in the physical environment, one that nonetheless had a substantial effect on survival, that selected for the hobbits’ reduction in stature. At the same time, new hunting opportunities and additional selective pressures must have driven their ability to fashion sophisticated weapons.\n\nThe ancestor of the hobbits may have been Homo erectus, a hominid lineage that has remained distinct from ours for approximately 2 million years. But, puzzlingly, features of the hobbits’ skeletons indicate that they had retained a mix of different morphologies, some dating back to a period 3 million years ago – long before the evolution of the morphologically different genus Homo. The history of the hobbits is likely to be longer and more complex than we currently imagine (Dennell and Roebroeks, 2005).\n\nDetermining whether the hobbits are descendants of Homo erectus, or of earlier lineages such as Homo habilis, requires DNA evidence. No DNA has yet been isolated from the hobbit bones that have been found so far, because the bones are water-soaked and poorly preserved. In the absence of such evidence it is not possible to do more than speculate how long it took the hobbits to evolve from larger ancestors. But, when better-preserved hobbit remains are discovered and DNA sequences are obtained from them, much light will be cast on the details of this case of rapid and continuing evolvability of some of our closest relatives.\n\nThe evolution of the hobbits was strongly influenced by the colonization of islands by their ancestors. Such colonizations are common sources of evolutionary changes in both animals and plants. Is it possible that similar colonization events in the future could bring about a similar diversification of human types?\n\nThe answer to this question depends on the extent and effect of gene flow among the members of our species. At the moment the differences among human groups are being reduced because of gene flow that has been made possible by rapid and easy travel. Thus it is extremely unlikely that different human groups will diverge genetically because they will not be isolated. But widespread gene flow may not continue in the future. Consider one possible scenario described in the next paragraph.\n\nIf global warming results in a planet with a warm pole-to-pole climate, a pattern that was typical of the Miocene, there will be a rapid rise in sea level by 80 metres as all the world’s glaciers melt (Williams and Ferrigno, 1999). Depending on the rapidity of the melt, the sea level rise will be accompanied by repeated tsunamis as pieces of the Antarctic ice cap that are currently resting on land slide into the sea. There will also be massive disturbance of the ocean’s circulation pattern, probably including the diversion or loss of the Gulf Stream. Such changes could easily reduce the world’s arable land substantially – for example, all of California’s Central Valley and much of the southeastern United States would be under water. If the changes occur swiftly, the accompanying social upheavals would be substantial, possibly leading to warfare over the remaining resources. Almost certainly there would be substantial loss of human life, made far worse if atomic war breaks out. If the changes occur sufficiently slowly it is possible that alterations in our behaviour and the introduction of new agricultural technologies, may soften their impact.\n\nWhat will be the evolutionary consequences to our species of such changes? Because of the wide current dispersal and portability of human technology, the ability to travel and communicate over long distances is unlikely to be lost completely as a result of such disasters unless the environmental disruption is extreme. But if a rapid decrease in population size were accompanied by societal breakdown and the loss of technology, the result could be geographic fragmentation of our species. There would then be a resumption of the genetic divergence among different human groups that had been taking place before the Age of Exploration (one extreme example of which is the evolution of the hobbits). Only under extremely adverse conditions, however, would the fragmentation persist long enough for distinctly different combinations of genes to become fixed in the different isolated groups. In all but the most extreme scenarios, technology and communication would become re-established over a span of a few generations, and gene flow between human groups would resume.\n\n3.3.3 Changes in the cultural environment\n\nBoth large and small changes in the physical environment can bring about evolutionary change. But even in the absence of such changes, cultural selective pressures that have acted on our species have had a large effect on our evolution and will continue to do so. To understand these pressures, we must put them into the context of hominid history. As we do so, we will see that cultural change has been a strong driving force of human evolution, and has also affected many other species with which we are associated.\n\nAbout 6 million years ago, in Africa, our evolutionary lineage separated from the lineage that led to chimpanzees and bonobos. The recent discovery in Kenya of chimpanzee teeth that are half a million years old (McBrearty and Jablonski, 2005) shows that the chimpanzee lineage has remained distinct for most of that time from our own lineage, though the process of actual separation of the gene pools may have been a complicated one (Patterson et al., 2006). There is much evidence that our remote ancestors were morphologically closer to chimpanzees and bonobos than to modern humankind ourselves. The early hominid Ardipithecus ramidus, living in East Africa 4.4 million years ago, had skeletal features and a brain size resembling those of chimpanzees (White et al., 1994). Its skeleton differed from those of chimpanzees in only two crucial respects: a slightly more anterior position of the foramen magnum, which is the opening at the bottom of the skull through which the spinal cord passes, and molars with flat crowns like those of modern humans rather than the highly cusped molars of chimpanzees. If we could resurrect an A. ramidus it would probably look very much like a chimpanzee to us – though there is no doubt that A. ramidus and present-day chimpanzees would not recognize each other as members of the same species.\n\nEvolutionary changes in the hominid line include a gradual movement in the direction of upright posture. The changes required a number of coordinated alterations in all parts of the skeleton but in the skull and the pelvis in particular. Perhaps the most striking feature of this movement towards upright posture is how gradual it has been. We can trace the change in posture through the gradual anterior movement of the skull’s foramen magnum that can be seen to have taken place from the oldest hominid fossils down to the most recent.\n\nA second morphological change is of great interest. Hominid brains have undergone a substantial increase in size, with the result that modern human brains have more than three times the volume of a chimpanzee brain. Most of these increases have taken place during the last 2.5 million years of our history. The increases took place not only in our own immediate lineage but also in at least one other extinct lineage that branched off about a million years ago – the lineage of Europe and the Middle East that included the pre-Neanderthals and Neanderthals. It is worth emphasizing, however, that this overall evolutionary ‘trend’ may have counterexamples, in particular the apparent substantial reduction in both brain and body size in the H. floresiensis lineage. The remarkable abilities of the hobbits make it clear that brain size is not the only determiner of hominid success.\n\nOur ability to manipulate objects and thus alter our environment has also undergone changes during the same period. A number of changes in the structure of hominid hands, such as the increase in brain size beginning at least 2.5 million years ago, have made them more flexible and sensitive. And our ability to communicate, too, has had a long evolutionary history, reflected in both physical and behavioural characteristics. The Neanderthals had a voice box indistinguishable from our own, suggesting that they were capable of speech (Arensburg et al., 1989). The ability of human children to learn a complex language quickly (and their enthusiasm for doing so) has only limited counterparts in other primates. Although some chimpanzees, bonobos and gorillas have shown remarkable understanding of human spoken language, their ability to produce language and to teach language to others is severely limited (Tagliatela et al., 2003).\n\nMany of the changes in the hominid lineage have taken place since the beginning of the current series of glaciations 2.5 million years ago. There has been an accelerating increase in the number of animal and plant extinctions worldwide during this period. These include extinctions in our own lineage, such as those of H. habilis, H. erectus and H. ergaster, and more recently the Neanderthals and H. floresiensis. These extinctions have been accompanied by a rapid rate of evolution in our own lineage.\n\nWhat are the cultural pressures that have contributed to this rapid evolution? I have argued elsewhere (Wills, 1993) that a feedback loop involving our brains, our bodies, our genes, and our rapidly changing cultural environment has been an important contributor to morphological and behavioural changes. Feedback loops are common in evolution and have led to many extreme results of sexual selection in animals and of interactions with pollinators among flowering plants. A ‘runaway brain’ feedback can explain why rapid changes have taken place in the hominid lineage.\n\nThe entire human and chimpanzee genomes are now available for comparison, opening up an astounding new world of possibilities for scientific investigation (Chimpanzee Sequencing and Analysis Consortium, 2005). Overall comparisons of the sequences show that some 10 million genetic changes separate us from chimpanzees. We have hardly begun to understand which of these changes have played the most essential role in our evolution. Even at such early stages in these genome-wide investigations, however, we can measure the relative rate of change of different classes of genes as they have diverged in the two lineages leading to humans and chimpanzees. It is now possible to examine the evolution of genes that are involved in brain function in the hominid lineage and to compare these changes with the evolution of the equivalent (homologous) genes in other primates.\n\nThe first such intergenomic comparisons have now been made between genes that are known to be involved in brain growth and metabolism and genes that affect development and metabolic processes in other tissues of the body. Two types of information have emerged, both of which demonstrate the rapid evolution of the hominid lineage.\n\nFirst, the genes that are expressed in brain tissue have undergone more regulatory change in the human lineage than they have in other primate lineages. Gene regulation determines whether and when a particular gene is expressed in a particular tissue. Such regulation, which can involve many different interactions between regulatory proteins and stretches of DNA, has a strong influence on how we develop from embryo to adult. As we begin to understand some of these regulatory mechanisms, it is becoming clear that they have played a key role in many evolutionary changes, including major changes in morphology and behaviour. We can examine the rate of evolution of these regulatory changes by comparing the ways in which members of this class of genes are expressed in the brains of ourselves and of our close relatives (Enard et al., 2002). One pattern that often emerges is that a given gene may be expressed at the same level (say high or low) in both chimpanzees and rhesus monkeys, but at a different level (say intermediate) in humans. Numerous genes show similar patterns, indicating that their regulation has undergone significantly more alterations in our lineage than in those of other primates. Unlike the genes involved in brain function, regulatory changes have not occurred preferentially in the hominid lineage in genes expressed in the blood and liver.\n\nSecond, genes that are implicated in brain function have undergone more meaningful changes in the human lineage than in other lineages. Genes that code for proteins undergo two types of changes: non-synonymous changes that alter the proteins that the genes code for, possibly changing their function, and synonymous changes that change the genes but have no effect on the proteins. When genes that are involved in brain function are compared among different mammalian lineages, significantly more potentially functional changes have occurred in the hominid lineage than in the other lineages (Clark et al., 2003). This finding shows clearly that in the hominid lineage strong natural selection has changed genes involved in brain function more rapidly than the changes that have taken place in other lineages.\n\nSpecific changes in genes that are involved in brain function can now be followed in detail. Evidence is accumulating that six microcephalin genes are involved in the proliferation of neuroblasts during early brain development. One of these genes, MCPH1, has been found to carry a specific haplotype at high frequency throughout human populations (Evans et al., 2005), and it has reached highest frequency in Asia (Fig. 3.1). The haplotype has undergone some further mutations and recombinations since it first arose about 37,000 years ago, and these show strong linkage disequilibrium. Other alleles at this locus do not show such a pattern of disequilibrium. Because disequilibrium breaks down with time, it is clear that this recent haplotype has spread as a result of strong natural selection.\n\n[Image]\n\nFig. 3.1 Global frequencies of microcephalin haplogroup D chromosomes (defined as having the derived C allele at the G37995C diagnostic SNP) in a panel of 1184 individuals.\n\n[Image]\n\nFig. 3.2 Worldwide frequencies of ASPM haplogroup D chromosomes (defined as having the derived G allele at the A44871G diagnostic polymorphism), based on a panel of 1186 individuals.\n\nAnother gene also associated with microcephaly, abnormal spindle-like microcephaly-associated (ASPM), shows even more recent evidence of extremely strong selection (Mekel-Bobrov et al., 2005). An allele found chiefly in Europe and the Middle East, but at much lower frequencies in Asia (Fig. 3.2), appears to have arisen as recently as 5800 years ago. The spread of this allele has been so rapid that it must confer a selective advantage of several percent on its carriers.\n\nAlthough both these alleles carry non-synonymous base changes, the allelic differences that are being selected may not be these changes but may be in linked regulatory regions. Further, a direct effect of these alleles on brain function has not been demonstrated. Nonetheless, the geographic patterns seen in these alleles indicate that natural selection is continuing to act powerfully on our species.\n\nHawks and coworkers (Hawks et al. 2007) have recently shown that the pattern of strong recent selection detectable by linkage disequilibrium extends to more than 2,000 regions of the human genome. They calculate that over the last 40,000 years our species has evolved at a rate 100 times as fast as our previous evolution. Such a rapid rate may require that many of the newly selected genes are maintained by frequency-dependent selection, to reduce the selective burden on our population.\n\n3.4 Ongoing human evolution\n\nAll these pieces of evidence from our past history show that humans have retained the ability to evolve rapidly. But are we continuing to evolve at the present time? Without a doubt! Although the evolutionary pressures that are acting on us are often different from those that acted on our ancestors a million years ago or even 10,000 years ago, we are still exposed to many of the same pressures. Selection for resistance to infectious disease continues. We have conquered, at least temporarily, many human diseases, but many others – such as tuberculosis, AIDS, malaria, influenza, and many diarrhoeal diseases-continue to be killers on a massive scale (see Chapter 14, this volume). Selection for resistance to these diseases is a continuing process because the disease organisms themselves are also evolving.\n\n3.4.1 Behavioural evolution\n\nAlthough the selective effects of psychological pressures are difficult to measure, they must also play a role. Michael Marmot and his colleagues have shown that an individual’s position in a work hierarchy has an impact on his or her health (Singh-Manoux et al., 2005). The members at the top level of the hierarchy live healthier lives than those at the bottom level – or even those who occupy positions slightly below the top level!\n\nReproduction also applies psychological pressures. The introduction of effective means of birth control has provided personal choice in reproduction for more people than at any time in the past. We can only surmise how selection for genes that influence reproductive decision-making will affect our future evolution. There is some evidence from twin studies, however, that heritability for reproductive traits, especially for age at first reproduction, is substantial (Kirk et al., 2001). Thus, rapid changes in population growth rates such as those being experienced in Europe and the former Soviet Union are likely to have evolutionary consequences.\n\nConcerns over dysgenic pressures on the human population resulting from the build-up of harmful genes (Muller, 1950) have abated. It is now realized that even in the absence of selection, harmful mutant alleles will accumulate only slowly in our gene pool and that many human characteristics have a complex genetic component so that it is impossible to predict the effects of selection on them. Alleles that are clearly harmful – sickle cell, Tay-Sachs, muscular dystrophy and others – will soon be amenable to replacement by functional alleles through precise gene surgery performed on germ line cells. Such surgery, even though it will be of enormous benefit to individuals, is unlikely to have much of an effect on our enormous gene pool unless it becomes extremely inexpensive.\n\nOne intriguing direction for current and future human evolution that has received little or no attention is selection for intellectual diversity. The measurement of human intellectual capabilities, and their heritability, is at a primitive stage. The heritability of IQ has received a great deal of attention, but a recent meta-analysis estimates broad heritability of IQ to be 0.5 and narrow heritability (the component of heritability that measures selectable phenotypic variation) to be as low as 0.34 (Devlin et al., 1997). But IQ is only one aspect of human intelligence, and other aspects of intelligence need investigation. Daniel Goleman has proposed that social intelligence, the ability to interact with others, is at least as important as IQ (Goleman, 1995), and Howard Gardner has explored multiple intelligences ranging from artistic and musical through political to mechanical (Gardner, 1993). All of us have a different mix of such intelligences. To the extent that genes contribute to them, these genes are likely to be polymorphic – that is, to have a number of alleles, each at appreciable frequency, in the human population.\n\nThis hypothesis of polymorphic genes involved in behaviour leads to two predictions. First, loci influencing brain function in humans should have more alleles than the same loci in chimpanzees. Note, however, that most of the functional polymorphic differences are likely to be found, not in structural genes, but in the regulatory regions that influence how these genes are expressed. Because of the difficulty of determining which genetic differences in the polymorphisms are responsible for the phenotypic effects, it may be some time before this prediction can be tested.\n\nThe second prediction is that some type of balancing selection, probably with a frequency-dependent component, is likely to be maintaining these alleles in the human population.\n\nWhen an allele has an advantage if it is rare but loses that advantage if it is common, it will tend to be maintained at the frequency at which there is neither an advantage nor a disadvantage. If we suppose that alleles influencing many behaviours or skills in the human population provide an advantage when they are rare, but lose that advantage when they are common, there will be a tendency for the population to accumulate these alleles at such intermediate frequencies. And, as noted earlier, if these genes are maintained by frequency dependence, the cost to the population of maintaining this diversity can be low.\n\nNumerous examples of frequency-dependent balancing selection have been found in populations. One that influences behaviours has been found in Drosophila melanogaster. Natural populations of this fly are polymorphic for two alleles at a locus (for, standing for forager) that codes for a protein kinase. A recessive allele at this locus, sitter, causes larvae to sit in the same spot while feeding. The dominant allele, rover, causes its carriers to move about while feeding. Neither allele can take over (reach fixation) in the population. Rover has an advantage when food is scarce, because rover larvae can find more food and grow more quickly than sitter larvae. Sitter has an advantage when food is plentiful. If they are surrounded by abundance, sitter larvae that do not waste time and effort moving about can mature more quickly than rovers (Sokolowski et al., 1997).\n\nIt will be fascinating to see whether behaviour-influencing polymorphisms such as those at the for locus are common in human populations. If so, one type of evolutionary change may be the addition of new alleles at these loci as our culture and technology become more complex and opportunities for new types of behaviours arise. It is striking that the common MCPH1 allele has not reached fixation in any human population, even though it has been under positive selection since before modern humans spread to Europe. It may be that this allele is advantageous when it is rare but loses that advantage when it becomes common. There appear to be no behavioral effects associated with this allele (Mekel-Bobrov et al., 2007), but detailed studies may reveal small differences. Natural selection can work on small phenotypic differences as well as large, and much of our recent evolution may have resulted from selection for genes with small effects.\n\n3.4.2 The future of genetic engineering\n\nThere has been much speculation about the effects of genetic engineering on the future of our species, including the possibility that a ‘genetic elite’ may emerge that would benefit from such engineering to the exclusion of other human groups (e.g., Silver, 1998). Two strong counterarguments to this viewpoint can be made.\n\nFirst, the number of genes that can potentially be modified in our species is immense. Assuming 50,000 genes per diploid human genome and 6 billion individuals, the number of genes in our gene pool is 3 × 10¹⁴. The task of changing even a tiny fraction of these genes would be enormous, especially since each such change could lead to dangerous and unexpected side effects. It is far more likely that our growing understanding of gene function will enable us to design specific drugs and other compounds that can produce desirable changes in our phenotypes and that these changes will be sufficiently easy and inexpensive that they will not be confined to a genetic elite (Wills, 1998). Even such milder phenotypic manipulations are fraught with danger, however, as we have seen from the negative effects that steroid and growth hormone treatments have had on athletes.\n\nSecond, the likelihood that a ‘genetic elite’ will become established seems remote. The modest narrow (selectable) heritability of IQ mentioned earlier shows the difficulty of establishing a genetic elite through selection. Heritabilities that are even lower are likely to be the rule for other physical or behavioural characteristics that we currently look upon as desirable.\n\nAttempts to establish groups of clones of people with supposedly desirable characters would also have unexpected and unpredictable effects, in this case because of the environment. Clones of Bill Gates or Mother Teresa, growing up at a different time and in a different place, would turn into people who reflected the influences of their unique upbringing, just as the originals of such hypothetical clones did. And, luckily, environmental effects can work to defang evil dysgenic schemes as well as utopian eugenic ones. ‘The Boys from Brazil’ notwithstanding, it seems likely that if clones of Adolf Hitler were to be adopted into well-adjusted families in healthy societies they would grow up to be nice, well-adjusted young men.\n\n3.4.3 The evolution of other species, including those on which we depend\n\nDiscussions of human evolution have tended to ignore the fact that we have greatly influenced the evolution of other species of animals and plants. These species have in turn influenced our own evolution. The abundant cereals that made the agricultural revolution possible were produced by unsung generations of primitive agriculturalists who carried out a process of long-continued artificial selection. Some results of such extremely effective selection are seen in Indian corn, which is an almost unrecognizable descendent of the wild grass teosinte, and domesticated wheat, which is an allohexaploid with genetic contributions from three different wild grasses. The current immense human population depends absolutely on these plants and also on other plants and animals that are the products of thousands of generations of artificial selection.\n\nOne consequence of climate change such as global warming is that the agriculture of the future will have to undergo rapid adaptations (see also Chapter 13, this volume). Southern corn leaf blight, a fungus that severely damaged corn production in the Southeast United States during the 1970s, was controlled by the introduction of resistant strains, but only after severe losses. If the climate warms, similar outbreaks of blight and other diseases that are prevalent in tropical and subtropical regions will become a growing threat to the world’s vast agricultural areas.\n\nOur ability to construct new strains and varieties of animals and plants that are resistant to disease, drought, and other probable effects of climate change depends on the establishment and maintenance of stocks of wild ancestral species. Such stocks are difficult to maintain for long periods because governments and granting agencies tend to lose interest and because societal upheavals can sometimes destroy them. Some of the stocks of wild species related to domestic crops that were collected by Russian geneticist Nikolai Vavilov in the early part of the twentieth century have been lost, taking with them an unknown number of genes of great potential importance. It may be possible to avoid such losses in the future through the construction of multiple seed banks and gene banks to safeguard samples of the planet’s genetic diversity. The Norwegian government recently opened a bunker on Svalbard, an Arctic island, designed to hold around 2 million seeds, representing all known varieties of the world’s crops. Unfortunately, however, there are no plans to replicate this stock centre elsewhere.\n\nTechnology may aid us in adjusting to environmental change, provided that our technological capabilities remain intact during future periods of rapid environmental change. To cite one such example, a transgenic tomato strain capable of storing excess salt in its leaves while leaving its fruit relatively salt-free has been produced by overexpression of an Arabidopsis transporter gene. The salt-resistant plants can grow at levels of salt 50 times higher than those found in normal soil (Zhang and Blumwald, 2001). The ability to produce crop plants capable of growing under extreme environmental conditions may enable us to go on feeding our population even as we are confronted with shrinking areas of arable land.\n\n3.5 Future evolutionary directions\n\nEven a large global catastrophe such as a 10 km asteroidal/cometary impact would not spell doom for our species if we would manage to spread to other solar systems by the time the impactor arrives. We can, however, postulate a number of scenarios, short of extinction, that will test our ability to survive as a species. I will not discuss here scenarios involving intelligent machines or more radical forms of technology-enabled human transformation.\n\n3.5.1 Drastic and rapid climate change without changes in human behaviour\n\nIf climatic change either due to slow (e.g., anthropogenic) or due to sudden (e.g., supervolcanic) causative agents is severe and the survivors are few in number, there may be no time to adapt. The fate of the early medieval Norse colonists in Greenland, who died out when the climate changed because they could not shift from eating meat to eating fish (Berglund, 1986) stands as a vivid example. Jared Diamond (2005) has argued in a recent book that climate change has been an important factor in several cases of societal collapse in human history.\n\n3.5.2 Drastic but slower environmental change accompanied by changes in human behaviour\n\nIf the environmental change occurs over generations rather than years or decades, there may be time for us to alter our behaviours deliberately. These behavioural changes will not be evolutionary changes, at least at first, although as we will see the ability to make such changes depends on our evolutionary and cultural history. Rather they will consist of changes in memes (Dawkins, 1976), the non-genetic learned or imitated behaviours that form an essential part of human societies and technology. The change that will have the largest immediate effect will be population control, through voluntary or coerced means or both. Such changes are already having an effect. The one-child policy enforced by the Chinese government, imperfect though it is in practice, has accelerated that country’s demographic transition and helped it to gain a 20-fold increase in per capita income over the last quarter century.\n\nSuch demographic transitions are taking place with increasing rapidity in most parts of the world, even in the absence of government coercion. Predictions of a human population of 12 billion by 2050 were made by the United Nations in 1960. These frightening projections envisioned that our population would continue to increase rapidly even after 2050. These estimates have now been replaced by less extreme predictions that average nine billion by 2050, and some of these revised projections actually predict a slow decline in world population after mid-century. Demographic transitions in sub-Saharan Africa and South Asia will lag behind the rest of the planet, but there is no reason to suppose that these regions will not catch up during this century as education, particularly the education of women, continues to spread. These demographic transitions are unlikely to be reversed in the future, as long as education continues to spread. Recently the chief of a small village on the remote island of Rinca in Indonesia complained to me that all his six children wanted to go to medical school, and he could not imagine how he could send them all.\n\nAccompanying the demographic transitions will be technological changes in how the planet is fed. If rising ocean levels cause the loss of immense amounts of arable land (including major parts of entire countries, such as low-lying Bangladesh), hordes of refugees will have to be fed and housed. Technology and alterations in our eating habits hold out some hope. Soy protein is similar in amino acid content to animal proteins, and its production has increased 400% in the past 30 years. This crop is already beginning to change our eating habits. And new agricultural infrastructure, such as intense hydroponic agriculture carried out under immense translucent geodesic domes with equipment for recycling water, will rapidly become adopted when the alternative is starvation.\n\nLittle will be done to confront these problems without a series of catastrophic events that make it clear even to the most reactionary societies and governments that drastic change is needed. These catastrophes are already beginning to throw into sharp relief current social behaviours that are inadequate for future challenges, such as a disproportionate use of the world’s limited resources by particular countries and restrictions on the free flow of information by dictatorial governments. Societal models based on national self-interest or on the preservation of power by a few will prove inadequate in the face of rapid and dramatic environmental changes. I will predict that – in spite of the widespread resistance to the idea – a global governmental organization with super-national powers, equivalent on a global scale to the European Union, will inevitably emerge. As we confront repeated catastrophes, we will see played out in the economic realm a strong selection against individual and societal behaviours that cannot be tolerated in a world of scarcity.\n\nDiscomfiting as such predictions may be to some, the accelerating rate of environmental change will make them inevitable. If we are to retain a substantial human population as the planet alters, our behaviours must alter as well. Otherwise, our societal upheavals may result in long-term ecological damage that can be reversed only after tens or hundreds of thousands of years.\n\nScream and argue and fight as we may about how to behave in the future, our past evolutionary history has provided most of us with the ability to change how we behave. This is our remarkable strength as a species.\n\n3.5.3 Colonization of new environments by our species\n\nA third future scenario, that of the colonization of other planets, is rapidly moving from the realm of science fiction to a real possibility. More than 200 extrasolar planets have been discovered in the last decade. These are mostly Jupiter-sized or larger, but it is safe to predict that within years or decades Earth-sized extrasolar planets, some of them showing evidence of life, will be found. The smallest extrasolar planet yet found is a recently discovered mere 5-Earth masses companion to Gliese 581 which lies in the habitable zone and is likely to possess surface water (Beaulieu et al., 2006). In view of the selection effects applicable to the surveys thus far, the discoveries of even smaller planets are inevitable.\n\nThis prediction is at variance with the argument presented by Ward and Brownlee (2000) that planets harbouring complex life are likely to be extremely rare in our galaxy. However, that argument was based on a biased interpretation of the available data and the most restrictive view on how planetary systems form (Kasting, 2001).\n\nNo more exciting moment of scientific discovery can be imagined than when we first obtain an image or spectrum of an Earth-sized planet with an oxygen-rich atmosphere circling a nearby star (soon to become possible with the advent of Darwin, Kepler, Gaia and several other terrestrial planet-seeking missions in the next several years). It is likely that the challenge of visiting and perhaps colonizing these planets is one that we as a species will be unable to resist.\n\nThe colonization of other planets will result in an explosive Darwinian adaptive radiation, involving both our species and the animals and plants that accompany us. Just as the mammals radiated into new ecological niches after the extinction of the dinosaurs, and the finches and land tortoises that Darwin encountered on the Galápagos Islands radiated adaptively as they spread to different islands, we will adapt in different ways to new planets that we explore and colonize.\n\nThe new planetary environments will be different indeed. How will we be able to colonize new planets peopled with indigenous life forms that have a different biochemistry and mechanism of inheritance from us? Could we, and the animals and plants that we bring with us, coexist with these indigenous and highly adapted life forms? Could we do so without damaging the ecology of the planets we will be occupying? And how will competition with these life forms change us? Will we be able to direct and accelerate these changes to ourselves by deliberately modifying the genes of these small populations of colonists?\n\nIf we are able to adapt to these new environments, then 10,000 or 100,000 years from now our species will be spread over so wide a region that no single environment-caused disaster would be able to wipe us all out. But our continued existence would still be fraught with danger. Will we, collectively, still recognize each other as human? What new and needless prejudices will divide us, and what new misunderstandings will lead to pointless conflict?\n\nSuggestions for further reading\n\nKareiva, P., Watts, S., McDonald, R., and Boucher, T. (2007). Domesticated nature: shaping landscapes and ecosystems for human welfare. Science, 316, 1866-1869. The likelihood that we will permanently alter the world’s ecosystems for our own benefit is explored in this paper.\n\nMyers, N. and Knoll, A.H. (2001). The biotic crisis and the future of evolution. Proc. Natl. Acad. Sci. (USA), 98, 5389-5392. Discomfiting predictions about the course of future evolution can be found in this paper.\n\nPalumbi, S.R. (2001). Humans as the world’s greatest evolutionary force. Science, 293, 1786-1790. The influence of humans on the evolution of other organisms is examined.\n\nUnfortunately, none of these authors deals with the consequences of changes in evolutionary pressures on our own species.\n\nReferences\n\nAgrawal, A., Eastman, Q.M., and Schatz, D.G. (1998). Implications of transposition mediated by V(D) J-recombination proteins RAG1 and RAG2 for origins of antigen-specific immunity. Nature, 394, 744-751.\n\nAlroy, J., Marshall, C.R., Bambach, R.K., Bezusko, K., Foote, M., Fursich, F.T., Hansen, T.A., Holland, S.M., Ivany, L.C., Jablonski, D., Jacobs, D.K., Jones, D.C., Kosnik, M.A., Lidgard, S., Low, S., Miller, A.I., Novack-Gottshall, P.M., Olszewski, T.D., Patzkowsky, M.E., Raup, D.M., Roy, K., Sepkoski, J.J., Jr, Ommers, M.G., Wagner, P.J., and Webber, A. (2001). Effects of sampling standardization on estimates of Phanerozoic marine diversification. Proc. Natl. Acad. Sci. (USA), 98, 6261–6266.\n\nArensburg, B., Tillier, A.M., Vandermeersch, B., Duday, H., Schepartz, L.A., and Rak, Y. (1989). A Middle Paleolithic hyoid bone. Nature, 338, 758–760.\n\nBarnosky, A.D., Bell, C.J., Emslie, S.D., Goodwin, H.T., Mead, J.I., Repenning, C.A., Scott, E., and Shabel, A.B. (2004). Exceptional record of mid-Pleistocene vertebrates helps differentiate climatic from anthropogenic ecosystem perturbations. Proc. Natl. Acad. Sci. (USA), 101, 9297–9302.\n\nBeaulieu, J.-P., Bennett, D.P., Fouque, P., Williams, A., Dominik, M., J0rgensen, U.G., Kubas, D., Cassan, A., Coutures, C., Greenhill, J., Hill, K., Menzies, J., Sackett, P.D., Albrow, M., Brillant, S., Caldwell, J.A.R., Calitz, J.J., Cook, K.H., Corrales, E., Desort, M., Dieters, S., Dominis, D., Donatowicz, J., Hoffman, M., Kane, S., Marquette, J.-B., Martin, R., Meintjes, P., Pollard, K., Sahu, K., Vinter, C., Wambsganss, J., Woller, K., Horne, K., Steele, I., Bramich, D.M., Burgdorf, M., Snodgrass, C., Bode, M., Udalski, A., Szymaski, M.K., Kubiak, M., Wickowski, T., Pietrzyski, G., Soszyski, I., Szewczyk, O., Wyrzykowski, L., Paczyski, B., Abe, F., Bond, I.A., Britton, T.R., Gilmore, A.C., Hearnshaw, J.B., Itow, Y., Kamiya, K., Kilmartin, P.M., Korpela, A.V., Masuda, K., Matsubara, Y., Motomura, M., Muraki, Y., Nakamura, S., Okada, C., Ohnishi, K., Rattenbury, N.J., Sako, T., Sato, S., Sasaki, M., Sekiguchi, T., Sullivan, D.J., Tristram, P.J., Yock, P.C.M., and Yoshioka, T. (2006). Discovery of a cool planet of 5.5 Earth masses through gravitational microlensing. Nature, 439, 437.\n\nBeerling, D.J., Lomax, B.H., Royer, D.L., Upchurch, G.R., Jr, and Kump, L.R. (2002). An atmospheric pCO 2 reconstruction across the Cretaceous-Tertiary boundary from leafmegafossils. Proc. Natl. Acad. Sci. (USA), 99, 7836–7840.\n\nBenton, M.J., and Twitchett, R.J. (2003) How to kill (almost) all life: the end-Permian extinction event, Trends Ecol. Evol. 18, 358–365.\n\nBerglund, J. (1986). The decline of the Norse settlements in Greenland. Arc. Anthropol., 23, 109–136.\n\nBrown, P., Sutikna, T., Morwood, M.J., Soejono, R.P., Jatmiko, Saptomo, E.W., and Due, R.A. (2004). A new small-bodied hominin from the Late Pleistocene of Flores, Indonesia. Nature, 431, 1055–1061.\n\nClark, A.G., Glanowski, S., Nielsen, R., Thomas, P.D., Kejariwal, A., Todd, M.A., Tanenbaum, D.M., Civello, D., Lu, F., Murphy, B., Ferriera, S., Wang, G., Zheng, X., White, T.J., Sninsky, J.J., Adams, M.D., and Cargill, M. (2003). Inferring nonneutral evolution from human-chimp-mouse orthologous gene trios. Science, 302, 1960 – 1963.\n\nChimpanzee Sequencing and Analysis Consortium, T.C.S. (2005). Initial sequence of the chimpanzee genome and comparison with the human genome. Nature, 437, 69–87.\n\nCook, L.M., Soltis, P.M., Brunsfeld, S.J., and Soltis, D.E. (1998). Multiple independent formations of Tragopogon tetraploids (Asteraceae): evidence from RAPD markers. Mol. Ecol., 7, 1293–1302.\n\nCulotta, E. and Pennisi, E. (2005). Evolution in action. Science, 310, 1878–1879.\n\nDawkins, R. (1976). The Selfish Gene (New York, NY: Oxford University Press).\n\nDennell, R. and Roebroeks, W. (2005). An Asian perspective on early human dispersal from Africa. Nature, 438, 1099–1104.\n\nDevlin, B., Daniels, M., and Roeder, K. (1997). The heritability of IQ. Nature, 388, 468–471.\n\nDiamond, Jared (2005). Collapse: How Societies Choose to Fail or Succeed. Viking, New York.\n\nEnard, W., Khaitovich, P., Klose, J., Zöllner, S., Heissig, F., Giavalisco, P., Nieselt-Struwe, K., Muchmore, E., Varki, A., Ravid, R., Doxiadis, G.M., Bontrop, R.E., and Pääbo, S. (2002). Intra- and interspecific variation in primate gene expression patterns. Science, 296, 340–343.\n\nEvans, P.D., Gilbert, S.L., Mekel-Bobrov, N., Vallender, E.J., Anderson, J.R., Vaez-Azizi, L.M., Tishkoff, S.A., Hudson, R.R., and Lahn, B.T. (2005). Microcephalin, a gene regulating brain size, continues to evolve adaptively in humans. Science, 309, 1717–1720.\n\nFlynn, J.J., Parrish, J.M., Rakotosamimanana, B., Simpson, W.F., Whatley, R.L., and Wyss, A.R. (1999). A Triassic fauna from Madagascar, including early dinosaurs. Science, 286, 763–765.\n\nGardner, H. (1993). Multiple Intelligences: The Theory in Practice (New York: Basic Books).\n\nGarrigan, D. and Hedrick, P.W. (2003). Perspective: detecting adaptive molecular polymorphism: lessons from the MHC. Evolution, 57, 1707–1722.\n\nGoleman, D. (1995). Emotional Intelligence (New York: Bantam Books).\n\nHawks, J., Wang, E.T., Cochran, G.M., Harpending, H.C. and Moyzis, R.K. (2007) Recent acceleration of human adaptive evolution. Proceedings of the National Academy of Sciences (US), 104, 20753–20758.\n\nHu, Y., Meng, J., Wang, Y., and Li, C. (2005). Large Mesozoic mammals fed on young dinosaurs. Nature, 433, 149–152.\n\nJi, Q., Luo, Z.-X., Yuan, C.-X., and Tabrum, A.R. (2006). A swimming mammaliaform from the middle Jurassic and ecomorphological diversification of early mammals. Science, 311, 1123–1127.\n\nJones, J.S. (1991). Is evolution over? If we can be sure about anything, it’s that humanity won’t become superhuman. New York Times, p.E17.\n\nKasting, J.F. (2001). Peter Ward and Donald Brownlee’s ‘Rare Earth’. Persp. Biol. Med., 44, 117–131.\n\nKirk, K.M., Blomberg, S.P., Duffy, D.L., Heath, A.C., Owens, I.P.F., and Martin, N.G. (2001). Natural selection and quantitative genetics of life-history traits in Western women: a twin study. Evolution, 55, 423–435.\n\nLamason, R.L., Mohideen, M.-A.P.K., Mest, J.R., Wong, A.C., Norton, H.L., Aros, M.C., Jurynec, M.J., Mao, X., Humphreville, V.R., Humbert, J.E., Sinha, S., Moore, J.L., Jagadeeswaran, P., Zhao, W., Ning, G., Makalowska, I., McKeigue, P.M., O’Donnell, D., Kittles, R., Parra, J., Mangini, N.J., Grunwald, D.J., Shriver, M.D., Canfield, V.A., and Cheng, K.C. (2005). SLC24A5, a putative cation exchanger, affects pigmentation in zebrafish and humans. Science, 310, 1782–1786.\n\nMcBrearty, S. and Jablonski, N.G. (2005). First fossil chimpanzee. Nature, 437, 105–108.\n\nMekel-Bobrov, N., Posthuma, D., Gilbert, S.L., Lind, P., Gosso, M.F., Luciano, M., Harris, S.E., Bates, T.C., Polderman, T.J.C., Whalley, L.J., Fox, H., Starr, J.M., Evans, P.D., Montgomery, G.W., Fernandes, C., Heutink, P., Martin, N.G., Boomsma, D.I., Deary, I.J., Wright, M.J., de Geus, E.J.C. and Lahn, B.T. (2007) The ongoing adaptive evolution of ASPM and Microcephalin is not explained by increased intelligence. Human Molecular Genetics 16, 600–608.\n\nMekel-Bobrov, N., Gilbert, S.L., Evans, P.D., Vallender, E.J., Anderson, J.R., Hudson, R.R., Tishkoff, S.A., and Lahn, B.T. (2005). Ongoing adaptive evolution of ASPM, a brain size determinant in Homo sapiens. Science, 309, 1720–1722.\n\nMuller, H.J. (1950). Our load of mutations. Am. J. Human Genet., 2, 111–176.\n\nPatterson, N., Richter, D.J., Gnerre, S., Lander, E.S., and Reich, D. (2006). Genetic evidence for complex speciation of humans and chimpanzees. Nature, 441, 1103–1108.\n\nSilver, L. (1998). Remaking Eden (New York: Harper).\n\nSingh-Manoux, A., Marmot, M.G., and Adler, N.E. (2005). Does subjective social status predict health and change in health status better than objective status? Psychosomatic Medicine, 67, 855–861.\n\nSokolowski, M.B., Pereira, H.S., and Hughes, K. (1997). Evolution of foraging behavior in Drosophila by density-dependent selection. Proc. Natl. Acad. Sci. (USA), 94, 7373–7377.\n\nSpira, A. and Multigner, L. (1998). Environmental factors and male infertility. Human Reprod., 13, 2041 -2042.\n\nSurovell, T., Waguespack, N., and Brantingham, P.J. (2005). Global archaeological evidence for proboscidean overkill. Proc. Natl. Acad. Sci. (USA), 102, 6231–6236.\n\nTagliatela, J.P., Savage-Rumbaugh, S., and Baker, L.A. (2003). Vocal production by a language-competent Pan paniscus. Int. J. Primatol., 24, 1 -17.\n\nWard, P. and Brownlee, D. (2000). Rare Earth: Why Complex Life Is Uncommon in the Universe (New York: Copernicus Books).\n\nWhite, T.D., Suwa, G., and Asfaw, B. (1994). Australopithecus ramidus, a new species of early hominid from Aramis, Ethiopia. Nature, 371, 306–312.\n\nWilliams, R.S. and Ferrigno, J. (1999). Estimated present-day area and volume of glaciers and maximum sea level rise potential. pp. 1–10. Satellite Image Atlas of Glaciers of the World. Washington DC: US Geological Survey.\n\nWills, C. (1993). The Runaway Brain: The Evolution of Human Uniqueness (New York: Basic Books).\n\nWills, C. (1998). Children of Prometheus: The Accelerating Pace of Human Evolution (Reading, MA: Perseus Books [formerly Addison-Wesley]).\n\nWills, C., Harms, K.E., Condit, R., King, D., Thompson, J., He, F., Muller-Landau, H.C., Ashton, P., Losos, E., Comita, L., Hubbell, S., LaFrankie, J., Bunyavejchewin, S., Dattaraja, H.S., Davies, S., Esufali, S., Foster, R., Gunatilleke, N., Gunatilleke, S., Hall, P., Itoh, A., John, R., Kiratiprayoon, S., de Lao, S.L., Massa, M., Nath, C., Noor, M.N.S., Kassim, A.R., Sukumar, R., Suresh, H.S., Sun, I.-F., Tan, S., Yamakura, T., and Zimmerman, J. (2006). Nonrandom processes maintain diversity in tropical forests. Science, 311, 527–531.\n\nZhang, H.-X. and Blumwald, E. (2001). Transgenic salt-tolerant tomato plants accumulate salt in foliage but not in fruit. Nat. Biotechnol., 19, 765–768.\n\nZink, R.M. and Slowinski, J.B. (1995). Evidence from molecular systematics for decreased avian diversification in the Pleistocene epoch. Proc. Natl. Acad. Sci. (USA), 92, 5832–5835.\n\n• 4 • Millennial tendencies in responses to apocalyptic threats\n\nJames J. Hughes\n\n4.1 Introduction\n\nAaron Wildavsky proposed in 1987 that cultural orientations such as egalitarianism and individualism frame public perceptions of technological risks, and since then a body of empirical research has grown to affirm the risk-framing effects of personality and culture (Dake, 1991; Gastil et al., 2005; Kahan, 2008). Most of these studies, however, have focused on relatively mundane risks, such as handguns, nuclear power, genetically modified food, and cellphone radiation. In the contemplation of truly catastrophic risks – risks to the future of the species from technology or natural threats – a different and deeper set of cognitive biases come into play, the millennial, utopian, or apocalyptic psychocultural bundle, a characteristic dynamic of eschatological beliefs and behaviours. This essay is an attempt to outline the characteristic forms millennialism has taken, and how it biases assessment of catastrophic risks and the courses of action necessary to address them.\n\nMillennialism is the expectation that the world as it is will be destroyed and replaced with a perfect world, that a redeemer will come to cast down the evil and raise up the righteous (Barkun, 1974; Cohn, 1970). Millennialism is closely tied to other historical phenomena, utopianism, apocalypticism, messianism, and millenarian violence. Western historians of millenialism have focused the most attention on the emergence of Christianity out of the messianic expectations of subjugated Jewry and subsequent Christian movements based on exegesis of the Book of Revelations expecting imminent return of Christ. But the millennial impulse is pancultural, found in many guises and with many common tropes from Europe to India to China, across the last several thousand years. When Chinese peasants followed religio-political revolutionaries claiming the mantle of the Coming Buddha, and when Mohammed birthed Islam preaching that the Last Judgement was imminent, they exhibited many similar features to medieval French peasants leaving their fields to follow would-be John the Baptists. Nor is the millennial impulse restricted to religious movements and beliefs in magical or supernatural agency. Revolutionary socialism and fascism embodied the same impulses and promises, although purporting to be based on science, das Volk, and the secular state instead of prophecy, the body of believers, and the Kingdom of Heaven (Rhodes, 1980; Rowley, 1983).\n\nIn this essay I will review some of the various ways in which the millennial impulse has manifested. Then I will parse contemporary secular expectations about catastrophic risks and utopian possibility for signs of these characteristic millennial dynamics. Finally, I will suggest that by avoiding the undertow of the psychocultural dysfunctions and cognitive biases that often accompany millennialism, we may be able to better anticipate the real benefits and threats that we face in this era of accelerating change and take appropriate prophylactic action to ensure a promising future for the human race.\n\n4.2 Types of millennialism\n\nWestern scholars have pointed to three theological positions among Christian millennialists that appear to have some general applicability to millennial typology, based on the role of human agency in ending Tribulations and bringing the Millennium.\n\n4.2.1 Premillennialism\n\nPremillennialism is the most familiar form of millennial thought in the United States and Europe today, characterized by the belief that everything will get awful before the millennium makes them better (Whalen, 2000). Christian premillennialists, among them many early Christians, have based their eschatological expectations on the Book of Revelations. They believed that the Antichrist will preside over a period of Tribulations, followed by God’s rescue of the righteous, the Rapture. Eventually Christ returns, defeats evil, judges all resurrected souls, and establishes a reign of the Kingdom of Heaven on Earth. Saved people will spend eternity in this new kingdom, and the unsaved will spend an eternity in damnation.\n\nThis doctrine was reintroduced among Protestants in the 1830s in the United States as ‘dispensationalism’ (Boyer, 1994; Crutchfield, 1992). Some dispensationalists became ‘Millerites’, influenced by the exegetical efforts of a nineteenth century lay scholar, William Miller, to interpret contemporary events as a fulfilment of a prophetic timeline in the Book of Revelations. Dispensationalism and Millerism inspired some successful sects that have flourished to this day, such as the Seventh-day Adventists and the Jehovah’s Witnesses, despite their failed prophecies of specific dates for the Second Coming.\n\nPremillennialism gained general acceptance among Christian evangelicals only in the twentieth century. Hal Lindsey’s The Late Great Planet Earth (1970) popularized the thinking of modern Millerites who saw the European Union, the re-creation of the state of Israel and other modern trends as fulfilment of the millennial timeline. Christian Right politicians such as Pat Robertson, a former Republican candidate for President, are premillennialists, interpreting daily events in the Middle East for the millions in their television and radio audience through the lens of Biblical exegesis. Premillennialism is also the basis of the extremely popular Left Behind novels by Jerry Jenkins and Tim LaHaye, and their film adaptations.\n\nPremillennialists are generally fatalists who do not believe human beings can influence the timing or outcome of the Tribulations, Rapture, and Second Coming (Wojcik, 1997). The best the believer can do is to save as many souls as possible before the end. A very similar doctrine can be found among certain Mahayana Buddhists who held that, after the passing of each Buddha, the world gradually falls into the age of mappo or the degeneracy of the Buddhist teachings. In the degenerate age enlightenment is nearly impossible, and the best that we can hope for is the intercession of previously enlightened, and now divine, beings to bring us to a Pure Land (Blum, 2002). The Japanese monk Nichiren Daishonin (1222-1282) founded one of the most successful schools of Japanese Buddhism using the idea of mappo as the rationale for his new dispensation. For Nichiren Buddhists, Japan will play a millennial role in the future as the basis for the conversion of the entire world to the Buddhist path (Stone, 1985).\n\nIn a secular context, Marxist futurism has often been appropriated into a form of premillennial expectation. According to classical Marxist eschatology the impersonal workings of capitalism and technological innovation will immiserate all the people of the world, wipe out all pre-existing identities and institutions, and then unite the world into revolutionary working-class movement. The role of self-conscious revolutionaries is only to explain this process to workers so that they understand their role in the unfolding of the inevitable historical telos and hastening the advent of the millennial worker’s paradise.\n\n4.2.2 Amillennialism\n\nAmillennialists believe that the millennial event has already occurred, or is occurring, in the form of some movement or institution, even though there are still bad things happening in the world (Hoekema, 2007; Riddlebarger, 2003). For Christian amillennialists the Millennium is actually the ongoing establishment of righteousness on Earth through the agency of the Church, struggling to turn back Satan and the Tribulations. Augustinian amillennialism was made the official doctrine of the early Christian Church, and premillennialism was declared heresy. The subsequent millenarian rebellions against church and civil authority, inspired by the Book of Revelations, reinforced the Catholic Church’s insistence that the Millennium would not be an abrupt, revolutionary event, but the gradual creation of the Kingdom of Heaven in each believer’s heart in a church-ruled world. In abstract, when the ‘Church Age’ ends Christ will return to judge humanity and take the saved to heaven for eternity, but attempts to predict the timeline are proscribed.\n\nAnother, more radical and recent example of amillennialism was found in the Oneida Community (1848-1881) in upstate New York, founded by John Humphrey Noyes (Klaw, 1993). Noyes believed that the Second Coming had occurred in 70 AD, and that all believers should begin to live as if in the Kingdom of Heaven, including forbidding monogamy and private property. Orthodox Communist or Maoists outside of the Soviet Union or China can be seen as secular amillennialists, believing Stalin’s and Mao’s regimes were paradises in which eventually all humanity would be able to share.\n\n4.2.3 Post-millennialism\n\nPost-millennialists believe that specific human accomplishments are necessary to bring the Millennium (Bock, 1999). Post-millennialist Christians argue that if Christians establish the Kingdom of Heaven on Earth through Christian rule this will hasten or be synonymous with the second coming. This doctrine has inspired both progressive ‘Social Gospel’ theologies, such as slavery abolitionism and anti-alcohol temperance, as well as theocratic movements such as the contemporary far Right ‘Christian Reconstruction’ and Dominionism.\n\nThe Buddhist Pali canon scriptures that describe the coming Buddha, Maitreya, are an example of post-millennialism (Hughes, 2007). The scripture foretells that humanity, repulsed by the horrors of an apocalyptic war, will build a utopian civilization thickly populated with billions of happy, healthy people who live for thousands of years in harmony with one another and with nature. The average age at marriage will be 500 years. The climate will always be good, neither too hot nor too cold. Wishing trees in the public squares will provide anything you need. The righteous king dissolves the government and turns over the property of the state to Maitreya. These millennial beliefs inspired a series of Buddhist uprisings in China (Naquin, 1976), and helped bring the Buddhist socialist movement in Burma to power from 1948 to 1962 (Malalgoda, 1970).\n\nThis worldview also corresponds to revolutionary Marxist-Leninism. Although the march of history is more or less assured, the working class may wander for centuries through the desert until the revolutionary vanguard can lead them to the Promised Land. Once socialism is established, it will gradually evolve into communism, in which the suppressed and distorted true human nature will become non-acquisitive and pro-social. Technology will provide such abundance that conflict over things will be unnecessary, and the state will wither. But revolutionary human agency is necessary to fulfil history.\n\n4.3 Messianism and millenarianism\n\nMessianism and millenarianism are defined here as forms of millennialism in which human agency, magical or revolutionary, is central to achieving the Millennium. Messianic movements focus on a particular leader or movement, while millenarian movements, such as many of the peasant uprisings of fifteenth and sixteenth Europe, believe revolutionary violence to smash the evil old order will help usher in the Millennium (Rhodes, 1980; Smith, 1999; Mason, 2002). Al Qaeda is, for instance, rooted in Islamic messianic eschatology; the Jihad to establish the global Caliphate is critical in the timeline for the coming of the Mahdi, or messiah. Osama bin Laden is only the latest of a long line of Arab leaders claiming, or being ascribed, the mantle of Mahdism (Cook, 2005; Furnish, 2005). In the Hindu and Buddhist messianic tradition there is the belief in the periodic emergence of Maha Purushas, ‘great men’, who arrive in times of need to provide either righteous rule or saintliness. Millenarian uprisings in China were often led by men claiming to be Maitreya, the next Buddha. Secular messiahs and revolutionary leaders are similarly often depicted in popular mythology as possessing extraordinary wisdom and abilities from an early age, validating their unique eschatological role. George Washington could never tell a lie and showed superhuman endurance at Valley Forge, whereas Chairman Mao was a modern Moses, leading his people on a Long March to Zion and guiding the masses with the wisdom in his Little Red Book.\n\n4.4 Positive or negative teleologies: utopianism and apocalypticism\n\nUtopianism and apocalypticism are defined here as the millennial impulse with, respectively, an optimistic and pessimistic eschatological expectation. By utopianism I mean the belief that historical trends are inevitably leading to a wonderful millennial outcome (Manuel and Manuel, 1979), including the Enlightenment narrative of inevitable human progress (Nash, 2000; Tuveson, 1949). By apocalypticism I do not mean simply the belief that something very bad may happen, since very bad events are simply a prelude to very good events for most millennialists, but that the bad event will be cataclysmic, or even the end of history.\n\nIn that sense, utopianism is the default setting of most millennial movements, even if the Tribulations are expected to be severe and indeterminately long. The promise of something better, at least for the righteous, is far more motivating than a guaranteed bad end. Even the most depressing religious eschatology, the Norse Ragnarok – at which humans and gods are defeated, and Earth and the heavens are destroyed – holds out a millennial promise that a new earth and Sun will emerge, and the few surviving gods and humans with live in peace and prosperity (Crossley-Holland, 1981).\n\nMillennial expectations of better times have not only been a comfort to people with hard, sad lives, an ‘opium for the masses’, but also, because of their mobilizing capacity, an essential catalyst of social change and political reform (Hobsbawm, 1959; Jacoby, 2005; Lanternari, 1965). From Moses’ mobilization of enslaved Jewry with a promise of a land of milk and honey, to medieval millenarian peasant revolts, to the Sioux Ghost Dance, to the integrationist millennialism of the African-American Civil Rights Movement, millenarian leaders have arisen out of repressive conditions to preach that they could lead their people to a new Zion. Sometimes the millennial movements are disastrously unsuccessful when they rely on supernatural methods for achieving their ends, as with the Ghost Dance (Mooney, 1991). Sometimes utopian and millennial currents contribute to social reform even in their defeat, as they did from the medieval peasant revolts through the rise of revolutionary socialism (Jacoby, 2005). Although movements for utopian social change were most successful when they focused on temporal, rather than millennial, goals through human, rather than supernatural, agency, expectations of utopian outcomes helped motivate participants to take risks on collective action against large odds.\n\nAlthough there have been few truly apocalyptic movements or faiths, those which foretell an absolute, unpleasant and unredeemed end of history, there have been points in history with widespread apocalyptic expectation. The stories of the Biblical flood and the destruction of Sodom and Gomorrah alerted Christians to the idea that God was quite willing to destroy almost all of humanity for our persistent sinfulness, well before the clock starts on the Tribulation-Millennium timeline. Although most mythic beliefs include apocalyptic periods in the past and future, as with Ragnarok or the Hindu-Buddhist view of a cyclical destruction – recreation of the universe, most myths make apocalypse a transient stage in human history.\n\nIt remained for more secular times for the idea of a truly cataclysmic end of history, with no redeeming Millennium, to become a truly popular current of thought (Heard, 1999; Wagar, 1982; Wojcik, 1997, 1999). Since the advent of the Nuclear Age, one apocalyptic threat after another, natural and man-made, has been added to the menu of ways that human history could end, from environmental destruction and weapons of mass destruction, to plague and asteroid strikes (Halpern, 2001; Leslie, 1998; Rees, 2004). In a sense, long-term apocalypticism is also now the dominant scientific worldview, insofar as most scientists see no possibility for intelligent life to continue after the Heat Death of the Universe (2002, Ellis; see also the Chapter 2 in this volume).\n\n4.5 Contemporary techno-millennialism\n\n4.5.1 The singularity and techno-millennialism\n\nJoel Garreau’s (2006) recent book on the psychoculture of accelerating change, Radical Evolution: The Promise and Peril of Enhancing Our Minds, Our Bodies – and What It Means to Be Human, is structured in three parts: Heaven, Hell and Prevail. In the Heaven scenario he focuses on the predictions of inventor Ray Kurzweil, summarized in his 2005 book, The Singularity Is Near. The idea of a techno-millennial ‘Singularity’ was coined in a 1993 paper by mathematician and science fiction author Vernor Vinge. In physics ‘singularities’ are the centres of black holes, within which we cannot predict how physical laws will work. In the same way, Vinge said, greater-than-human machine intelligence, multiplying exponentially, would make everything about our world unpredictable. Most Singularitarians, like Vinge and Kurzweil, have focused on the emergence of superhuman machine intelligence. But thbe even more fundamental concept is exponential technological progress, with the multiplier quickly leading to a point of radical social crisis. Vinge projected that self-willed artificial intelligence would emerge within the next 30 years, by 2023, with either apocalyptic or millennial consequences. Kurzweil predicts the Singularity for 2045.\n\nThe most famous accelerating trend is ‘Moore’s Law’, articulated by Intel co-founder Gordon Moore in 1965, which is the observation that the number of transistors that can be fit on a computer chip has doubled about every 18 months since their invention. Kurzweil goes to great lengths to document that these trends of accelerating change also occur in genetics, mechanical miniaturization, and telecommunications, and not just in transistors. Kurzweil projects that the ‘law of accelerating returns’ from technological change is ‘so rapid and profound it represents a rupture in the fabric of human history’. For instance, Kurzweil predicts that we will soon be able to distribute trillions of nanorobots in our brains, and thereby extend our minds, and eventually upload our minds into machines. Since lucky humans will at that point merge with superintelligence or become superintelligent, some refer to the Singularity as the ‘Techno-rapture’, pointing out the similarity of the narrative to the Christian Rapture; those foresighted enough to be early adopters of life extension and cybernetics will live long enough to be uploaded and ‘vastened’ (vastly mental abilities) after the Singularity. The rest of humanity may however be ‘left behind’.\n\nThis secular ‘left behind’ narrative is very explicit in the Singularitarian writings of computer scientist Hans Moravec (1990, 2000). For Moravec the human race will be superseded by our robot children, among whom some of us may be able to expand to the stars. In his Robot: Mere Machine to Transcendent Mind, Moravec (2000, pp. 142–162) says\n\n‘Our artificial progeny will grow away from and beyond us, both in physical distance and structure, and similarity of thought and motive. In time their activities may become incompatible with the old Earth’s continued existence … An entity that fails to keep up with its neighbors is likely to be eaten, its space, materials, energy, and useful thoughts reorganized to serve another’s goals. Such a fate may be routine for humans who dally too long on slow Earth before going Ex.\n\nHere we have Tribulations and damnation for the late adopters, in addition to the millennial utopian outcome for the elect.\n\nAlthough Kurzweil acknowledges apocalyptic potentials – such as humanity being destroyed by superintelligent machines – inherent in these technologies, he is nonetheless uniformly utopian and enthusiastic. Hence Garreau’s labelling Kurzweil’s the ‘Heaven’ scenario. While Kurzweil (2005) acknowledges his similarity to millennialists by, for instance, including a tongue-in-cheek picture in The Singularity Is Near of himself holding a sign with that slogan, referencing the classic cartoon image of the EndTimes street prophet, most Singularitarians angrily reject such comparisons insisting their expectations are based solely on rational, scientific extrapolation.\n\nOther Singularitarians, however, embrace parallels with religious millennialism. John Smart, founder and director of the California-based Acceleration Studies Foundation, often notes the similarity between his own ‘Global Brain’ scenario and the eschatological writings of the Jesuit palaeontologist Teilhard de Chardin (2007). In the Global Brain scenario, all human beings are linked to one another and to machine intelligence in the emerging global telecommunications web, leading to the emergence of collective intelligence. This emergent collectivist form of Singularitarianism was proposed also by Peter Russell (1983) in The Global Brain, and Gregory Stock (1993) in Metaman. Smart (2007) argues that the scenario of an emergent global human-computer meta-mind is similar to Chardin’s eschatological idea of humanity being linked in a global ‘noosphere’, or info-sphere, leading to a post-millennial ‘Omega Point’ of union with God. Computer scientist Juergen Schmidhuber (2006) also has adopted Chardin’s ‘Omega’ to refer to the Singularity.\n\nFor most Singularitarians, as for most millennialists, the process of technological innovation is depicted as autonomous of human agency, and wars, technology bans, energy crises or simple incompetence are dismissed as unlikely to slow or stop the trajectory. Kurzweil (2006) insists, for instance, that the accelerating trends he documents have marched unhindered through wars, plagues and depressions. Other historians of technology (Lanier, 2000; Seidensticker, 2006; Wilson, 2007) argue that Kurzweil ignores techno-trends which did stall, due to design challenges and failures, and to human factors that slowed the diffusion of new technologies, factors which might also slow or avert greater-than-human machine intelligence. Noting that most predictions of electronic transcendence fall within the predictor’s expected lifespan, technology writer Kevin Kelly (2007)suggests that people who make such predictions have a cognitive bias towards optimism.\n\nThe point of this essay is not to parse the accuracy or empirical evidence for exponential change or catastrophic risks, but to examine how the millennialism that accompanies their consideration biases assessment of their risks and benefits, and the best courses of action to reduce the former and ensure the latter. There is of course an important difference between fear of a civilization-ending nuclear war, grounded in all-too-real possibility, and fear of the end of history from a prophesied supernatural event. I do not mean to suggest that all discussion of utopian and catastrophic possibilities are merely millennialist fantasies, but rather that recognizing millennialist dynamics permits more accurate risk/benefit assessments and more effective prophylactic action.\n\n4.6 Techno-apocalypticism\n\nIn Radical Evolution Joel Garreau’s Hell scenario is centred on the Luddite apocalypticism of the techno-millennial apostate, Bill Joy, former chief scientist and co-founder of Sun Microsystems. In the late 1990s Joy began to believe that genetics, robotics, and nanotechnology posed novel apocalyptic risks to human life. These technologies, he argued, posed a different kind of threat because they could self-replicate; guns do not breed and shoot people on their own, but a rogue bioweapon could. His essay ‘Why the Future Doesn’t Need Us,’ published in April 2000 in Wired magazine, called for a global, voluntary ‘relinquishment’ of these technologies.\n\nGreens and others of an apocalyptic frame of mind were quick to seize on Joy’s essay as an argument for the enacting of bans on technological innovation, invoking the ‘precautionary principle’, the idea that a potentially dangerous technology should be fully studied for its potential impacts before being deployed. The lobby group ETC argued in its 2003 report ‘The Big Down’ that nanotechnology could lead to a global environmental and social catastrophe, and should be placed under government moratorium. Anxieties about the apocalyptic risks of converging bio-, nano- and information technologies have fed a growing Luddite strain in Western culture (Bailey 2001a, 2001b), linking Green and anarchist advocates for neo-pastoralism (Jones, 2006; Mander, 1992; Sale, 2001; Zerzan, 2002) to humanist critics of techno-culture (Ellul, 1967; Postman, 1993; Roszak, 1986) and apocalyptic survivalists to Christian millennialists. The neo-Luddite activist Jeremy Rifkin has, for instance, built coalitions between secular and religious opponents of reproductive and agricultural biotechnologies, arguing that the encroachment into the natural order will have apocalyptic consequences. Organizations such as the Chicago-based Institute on Biotechnology &The Human Future, which brings together bio- and nano-critics from the Christian Right and the secular Left, represent the institutionalization of this new Luddite apocalypticism advocating global bans on ‘genocidal’ lines of research (Annas, Andrews, and Isasi, 2002).\n\nJoy has, however, been reluctant to endorse Luddite technology bans. Joy and Kurzweil are entrepreneurs and distrust regulatory solutions. Joy and Kurzweil also share assumptions about the likelihood and timing of emerging technologies, differing only in their views on the likelihood of millennial or apocalyptic outcomes. But they underlined their similarity of worldview by issuing a startling joint statement in 2005 condemning the publication of the genome of the 1918 influenza virus, which they viewed as a cookbook for a potential bioterror weapon (Kurzweil and Joy, 2005). Disturbing their friends in science and biotech, leery of government mandates for secrecy, they called for ‘international agreements by scientific organizations to limit such publications’ and ‘a new Manhattan Project to develop specific defences against new biological viral threats’.\n\nIn the 1990s anxieties grew about the potential for terrorists to use recombinant bioengineering to create new bioweapons, especially as bioweapon research in the former Soviet Union came to light. In response to these threats the Clinton administration and US Congress started major bioterrorism preparedness initiatives in the 1990s, despite warnings from public health advocates such as Laurie Garrett (1994, 2000) that monies would be far better spent on global public health initiatives to prevent, detect, and combat emerging infectious diseases. After 9/11 the Bush administration, motivated in part by the millennial expectations of both the religious Right and secular neo-conservatives, focused even more attention on the prevention of relatively low probability/low lethality bioterrorism than on the higher probability/lethality prospects of emerging infectious diseases such pandemic flu. Arguably apocalyptic fears around bioterrorism, combined with the influence of the neo-conservatives and biotech lobbies, distorted public health priorities. Perhaps conversely we have not yet had sufficient apocalyptic anxiety about emerging plagues to force governments to take a comprehensive, proactive approach to public health. (Fortunately efforts at infectious disease monitoring, gene sequencing and vaccine production are advancing nonetheless; a year after Kurzweil and Joy’s letter a team at the US National Institutes of Health had used the flu genome to develop a vaccine for the strain [NIH, 2006].)\n\nAn example of a more successful channelling of techno-apocalyptic energies into effective prophylaxis was the Millennium Bug or Y2K phenomenon. In the late 1990s a number of writers began to warn that a feature of legacy software systems from the 1960s and 1970s, which coded years with two digits instead of four, would lead to widespread technology failure in the first seconds of 2000. The chips controlling power plants, air traffic, and the sluice gates in sewer systems would suddenly think the year was 1900 and freeze. Hundreds of thousands of software engineers around the world were trained to analyse 40-year-old software languages and rewrite them. Hundreds of billions of dollars were spent worldwide on improving information systems, disaster preparedness, and on global investment in new hardware and software, since it was often cheaper simply to replace than to repair legacy systems (Feder, 1999; Mussington, 2002). Combined with the imagined significance of the turn of the Millennium, Christian millennialists saw the crisis as a portent of the EndTimes (Schaefer, 2004), and secular apocalyptics bought emergency generators, guns and food in anticipation of a prolonged social collapse (CNN, 1998; Kellner, 1999; Tapia, 2003). Some anti-technology Y2K apocalyptics argued for widespread technological relinquishment – getting off the grid and returning to a nineteenth century lifestyle.\n\nThe date 1 January 2000 was as unremarkable as all predicted millennial dates have been, but in this case, many analysts believe potential catastrophes were averted due to the proactive action from governments, corporations, and individual consumers (U.S. Senate, 2000), motivated in part by millennial anxieties. Although the necessity and economic effects of pre-Y2K investments in information technology modernization remain controversial, some subsequent economic and productivity gains were probably accrued (Kliesen, 2003). Althoughthe size and cost of the Y2K preparations may not have been optimal, the case is still one of proactive policy and technological innovation driven in part by millennial/apocalyptic anxiety. Similar dynamics can be observed around the apocalyptic concerns over ‘peak oil’, ‘climate change’, and the effects of environmental toxins, which have helped spur action on conservation, alternative energy sources, and the testing and regulation of novel industrial chemicals (Kunstler, 2006).\n\n4.7 Symptoms of dysfunctional millennialism in assessing future scenarios\n\nSome critics denigrate utopian, millennial, and apocalyptic impulses, both religious and secular, seeing them as irrational at best, and potentially murderous and totalitarian at worst. They certainly can manifest in the dangerous and irrational ways as I have catalogued in this essay. But they are also an unavoidable accompaniment to public consideration of catastrophic risks and techno-utopian possibilities. We may aspire to a purely rational, technocratic analysis, calmly balancing the likelihoods of futures without disease, hunger, work or death, on the one hand, against the likelihoods of worlds destroyed by war, plagues or asteroids, but few will be immune to millennial biases, positive or negative, fatalist or messianic. Some of these effects can be positive. These mythopoetic interpretations of the historical moment provide hope and meaning to the alienated and lost. Millennialist energies can overcome social inertia and inspire necessary prophylaxis and force recalcitrant institutions to necessary action and reform. In assessing the prospects for catastrophic risks, and potentially revolutionary social and technological progress, can we embrace millennialism and harness its power without giving in to magical thinking, sectarianism, and overly optimistic or pessimistic cognitive biases?\n\nI believe so; understanding the history and manifestations of the millennial impulse, and scrutinizing even our most purportedly scientific and rational ideas for their signs, should provide some correction for their downsides. Based on the discussion so far, I would identify four dysfunctional manifestations of millennialism to watch for. The first two are the manic and depressive errors of millennialism, tendencies to utopian optimism and apocalyptic pessimism. The other two dysfunctions have to do with the role of human agency, a tendency towards fatalist passivity on the one hand, believing that human action can have no effect on the inevitable millennial or apocalyptic outcomes, and the messianic tendency on the other hand, the conviction that specific individuals, groups, or projects have a unique historical role to play in securing the Millennium.\n\nOf course, one may acknowledge these four types of millennialist biases without agreeing whether a particular assessment or strategy reflects them. A realistic assessment may in fact give us reasons for great optimism or great pessimism. Apocalyptic anxiety during the 1962 Cuban missile confrontation between the United States and the Soviet Union was entirely warranted, whereas historical optimism about a New World Order was understandable during the 1989–1991 collapse of the Cold War. Sometimes specific individuals (Gandhis, Einsteins, Hitlers, etc.) do have a unique role to play in history, and sometimes (extinction from gamma-ray bursts from colliding neutron stars or black holes) humanity is completely powerless in the face of external events. The best those who ponder catastrophic risks can do is practise a form of historically informed cognitive therapy, interrogating our responses to see if we are ignoring counterfactuals and alternative analyses that might undermine our manic, depressive, fatalist, or messianic reactions.\n\nOne symptom of dysfunctional millennialism is often dismissal of the possibility that political engagement and state action could affect the outcome of future events. Although there may be some trends or cataclysms that are beyond all human actions, all four millennialist biases – utopian, apocalyptic, fatalist, and messianic – underestimate the potential and importance of collective action to bring about the Millennium or prevent apocalypse. Even messianists are only interested in public approbation of their own messianic mission, not winning popular support for a policy. So it is always incumbent on us to ask how engaging with the political process, inspiring collective action, and changing state policy could steer the course of history. The flip side of undervaluing political engagement as too uncertain, slow, or ineffectual is a readiness to embrace authoritarian leadership and millenarian violence in order to achieve quick, decisive, and far-sighted action.\n\nMillennialists also tend to reduce the complex socio-moral universe into those who believe in the eschatological worldview and those who do not, which also contributes to political withdrawal, authoritarianism and violence. For millennialists society collapses into friends and enemies of the Singularity, the Risen Christ, or the Mahdi, and their enemies may be condemning themselves or all of humanity to eternal suffering. Given the stakes on the table – the future of humanity – enemies of the Ordo Novum must be swept aside. Apostates and the peddlers of mistaken versions of the salvific faith are even more dangerous than outright enemies, since they can fatally weaken and mislead the righteous in their battle against Evil. So the tendency to demonize those who deviate can unnecessarily alienate potential allies and lead to tragic violence. The Jones Town suicides, the Oklahoma City bombing, Al Qaeda, and Aum Shinrikyo are contemporary examples of a millennial logic in which the murder is required to fight evil and heresies, and wake complacent populations to imminent millennial threats or promises (Hall, 2000; Mason, 2002; Whitsel, 1998). Whenever contemporary millenarians identify particular scientists, politicians, firms, or agencies as playing a special role in their eschatologies, as specific engineers did for the Unabomber, we can expect similar violence in the future. A more systemic and politically engaged analysis, on the other hand, would focus on regulatory approaches addressed at entire fields of technological endeavours rather than specific actors, and on the potential for any scientist, firm, or agency to contribute to both positive and negative outcomes.\n\n4.8 Conclusions\n\nThe millennial impulse is ancient and universal in human culture and is found in many contemporary, purportedly secular and scientific, expectations about the future. Millennialist responses are inevitable in the consideration of potential catastrophic risks and are not altogether unwelcome. Secular techno-millennials and techno-apocalyptics can play critical roles in pushing reluctant institutions towards positive social change or to enact prophylactic policies just as religious millennialists have in the past. But the power of millennialism comes with large risks and potential cognitive errors which require vigilant self-interrogation to avoid.\n\nSuggestions for further reading\n\nBaumgartner, F.J. (1999). Longing for the End: A History of Millennialism in Western Civilization (London: St. Martin’s Press). A history of apocalyptic expectations from Zoroastrianism to Waco.\n\nCohn, N. (1999). The Pursuit of the Millennium: Revolutionary Millenarians and Mystical Anarchists of the Middle Ages (New York: Oxford University Press, 1961, 1970, 1999). The classic text on medieval millennialism. Devotes much attention to Communism and Nazism.\n\nHeard, A. (1999). Apocalypse Pretty Soon: Travels in End-time America (New York: W. W. Norton &Company). A travelogue chronicling a dozen contemporary millennial groups, religious and secular, from UFO cults and evangelical premillennialists to transhumanists and immortalists.\n\nLeslie, J. (1998). The End of the World: The Science and Ethics of Human Extinction. London, New York. Routledge. A catalogue of real apocalyptic threats facing humanity.\n\nManuel, F.E. and Fritzie, P.M. (1979). Utopian Thought in the Western World (Cambridge: Harvard University Press). The classic study of utopian thought and thinkers, from Bacon to Marx.\n\nNoble, D. (1998). The Religion of Technology: The Divinity of Man and the Spirit of Invention (New York: Alfred A. Knopf). A somewhat overwrought attempt to unveil the millennial and religious roots of the space programme, artificial intelligence, and genetic engineering. Argues that there is a continuity of medieval pro-technology theologies with contemporary techno-millennialism.\n\nOlson, T. (1982). Millennialism, Utopianism, and Progress (Toronto: University of Toronto Press). Argues millennialism and utopianism were separate traditions that jointly shaped the modern secular idea of social progress, and post-millennialist Social Gospel religious movements.\n\nReferences\n\nAnnas, G.J., Andrews, L., and Isasi, R. (2002). Protecting the endangered human: toward an international treaty prohibiting cloning and inheritable alterations. Am. J. Law Med., 28, 151–178.\n\nBailey, R. (2001a). Rebels against the future: witnessing the birth of the global anti-technology movement. Reason, 28 February. http://www.reason.com/news/show/34773.html\n\nBailey, R. (2001b). Rage against the machines: witnessing the birth of the neo-Luddite movement. Reason, July. http://www.reason.com/news/show/28102.html\n\nBarkun, M. (1974). Disaster and the Millennium (New Haven, IN: Yale University Press).\n\nBlum, M.L. (2002). The Origins and Development of Pure Land Buddhism (New York: Oxford University Press).\n\nBock, D.L. (1999). Three Views on the Millennium and Beyond. Premillennialism, Postmillennialism, Amillennialism. Zondervan. Grand Rapids, Michigca (Mi).\n\nBoyer, P. (1994). When Time Shall Be No More: Prophecy Belief in Modern American Culture. Belknap, Cambridge MA.\n\nCamp, G.S. (1997). Selling Fear: Conspiracy Theories and End-time Paranoia. Baker. Grand Rapids MI.\n\nCNN. (1998). Survivalists try to prevent millennium-bug bite. October 10. http://www.cnn.com/US/9810/10/y2k.survivalists/\n\nCohn, N. (1970). The Pursuit of the Millennium: Revolutionary Millenarians and Mystical Anarchists of the Middle Ages (New York: Oxford University Press).\n\nCook, D. (2005). Contemporary Muslim Apocalyptic Literature (Syracuse, NY: Syracuse University Press).\n\nCrossley-Holland, K. (1981). The Norse Myths (Tandem Library).\n\nCrutchfield, L. (1992). Origins of Dispensationalism: The Darby Factor (University Press of America).\n\nDake, K. (1991). Orienting dispositions in the perception of risk: an analysis of contemporary worldviews and cultural biases. J. Cross-cultural Psychol., 22, 61–82.\n\nEllis, G.F.R. (2002). The Far-future Universe: Eschatology from a Cosmic Perspective (Templeton Foundation).\n\nEllul, J. (1967). The Technological Society (Vintage).\n\nETC. (2003). The Big Down. ETC. http://www.etcgroup.org/documents/TheBigDown.pdf\n\nFeder, B.J. (1999). On the year 2000 front, humans are the big wild cards. New York Times, 28 December.\n\nFurnish, T. (2005). Holiest Wars: Islamic Mahdis, Their Jihads, and Osama bin Laden (Westport: Praeger).\n\nGarreau, J. (2006). Radical Evolution: The Promise and Peril of Enhancing Our Minds, Our Bodies-and What It Means to Be Human (New York: Broadway).\n\nGarrett, L. (1994). The Coming Plague: Newly Emerging Diseases in a World Out of Balance (New York: Farrar, Straus &Cudahy).\n\nGarrett, L. (2000). Betrayal of Trust: The Collapse of Global Public Health (Hyperion).\n\nGastil, J., Braman, D., Kahan, D.M., and Slovic, P. (2005). The ‘Wildavsky Heuristic’: The Cultural Orientation of Mass Political Opinion. Yale Law School, Public Law Working Paper No. 107. http://ssrn.com/abstract=834264\n\nHall, J.R. (2000). Apocalypse Observed: Religious Movements and Violence in North America, Europe, and Japan (London: Routledge).\n\nHalpern, P. (2001). Countdown to Apocalypse: A Scientific Exploration of the End of the World (New York, NY: Basic Books).\n\nHeard, A. (1999). Apocalypse Pretty Soon: Travels in End-time America (New York: W. W. Norton &Company).\n\nHobsbawm, E.J. (1959). Primitive Rebels: Studies in Archaic Forms of Social Movement in the 19th and 20th Centuries (New York: W. W. Norton &Company).\n\nHoekema, A. (2007). Amillenialism. http://www.the-highway.com/amila\\_Hoekema.html\n\nHughes, J.J. (2007). The compatibility of religious and transhumanist views of metaphysics, suffering, virtue and transcendence in an enhanced future. The Global Spiral, 8(2). http://metanexus.net/magazine/tabid/68/id/9930/Default.aspx\n\nJacoby, R. (2005). Picture Imperfect: Utopian Thought for an Anti-Utopian Age (New York: Columbia University Press).\n\nJones, S. (2006). Against Technology: From the Luddites to Neo-Luddism (NY: Routledge).\n\nJoy, B. (2000). Why the future doesn’t need us. Wired, April. http://www.wired.com/wired/archive/8.04/joy.html\n\nKahan, D.M. (2008). Two conceptions of emotion in risk regulation. University of Pennsylvania Law Review, 156. http://ssrn.com/abstract=962520\n\nKaplan, J. (1997). Radical Religion in America: Millenarian Movements from the Far Right to the Children of Noah (Syracuse, NY: Syracuse University Press).\n\nKatz, D.S. and Popkin, R.H. (1999). Messianic Revolution: Radical Religious Politics to the End of the Second Millennium (New York: Hill and Wang).\n\nKellner, M.A. (1999). Y2K: Apocalypse or Opportunity? (Wheaton Illinois: Harold Shaw Publications).\n\nKelly, K. (2007). The Maes-Garreau Point. The Technium, March 14. http://www.kk.org/thetechnium/archives/2007/03 /the\\_maesgarreau.php\n\nKing, M.L. (1950). The Christian Pertinence of Eschatological Hope. http://www.stanford.edu/group/King/publications/papers/vol1/500215-The\\_Christian\\_Pertinence\\_of\\_Eschatological\\_Hope.htm\n\nKlaw, S. (1993). Without Sin: The Life and Death of the Oneida Community (New York: Allen Lane, Penguin Press).\n\nKliesen, K.L. (2003). Was Y2K behind the business investment boom and bust? Review, Federal Reserve Bank of St. Louis, January: 31–42.\n\nKunstler, J.H. (2006). The Long Emergency: Survivingthe End of Oil, Climate Change, and Other Converging Catastrophes of the Twenty-First Century (New York: Grove Press).\n\nKurzweil, R. (2005). The Singularity is Near (New York: Viking).\n\nKurzweil, R. (2006). Questions and answers on the singularity. Non-Prophet, January 8. http://nonprophet.typepad.com/nonprophet/2006/01/guest\\_blogger\\_r.html\n\nKurzweil, R. and Joy, B. (2005). Recipe for Destruction. New York Times, 17 October. http://www.nytimes.com/2005/10/17/opinion/17kurzweiljoy.html\n\nLanier, J. (2000). One half of a manifesto. Edge, 74. http://www.edge.org/documents/archive/edge74.html\n\nLanternari, V. (1965). The Religions of the Oppressed: A Study of Modern Messianic Cults (New York: Alfred A. Knopf).\n\nLeslie, J. (1998). The End of the World: The Science and Ethics of Human Extinction (London, New York: Routledge).\n\nLindsey, H. (1970). The Late Great Planet Earth (Zondervan).\n\nMalalgoda, K. (1970). Millennialism in Relation to Buddhism. Comp. Studies Soc. History, 12(4), 424–441.\n\nMander, J. (1992). In the Absence of the Sacred: The Failure of Technology and the Survival of the Indian Nations (San Francisco, California: Sierra Club Books).\n\nManuel, F.E. and Manuel, F.P. (1979). Utopian Thought in the Western World (Cambridge, MA: Belknap Press).\n\nMason, C. (2002). Killing for Life: The Apocalyptic Narrative of Pro-life Politics (Cornell: Cornell University Press).\n\nMooney, J. (1991). The Ghost-Dance Religion and the Sioux Outbreak of 1890 (University of Nebraska Press).\n\nMoravec, H. (1990). Mind Children: The Future of Robot and Human Intelligence (Harvard: Harvard University Press).\n\nMoravec, H. (2000). Robot: Mere Machine to Transcendent Mind (OX: Oxford University Press).\n\nMussington, D. (2002). Concepts for Enhancing Critical Infrastructure Protection: Relating Y2K to Cip Research and Development (Washington, DC: Rand Corporation).\n\nNaquin, S. (1976). Millenarian Rebellion in China: The Eight Trigrams Uprising of 1813 (New Haven, CT: Yale University Press).\n\nNash, D. (2000). The failed and postponed millennium: secular millennialism since the enlightenment. J. Religious History, 24(1), 70–86.\n\nNational Institute of Allergy and Infectious Diseases. (2006). Experimental Vaccine Protects Mice Against Deadly 1918 Flu Virus. NIH News. http://www.nih.gov/news/pr/oct2006/niaid-17.htm\n\nPostman, N. (1993). Technopoly: The Surrender of Culture to Technology (NY: Vintage).\n\nRhodes, J.M. (1980). The Hitler Movement: A Modern Millenarian Revolution (Stanford, CA: Hoover Institution Press, Stanford University).\n\nRees, M. (2004). Our Final Hour (New York, NY: Basic Books).\n\nRiddlebarger, K. (2003). A Case for Amillennialism: Understanding the End Times (Grand Rapids, MI: Baker Books).\n\nRoszak, T. (1986). The Cult of Information: A Neo-Luddite Treatise on High-tech, Artificial Intelligence, and the True Art of Thinking (Berkeley: University of California Press).\n\nRowley, D. (1983). Redeemer Empire: Russian Millenarianism. Am. Historical Rev., 104(5). http://www.historycooperative.org/journals/ahr/104.5/ah001582.html\n\nRussell, P. (1983). The Global Brain: Speculation on the Evolutionary Leap to Planetary Consciousness (Los Angels: Tarcher).\n\nSale, K. (2001). Rebels Against the Future: The Luddites and Their War on the Industrial Revolution: Lessons for the Computer Age (New York, NY: Basic Books).\n\nSchaefer, N.A. (2004). Y2K as an endtime sign: apocalypticism in America at the finde-millennium. J. Popular Cult., 38(1), 82–105.\n\nSchmidhuber, J. (2006). New millennium AI and the convergence of history. In Duch, W. and Mandziuk, J. (eds.), Challenges to Computational Intelligence (Berlin: Springer). http://arxiv.org/abs/cs.AI/0606081\n\nSeidensticker, B. (2006). Future Hype: The Myths of Technology Change (San Francisco, CA: Berrett-Koehler Publishers).\n\nSeidensticker, B. (2005). Brief History of Intellectual Discussion Of Accelerating Change. http://arxiv.org/abs/cs.AI/0606081\n\nSmart, J. (2007). Why ‘Design’ (A Universe Tuned for Life and Intelligence) Does Not Require a Designer, and Teleology (a Theory of Destiny) is Not a Theology – Understanding the Paradigm of Evolutionary Development. http://www.accelerationwatch.com/\n\nSmith, C. (1999). ‘Do Apocalyptic World Views Cause Violence?’ The Religious Movements Homepage Project at the University of Virginia. http://religiousmovements.lib.virginia.edu/nrms/millennium/violence.html\n\nSpecial Committee on the Year 2000 Technology Problem. (2000). Y2K Aftermath – Crisis Averted: Final Committee Report. February 2000. U.S. Senate. http://www.senate.gov/~bennett/issues/documents/y2kfinalreport.pdf\n\nStock, G. (1993). Metaman: The Merging of Humans and Machines into a Global Superorganism (New York: Simon &Schuster).\n\nStone, J. (1985). Seeking enlightenment in the last age: Mappo Thought in Kamakura Buddhism. Eastern Buddhist, 18(1), 28–56.\n\nTapia, A.H. (2003). Technomillennialism: a subcultural response to the technological threat of Y2K. Sci., Technol. Human Values, 28(4), 483–512.\n\nThompson, D. (1997). The End of Time: Faith and Fear in the Shadow of the Millennium (Hanover, NH: University Press of New England).\n\nTuveson, E.L. (1949). Millennium and Utopia: A Study in the Background of the Idea of Progress (Berkeley, CA: University of California Press).\n\nVinge, V. (1993). The Coming Technological Singularity: How to Survive in the Post-Human Era. Presented at the VISION-21 Symposium sponsored by NASA Lewis Research Center and the Ohio Aerospace Institute, 30–31 March 1993. http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html\n\nWagar, W.W. (1982). Terminal Visions: The Literature of Last Things (Bloomington: Indiana University Press).\n\nWhalen, R.K. (2000). Premillennialism. In Landes, R.A. (ed.), The Encyclopedia of Millennialism and Millennial Movements, pp. 331 (New York: Routledge).\n\nWhitsel, B. (1998). The Turner Diaries and Cosmotheism: William Pierce’s Theology of Revolution. Nova Religio, 1(2), 183–197.\n\nWilson, D. (2007). Where’s My Jetpack?: A Guide to the Amazing Science Fiction Future that Never Arrived (Bloomsbury).\n\nWildavsky, A.B. (1987). Choosing preferences by constructing institutions: a cultural theory of preference formation. Am. Polit. Sci. Rev., 81, 3–21.\n\nWildavsky, A.B. and Douglas, M. (1982). Risk and Culture: An Essay on the Selection of Technological and Environmental Dangers (Berkeley, CA: University of California Press).\n\nWojcik, D. (1997). The End of the World as We Know It: Faith, Fatalism, and Apocalypse in America (New York: New York University Press).\n\nWojcik, D. (1999). Secular apocalyptic themes in the Nuclear Era. In D. Wojcik (ed.), The End of the World As We Know It: Faith, Fatalism, and Apocalypse in America (New York: New York University Press).\n\nZerzan, J. (2002). Running on Emptiness: The Pathology of Civilization (Los Angeles, CA: Feral House).\n\n• 5 • Cognitive biases potentially affecting judgement of global risks\n\nEliezer Yudkowsky\n\n5.1 Introduction\n\nAll else being equal, not many people would prefer to destroy the world. Even faceless corporations, meddling governments, reckless scientists, and other agents of doom, require a world in which to achieve their goals of profit, order, tenure, or other villainies. If our extinction proceeds slowly enough to allow a moment of horrified realization, the doers of the deed will likely be quite taken aback on realizing that they have actually destroyed the world. Therefore I suggest that if the Earth is destroyed, it will probably be by mistake.\n\nThe systematic experimental study of reproducible errors of human reasoning, and what these errors reveal about underlying mental processes, is known as the heuristics and biases programme in cognitive psychology. This programme has made discoveries highlyrelevant to assessors of global catastrophic risks. Suppose you are worried about the risk of Substance P, an explosive of planet-wrecking potency which will detonate if exposed to a strong radio signal. Luckily there is a famous expert who discovered Substance P, spent the last 30 years working with it, and knows it better than anyone else in the world. You call up the expert and ask how strong the radio signal has to be. The expert replies that the critical threshold is probably around 4000 terawatts. ‘Probably?’ you query. ‘Can you give me a 98% confidence interval?’ ‘Sure’, replies the expert. ‘I’m 99% confident that the critical threshold is above 500 terawatts, and 99% confident that the threshold is below 80,000 terawatts.’ ‘What about 10 terawatts?’ you ask. ‘Impossible’, replies the expert.\n\nThe above methodology for expert elicitation looks perfectly reasonable, the sort of thing any competent practitioner might do when faced with such a problem. Indeed, this methodology was used in the Reactor Safety Study (Rasmussen, 1975), now widely regarded as the first major attempt at probabilistic risk assessment. But the student of heuristics and biases will recognize at least two major mistakes in the method – not logical flaws, but conditions extremely susceptible to human error. I shall return to this example in the discussion of anchoring and adjustments biases (Section 5.7).\n\nThe heuristics and biases programme has uncovered results that may startle and dismay the unaccustomed scholar. Some readers, first encountering the experimental results cited here, may sit up and say: ‘Is that really an experimental result? Are people really such poor guessers? Maybe the experiment was poorly designed, and the result would go away with such-and-such manipulation.’ Lacking the space for exposition, I can only plead with the reader to consult the primary literature. The obvious manipulations have already been tried, and the results found to be robust.\n\n5.2 Availability\n\nSuppose you randomly sample a word of three or more letters from an English text. Is it more likely that the word starts with an R (‘rope’), or that R is its third letter (‘park’)?\n\nA general principle underlying the heuristics and biases programme is that human beings use methods of thought – heuristics – which quickly return good approximate answers in many cases; but which also give rise to systematic errors called biases. An example of a heuristic is to judge the frequency or probability of an event by its availability, the ease with which examples of the event come to mind. R appears in the third-letter position of more English words than in the first-letter position, yet it is much easier to recall words that begin with ‘R’ than words whose third letter is ‘R’. Thus, a majority of respondents guess that words beginning with ‘R’ are more frequent, when the reverse is the case (Tversky and Kahneman, 1973).\n\nBiases implicit in the availability heuristic affect estimates of risk. A pioneering study by Lichtenstein et al. (1978) examined absolute and relative probability judgements of risk. People know in general terms which risks cause large numbers of deaths and which cause few deaths. However, asked to quantify risks more precisely, people severely overestimate the frequency of rare causes of death and severely underestimate the frequency of common causes of death. Other repeated errors were also apparent: accidents were judged to cause as many deaths as disease. (Diseases cause about 16 times as many deaths as accidents.) Homicide was incorrectly judged a more frequent cause of death than diabetes or stomach cancer. A follow-up study by Combs and Slovic (1979) tallied reporting of deaths in two newspapers, and found that errors in probability judgements correlated strongly (.85 and .89) with selective reporting in newspapers.\n\nPeople refuse to buy flood insurance even when it is heavily subsidized and priced far below an actuarially fair value. Kates 1962 suggest that underreaction to threats of flooding may arise from ‘the inability of individuals to conceptualize floods that have never occurred … Men on flood plains appear to be very much prisoners of their experience … Recently experienced floods appear to set an upward bound to the size of loss with which managers believe they ought to be concerned’. Burton et al. (1978) report that when dams and levees are built, they reduce the frequency of floods, and thus apparently create a false sense of security, leading to reduced precautions. While building dams decreases the frequency of floods, damage per flood is so much greater afterwards that the average yearly damage increases.\n\nIt seems that most people do not extrapolate from experienced small hazards to a possibility of large risks; rather, the past experience of small hazards sets a perceived upper bound on risks. A society well protected against minor hazards will take no action against major risks (building on flood plains once the regular minor floods are eliminated). A society subject to regular minor hazards will treat those minor hazards as an upper bound on the size of the risks (guarding against regular minor floods but not occasional major floods).\n\nRisks of human extinction may tend to be underestimated since, obviously, humanity has never yet encountered an extinction event. ¹\n\n5.3 Hindsight bias\n\nHindsight bias is when subjects, after learning the eventual outcome, give a much higher estimate for the predictability of that outcome than subjects who predict the outcome without advance knowledge. Hindsight bias is sometimes called the I-knew-it-all-along effect.\n\nFischhoff and Beyth (1975) presented students with historical accounts of unfamiliar incidents such as a conflict between the Gurkhas and the British in 1814. Given the account as background knowledge, five groups of students were asked what they would have predicted as the probability for each of four outcomes: British victory, Gurkha victory, stalemate with a peace settlement, or stalemate with no peace settlement. Four experimental groups were, respectively, told that these four outcomes were the historical outcome. The fifth, control group was not told any historical outcome. In every case, a group told an outcome assigned substantially higher probability to that outcome, than did any other group or the control group.\n\nHindsight bias is important in legal cases, where a judge or jury must determine whether a defendant was legally negligent in failing to foresee a hazard (Sanchiro, 2003). In an experiment based on an actual legal case, Kamin and Rachlinski (1995) asked two groups to estimate the probability of flood damage caused by blockage of a city-owned drawbridge. The control group was told only the background information known to the city when it decided not to hire a bridge watcher. The experimental group was given this information, plus the fact that a flood had actually occurred. Instructions stated that the city was negligent if the foreseeable probability of flooding was greater than 10%. As many as 76% of the control group concluded the flood was so unlikely that no precautions were necessary; 57% of the experimental group concluded the flood was so likely that failure to take precautions was legally negligent. A third experimental group was told the outcome and also explicitly instructed to avoid hindsight bias, which made no difference: 56% concluded the city was legally negligent. Judges cannot simply instruct juries to avoid hindsight bias; that debiasing manipulation has no significant effect.\n\nWhen viewing history through the lens of hindsight, we vastly underestimate the cost of preventing catastrophe. In 1986, the space shuttle Challenger exploded for reasons eventually traced to an O-ring losing flexibility at low temperature (Rogers et al., 1986). There were warning signs of a problem with the O-rings. But preventing the Challenger disaster would have required, not attending to the problem with the O-rings, but attending to every warning sign which seemed as severe as the O-ring problem, without benefit of hindsight.\n\n5.4 Black Swans\n\nTaleb (2005) suggests that hindsight bias and availability bias bear primary responsibility for our failure to guard against what he calls Black Swans. Black Swans are an especially difficult version of the problem of the fat tails: sometimes most of the variance in a process comes from exceptionally rare, exceptionally huge events. Consider a financial instrument that earns $10 with 98% probability, but loses $1000 with 2% probability; it is a poor net risk, but it looks like a steady winner. Taleb(2001) gives the example of a trader whose strategy worked for 6 years without a single bad quarter, yielding close to $80 million – then lost $300 million in a single catastrophe.\n\nAnother example is that of Long-term Capital Management (LTCM), a hedge fund whose founders included two winners of the Nobel Prize in Economics. During the Asian currency crisis and Russian bond default of 1998, the markets behaved in a literally unprecedented fashion, assigned a negligible probability by LTCM’s historical model. As a result, LTCM began to lose $100 million per day, day after day. On a single day in 1998, LTCM lost more than $500 million (Taleb, 2005).\n\nThe founders of LTCM later called the market conditions of 1998 a’10-sigma event’. But obviously it was not that improbable. Mistakenly believing that the past was predictable, people conclude that the future is predictable. As Fischhoff(1982) puts it:\n\nWhen we attempt to understand past events, we implicitly test the hypotheses or rules we use both to interpret and to anticipate the world around us. If, in hindsight, we systematically underestimate the surprises that the past held and holds for us, we are subjecting those hypotheses to inordinately weak tests and, presumably, finding little reason to change them.\n\nThe lesson of history is that swan happens. People are surprised by catastrophes lying outside their anticipation, beyond their historical probability distributions. Then why are we so taken aback when Black Swans occur? Why did LTCM borrow a leverage of $125 billion against $4.72 billion of equity, almost ensuring that any Black Swan would destroy them?\n\nBecause of hindsight bias, we learn overly specific lessons. After September 11, the US Federal Aviation Administration prohibited box-cutters on airplanes. The hindsight bias rendered the event too predictable in retrospect, permitting the angry victims to find it the result of ‘negligence’ -such as intelligence agencies’ failure to distinguish warnings of Al Qaeda activity amid a thousand otherwarnings. We learned not to allow hijacked planes to overfly our cities. We did not learn the lesson: ‘Black Swans do occur; do what you can to prepare for the unanticipated.’\n\nTaleb(2005) writes:\n\nIt is difficult to motivate people in the prevention of Black Swans … Prevention is not easily perceived, measured, or rewarded; it is generally a silent and thankless activity. Just consider that a costly measure is taken to stave off such an event. One can easily compute the costs while the results are hard to determine. How can one tell its effectiveness, whether the measure was successful or if it just coincided with no particular accident? … Job performance assessments in these matters are not just tricky, but may be biased in favor of the observed ‘acts of heroism’. History books do not account for heroic preventive measures.\n\n5.5 The conjunction fallacy\n\nLinda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.\n\nRank the following statements from most probable to least probable:\n\n1. Linda is a teacher in an elementary school.\n\n2. Linda works in a bookstore and takes Yoga classes.\n\n3. Linda is active in the feminist movement.\n\n4. Linda is a psychiatric social worker.\n\n5. Linda is a member of the League of Women Voters.\n\n6. Linda is a bank teller.\n\n7. Linda is an insurance salesperson.\n\n8. Linda is a bank teller and is active in the feminist movement.\n\nAmong the 88 undergraduate subjects, 89% ranked statement (8) as more probable than (6) (Tversky and Kahneman, 1982). Since the given description of Linda was chosen to be similar to a feminist and dissimilar to a bank teller, (8) is more representative of Linda’s description. However, ranking (8) as more probable than (6) violates the conjunction rule of probability theory which states that p(A&B) ≤ p(A). Imagine a sample of 1000 women; surely more women in this sample are bank tellers than are feminist bank tellers.\n\nCould the conjunction fallacy rest on subjects interpreting the experimental instructions in an unanticipated way? Perhaps subjects think that by ‘probable’ is meant the probability of Linda’s description given statements (6) and (8), rather than the probability of (6) and (8) given Linda’s description. It could also be that subjects interpret (6) to mean ‘Linda is a bank teller and is not active in the feminist movement’. Although many creative alternative hypotheses have been invented to explain away the conjunction fallacy, the conjunction fallacy has survived all experimental tests meant to disprove it; see, for example, Sides et al. (2002) for a summary. For example, the following experiment excludes both of the alternative hypotheses proposed earlier.\n\nConsider a regular six-sided die with four green faces and two red faces. The die will be rolled 20 times and the sequence of greens (G) and reds (R) will be recorded. You are asked to select one sequence, from a set of three, and you will win $ 25 if the sequence you chose appears on successive rolls of the die. Please check the sequence of greens and reds on which you prefer to bet.\n\n1. RGRRR\n\n2. GRGRRR\n\n3. GRRRRR\n\nA total of 125 undergraduates at University of British Columbia (UBC) and Stanford University played this gamble with real pay-offs. Among them, 65% of subjects chose sequence (2) (Tversky and Kahneman, 1983). Sequence (2) is most representative of the die, since the die is mostly green and sequence (2) contains the greatest proportion of green faces. However, sequence (1) dominates sequence (2) because (1) is strictly included in (2)-toget (2) you must roll (1) preceded by a green face.\n\nIn the above-mentioned task, the exact probabilities for each event could in principle have been calculated by the students. However, rather than go to the effort of a numerical calculation, it would seem that (at least 65% of) the students made an intuitive guess, based on which sequence seemed most ‘representative’ of the die. Calling this ‘the representativeness heuristic’ does not imply that students deliberately decided that they would estimate probability by estimating similarity. Rather, the representativeness heuristic is what produces the intuitive sense that sequence (2) ‘seems more likely’ than sequence (1). In other words, the ‘representativeness heuristic’ is a built-in feature of the brain for producing rapid probability judgements, rather than a consciously adopted procedure. We are not aware of substituting judgement of representativeness for judgement of probability.\n\nThe conjunction fallacy similarly applies to futurological forecasts. Two independent sets of professional analysts at the Second International Congress on Forecasting were asked to rate, respectively, the probability of ‘a complete suspension of diplomatic relations between the United States and the Soviet Union sometime in 1983’ or ‘a Russian invasion of Poland, and a complete suspension of diplomatic relations between the United States and the Soviet Union sometime in 1983.’ The second set of analysts responded with significantly higher probabilities (Tversky and Kahneman 1983).\n\nIn a study by Johnson et al. (1993), MBA students at Wharton were scheduled to travel to Bangkok as part of their degree programme. Several groups of students were asked how much they were willing to pay for terrorism insurance. One group of subjects was asked how much they were willing to pay for terrorism insurance covering the flight from Thailand to the United States. A second group of subjects was asked how much they were willing to pay for terrorism insurance covering the round-trip flight. A third group was asked how much they were willing to pay for terrorism insurance that covered the complete trip to Thailand. These three groups responded with average willingness to pay of $17.19, $13.90, and $7.44, respectively.\n\nAccording to probability theory, adding a detail to a hypothesis must render the hypothesis less probable. It is less probable that Linda is a feminist bank teller than that she is a bank teller, since all feminist bank tellers are necessarily bank tellers. Yet human psychology seems to follow the rule that adding a detail can make the story more plausible.\n\nPeople might pay more for international diplomacy intended to prevent nanotechnological warfare by China than for an engineering project to defend against nanotechnological attack from any source. The second threat scenario is less vivid and alarming, but the defence is more useful because it is more vague. More valuable still would be strategies which make humanity harder to extinguish without being specific to nanotechnological threats – such as colonizing space, or see Chapter 15 (this volume) on Artificial Intelligence. Security expert Bruce Schneier observed (both before and after the 2005 hurricane in New Orleans) that the US government was guarding specific domestic targets against ‘movie-plot scenarios’ of terrorism, at the cost of taking away resources from emergency-response capabilities that could respond to any disaster (Schneier, 2005).\n\nOverly detailed reassurances can also create false perceptions of safety: ‘X is not an existential risk and you don’t need to worry about it, because of A, B, C, D, and E’; where the failure of any one of propositions A, B, C, D, or E potentially extinguishes the human species. ‘We don’t need to worry about nanotechnological war, because a UN commission will initially develop the technology and prevent its proliferation until such time as an active shield is developed, capable of defending against all accidental and malicious outbreaks that contemporary nanotechnology is capable of producing, and this condition will persist indefinitely.’ Vivid, specific scenarios can inflate our probability estimates of security, as well as misdirecting defensive investments into needlessly narrow or implausibly detailed risk scenarios.\n\nMore generally, people tend to overestimate conjunctive probabilities and underestimate disjunctive probabilities (Tversky and Kahneman, 1974). That is, people tend to overestimate the probability that, for example, seven events of 90% probability will all occur. Conversely, people tend to underestimate the probability that at least one of seven events of 10% probability will occur. Someone judging whether to, for instance, incorporate a new start-up, must evaluate the probability that many individual events will all go right (there will be sufficient funding, competent employees, customers will want the product) while also considering the likelihood that at least one critical failure will occur (the bank refuses a loan, the biggest project fails, the lead scientist dies). This may help explain why only 44% of entrepreneurial ventures² survive after 4 years (Knaup, 2005).\n\nDawes (1988, p. 133) observes: ‘In their summations lawyers avoid arguing from disjunctions (’either this or that or the other could have occurred, all of which would lead to the same conclusion’) in favor of conjunctions. Rationally, of course, disjunctions are muchmore probable than are conjunctions.’\n\nThe scenario of humanity going extinct in the next century is a disjunctive event. It could happen as a result of any of the existential risks discussed in this book – or some other cause which none of us foresaw. Yet for a futurist, disjunctions make for an awkward and unpoetic-sounding prophecy.\n\n5.6 Confirmation bias\n\nPeter Wason (1960) conducted a now-classic experiment that became known as the ‘2-4-6’ task. Subjects had to discover a rule, known to the experimenter but not to the subject – analogous to scientific research. Subjects wrote three numbers, such as ‘2-4-6’or’10-12-14’, on cards, and the experimenter told them whether the tripletfit the rule or did not fit the rule. Initially subjects were given the triplet 2–4-6, and told that this triplet fitted the rule. Subjects could continue testing triplets until they felt sure they knew the experimenter’s rule, at which point the subject announced the rule.\n\nAlthough subjects typically expressed high confidence in their guesses, only 21% of Wason’s subjects guessed the experimenter’s rule, and replications of Wason’s experiment usually report success rates of around 20%. Contrary to the advice of Karl Popper, subjects in Wason’s task try to confirm their hypotheses rather than falsifying them. Thus, someone who forms the hypothesis ‘Numbers increasing by two’ will test the triplets 8–10-12 or 2022–24, hear that they fit, and confidently announce the rule. Someone who forms the hypothesis X-2X-3X will test the triplet 3–6-9, discover that it fits, and then announce that rule. In every case the actual rule is the same: the three numbers must be in ascending order. In some cases subjects devise, ‘test’, and announce rules far more complicated than the actual answer.\n\nWason’s 2–4-6 task is a ‘cold’ form of confirmation bias; people seek confirming but not falsifying evidence. ‘Cold’ means that the 2–4-6 task is an emotionally neutral case of confirmation bias; the belief held is logical, not emotional. ‘Hot’ refers to cases where the belief is emotionally charged, such as political argument. Unsurprisingly, ‘hot’ confirmation biases are stronger – larger in effect and more resistant to change. Active, effortful confirmation biases are labelled motivated cognition (more ordinarily known as ‘rationalization’). As stated by Brenner et al. (2002, p.503) in ‘Remarks on Support Theory’:\n\nClearly, in many circumstances, the desirability of believing a hypothesis may markedly influence its perceived support … Kunda (1990) discusses how people who are motivated to reach certain conclusions attempt to construct (in a biased fashion) a compelling case for their favored hypothesis that would convince an impartial audience. Gilovich (2000) suggests that conclusions a person does not want to believe are held to a higher standard than conclusions a person wants to believe. In the former case, the person asks if the evidence compels one to accept the conclusion, whereas in the latter case, the person asks instead if the evidence allows one to accept the conclusion.\n\nWhen people subject disagreeable evidence to more scrutiny than agreeable evidence, this is known as motivated scepticism or disconfirmation bias. Disconfirmation bias is especially destructive for two reasons: (1) Two biased reasoners considering the same stream of evidence can shift their beliefs in opposite directions – both sides selectively accepting only favourable evidence. Gathering more evidence may not bring biased reasoners to agreement. (2) People who are more skilled sceptics – who know a larger litany of logical flaws – but apply that skill selectively, may change their minds more slowly than unskilled reasoners.\n\nTaber and Lodge (2000) examined the prior attitudes and attitude changes of students – when exposed to political literature for and against gun control and affirmative action, two issues of particular salience in the political life of the United States. The study tested six hypotheses using two experiments:\n\n1. Prior attitude effect. Subjects who feel strongly about an issue – even when encouraged to be objective – will evaluate supportive arguments more favourably than contrary arguments.\n\n2. Disconfirmation bias. Subjects will spend more time and cognitive resources denigrating contrary arguments than supportive arguments.\n\n3. Confirmation bias. Subjects free to choose their information sources will seek out supportive rather than contrary sources.\n\n4. Attitude polarization. Exposing subjects to an apparently balanced set of pro and con arguments will exaggerate their initial polarization.\n\n5. Attitude strength effect. Subjects voicing stronger attitudes will be more prone to the above biases.\n\n6. Sophistication effect. Politically knowledgeable subjects, because they possess greater ammunition with which to counter-argue incongruent facts and arguments, will be more prone to the above biases.\n\nIronically, Taber and Lodge’s experiments confirmed all six of the authors’ prior hypotheses. Perhaps you will say: ‘The experiment only reflects the beliefs the authors started out with – it is just a case of confirmation bias.’ If so, then by making you a more sophisticated arguer – by teaching you another bias of which to accuse people – I have actually harmed you; I have made you slower to react to evidence. I have given you another opportunity to fail each time you face the challenge of changing your mind.\n\nHeuristics and biases are widespread in human reasoning. Familiarity with heuristics and biases can enable us to detect a wide variety of logical flaws that might otherwise evade our inspection. But, as with any ability to detect flaws in reasoning, this inspection must be applied even-handedly – both to our own ideas and the ideas of others; to ideas which discomfort us and also to ideas which comfort us. Awareness of human fallibility is a dangerous knowledge, if you remind yourself of the fallibility of those who disagree with you. If I am selective about which arguments I inspect for errors, or even how hard I inspect for errors, then every new rule of rationality I learn, every new logical flaw I know how to detect, makes me that much stupider. Intelligence, to be useful, must be used for something other than defeating itself.\n\nYou cannot ‘rationalize’ what is not rational to begin with – as if lying were called ‘truthization’. There is no way to obtain more truth for a proposition by bribery, flattery, or the most passionate argument – you can make more people believe the proposition, but you cannot make it more true. To improve the truth of our beliefs we must change our beliefs. Not every change is an improvement, but every improvement is necessarily a change.\n\nOur beliefs are more swiftly determined than we think. Griffin and Tversky (1992) discreetly approached 24 colleagues faced with a choice between two job offers and asked them to estimate the probability that they would choose each job offer. The average confidence in the choice assigned the greater probability was a modest 66%. Yet only 1 of 24 respondents chose the option initially assigned the lower probability, yielding an overall accuracy of 96% (one of few reported instances of human under confidence).\n\nThe moral may be that once you can guess what your answer will be – once you can assign a greater probability to your answering one way than another – you have, in all probability, already decided. And if you were honest with yourself, you would often be able to guess your final answer within seconds of hearing the question. We change our minds less often than we think. How fleeting is that brief unnoticed moment when we cannot yet guess what our answer will be, the tiny fragile instant when there is a chance for intelligence to act – in questions of choice, as in questions of fact.\n\nThor Shenkel said: ‘It ain’t a true crisis of faith unless things could just as easily go either way.’\n\nNorman R.F. Maier said: ‘Do not propose solutions until the problem has been discussed as thoroughly as possible without suggesting any.’ Robyn Dawes, commenting on Maier, said: ‘I have often used this edict with groups I have led – particularly when they face a very tough problem, which is when group members are most apt to propose solutions immediately.’\n\nIn computer security, a ‘trusted system’ is one that you are in fact trusting, not one that is in fact trustworthy. A ‘trusted system’ is a system which, if it is untrustworthy, can cause a failure. When you read a paper which proposes that a potential global catastrophe is impossible, or has a specific annual probability, or can be managed using some specific strategy, then you trust the rationality of the authors. You trust the authors’ ability to be driven from a comfortable conclusion to an uncomfortable one, even in the absence of overwhelming experimental evidence to prove a cherished hypothesis wrong. You trust that the authors did not unconsciously look just a little bit harder for mistakes in equations that seemed to be leaning the wrong way, before you ever saw the final paper.\n\nHowever, if authority legislates that the mere suggestion of an existential risk is enough to shut down a project, or if it becomes a de facto truth of the political process that no possible calculation can overcome the burden of a suggestion once made, no scientist will ever again make a suggestion, which is worse. I do not know how to solve this problem. But I think it would be well for estimators of existential risks to know something about heuristics and biases in general, and disconfirmation bias in particular.\n\n5.7 Anchoring, adjustment, and contamination\n\nAn experimenter spins a ‘Wheel of Fortune’ device as you watch, and the Wheel happens to come up pointing to (version one) the number 65 or (version two) the number 15. The experimenter then asks you whether the percentage of African countries in the United Nations is above or below this number. After you answer, the experimenter asks you your estimate of the percentage of African countries in the United Nations.\n\nTversky and Kahneman (1974) demonstrated that subjects who were first asked if the number was above or below 15, later generated substantially lower percentage estimates than subjects first asked if the percentage was above or below 65. The groups’ median estimates of the percentage of African countries in the United Nations were 25 and 45, respectively. This, even though the subjects had watched the number being generated by an apparently random device, the Wheel of Fortune, and hence believed that the number bore no relation to the actual percentage of African countries in the United Nations. Payoffs for accuracy did not change the magnitude of the effect. Tversky and Kahneman hypothesized that this effect was due to anchoring and adjustment; subjects took the initial uninformative number as their starting point, or anchor, and then adjusted the number up or down until they reached an answer that sounded plausible to them; then they stopped adjusting. The result was under-adjustment from the anchor.\n\nIn the example that opens this chapter, we first asked the expert on Substance P to guess the actual value for the strength of radio signal that would detonate Substance P, and only afterwards asked for confidence bounds around this value. This elicitation method leads people to adjust upwards and downwards from their starting estimate, until they reach values that ‘sound improbable’ and stop adjusting. This leads to under-adjustment and too-narrow confidence bounds.\n\nFollowing the study by Tversky and Kahneman (1974), continued research showed a wider range of anchoring and pseudo-anchoring effects. Anchoring occurred even when they represented utterly implausible answers to the question; for example, asking subjects to estimate the year Einstein first visited the United States, after considering anchors of 1215 or 1992. These implausible anchors produced anchoring effects just as large as more plausible anchors such as 1905 or 1939 (Strack and Mussweiler, 1997). Walking down the supermarket aisle, you encounter a stack of cans of canned tomato soup, and a sign saying ‘Limit 12 per customer’. Does this sign actually prompt people to buy more cans of tomato soup? According to empirical experiment, it does (Wansink et al., 1998).\n\nSuch generalized phenomena became known as contamination effects, since it turned out that almost any information could work its way into a cognitive judgement (Chapman and Johnson, 2002). Attempted manipulations to eliminate contamination include paying subjects for correct answers (Tversky and Kahneman, 1974), instructing subjects to avoid anchoring on the initial quantity (Quattrone et al., 1981), and facing real-world problems (Wansink et al., 1998). These manipulations did not decrease, or only slightly decreased, the magnitude of anchoring and contamination effects. Furthermore, subjects asked whether they had been influenced by the contaminating factor typically did not believe they had been influenced, when experiment showed they had been (Wilson et al., 1996).\n\nA manipulation which consistently increases contamination effects is placing the subjects in cognitively ‘busy’ conditions such as rehearsing a word-string while working (Gilbert et al., 1988) or asking the subjects for quick answers (Gilbert and Osborne, 1989). Gilbert et al. (1988) attribute this effect to the extra task interfering with the ability to adjust away from the anchor; that is, less adjustment was performed in the cognitively busy condition. This decreases adjustment, hence increases the under-adjustment effect known as anchoring.\n\nTo sum up, information that is visibly irrelevant still anchors judgements and contaminates guesses. When people start from information known to be irrelevant and adjust until they reach a plausible-sounding answer, they under-adjust. People under-adjust more severely in cognitively busy situations and other manipulations that make the problem harder. People deny they are anchored or contaminated, even when experiment shows they are. These effects are not diminished or only slightly diminished by financial incentives, explicit instruction to avoid contamination, and real-world situations.\n\nNow consider how many media stories on Artificial Intelligence cite the Terminator movies as if they were documentaries, and how many media stories on brain-computer interfaces mention Star Trek’s Borg.\n\nIf briefly presenting an anchor has a substantial effect on subjects’ judgements, how much greater an effect should we expect from reading an entire book or watching a live-action television show? In the ancestral environment, there were no moving pictures; whatever you saw with your own eyes was true. People do seem to realize, so far as conscious thoughts are concerned, that fiction is fiction. Media reports that mention Terminator do not usually treat Cameron’s screenplay as a prophecy or a fixed truth. Instead the reporter seems to regard Cameron’s vision as something that, having happened before, might well happen again – the movie is recalled (is available)asifit were an illustrative historical case. I call this mix of anchoring and availability the logical fallacy of generalization from fictional evidence³ .\n\nStorytellers obey strict rules of narrative unrelated to reality. Dramatic logic is not logic. Aspiring writers are warned that truth is no excuse: you may not justify an unbelievable event in your fiction by citing an instance of real life. A good story is painted with bright details, illuminated by glowing metaphors; a storyteller must be concrete, as hard and precise as stone. But in forecasting, every added detail is an extra burden! Truth is hard work and not the kind of hard work done by storytellers. We should avoid not only being duped by fiction – failing to expend the mental effort necessary to ‘unbelieve’ it – but also being contaminatedby fiction, letting it anchor our judgements. And we should be aware that we are not always aware of this contamination. Not uncommonly in a discussion of existential risk, the categories, choices, consequences, and strategies derive from movies, books, and television shows. There are subtler defeats, but this is outright surrender.\n\n5.8 The affect heuristic\n\nThe affect heuristic refers to the way in which subjective impressions of ‘goodness’ or ‘badness’ can act as a heuristic, capable of producing fast perceptual judgements, and also systematic biases.\n\nIn a study by Slovic et al. (2002), two groups of subjects evaluated a scenario in which an airport had to decide whether to spend money to purchase new equipment, while critics argued that money should be spent on other aspects of airport safety. The response scale ranged from zero (would not support at all) to 20 (very strong support). A measure that was described as ‘Saving 150 lives’ had mean a support of 10.4, whereas a measure that was described as ‘Saving 98% of 150 lives’ had a mean support of 13.6. Even ‘Saving 85%of 150 lives’ had higher support than simply ‘Saving 150 lives’. The hypothesis motivating the experiment was that saving 150 lives sounds diffusely good and is therefore only weakly evaluable, whereas saving 98% of something is clearly very good because it is so close to the upper bound on the percentage scale.\n\nFinucane et al. (2000) wondered if people conflated their assessments of the possible benefits of a technology such as nuclear power, and their assessment of possible risks, into an overall good or bad feeling about the technology. Finucane et al. tested this hypothesis by providing four kinds of information that would increase or decrease perceived risk or perceived benefit. There was no logical relation between the information provided (e.g., about risks) and the non-manipulated variable (e.g., benefits). In each case, the manipulated information produced an inverse effect on the affectively inverse characteristic. Providing information that increased perception of risk decreased perception of benefit. Similarly, providing information that decreased perception of benefit increased perception of risk. Finucane et al. also found that time pressure greatly increased the inverse relationship between perceived risk and perceived benefit – presumably because time pressure increased the dominance of the affect heuristic over analytical reasoning.\n\nGanzach (2001) found the same effect in the realm of finance: analysts seemed to base their judgements of risk and return for unfamiliar stocks upon a global affective attitude. Stocks perceived as ‘good’ were judged to have low risks and high return; stocks perceived as ‘bad’ were judged to have low return and high risks. That is, for unfamiliar stocks, perceived risk and perceived return were negatively correlated, as predicted by the affect heuristic. ⁴ For familiar stocks, perceived risk and perceived return were positively correlated; riskier stocks were expected to produce higher returns, as predicted by ordinary economic theory. (If a stock is safe, buyers pay a premium for its safety and it becomes more expensive, driving down the expected return.)\n\nPeople typically have sparse information in considering future technologies. Thus it is not surprising that their attitudes should exhibit affective polarization. When I first began to think about such matters, I rated biotechnology as having relatively smaller benefits compared to nanotechnology, andI worried more about an engineered supervirus than about misuse of nanotechnology. Artificial Intelligence, from which I expected the largest benefits of all, gave me not the least anxiety. Later, after working through the problems in much greater detail, my assessment of relative benefit remained much the same, but my worries had inverted: the more powerful technologies, with greater anticipated benefits, now appeared to have correspondingly more difficult risks. In retrospect this is what one would expect. But analysts with scanty information may rate technologies affectively, so that information about perceived benefit seems to mitigate the force of perceived risk.\n\n5.9 Scope neglect\n\nMigrating birds (2000/20,000/200,000) die each year by drowning in uncovered oil ponds, which the birds mistake for water bodies. These deaths could be prevented by covering the oil ponds with nets. How much money would you be willing to pay to provide the needed nets?\n\nThree groups of subjects considered three versions of the above question, asking them how high a tax increase they would accept to save 2,000, 20,000, or 200,000 birds. The response – known as Stated Willingness-to-Pay (SWTP) – had a mean of $80 for the 2000-bird group, $78 for 20,000 birds, and $88 for 200,000 birds (Desvousges et al., 1993). This phenomenon is known as scope insensitivity or scope neglect.\n\nSimilar studies have shown that Toronto residents would a pay little more to clean up all polluted lakes in Ontario than polluted lakes in a particular region of Ontario (Kahneman, 1986); and that residents of four western US states would pay only 28% more to protect all 57 wilderness areas in those states than to protect a single area (McFadden and Leonard, 1995).\n\nThe most widely accepted explanation for scope neglect appeals to the affect heuristic. Kahneman et al. (1999, pp. 212–213) write:\n\nThe story constructed by Desvouges et al. probably evokes for many readers a mental representation of a prototypical incident, perhaps an image of an exhausted bird, its feathers soaked in black oil, unable to escape. The hypothesis of valuation by prototype asserts that the affective value of this image will dominate expressions of the attitude to the problem – including the willingness to pay for a solution. Valuation by prototype implies extension neglect.\n\nTwo other hypotheses accounting for scope neglect include purchase of moral satisfaction (Kahneman and Knetsch, 1992) and good cause dump (Harrison, 1992). ‘Purchase of moral satisfaction’ suggests that people spend enough money to create a ‘warm glow’ in themselves, and the amount required is a property of the person’s psychology, having nothing to do with birds. ‘Good cause dump’ suggests that people have some amount of money they are willing to pay for ‘the environment’, and any question about environmental goods elicits this amount.\n\nScope neglect has been shown to apply to human lives. Carson and Mitchell (1995) report that increasing the alleged risk associated with chlorinated drinking water from 0.004 to 2.43 annual deaths per 1000 (a factor of 600) increased stated willingness to pay from $3.78 to $15.23 (a factor of 4). Baron and Greene (1996) found no effect from varying lives saved by a factor of 10.\n\nFetherstonhaugh et al. (1997), in a paper titled ‘Insensitivity to the value of human life: a study of psychophysical numbing’, found evidence that our perception of human deaths, and valuation of human lives, obeys Weber’s Law – meaning that we use a logarithmic scale. And indeed, studies of scope neglect in which the quantitative variations are huge enough to elicit any sensitivity at all, show small linear increases in Willingness-to-Pay corresponding to exponential increases in scope. Kahneman et al. (1999) interpret this as an additive effect of scope affect and prototype affect – the prototype image elicits most of the emotion, and the scope elicits a smaller amount of emotion which is added (not multiplied) with the first amount.\n\nAlbert Szent-Györgyi, famous Hungarian physiologist and the discoverer of vitamin C, said: ‘I am deeply moved if I see one man suffering and would risk my life for him. Then I talk impersonally about the possible pulverization of our big cities, with a hundred million dead. I am unable to multiply one man’s suffering by a 100 million.’ Human emotions take place within an analogous brain. The human brain cannot release enough neurotransmitters to feel emotion a 1000 times as strong as the grief of one funeral. A prospective risk going from 10,000,000 deaths to 100,000,000 deaths does not multiply by 10 the strength of our determination to stop it. It adds one more zero on paper for our eyes to glaze over, an effect so small that one must usually jump several orders of magnitude to detect the difference experimentally.\n\n5.10 Calibration and overconfidence\n\nWhat confidence do people place in their erroneous estimates? In Section 5.2 on Availability, I discussed an experiment on perceived risk, in which subjects overestimated the probability of newsworthy causes of death in a way that correlated to their selective reporting in newspapers. Slovic et al. (1982, p. 472) also observed:\n\nA particularly pernicious aspect of heuristics is that people typically have great confidence in judgments based upon them. In another followup to the study on causes of death, people were asked to indicate the odds that they were correct in choosing the more frequent of two lethal events (Fischhoff, Slovic, and Lichtenstein, 1977)-In Experiment 1, subjects were reasonably well calibrated when they gave odds of 1:1, 1.5:1, 2:1, and 3:1. That is, their percentage of correct answers was close to the appropriate percentage correct, given those odds. However, as odds increased from 3:1 to 100:1, there was little or no increase in accuracy. Only 73% of the answers assigned odds of 100:1 were correct (instead of 99.1%). Accuracy ‘jumped’ to 81% at 1000:1 and to 87% at 10,000:1. For answers assigned odds of 1,000,000:1 or greater, accuracy was 90%; the appropriate degree of confidence would have been odds of 9:1 – In summary, subjects were frequently wrong at even the highest odds levels. Moreover, they gave many extreme odds responses. More than half of their judgments were greater than 50:1. Almost one-fourth were greater than 100:1 – 30% of the respondents in Experiment 1 gave odds greater than 50:1 to the incorrect assertion that homicides are more frequent than suicides.\n\nThis extraordinary-seeming result is quite common within the heuristics and biases literature, where it is known as overconfidence. Suppose I ask you for your best guess as to an uncertain quantity, such as the number of ‘Physicians and Surgeons’ listed in the Yellow Pages of the Boston phone directory, or total US egg production in millions. You will generate some value, which surely will not be exactly correct; the true value will be more or less than your guess. Next I ask you to name a lower bound such that you are 99% confident that the true value lies above this bound and an upper bound such that you are 99% confident the true value lies beneath this bound. These two bounds form your 98% confidence interval. If you are well calibrated, then on a test with 100 such questions, around 2 questions will have answers that fall outside your 98% confidence interval.\n\nAlpert and Raiffa (1982) asked subjects a collective total of 1000 general knowledge questions like those described above; 426 of the true values lay outside the subjects’ 98% confidence intervals. If the subjects were properly calibrated there would have been approximately 20 surprises. Put another way: Events to which subjects assigned a probability of 2% happened 42.6%of the time.\n\nAnother group of 35 subjects was asked to estimate 99.9% confident upper and lower bounds. They received 40% surprises. Another 35 subjects were asked for ‘minimum’ and ‘maximum’ values and were surprised 47% of the time. Finally, a fourth group of 35 subjects were asked for ‘astonishingly low’ and ‘astonishingly high’ values; they recorded 38% surprises.\n\nIn a second experiment, a new group of subjects was given a first set of questions, scored, provided with feedback, told about the results of previous experiments, had the concept of calibration explained to them at length, and then asked to provide 98% confidence intervals for a new set of questions. The post-training subjects were surprised 19% of the time, a substantial improvement over their pre-training score of 34% surprises, but still a far cry from the well-calibrated value of 2% surprises.\n\nSimilar failure rates have been found for experts. Hynes and Vanmarke (1976) asked seven internationally known geotechnical engineers to predict the height of an embankment that would cause a clay foundation to fail and to specify confidence bounds around this estimate that were wide enough to have a 50% chance of enclosing the true height. None of the bounds specified by the engineers enclosed the true failure height. Christensen-Szalanski and Bushyhead (1981) reported physician estimates for the probability of pneumonia for 1531 patients examined because of a cough. At the highest calibrated bracket of stated confidences, with average verbal probabilities of 88%, the proportion of patients actually having pneumonia was less than 20%.\n\nIn the words of Alpert and Raiffa (1982, p. 301): ‘For heaven’s sake, Spread Those Extreme Fractiles! Be honest with yourselves! Admit what you don’t know!’\n\nLichtenstein et al. (1982) reviewed the results of 14 papers on 34 experiments performed by 23 researchers studying human calibration. The overwhelmingly strong result was that people are overconfident. In the modern field, overconfidence is no longer noteworthy; but it continues to show up, in passing, in nearly any experiment where subjects are allowed to assign extreme probabilities.\n\nOverconfidence applies forcefully to the domain of planning, where it is known as the planning fallacy. Buehler et al. (1994) asked psychology students to predict an important variable – the delivery time of their psychology honours thesis. They waited until students approached the end of their year-long projects, then asked the students when they realistically expected to submit their thesis and also when they would submit the thesis ‘if everything went as poorly as it possibly could’. On average, the students took 55 days to complete their thesis, 22 days longer than they had anticipated, and 7 days longer than their worst-case predictions.\n\nBuehler et al. (1995) asked students for times by which they were 50% sure, 75% sure, and 99% sure that they would finish their academic project. Only 13% of the participants finished their project by the time assigned a 50% probability level, only 19% finished by the time assigned a 75% probability, and 45% finished by the time of their 99% probability level. Buehler et al. (2002) wrote: ‘The results for the 99% probability level are especially striking: Even when asked to make a highly conservative forecast, a prediction that they felt virtually certain that they would fulfill, students’ confidence in their time estimates far exceeded their accomplishments.’\n\nNewby-Clark et al. (2000) found that asking subjects for their predictions based on realistic ‘best guess’ scenarios and asking subjects for their hoped-for ‘best case’ scenarios produced indistinguishable results. When asked for their ‘most probable’ case, people tend to envision everything going exactly as planned, with no unexpected delays or unforeseen catastrophes: the same vision as their ‘best case’. Reality, it turns out, usually delivers results somewhat worse than the ‘worst case’.\n\nThis chapter discusses overconfidence after discussing the confirmation bias and the sub-problem of the disconfirmation bias. The calibration research is dangerous knowledge – so tempting to apply selectively. ‘How foolish my opponent is, to be so certain of his arguments! Doesn’t he know how often people are surprised on their certainties?’ If you realize that expert opinions have less force than you thought, you had better also realize that your own thoughts have much less force than you thought, so that it takes less force to compel you away from your preferred belief. Otherwise you become slower to react to incoming evidence. You are left worse off than if you had never heard of calibration. That is why – despite frequent great temptation – I avoid discussing the research on calibration unless I have previously spoken of the confirmation bias, so that I can deliver this same warning.\n\nNote also that a confidently expressed expert opinion is quite a different matter from a calculation made strictly from actuarial data, or strictly from a precise, precisely confirmedmodel. Of all the times an expert has ever stated, even from strict calculation, that an event has a probability of 10⁻⁶, they have undoubtedly been wrong more often than one time in a million. But if combinatorics could not correctly predict that a lottery ticket has a 10⁻⁸chance of winning, ticket sellers would go broke.\n\n5.11 Bystander apathy\n\nMy last bias comes, not from the field of heuristics and biases, but from the field of social psychology. A now-famous series of experiments by Latane and Darley (1969) uncovered the bystander effect, also known as bystander apathy, in which larger numbers of people are less likely to act in emergencies – not only individually, but also collectively. Among subjects alone in a room, on noticing smoke entering from under a door, 75% of them left the room to report it. When three naïve subjects were present, the smoke was reported only 38% of the time. A naive subject in the presence of two confederates who purposely ignored the smoke, even when the room became hazy, left to report the smoke only 10% of the time. A college student apparently having an epileptic seizure was helped 85% of the time by a single bystander and 31% of the time by five bystanders.\n\nThe bystander effect is usually explained as resulting from diffusion of responsibility and pluralistic ignorance. Being part of a group reduces individual responsibility. Everyone hopes that someone else will handle the problem instead, and this reduces the individual pressure to the point that no one does anything. Support for this hypothesis is adduced from manipulations in which subjects believe that the victim is especially dependent on them; this reduces the bystander effect or negates it entirely. Cialdini (2001) recommends that if you are ever in an emergency, you single out one bystander, and ask that person to help – thereby overcoming the diffusion.\n\nPluralistic ignorance is a more subtle effect. Cialdini (2001, p.114) writes:\n\nVery often an emergency is not obviously an emergency. Is the man lying in the alley a heart-attack victim or a drunk sleeping one off? … In times of such uncertainty, the natural tendency is to look around at the actions of others for clues. We can learn from the way the other witnesses are reacting whether the event is or is not an emergency. What is easy to forget, though, is that everybody else observing the event is likely to be looking for social evidence, too. Because we all prefer to appear poised and unflustered among others, we are likely to search for that evidence placidly, with brief, camouflaged glances at those around us. Therefore everyone is likely to see everyone else looking unruffled and failing to act.\n\nThe bystander effect is not about individual selfishness or insensitivity to the suffering of others. Alone subjects do usually act. Pluralistic ignorance can explain, and individual selfishness cannot explain, subjects failing to react to a room filling up with smoke. In experiments involving apparent dangers to either others or the self, subjects placed with non-reactive confederates frequently glance at the non-reactive confederates.\n\nI am sometimes asked: ‘If existential risk X is real, why aren’t more people doing something about it?’ There are many possible answers, a few of which I have touched on here. People may be overconfident and over-optimistic. They may focus on overly specific scenarios for the future, to the exclusion of all others. They may not recall any past extinction events in memory. They may overestimate the predictability of the past and hence underestimate the surprise of the future. They may not realize the difficulty of preparing for emergencies without benefit of hindsight. They may prefer philanthropic gambles with higher pay-off probabilities, neglecting the value of the stakes. They may conflate positive information about the benefits of a technology as negative information about its risks. They may be contaminated by movies, where the world ends up being saved. They may purchase moral satisfaction more easily by giving to other charities. Otherwise, the extremely unpleasant prospect of human extinction may spur them to seek arguments that humanity will not go extinct, without an equally frantic search for reasons why we would.\n\nBut if the question is, specifically, ‘Why aren’t more people doing something about it?’, one possible component is that people are asking that very question – darting their eyes around to see if anyone else is reacting to the emergency, meanwhile trying to appear poised and unflustered. If you want to know why others are not responding to an emergency, before you respond yourself, you may have just answered your own question.\n\n5.12 A final caution\n\nEvery true idea which discomforts you will seem to match the pattern of at least one psychological error.\n\nRobert Pirsig said: ‘The world’s biggest fool can say the sun is shining, but that doesn’t make it dark out.’ If you believe someone is guilty of a psychological error, then demonstrate your competence by first demolishing their consequential factual errors. If there are no factual errors, then what matters the psychology? The temptation of psychology is that, knowing a little psychology, we can meddle in arguments where we have no technical expertise – instead of sagely analysing the psychology of the disputants.\n\nIf someone wrote a novel about an asteroid strike destroying modern civilization, then the reader might criticize that novel as dystopian, apocalyptic, and symptomatic of the author’s naïve inability to deal with a complex technological society. We should recognize this as a literary criticism and not a scientific one; it is about good or bad novels and not about good or bad hypotheses. To quantify the annual probability of an asteroid strike in real life, one must study astronomy and the historical record (while avoiding the case-specific biases; see Chapters 1 and 11, this volume): no amount of literary criticism can put a number on it. Garreau (2005) seems to hold that a scenario of a mind slowly increasing in capability, is more mature and sophisticated than a scenario of extremely rapid intelligence increase. But that is a technical question, not a matter of taste; no amount of psychologizing can tell you the exact slope of that curve.\n\nIt is harder to abuse heuristics and biases than psycho-analysis. Accusing someone of conjunction fallacy leads naturally into listing the specific details that you think are burdensome and drive down the joint probability. Even so, do not lose track of the real-world facts of primary interest; do not let the argument become about psychology.\n\nDespite all dangers and temptations, it is better to know about psychological biases than not to know. Otherwise we will walk directly into the whirling helicopter blades of life. But be very careful not to have too much fun accusing others of biases. That is the road that leads to becoming a sophisticated arguer – someone who, faced with any discomforting argument, finds at once a bias in it; the one whom you must watch above all is yourself.\n\nJerry Cleaver said: ‘What does you in is not failure to apply some high-level, intricate, complicated technique. It’s overlooking the basics. Not keeping your eye on the ball.’\n\nAnalyses should finally centreon testable real-world assertions. Do not take your eyes off the ball.\n\n5.13 Conclusion\n\nWhy should there be an organized body of thinking about global catastrophic and existential risks? Falling asteroids are not like engineered superviruses; physics disasters are not like nanotechnological wars. Why not consider each of these problems separately?\n\nIf someone proposes a physics disaster, then the committee convened to analyse the problem must obviously include physicists. But someone on that committee should also know how terribly dangerous it is to have an answer in your mind before you finish asking the question. Someone on that committee should remember the reply of Enrico Fermi to Leo Szilard’s proposal that a fission chain reaction could be used to build nuclear weapons. (The reply was ‘Nuts!’ – Fermi considered the possibility so remote as to not be worth investigating.) Someone should remember the history of errors in physics calculations: the Castle Bravo nuclear test that produced a 15-megaton explosion, instead of 4 to 8, because of an unconsidered reaction in lithium-7: they correctly solved the wrong equation, failed to think of all the terms that needed to be included, and at least one person in the expanded fallout radius died. Someone should remember Lord Kelvin’s careful proof, using multiple, independent quantitative calculations from well-established theories, that the Earth could not possibly have existed for as much as 40 million years. Someone should know that when an expert says the probability is ‘a million to one’ without using actuarial data or calculations from a precise, precisely confirmed model, the calibration is probably more like 20 to 1 (although this is not an exact conversion).\n\nAny existential risk evokes problems that it shares with all other existential risks, in addition to the domain-specific expertise required for the specific existential risk (see more details in Chapter 1, this book). Someone on the physics-disaster committee should know what the term ‘existential risk’ means and should possess whatever skills the field of existential risk management has accumulated or borrowed. For maximum safety, that person should also be a physicist. The domain-specific expertise and the expertise pertaining to existential risks should combine in one person. I am sceptical that a scholar of heuristics and biases, unable to read physics equations, could check the work of physicists who knew nothing of heuristics and biases.\n\nOnce upon a time I made up overly detailed scenarios, without realizing that every additional detail was an extra burden. I really did think that I could say there was a 90% chance of Artificial Intelligence being developed between 2005 and 2025, with the peak in 2018. This statement now seems to me like complete gibberish. Why did I ever think I could generate a tight probability distribution over a problem like that? Where did I even get those numbers in the first place?\n\nI once met a lawyer who had made up his own theory of physics. I said to the lawyer: ‘You cannot invent your own physics theories without knowing math and studying for years; physics is hard.’ He replied: ‘But if you really understand physics you can explain it to your grandmother, Richard Feynman told me so.’ And I said to him: ‘Would you advise a friend to argue his own court case?’ At this he fell silent. He knew abstractly that physics was difficult, but I think it had honestly never occurred to him that physics might be as difficult as lawyering.\n\nOne of many biases not discussed in this chapter describes the biasing effect of not knowing what we do not know. When a company recruiter evaluates his own skill, he recalls in his mind the performance of candidates he hired, many of whom subsequently excelled; therefore the recruiter thinks highly of his skill. But the recruiter never sees the work of candidates not hired. Thus I must warn that this paper touches upon only a small subset of heuristics and biases; for when you wonder how much you have already learned, you will recall the few biases this chapter does mention, rather than the many biases it does not. Brief summaries cannot convey a sense of the field, the larger understanding which weaves a set of memorable experiments into a unified interpretation. Many highly relevant biases, such as need for closure, I have not even mentioned. The purpose of this chapter is not to teach the knowledge needful to a student of existential risks but to intrigue you into learning more.\n\nThinking about existential risks falls prey to all the same fallacies that prey upon thinking in general. But the stakes are much, much higher. A common result in heuristics and biases is that offering money or other incentives does not eliminate the bias. (Kachelmeier and Shehata [1992] offered subjects living in the People’s Republic of China the equivalent of three months’ salary.) The subjects in these experiments do not make mistakes on purpose; they make mistakes because they do not know how to do better. Even if you told them the survival of humankind was at stake, they still would not thereby know how to do better. (It might increase their need for closure, causing them to do worse.) It is a terribly frightening thing, but people do not become any smarter, just because the survival of humankind is at stake.\n\nIn addition to standard biases, I have personally observed what look like harmful modes of thinking specific to existential risks. The Spanish flu of 1918 killed 25–50 million people. World War II killed 60 million people; 10⁷ is the order of the largest catastrophes in humanity’s written history. Substantially larger numbers, such as 500 million deaths, and especially qualitatively different scenarios such as the extinction of the entire human species, seem to trigger a different mode of thinking – enter into a ‘separate magisterium’. People who would never dream of hurting a child hear of an existential risk, and say, ‘Well, maybe the human species doesn’t really deserve to survive.’\n\nThere is a saying in heuristics and biases that people do not evaluate events, but descriptions of events – what is called non-extensional reasoning. The extension of humanity’s extinction includes the death of yourself, your friends, your family, your loved ones, your city, your country, your political fellows. Yet people who would take great offence at a proposal to wipe the country of Britain from the map, to kill every member of the Democratic Party in the United States, to turn the city of Paris to glass – who would feel still greater horror on hearing the doctor say that their child had cancer – these people will discuss the extinction of humanity with perfect calm. The phrase ‘Extinction of humanity’, as words on paper, appears in fictional novels or is discussed in philosophy books – it belongs to a different context compared to the Spanish flu. We evaluate descriptions of events, not extensions of events. The cliché phrase end of the world invokes the magisterium of myth and dream, of prophecy and apocalypse, of novels and movies. The challenge of existential risks to rationality is that, the catastrophes being so huge, people snap into a different mode of thinking. Human deaths are suddenly no longer bad, and detailed predictions suddenly no longer require any expertise, and whether the story is told with a happy ending or a sad ending is a matter of personal taste in stories.\n\nBut that is only an anecdotal observation of mine. I thought it better that this essay should focus on mistakes well documented in the literature – the general literature of cognitive psychology, because there is not yet experimental literature specific to the psychology of existential risks. There should be.\n\nIn the mathematics of Bayesian decision theory there is a concept of information value – the expected utility of knowledge. The value of information emerges from the value of whatever it is about; if you double the stakes, you double the value of information about the stakes. The value of rational thinking works similarly – the value of performing a computation that integrates the evidence is calculated much the same way as the value of the evidence itself (Good, 1952; Horvitz et al., 1989).\n\nNo more than Albert Szent-Györgyi could multiply the suffering of one human by a 100 million can I truly understand the value of clear thinking about global risks. Scope neglect is the hazard of being a biological human, running on an analogous brain; the brain cannot multiply by 6 billion. And the stakes of existential risk extend beyond even the 6 billion humans alive today, to all the stars in all the galaxies that humanity and humanity’s descendants may some day touch. All that vast potential hinges on our survival here, now, in the days when the realm of humankind is a single planet orbiting a single star. I cannot feel our future. All I can do is try to defend it.\n\nAcknowledgement\n\nI thank Michael Roy Ames, Nick Bostrom, Milan M. Ćirković, Olie Lamb, Tamas Martinec, Robin Lee Powell, Christian Rovner, and Michael Wilson for their comments, suggestions, and criticisms. Needless to say, any remaining errors in this chapter are my own.\n\nSuggestions for further reading\n\nDawes, R. (1988). Rational Choice in an Uncertain World: The Psychology of Intuitive Judgment (San Diego, CA: Harcourt, Brace, Jovanovich). First edition 1988 by Dawes and Kagan, second edition 2001 by Dawes and Hastie. This book aims to introduce heuristics and biases to an intelligent general audience. (For example, Bayes’s Theorem is explained, rather than assumed, but the explanation is only a few pages.) A good book for quickly picking up a sense of the field.\n\nKahneman, D., Slovic, P., and Tversky, A. (eds.) (1982). Judgment Under Uncertainty: Heuristics and Biases (New York: Cambridge University Press). This is the edited volume that helped establish the field, written with the outside academic reader firmly in mind. Later research has generalized, elaborated, and better explained the phenomena treated in this volume, but the basic results given are still standing strong.\n\nKahneman, D. and Tversky, A. (eds.) (2000). Choices, Values, and Frames (Cambridge: Cambridge University Press). Gilovich, T. Griffin, D. and Kahneman, D. (2003). Heuristics and Biases. These two edited volumes overview the field of heuristics and biases in its current form. They are somewhat less accessible to a general audience.\n\nReferences\n\nAlpert, M. and Raiffa, H. (1982). A progress report on the training of probability assessors. In Kahneman, D., Slovic, P., and Tversky, A. (eds.), Judgement Under Uncertainty: Heuristics and Biases, pp. 294–305 (Cambridge: Cambridge University Press).\n\nAmbrose, S.H. (1998). Late Pleistocene human population bottlenecks, volcanic winter, and differentiation of modern humans. J. Human Evol., 34, 623–651.\n\nBaron, J. and Greene, J. (1996). Determinants of insensitivity to quantity in valuation of public goods: contribution, warm glow, budget constraints, availability, and prominence. J. Exp. Psychol.: Appl., 2, 107–125.\n\nBostrom, N. (2001). Existential risks: analyzing human extinction scenarios. J. Evol. Technol., 9.\n\nBrenner, L.A., Koehler, D.J., and Rottenstreich, Y. (2002). Remarks on support theory: recent advances and future directions. In Gilovich, T., Griffin, D., and Kahneman, D. (eds.), Heuristics and Biases: The Psychology of Intuitive Judgment, pp. 489–509 (Cambridge: Cambridge University Press).\n\nBuehler, R., Griffin, D., and Ross, M. (1994). Exploring the ‘planning fallacy’: why people underestimate their task completion times. J. Personal. Social Psychol., 67, 366–381.\n\nBuehler, R., Griffin, D., and Ross, M. (1995). It’s about time: optimistic predictions in work and love. Eur. Rev. Social Psychol., 6, 1 -32.\n\nBuehler, R., Griffin, D., and Ross, M. (2002). Inside the planning fallacy: the causes and consequences of optimistic time predictions. In Gilovich, T., Griffin, D., and Kahneman, D. (eds.), Heuristics and Biases: The Psychology of Intuitive Judgment, pp. 250–270 (Cambridge: Cambridge University Press).\n\nBurton, I., Kates, R., and White, G. (1978). Environment as Hazard (New York: Oxford University Press).\n\nCarson, R.T. and Mitchell, R.C. (1995). Sequencing and nesting in contingent valuation surveys. J. Environ. Econ. Manag., 28(2), 155–173.\n\nChapman, G.B. and Johnson, E.J. (2002). Incorporating the irrelevant: anchors in judgments of belief and value. In Gilovich, T., Griffin, D., and Kahneman, D. (eds.), Heuristics and Biases: The Psychology of Intuitive Judgment, pp. 120–138 (Cambridge: Cambridge University Press.\n\nChristensen-Szalanski, J.J.J. and Bushyhead, J.B. (1981). Physicians’use of probabilistic information in a real clinical setting. J. Exp. Psychol. Human Percept. Perf., 7, 928–935. Cialdini, R.B. (2001). Influence: Science and Practice (Boston, MA: Allyn and Bacon).\n\nCombs, B. and Slovic, P. (1979). Causes of death: Biased newspaper coverage and biased judgments. Journalism Quarterly, 56, 837–843.\n\nDawes, R.M. (1988). Rational Choice in an Uncertain World (San Diego, CA: Harcourt, Brace, Jovanovich).\n\nDesvousges, W.H., Johnson, F.R., Dunford, R.W., Boyle, K.J., Hudson, S.P., and Wilson, N. (1993). Measuring natural resource damages with contingent valuation: tests of validity and reliability. In Hausman, J.A. (ed.), Contingent Valuation: A Critical Assessment, pp. 91–159 (Amsterdam: North Holland).\n\nFetherstonhaugh, D., Slovic, P., Johnson, S., and Friedrich, J. (1997). Insensitivity to the value of human life: a study of psychophysical numbing. J. Risk Uncertainty, 14, 238–300.\n\nFinucane, M.L., Alhakami, A., Slovic, P., and Johnson, S.M. (2000). The affect heuristic in judgments of risks and benefits. J. Behav. Decision Making, 13(1), 1–17.\n\nFischhoff, B. (1982). For those condemned to study the past: heuristics and biases in hindsight. In Kahneman, D., Slovic, P., and Tversky, A. (eds.), Judgement Under Uncertainty: Heuristics and Biases, pp. 306–354 (Cambridge: Cambridge University Press).\n\nFischhoff, B. and Beyth, R. (1975). I knew it would happen: remembered probabilities of once-future things. Organ. Behav. Human Perf., 13, 1–16. Fischhoff, B., Slovic, P., and Lichtenstein, S. (1977). Knowing with certainty: the appropriateness of extreme confidence. J. Exp. Psychol Human Percept. Perf., 3, 522–564.\n\nGanzach, Y. (2001). Judging risk and return of financial assets. Organ. Behav. Human Decision Processes, 83, 353–370. Garreau, J. (2005). Radical Evolution: The Promise and Peril of Enhancing Our Minds, Our Bodies-and What It Means to Be Human (New York: Doubleday).\n\nGilbert, D.T. and Osborne, R.E. (1989). Thinking backward: Some curable and incurable consequences of cognitive busyness. J. Person. Social Psychol., 57, 940–949.\n\nGilbert, D.T., Pelham, B.W., and Krull, D.S. (1988). On cognitive busyness: when person perceivers meet persons perceived. J. Person. Social Psychol., 54, 733–740.\n\nGilovich, T. (2000). Motivated Skepticism and Motivated Credulity: Differential Standards of Evidence in the Evaluation of Desired and Undesired Propositions. Presented at the 12th Annual Convention of the American Psychological Society, Miami Beach, FL.\n\nGilovich, T., Griffin, D., and Kahneman, D. (eds.) (2003). Heuristics and Biases: The Psychology of Intuitive Judgment (Cambridge: Cambridge University Press).\n\nGood, I.J. (1952). Rational decisions. J. Royal Statist. Soc, Series B, 14, 107–114.\n\nGriffin, D. and Tversky, A. (1992). The weighing of evidence and the determinants of confidence. Cogn. Psychol., 24, 411–435.\n\nHarrison, G.W. (1992). Valuing public goods with the contingent valuation method: a critique of Kahneman and Knestch. J. Environ. Econ. Manag., 23, 248–257.\n\nHorvitz, E.J., Cooper, G.F., and Heckerman, D.E. (1989). Reflection and action under scarce resources: theoretical principles and empirical study. Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, pp. 1121–1127 (Detroit, MI).\n\nHynes, M.E. and Vanmarke, E.K. (1976). Reliability of embankment performance predictions. Proceedings of the ASCE Engineering Mechanics Division Specialty Conference (Waterloo, Ontario: University of Waterloo Press).\n\nJohnson, E., Hershey, J., Meszaros, J., and Kunreuther, H. (1993). Framing, probability distortions and insurance decisions. J. Risk Uncertainty, 7, 35–51.\n\nKachelmeier, S.J. and Shehata, M. (1992). Examining risk preferences under high monetary incentives: experimental evidence from the People’s Republic of China. Am. Econ. Rev., 82, 1120–1141.\n\nKahneman, D. (1986). Comments on the contingent valuation method. In Cummings, R.G., Brookshire, D.S., and Schulze, W.D. (eds.), Valuing Environmental Goods: A State of the Arts Assessment of the Contingent Valuation Method, pp. 185–194 (Totowa, NJ: Roweman and Allanheld).\n\nKahneman, D. and Knetsch, J.L. (1992). Valuing public goods: the purchase of moral satisfaction. J. Environ. Econ. Manag., 22, 57–70.\n\nKahneman, D., Ritov, I., and Schkade, D.A. (1999). Economic preferences or attitude expressions?: An analysis of dollar responses to public issues. J. Risk Uncertainty, 19, 203–235.\n\nKahneman, D., Slovic, P., and Tversky, A. (eds.) (1982). Judgment Under Uncertainty: Heuristics and Biases (New York: Cambridge University Press).\n\nKahneman, D. and Tversky, A. (eds.) (2000). Choices, Values, and Frames (Cambridge: Cambridge University Press).\n\nKamin, K. and Rachlinski, J. (1995). Ex post ≠ ex ante: determining liability in hindsight. Law Human Behav., 19(1), 89–104.\n\nKates, R. (1962). Hazard and choice perception in flood plain management. Research Paper No. 78. Chicago, IL: University of Chicago, Department of Geography.\n\nKnaup, A. (2005). Survival and longevity in the business employment dynamics data. Monthly Labor Rev., May 2005, 50–56.\n\nKunda, Z. (1990). The case for motivated reasoning. Psychol. Bull., 108(3), 480–498.\n\nLatane, B. and Darley, J. (1969). Bystander ‘Apathy’. Am. Scientist, 57, 244–268.\n\nLichtenstein, S., Fischhoff, B., and Phillips, L.D. (1982). Calibration of probabilities: the state of the art to 1980. In Kahneman, D., Slovic, P., and Tversky, A. (eds .), Judgment Under Uncertainity: Heuristics and Biases, pp. 306–334 (New York: Cambridge University Press).\n\nLichtenstein, S., Slovic, P., Fischhoff, B., Layman, M., and Combs, B. (1978). Judged frequency of lethal events. J. Exp. Psychol.: Human Learn. Memory, 4(6), 551–578.\n\nMcFadden, D. and Leonard, G. (1995). Issues in the contingent valuation of environmental goods: methodologies for data collection and analysis. In Hausman, J.A. (ed.), Contingent Valuation: A Critical Assessment, pp. 165–208 (Amsterdam: North Holland).\n\nNewby-Clark, I.R., Ross, M., Buehler, R., Koehler, D.J., and Griffin, D. (2000). People focus on optimistic and disregard pessimistic scenarios while predicting their task completion times. J. Exp. Psychol. Appl., 6, 171–182.\n\nQuattrone, G.A., Lawrence, C.P., Finkel, S.E., and Andrus, D.C. (1981). Explorations in anchoring: the effects of prior range, anchor extremity, and suggestive hints. Manuscript, Stanford University.\n\nRasmussen, N.C. (1975). Reactor Safety Study: An Assessment of Accident Risks in U.S. Commercial Nuclear Power Plants. NUREG-75/014, WASH-1400 (Washington, DC: U.S. Nuclear Regulatory Commission)\n\nRogers, W.P., Armstrong, N., Acheson, D.C., Covert, E.E., Feynman, R.P., Hotz, R.B., Kutyna, D.J., Ride, S.K., Rummel, R.W., Suffer, D.F., Walker, A.B.C., Jr., Wheelon, A.D., Yeager, C., Keel, A.G., Jr. (1986). Report of the Presidential Commission on the Space Shuttle Challenger Accident. Presidential Commission on the Space Shuttle Challenger Accident. Washington, DC.\n\nSanchiro, C. (2003). Finding Error. Mich. St. L. Rev. 1189.\n\nSchneier, B. (2005). Security lessons of the response to hurricane Katrina. http://www.schneier.com/blog/archives/2005/09/security\\_lesson.html. Viewed on 23 January 2006.\n\nSides, A., Osherson, D., Bonini, N., and Viale, R. (2002). On the reality of the conjunction fallacy. Memory Cogn., 30(2), 191–198.\n\nSlovic, P., Finucane, M., Peters, E., and MacGregor, D. (2002). Rational actors or rational fools: implications of the affect heuristic for behavioral economics. J. Socio-Econ., 31, 329–342.\n\nSlovic, P., Fischoff, B., and Lichtenstein, S. (1982). Facts versus fears: understanding perceived risk. In Kahneman, D., Slovic, P., and Tversky, A. (eds.), Judgment Under Uncertainity: Heuristics and Biases, pp. 463–492 (Cambridge: Cambridge University Press).\n\nStrack, F. and Mussweiler, T. (1997). Explaining the enigmatic anchoring effect: mechanisms of selective accessibility. J. Person. Social Psychol., 73, 437–446.\n\nTaber, C.S. and Lodge, M. (2000). Motivated skepticism in the evaluation of political beliefs. Presented at the 2000 meeting of the American Political Science Association.\n\nTaleb, N. (2001). Fooled by Randomness: The Hidden Role of Chance in Life and in the Markets, pp. 81–85 (New York: Textre).\n\nTaleb, N. (2005). The Black Swan: Why Don’t We Learn that We Don’t Learn? (New York: Random House).\n\nTversky, A. and Kahneman, D. (1973). Availability: a heuristic for judging frequency and probability. Cogn. Psychol., 4, 207–232.\n\nTversky, A. and Kahneman, D. (1974). Judgment under uncertainty: heuristics and biases. Science, 185, 251 -284.\n\nTversky, A. and Kahneman, D. (1982). Judgments of and by representativeness. In Kahneman, D., Slovic, P., and Tversky, A. (eds.), Judgement Under Uncertainty: Heuristics and Biases, pp. 84–98 (Cambridge: Cambridge University Press).\n\nTversky, A. and Kahneman, D. (1983). Extensional versus intuitive reasoning: the conjunction fallacy in probability judgment. Psychol. Rev., 90, 293–315.\n\nWansink, B., Kent, R.J., and Hoch, S.J. (1998). An anchoring and adjustment model of purchase quantity decisions. J. Market. Res., 35 (February), 71–81.\n\nWason, P.C. (1960). On the failure to eliminate hypotheses in a conceptual task. Quarterly J. Exp. Psychol., 12, 129–140.\n\nWilson, T.D., Houston, C., Etling, K.M., and Brekke, N. (1996). A new look at anchoring effects: basic anchoring and its antecedents. J. Exp. Psychol.: General, 4, 387–402.\n\n• 6 • Observation selection effects and global catastrophic risks\n\nMilan M. Ćirković\n\nTreason doth never prosper: what’s the reason? Why if it prosper, none dare call it treason.\n\nSir John Harrington (1561 – 1612)\n\n6.1 Introduction: anthropic reasoning and global risks\n\nDifferent types of global catastrophic risks (GCRs) are studied in various chapters of this book by direct analysis. In doing so, researchers benefit from a detailed understanding of the interplay of the underlying causal factors. However, the causal network is often excessively complex and difficult or impossible to disentangle. Here, we would like to consider limitations and theoretical constraints on the risk assessments which are provided by the general properties of the world in which we live, as well as its contingent history. There are only a few of these constraints, but they are important because they do not rely on making a lot of guesses about the details of future technological and social developments. The most important of these are observation selection effects.\n\nPhysicists, astronomers, and biologists have been familiar with the observational selection effect for a long time, some aspects of them (e.g., Malmquist bias in astronomy¹ or Signor-Lipps effect in paleontology²) being the subject of detailed mathematical modelling. In particular, cosmology is fundamentally incomplete without taking into account the necessary ‘anthropic bias’: the conditions we observe in fundamental physics, as well as in the universe at large, seem atypical when judged against what one would expect as ‘natural’ according to our best theories, and require an explanation compatible with our existence as intelligent observers at this particular epoch in the history of the universe. In contrast, the observation selection effects are still often overlookedin philosophy and epistemology, and practically completely ignoredin risk analysis, since they usually do not apply to conventional categories of risk (such as those usedin insurance modelling). Recently, Bostrom (2002a) laid foundations for a detailed theory of observation selection effects, which has applications for both philosophy and several scientific areas including cosmology, evolution theory, thermodynamics, traffic analysis, game theory problems involving imperfect recall, astrobiology, andquantum physics. The theory of observation selection effects can tell us what we should expect to observe, given some hypothesis about the distribution of observers in the world. By comparing such predictions to our actual observations, we get probabilistic evidence for or against various hypotheses. ³\n\nMany conclusions pertaining to GCRs can be reached by taking into account observation selection effects. For instance, people often erroneously claim that we should not worry too much about existential disasters, since none has happened in the last thousand or even million years. This fallacy needs to be dispelled. Similarly, the conclusion that we are endangered primarily by our own activities and their consequences can be seen most clearly only after we filter out selection effects from our estimates.\n\nIn the rest of this chapter, we shall consider several applications of the anthropic reasoning to evaluation of our future prospects: first the anthropic overconfidence argument stemming from the past-future asymmetry in presence of intelligence observers (Section 6.2) and then the (in) famous Doomsday Argument (DA; Section 6.3). We proceed with Fermi’s paradox and some specific risks related to the concept of extraterrestrial intelligence (Section 6.4) and give a brief overview of the Simulation Argument in connection with GCRs in Section 6.5 before we pass on to concluding remarks.\n\n6.2 Past-future asymmetry and risk inferences\n\nOne important selection effect in the study of GCRs arises from the breakdown of the temporal symmetry between past and future catastrophes when our existence at the present epoch and the necessary conditions for it are taken into account. In particular, some of the predictions derived from past records are unreliable due to observation selection, thus introducing an essential qualification to the general and often uncritically accepted gradualist principle that ‘the past is a key to the future’. This resulting anthropic overconfidence bias is operative in a wide range of catastrophic events, and leads to potentially dangerous underestimates of the corresponding risk probabilities. After we demonstrate the effect on a toy model applied to a single catastrophic event situation in Section 6.2.1, we shall develop the argument in more detail in Section 6.2.2, while considering its applicability conditions for various types of GCRs in Section 6.2.3. Finally, we show that with the help of additional astrobiological information, we may do even better and constrain the probabilities of some very specific exogenous risks in Section 6.2.4. ⁴\n\n[Image]\n\nFig.6.1 A schematic presentation of the single-event toy model. The evidence E consists of our present-day existence.\n\n6.2.1 A simplified model\n\nConsider the simplest case of a single very destructive global catastrophe, for instance, a worse-than-Toba super-volcanic eruption (see Chapter 10, this volume). The evidence we take into account in a Bayesian manner is the fact of our existence at the present epoch; this, in turn, implies the existence of a complicated web of evolutionary processes upon which our emergence is contingent; we shall neglect this complication in the present binary toy model and shall return to it in the next subsection. The situation is schematically shown in Fig. 6.1. The a priori probability of catastrophe is P and the probability of human extinction (or a sufficiently strong perturbation leading to divergence of evolutionary pathways from the morphological subspace containing humans) upon the catastrophic event is Q . We shall suppose that the two probabilities are (1) constant, (2) adequately normalized, and (3) applicable to a particular well-defined interval of past time. Event B₂ is the occurrence of the catastrophe, and by E we denote the evidence of our present existence.\n\nThe direct application of the Bayes formula for expressing conditional probabilities in form\n\n[Image]\n\nusing our notation, yields the a posteriori probability as\n\n[Image]\n\nBy simple algebraic manipulation, we can show that\n\n[Image]\n\nthat is, we tend to underestimate the true catastrophic risk. It is intuitively clear why: the symmetry between past and future is broken by the existence of an evolutionary process leading to our emergence as observers at this particular epoch in time. We can expect a large catastrophe tomorrow, but we cannot – even without any empirical knowledge – expect to find traces of a large catastrophe that occurred yesterday, since it would have pre-empted our existence today.\n\nNote that\n\n[Image]\n\nVery destructive events completely destroy predictability! An obvious consequence is that absolutely destructive events, which humanity has no chance of surviving at all (Q = 0), completely annihilate our confidence in predicting from past occurrences. This almost trivial conclusion is not, however, widely appreciated.\n\nThe issue at hand is the possibility of vacuum phase transition (see Chapter 16, this volume). This is an example par excellence of the Q = 0 event: its ecological consequences are such that the extinction not only of humanity but also of the terrestrial biosphere is certain. ⁵ However, the anthropic bias was noticed neither by Hut and Rees (1983), nor by many of subsequent papers citing it. Instead, these authors suggested that the idea of high-energy experiments triggering vacuum phase transition can be rejected by comparison with the high-energy events occurring in nature. Since the energies of particle collisions taking place, for instance, in interactions between cosmic rays and the Earth’s atmosphere or the solid mass of the Moon are still orders of magnitude higher than those achievable in human laboratories in the near future, and with plausible general assumptions on the scaling of the relevant reaction cross-sections with energy, Hut and Rees concluded that in view of the fact that the Earth (and the Moon) survived the cosmic-ray bombardment for about 4.5 Gyr, we are safe for the foreseeable future. In other words, their argument consists of the claim that the absence of catastrophic event of this type in our past light cone gives us the information that the probability P (or its rate per unit time p) has to be so extremely small, that any fractional increase caused by human activities (like building and operating of a new particle collider) is insignificant. If, for example, p is 10⁻⁵⁰ per year, then its doubling or even a 1000-fold increase by deliberate human activities is arguably unimportant. Thus, we can feel safe with respect to the future on the basis of our observations about the past. As we have seen, there is a hole in this argument: all observers everywhere will always find that no such disaster has occurred in their backward light cone – and this is true whether such disasters are common or rare.\n\n6.2.2 Anthropic overconfidence bias\n\nIn order to predict the future from records of the past, scientists use a wide variety of methods with one common feature: the construction of an empirical distribution function of events of a specified type (e.g., extraterrestrial impacts, supernova/gamma-burst explosions, or super-volcanic eruptions). In view of the Bayesian nature of our approach, we can dub this distribution function the a posteriori distribution function. Of course, deriving such function from observed traces is often difficult and fraught with uncertainty. For instance, constructing the distribution function of asteroidal/cometary impactors from the sample of impact craters discovered on Earth (Earth Impact Database, 2005) requires making physical assumptions, rarely uncontroversial, about physical parameters of impactors such as their density, as well as astronomical (velocity distribution of Earth-crossing objects) and geological (the response of continental or oceanic crust to a violent impact, formation conditions of impact glasses) input.\n\nHowever, the a posteriori distribution is not the end of the story. What we are interested in is the ‘real’ distribution of chances of events (or their causes), which is ‘given by Nature’ but not necessarily completely revealed by the historical record. This underlying objective characteristic of a system can be called its a priori distribution function. It reflects the (evolving) state of the system considered without reference to incidental spatio-temporal specifics. Notably, the a priori distribution function describes the stochastic properties of a chance-generating system in nature rather than the contingent outcomes of that generator in the particular history of a particular place (in this case planet Earth). The relationship between a priori and a posteriori distribution functions for several natural catastrophic hazards is shown in a simplified manner in Table 6.1. Only a priori distribution is useful for predicting the future, since it is not constrained by observation selection effects.\n\nTable 6.1 Examples of Natural Hazards Potentially Comprising GCRs and Two Types of Their Distribution Functions\n\n[Image]\n\n[Image]\n\nFig. 6.2 A sketch of the common procedure for deriving predictions about the future from the past records. This applies to quite benign events as well as to GCRs, but only in the latter case do we need to apply the correction symbolically shown in dashed-line box. Steps framed by dashed line are – surprisingly enough – usually not performed in the standard risk analysis; they are, however, necessary in order to obtain unbiased estimates of the magnitude of natural GCRs.\n\nThe key insight is that the inference to the inherent (a priori) distribution function from the reconstructed empirical (a posteriori) distribution must take account of an observation selection effect (Fig 6.2). Catastrophic events exceeding some severity threshold eliminate all observers and are hence unobservable. Some types of catastrophes may also make the existence of observers on a planet impossible in a subsequent interval of time, the size of which might be correlated with the magnitude of the catastrophe. Because of this observation selection effect, the events reflected in our historical record are not sampled from the full events space but rather from just the part of the events space that lies beneath the ‘anthropic compatibility boundary’ drawn on the time-severity diagram for each type of catastrophe. This biased sampling effect must be taken into account when we seek to infer the objective chance distribution from the observed empirical distribution of events. Amazingly, it is usually not taken into account in most of the real analyses, perhaps ‘on naive ergodic grounds’. ⁶\n\nThis observation selection effect is in addition to what we might call ‘classical’ selection effects applicable to any sort of event (e.g., removal of traces of events in the distant part by erosion and other instances of the natural entropy increase; see Woo, 1999). Even after these classical selection effects have been taken into account in the construction of an empirical (a posteriori) distribution, the observation selection effects remain to be corrected for in order to derive the a priori distribution function.\n\n6.2.3 Applicability class of risks\n\nIt seems obvious that the reasoning sketched above applies to GCRs of natural origin since, with one partial exception there is no unambiguous way of treating major anthropogenic hazards (like the global nuclear war or misuse of biotechnology) statistically. This is a necessary, but not yet sufficient, condition for the application of this argument. In order to establish the latter, we need natural catastrophic phenomena which are\n\n• sufficiently destructive (at least in a part of the severity spectrum)\n\n• sufficiently random (in the epistemic sense) and\n\n• leaving traces in the terrestrial (or in general local) record allowing statistical inference.\n\nThere are many conceivable threats satisfying these broad desiderata. Some examples mentioned in the literature comprise the following:\n\n1. asteroidal/cometary impacts (severity gauged by the Turin scale or the impact crater size)\n\n2. super-volcanism episodes (severity gauged by the so-called volcanic explosivity index (VEI) or a similar measure)\n\n3. Supernovae/gamma-ray bursts (severity gauged by the distance/ intrinsic power)\n\n4. superstrong Solar flares (severity gauged by the spectrum/intrinsic power of electromagnetic and corpuscular emissions).\n\nThe crucial point here is to have events sufficiently influencing our past, but without too much information which can be obtained externally to the terrestrial biosphere. Thus, there are differences between kinds of catastrophic events in this regard. For instance, the impact history of the Solar System (or at least the part where the Earth is located) is, in theory, easier to be obtained for the Moon, where erosion is orders of magnitude weaker than on Earth. In practice, in the current debates about the rates of cometary impacts, it is precisely the terrestrial cratering rates that are used as an argument for or against existence of a large dark impactor population (see Napier, 2006; Chapter 11 in this volume), thus offering a good model on which the anthropic bias can, at least potentially, be tested. In addition to the impact craters, there is a host of other traces one attempts to find in field work which contribute to the building of the empirical distribution function of impacts, notably searching for chemical anomalies or shocked glasses (e.g., Schultz et al., 2004).\n\nSupernovae/gamma-ray bursts distribution frequencies are also inferred (albeit much less confidently!) from observations of distant regions, notably external galaxies similar to the Milky Way. On one hand, finding local traces of such events in the form of geochemical anomalies (Dreschhoff and Laird, 2006) is excessively difficult and still very uncertain. This external evidence decreases the importance of the Bayesian probability shift. On the other hand, the destructive capacities of such events have been known and discussed for quite some time (see Chapter 12, this volume; Hunt, 1978; Ruderman, 1974; Schindewolf, 1962), and have been particularly enhanced recently by successful explanation of hitherto mysterious gamma-ray bursts as explosions occurring in distant galaxies (Scalo and Wheeler, 2002). The possibility of such cosmic explosions causing a biotic crisis and possibly even a mass extinction episode has returned with a vengeance (Dar et al., 1998; Melott et al., 2004).\n\nSuper-volcanic episodes (see Chapter 10, this volume) – both explosive pyroclastic and non-explosive basaltic eruptions of longer duration – are perhaps the best example of global terrestrial catastrophes (which is the rationale for choosing it in the toy model above). They are interesting for two additional recently discovered reasons: (1) Super-volcanism creating Siberian basaltic traps almost certainly triggered the end-Permian mass extinction (251.4 ± 0.7 Myr before present), killing up to 96% of the terrestrial non-bacterial species (e.g., Benton, 2003; White, 2002). Thus, its global destructive potential is today beyond doubt. (2) Super-volcanism is perhaps the single almost-realized existential catastrophe: the Toba super-eruption probably reduced human population to approximately 1000 individuals, nearly causing the extinction of humanity (Ambrose, 1998; Rampino and Self, 1992). In that light, we would do very well to consider seriously this threat which, ironically in views of historically well-known calamities like destructions of Santorini, Pompeii, or Tambora, has become an object of concern only very recently (e.g., McGuire, 2002; Roscoe, 2001).\n\nAs we have seen, one frequently cited argument in the debate on GCRs, the one of Hut and Rees (1983), actually demonstrates how misleading (but comforting!) conclusions about risk probabilities can be reached when anthropic overconfidence bias is not taken into account.\n\n6.2.4 Additional astrobiological information\n\nThe bias affecting the conclusions of Hut and Rees (1983) can be at least partially corrected by using the additional information coming from astrobiology, which has been recently done by Tegmark and Bostrom (2005). Astrobiology is the nascent and explosively developing discipline that deals with three canonical questions: How does life begin and develop? Does life exist elsewhere in the universe? What is the future of life on Earth and in space? One of the most interesting of many astrobiological results of recent years has been the study by Lineweaver (2001), showing that the Earth-like planets around other stars in the Galactic Habitable Zone (GHZ; Gonzalez et al., 2001) are, on average, 1.8 ± 0.9 Gyr older than our planet (see also the extension of this study by Lineweaver et al., 2004). His calculations are based on the tempo of chemical enrichment as the basic precondition for the existence of terrestrial planets. Moreover, Lineweaver’s results enable constructing a planetary age distribution, which can be used to constrain the rate of particularly destructive catastrophes, like the vacuum decay or a strangelet catastrophe.\n\nThe central idea of the Tegmark and Bostrom study is that planetary age distribution, as compared to the Earth’s age, bounds the rate for many doomsday scenarios. If catastrophes that permanently destroy or sterilize a cosmic neighbourhood were very frequent, then almost all intelligent observers would arise much earlier than we did, since the Earth is a latecomer within the habitable planet set. Using the Lineweaver data on planetary formation rates, it is possible to calculate the distribution of birth rates for intelligent species under different assumptions about the rate of sterilization by catastrophic events. Combining this with the information about our own temporal location enables the rather optimistic conclusion that the cosmic (permanent) sterilization rate is at the most of order of one per 10⁹ years.\n\nHow about catastrophes that do not permanently sterilize a cosmic neighbourhood (preventing habitable planets from surviving and forming in that neighbourhood)? Most catastrophes are obviously in this category. Is biological evolution on the other habitable planets in the Milky Way influenced more or less by catastrophes when compared to the Earth? We cannot easily say, because the stronger the catastrophic stress is (the larger analogue of our probability 1 – Q is on average), the less useful information can we extract about the proximity – or else – of our particular historical experience to what is generally to be expected. However, future astrobiological studies could help us to resolve this conundrum. Some data already exist. For instance, one recently well-studied case is the system of the famous nearby Sun-like star Tau Ceti which contains both planets and a massive debris disc, analogous to the Solar System Kuiper belt. Modelling of Tau Ceti’s dust disc observations indicate, however, that the mass of the colliding bodies up to 10 km in size may total around 1.2 Earth-masses, compared with 0.1 Earth-masses estimated to be in the Solar System’s Edgeworth-Kuiper Belt (Greaves et al., 2004). Thus, Tau Ceti’s dust disc may have around 10 times more cometary and asteroidal material than is currently found in the Solar System – in spite of the fact that Tau Ceti seems to be about twice as old as the Sun (and it is conventionally expected the amount of such material to decrease with time). Why the Tau Ceti System would have a more massive cometary disc than the Solar System is not fully understood, but it is reasonable to conjecture that any hypothetical terrestrial planet of this extrasolar planetary system has been subjected to much more severe impact stress than the Earth has been during the course of its geological and biological history. ⁷\n\n6.3 Doomsday Argument\n\nThe Doomsday Argument (DA) is an anthropic argument purporting to show that we have systematically underestimated the probability that humankind will become extinct relatively soon. Originated by the astrophysicist Brandon Carter and developed at length by the philosopher John Leslie, ⁸ DA purports to show that we have neglected to fully take into account the indexical information residing in the fact about when in the history of the human species we exist. Leslie (1996) – in what can be considered the first serious study of GCRs facing humanity and their philosophical aspects – gives a substantial weight to DA, arguing that it prompts immediate re-evaluation of probabilities of extinction obtained through direct analysis of particular risks and their causal mechanisms.\n\nThe core idea of DA can be expressed through the following thought experiment. Place two large urns in front of you, one of which you know contains 10 balls, the other a million, but you do not know which is which. The balls in each urn are numbered 1, 2, 3, 4, … Now take one ball at random from the left urn; it shows the number 7. This clearly is a strong indication that the left urn contains only 10 balls. If the odds originally were 50:50 (identically looking urns), an application of Bayes’ theorem gives the posterior probability that the left urn is the one with only 10 balls as Pp\\_(ost) (n = 10) = 0.99999. Now consider the case where instead of two urns you have two possible models of humanity’s future, and instead of balls you have human individuals, ranked according to birth order. One model suggests that the human race will soon become extinct (or at least that the number of individuals will be greatly reduced), and as a consequence the total number of humans that ever will have existed is about 100 billion. Even the vociferous optimists would not put the prior probability of such a development excessively low – certainly not lower than the probability of the largest certified natural disaster (so-called ‘asteroid test’) of about 10⁻⁸ per year. The other model indicates that humans will colonize other planets, spread through the Galaxy, and continue to exist for many future millennia; we consequently can take the number of humans in this model to be of the order of, say, 10¹⁸. As a matter of fact, you happen to find that your rank is about 60 billion. According to Carter and Leslie, we should reason in the same way as we did with the urn balls. That you should have a rank of 60 billion is much more likely if only 100 billion humans ever will have lived than if the number was 10¹⁸. Therefore, by Bayes’ theorem, you should update your beliefs about mankind’s prospects and realize that an impending doomsday is much more probable than you thought previously. ⁹\n\nIts underlying idea is formalized by Bostrom (1999, 2002a) as the Self-sampling Assumption (SSA):\n\nSSA: One should reason as if one were a random sample from the set of all observers in one’s reference class.\n\nIn effect, it tells us that there is no structural difference between doing statistics with urn balls and doing it with intelligent observers. SSA has several seemingly paradoxical consequences, which are readily admitted by its supporters; for a detailed discussion, see Bostrom (2001). In particular, the reference class problem (’what counts as an observer?’) has been plaguing the entire field of anthropic reasoning. A possible response to it is an improved version of SSA, ‘Strong SSA’ (SSSA):\n\nSSSA: One should reason as if one’s present observer-moment were a random sample from the set of all observer-moments in its reference class.\n\nIt can be shown that by taking more indexical information into account than SSA does (SSA considers only information about which observer you are, but you also have information about, for example, which temporal part of this observer = observer-moment you are at the current moment), it is possible to relativize your reference class so that it may contain different observers at different times, depending partly on your epistemic situation on the occasion. SSA, therefore, describes the correct way of assigning probabilities only in certain special cases; and revisiting the existing arguments for SSA, we find that this is all they establish. In particular, DA is inconclusive. It is shown to depend on particular assumptions about the part of one’s subjective prior probability distribution that has to do with indexical information – assumptions that one is free to reject, and indeed, arguably, ought to reject in light of their strongly counterintuitive consequences. Thus, applying the argument to our actual case may be a mistake; at least, a serious methodological criticism could be made of such an inference.\n\n6.4 Fermi’s paradox\n\nFermi’s paradox (also known as the ‘Great Silence’ problem) consists in the tension between (1) naturalistic origin of life and intelligence, as well as astrophysical sizes and ages of our Galaxy and(2) the absence of extraterrestrials in the Solar System, or any other traces of extraterrestrial intelligent activities in the universe. ¹⁰ In particular, the lack of macroengineering (or astroengineering) activities observable from interstellar distances tells us that it is not the case that life evolves on a significant fraction of Earth-like planets and proceeds to develop advanced technology, using it to colonize the universe or perform astroengineering feats in ways that would have been detected with our current instrumentation. The characteristic time for colonization of the Galaxy, according to Fermi’s argument, is 10⁶-10⁸ years, making the fact that the Solar System is not colonizedhard to explain, if not for the absence of extraterrestrial cultures. There must be (at least) one Great Filter – an evolutionary step that is extremely improbable – somewhere on the line between Earth-like planet and colonizing-in-detectable-ways civilization (Hanson, 1999). If the Great Filter is not in our past, we must fear it in our (near) future. Maybe almost every civilization that develops a certain level of technology causes its own extinction.\n\nFermi’s paradox has become significantly more serious, even disturbing, of late. This is due to several independent lines of scientific and technological advances occurring during the last two decades:\n\n• The discovery of nearly 250 extrasolar planets so far, on an almost weekly basis (for regular updates see http://www.obspm.fr/planets). Although most of them are ‘hot Jupiters’ and not suitable for life as we know it (some of their satellites could still be habitable, however; see Williams et al., 1997), many other exoworlds are reported to be parts of systems with stable circumstellar habitable zones (Asghari et al., 2004; Beaugé et al., 2005; Noble et al., 2002). It seems that only the selection effects and capacity of present-day instruments stand between us and the discovery of Earth-like extrasolar planets, envisioned by the new generation of orbital observatories.\n\n• Improved understanding of the details of chemical and dynamical structure of the Milky Way and its GHZ. In particular, the already mentioned calculations of Lineweaver (2001; Lineweaver et al., 2004) on the histories of Earth-like planet formation show their median age as 6.4± 0.7 Gyr, significantly larger than the Earth’s age.\n\n• Confirmation of the relatively rapid origination of life on early Earth (e.g., Mojzsis et al., 1996); this rapidity, in turn, offers weak probabilistic support to the idea of many planets in the Milky Way inhabited by at least simple life forms (Lineweaver and Davis, 2002).\n\n• Discovery of extremophiles and the general resistance of simple life forms to much more severe environmental stresses than it was hitherto thought possible (Cavicchioli, 2002). These include representatives of all three great domains of terrestrial life (Bacteria, Archaea, and Eukarya), showing that the number and variety of cosmic habitats for life are probably much larger than conventionally imagined.\n\n• Our improved understanding in molecular biology and biochemistry leading to heightened confidence in the theories of naturalistic origin of life (Bada, 2004; Ehrenfreund et al., 2002; Lahav et al., 2001). The same can be said, to a lesser degree, for our understanding of the origin of intelligence and technological civilization (e.g., Chernavskii, 2000).\n\n• Exponential growth of the technological civilization on Earth, especially manifested through Moore’s Law and other advances in information technologies (see, for instance, Bostrom, 2000; Schaller, 1997).\n\n• Improved understanding of the feasibility of interstellar travel in both the classical sense (e.g., Andrews, 2003) and in the more efficient form of sending inscribed matter packages over interstellar distances (Rose and Wright, 2004).\n\n• Theoretical grounding for various astroengineering/macroengineering projects (Badescu and Cathcart, 2000, 2006; Korycansky et al., 2001) potentially detectable over interstellar distances. Especially important in this respect is the possible synergistic combination of astroengineering and computation projects of advanced civilizations, like those envisaged by Sandberg (1999).\n\nAlthough admittedly uneven and partially conjectural, this list of advances and developments (entirely unknown at the time of Tsiolkovsky’s and Fermi’s original remarks and even Viewing’s, Hart’s and Tipler’s later re-issues) testifies that Fermi’s paradox is not only still with us more than half a century later, but that it is more puzzling and disturbing than ever.\n\nThere is a tendency to interpret Fermi’s paradox as an argument against contemporary Search for Extra-Terrestrial Intelligence (SETI) projects (e.g., Tipler, 1980). However, this is wrong, since the argument is at best inconclusive – there are many solutions which retain both the observed’Great Silence’ and the rationale for engaging in vigorous SETI research (Gould 1987; Webb, 2002). Furthermore, it is possible that the question is wrongly posed; in an important recent paper, the distinguished historian of science Steven J. Dick argued that there is a tension between SETI, as conventionally understood, and prospects following exponential growth of technology as perceived in recent times on Earth (Dick, 2003, p. 66):\n\n[I]f there is a flaw in the logic of the Fermi paradox and extraterrestrials are a natural outcome of cosmic evolution, then cultural evolution may have resulted in a postbiological universe in which machines are the predominant intelligence. This is more than mere conjecture; it is a recognition of the fact that cultural evolution-the final frontier of the Drake Equation – needs to be taken into account no less than the astronomical and biological components of cosmic evolution. [emphasis in the original]\n\nIt is easy to understand the necessity of redefining SETI studies in general and our view of Fermi’s paradox in particular in this context. For example, post-biological evolution makes those behavioural and social traits like territoriality or expansion drive (to fill the available ecological nïche) which are – more or less successfully – ‘derived from nature’, lose their relevance. Other important guidelines must be derived which will encompass the vast realm of possibilities stemming from the concept of post-biological evolution. In addition, we have witnessed substantial research leading to a decrease in confidence in the so-called Carter’s (1983) ‘anthropic’ argument, the other mainstay of SETI scepticism (Ćirković et al., 2007; Livio, 1999; Wilson, 1994). All this is accompanied by an increased public interest in astrobiology and related issues (such as Cohen and Stewart, 2002; Grinspoon, 2003; Ward and Brownlee, 2000).\n\n6.4.1 Fermi’s paradox and GCRs\n\nFaced with the aggravated situation vis-à-vis Fermi’s paradox the solution is usually sought in either (1) some version of the ‘rare Earth’ hypothesis (i.e., the picture which emphasizes inherent uniqueness of evolution on our planet and hence uniqueness of human intelligence and technological civilization in the Galactic context), or (2) ‘neo-catastrophic’ explanations (ranging from the classical ‘mandatory self-destruction’ explanation, championed for instance by disenchanted SETI pioneers from the Cold War epoch like Sebastian von Hoerner or Iosif Shklovsky, to the modern emphasis on mass extinctions in the history of life and the role of catastrophic impacts, gamma-ray bursts, and similar dramatic events). Both these broad classes of hypotheses are unsatisfactory on several counts: for instance, the ‘rare Earth’ hypotheses reject the usual Copernican assumption (the Earth is a typical member of the planetary set), and neo-catastrophic explanations usually fail to pass the non-exclusivity requirement¹¹ (but see Ćirković, 2004, 2006). None of these is a clear, straight forward solution. It is quite possible that a ‘patchwork solution’, comprised of a combination of suggested and other solutions, remains our best option for solving this deep astrobiological problem. This motivates the continuation of the search for plausible explanations of Fermi’s paradox. It should be emphasized that even the founders of ‘rare Earth’ picture readily admit that simple life forms are ubiquitous throughout the universe (Ward and Brownlee, 2000). It is clear that with the explosive development of astrobiological techniques, very soon we shall be able to directly test this default conjecture.\n\nOn the other hand, neo-catastrophic explanations pose important dilemmas related to GCRs – if the ‘astrobiological clock’ is quasiperiodically reset by exogenous events (like Galactic gamma-ray bursts; Annis, 1999; Ćirković, 2004, 2006), how dangerous is it to be living at present? Seemingly paradoxically, our prospects are quite bright under this hypothesis, since (1) the frequency of forcing events decreases in time and (2) exogenous forcing implies ‘astrobiological phase transition’ – namely that we are currently located in the temporal window enabling emergence and expansion of intelligence throughout the Galaxy. This would give a strong justification to our present and future SETI projects (Ćirković, 2003). Moreover, this class of solutions of Fermi’s paradox does not suffer from usual problems like assuming something about arguably nebulous extraterrestrial sociology in contrast to solutions such as the classical ‘Zoo’ or ‘Interdict’ hypotheses (Ball, 1973; Fogg, 1987).\n\nSomewhat relatedto this issue is Olum’s anthropic argument dealing with the recognition that, if large interstellar civilizations are physically possible, they should, in an infinite universe strongly suggested by modern cosmology, predominate in the total tally of observers (Olum, 2004). As shown by Ćirković (2006), neo-catastrophic solution based on the GRB-forcing of astrobiological timescales can successfully resolve this problem which, as many other problems in astrobiology, including Carter’s argument, is based upon implicit acceptance of insidious gradualist assumptions. In particular, while in the equilibrium state most of observers would indeed belong to large (in an appropriately loose sense) civilizations, it is quite reasonable to assume that such an equilibrium has not been established yet. On the contrary, we are located in the phase-transition epoch, in which all civilizations are experiencing rapid growth and complexification. Again, neo-catastrophic scenarios offer a reasonable hope for the future of humanity, in agreement with all our empirical evidence.\n\nThe relevance of some of particular GCRs discussed in this book to Fermi’s paradox has been repeatedly addressed in recent years (e.g., Chapter 10, this volume; Rampino, 2002). It seems that the promising way for future investigations is formulation of joint ‘risk function’ describing all (both local and correlated) risks facing a habitable planet; such a multi-component function will act as a constraint to the emergence of intelligence and in conjunction with the planetary formation rates, this should give us specific predictions on the number and spatiotemporal distribution of SETI targets.\n\n6.4.2 Risks following from the presence of extraterrestrial intelligence\n\nA particular GCR not covered elsewhere in this book is the one of which humans have been at least vaguely aware since 1898 and the publication of H.G. Wells’ The War of the Worlds (Wells, 1898) – conflict with hostile extraterrestrial intelligent beings. The famous Orson Welles radio broadcast for Halloween on 30 October 1938 just reiterated the presence of this threat in the mind of humanity. The phenomenon of the mass hysteria displayed on that occasion has proved a goldmine for psychologists and social scientists (e.g., Bulgatz, 1992; Cantril, 1947) and the lessons are still with us. However, we need to recognize that analysing various social and psychological reactions to such bizarre events could induce disconfirmation bias (see Chapter 5 on cognitive biases in this volume) in the rational consideration of the probability, no matter how minuscule, of this and related risks.\n\nThe probability of this kind of GCR obviously depends on how frequent extraterrestrial life is in our astrophysical environment. As discussed in the preceding section, opinions wildly differ on this issue. ¹² Apart from a couple of ‘exotic’ hypotheses (‘Zoo’, ‘Interdict’, but also the simulation hypotheses below), most researchers would agree that the average distance between planets inhabited by technological civilizations in the Milky Way is at least of the order of 10² parsecs. ¹³ This directs us to the second relevant issue for this particular threat: apart from the frequency of extraterrestrial intelligence (which is a necessary, but not sufficient condition for this GCR), the reality of the risk depends on the following:\n\n1. the feasibility of conflict over huge interstellar distances\n\n2. the magnitude of threat such a conflict would present for humanity in the sense of general definition of GCRs, and\n\n3. motivation and willingness of intelligent communities to engage in this form of conflict.\n\nItem (1) seems doubtful, to say the least, if the currently known laws of physics hold without exception; in particular, the velocity limit ensures that such conflict would necessary take place over timescales measured by at least centuries and more probably millennia or longer (compare the timescales of wars between terrestrial nations with the transportation timescales on Earth!). The limitations of computability in chaotic systems would obviate the detailed strategic thinking and planning on such long timescales even for superintelligences employed by the combatants. In addition, the nature of clumpy astronomical distribution of matter and resources, which are tightly clustered around the central star(s) of planetary systems, ensures that a takeover of an inhabited and industrialized planetary system would be possible only in the case of large technological asymmetry between the parties in the conflict. We have seen, in the discussion of Fermis paradox that, given observable absence of astroengineering activities, such an asymmetry seems unlikely. This means, among other things, that even if we encounter hostile extraterrestrials, the conflict need not jeopardize the existence of human (or post-human) civilization in the Solar System and elsewhere. Finally, factor (3) is even more unlikely and not only for noble, but at present hardly conceivable, ethical reasons. If we take seriously the lessons of sociobiology that suggest that historical human warfare is part of the ‘Darwinian baggage inherited by human cultures, an obvious consequence is that, with the transition to a post-biological phase of our evolution, any such archaic impulses will be obviated. Per analogiam, this will apply to other intelligent communities in the Milky Way. On the other hand, the resources of even our close astronomical environment are so vast, as is the space of efficiency-improving technologies, that no real ecological pressures can arise to prompt imperial-style expansion and massive colonization over interstellar distances. Even if such unlikely pressures arise, it seems clear that the capacities of seizing defended resources would always (lacking the already mentioned excessive technological asymmetry) be far less cost-effective than expansion into the empty parts of the Milky Way and the wider universe.\n\nThere is one particular exception to this generally optimistic view on the (im)possibilities of interstellar warfare which can be worrying: the so-called ‘deadly probes’ scenario for explaining Fermi’s paradox (e.g., Brin, 1983; for fictional treatments of this idea see Benford, 1984; Schroeder, 2002). If the first or one of the first sets of self-replicating von Neumann probes to be released by Galactic civilizations was either programmed to destroy other civilizations or mutated to the same effect (see Benford, 1981), this would explain the ‘Great Silence’ by another non-exclusive risk. In the words of Brin (1983) ‘[i]ts logic is compellingly self-consistent. The ‘deadly probes scenario seems to be particularly disturbing in conjunction with the basic theme of this book, since it shares some of the features of conventional technological optimism vis-à-vis the future of humanity: capacity of making self-replicating probes, AI, advanced spaceflight propulsion, probably also nanotechnology.\n\nIt is unfortunate that the ‘deadly probes’ scenario has not to date been numerically modelled. If is to be hoped that future astrobiological and SETI research will explore these possibilities in the more serious and quantitative manner. In the same time, our astronomical SETI efforts, especially those aimed at the discovery of astroengineering projects (Freitas, 1985; Ćirković and Bradbury, 2006) should be intensified. The discovery of any such project or artefact (see Arnold, 2005) could, in fact, gives us strong probabilistic argument against the ‘deadly probes’ risk and thus be of long-term assuring comfort. ¹⁴\n\nA related, but distinct, set of threats follows from the possible inadvertent activities of extraterrestrial civilizations which can bring the destruction to humanity. A clear example of such activities are quantum field theory-related risks (see Chapter 16, this volume), especially the vacuum decay triggering. A ‘new vacuum’ bubble produced anywhere in the visible universe – say by powerful alien particle accelerators – would expand at the speed of light, possibly encompassing the Earth and humanity at some point. Clearly, such an event could, in principle, have happened somewhere within our cosmological horizon long ago, the expanding bubble not yet having reached our planet. Fortunately, at least with a set of rather uncontroversial assumptions, the reasoning of Tegmark and Bostrom explained in Section 6.2.4 above applies to this class of events, and the relevant probabilities can be rather tightly constrained by using additional astrobiological information. The conclusion is optimistic since it gives a very small probability that humanity will be destroyed in this manner in the next billion years.\n\n6.5 The Simulation Argument\n\nA particular speculative application of the theory of observation selection leads to the so-called Simulation Argument of Bostrom (2003). If we accept the possibility that a future advanced human (post-human) civilization might have the technological capability of running ‘ancestor-simulations’ – computer simulations of people like our historical predecessors sufficiently detailed for the simulated people to be conscious – we run into an interesting consequence illuminated by Bostrom (2003). Starting from a rather simple reasoning for the fraction of observers living in simulations (fsim )\n\n[Image]\n\nBostrom reaches the intriguing conclusion that this commits one to the belief that either (1) we are living in the simulation, or (2) we are almost certain never to reach the post-human stage, or (3) almost all post-human civilizations lack individuals who run significant numbers of ancestor-simulations, that is, computer-emulations of the sort of human-like creatures from which they evolved. Disjunct (3) looks at first glance most promising, but it should be clear that it suggests a quite uniform or monolithic social organization of the future, which could be hallmark of totalitarianism, a GCR in its own right (see Chapter 22, this volume). The conclusion of the Simulation Argument appears to be a pessimistic one, for it narrows down quite substantially the range of positive future scenarios that are tenable in light of the empirical information we now have. The Simulation Argument increases the probability that we are living in a simulation (which may in many subtle ways affect our estimates of how likely various outcomes are) and it decreases the probability that the post-human world would contain lots of free individuals who have large computational resources and human-like motives. But how does it threaten us right now?\n\nIn a nutshell, the simulation risk lies in disjunct (1), which implies the possibility that the simulation we inhabit could be shut down. As Bostrom (2002b, P. 7) writes: ‘While to some it may seem frivolous to list such a radical or “philosophical” hypothesis next the concrete threat of nuclear holocaust, we must seek to base these evaluations on reasons rather than untutored intuition.’ Until a refutation appears of the argument presented in Bostrom (2003), it would be intellectually dishonest to neglect to mention simulation-shutdown as a potential extinction mode. Until a refutation appears of the argument presented in Bostrom (2003), it would intellectually dishonest to neglect to mention simulation-shutdown as a potential extinction mode.\n\nA decision to terminate our simulation, taken on the part of the post-human director (under which we shall subsume any relevant agency), may be prompted by our actions or by any number of exogenous factors. Such exogenous factors may include generic properties of such ancestor-simulations such as fixed temporal window or fixed amount of allocated computational resources, or emergent issues such as a realization of a GCR in the director’s world. Since we cannot know much about these hypothetical possibilities, let us pick one that is rather straight forward to illustrate how a risk could emerge: the energy cost of running an ancestor-simulation.\n\nFrom the human experience thus far, especially in sciences such as physics and astronomy, the cost of running large simulations may be very high, though it is still dominated by the capital cost of computer processors and human personnel, not the energy cost. However, as the hardware becomes cheaper and more powerful and the simulating tasks more complex, we may expect that at some point in future the energy cost will become dominant. Computers necessarily dissipate energy as heat, as shown in classical studies of Landauer (1961) and Brillouin (1962) with the finite minimum amount of heat dissipation required per processing of 1 bit of information. ¹⁵ Since the simulation of complex human society will require processing a huge amount of information, the accompanying energy cost is necessarily huge. This could imply that the running of ancestor-simulations is, even in advanced technological societies, expensive and/or subject to strict regulation. This makes the scenario in which a simulation runs until it dissipates a fixed amount of energy allocated in advance (similar to the way supercomputer or telescope resources are allocated for today’s research) more plausible. Under this assumption, the simulation must necessarily either end abruptly or enter a prolonged phase of gradual simplification and asymptotic dying-out. In the best possible case, the simulation is allocated a fixed faction of energy resources of the director’s civilization. In such case, it is, in principle, possible to have a simulation of indefinite duration, linked only to the (much more remote) options for ending of the director’s world. On the other hand, our activities may make the simulation shorter by increasing the complexity of present entities and thus increasing the running cost of the simulation. ¹⁶\n\n6.6 Making progress in studying observation selection effects\n\nIdentifying and correcting for observation selection biases plays the same role as correcting for other biases in risk analysis, evaluation, and mitigation: we need to know the underlying mechanisms of risk very precisely in order to make any progress towards making humanity safe. Although we are not very well prepared for any of the emergencies discussed in various chapters of this book, a general tendency easily emerges: some steps in mitigation have been made only for risks that are rationally sufficiently understood in as objective manner as possible: pandemics, nuclear warfare, impacts, and so on. We have shown that the observation selection acts to decrease the perceived probability of future risks in several wide classes, giving us a false sense of security.\n\nThe main lesson is that we should be careful not to use the fact that life on Earth has survived up to this day and that our humanoid ancestors did not go extinct in some sudden disaster to infer that the Earth-bound life and humanoid ancestors are highly resilient. Even if on the vast majority of Earth-like planets life goes extinct before intelligent life forms evolve, we should still expect to find ourselves on one of the exceptional planets that were lucky enough to escape devastation. In particular, the case of Tau Ceti offers a glimpse of what situation in many other places throughout the universe may be like. With regard to some existential risks, our past success provides no ground for expecting success in the future.\n\nAcknowledgement\n\nI thank Rafaela Hildebrand, Cosma R. Shalizi, Alexei Turchin, Slobodan Popović, Zoran Knežević, Nikola Božić, Momˇilo Jovanović, Robert J. Bradbury, Danica Ćirković, Zoran Živković and Irena Diklić for useful discussion and comments.\n\nSuggestions for further reading\n\nBostrom, N. (2002). Anthropic Bias: Observation Selection Effects (New York: Routledge). A comprehensive summary of the theory of observation selection effects with some effective examples and a chapter specifically devoted to the Doomsday Argument.\n\nBostrom, N. (2003). Are you living in a computer simulation? Philosophical Quarterly, 53, 243–255. Quite accessible original presentation of the Simulation Argument.\n\nGrinspoon, D. (2003). Lonely Planets: The Natural Philosophy of Alien Life (New York: HarperCollins). The most comprehensive and lively written treatment of various issues related to extraterrestrial life and intelligence, as well as excellent introduction into astrobiology. Contains a popular-level discussion of physical, chemical, and biological preconditions for observership.\n\nWebb, S. (2002). Where Is Everybody? Fifty Solutions to the Fermi’s Paradox (New York: Copernicus). The most accessible and comprehensive introduction into the various proposed solutions of Fermi’s paradox.\n\nReferences\n\nAmbrose, S.H. (1998). Late Pleistocene human population bottlenecks, volcanic winter, and differentiation of modern humans. J. Human Evol., 34, 623–651.\n\nAndrews, D.G. (2003). Interstellar Transportation using Today’s Physics. AIAA Paper 2003–4691. Report to 39th Joint Propulsion Conference &Exhibit.\n\nAnnis, J. (1999). An astrophysical explanation for the great silence. J. Brit. Interplan. Soc., 52, 19–22.\n\nArnold, L.F.A. (2005). Transit lightcurve signatures of artificial objects. Astrophys. J., 627, 534–539.\n\nAsghari, N. et al. (2004). Stability of terrestrial planets in the habitable zone of Gl 777 A, HD 72659, Gl 614, 47 UMa and HD 4208. Astron. Astrophys., 426, 353–365.\n\nBada, J.L. (2004). How life began on Earth: a status report. Earth Planet. Sci. Lett., 226, 1–15.\n\nBadescu, V. and Cathcart, R.B. (2000). Stellar engines for Kardashev’s type II civilisations. J. Brit. Interplan. Soc., 53, 297–306.\n\nBadescu, V. and Cathcart, R.B. (2006). Use of class A and class C stellar engines to control Sun’s movement in the galaxy. Acta Astronautica, 58, 119–129.\n\nBalashov, Yu. (1991). Resource Letter AP-1: The Anthropic Principle. Am. J. Phys., 59, 1069–1076.\n\nBall, J.A. (1973). The Zoo hypothesis. Icarus, 19, 347–349.\n\nBarrow, J.D. and Tipler, F.J. (1986). The Anthropic Cosmological Principle (New York: Oxford University Press).\n\nBaxter, S. (2000). The planetarium hypothesis: a resolution of the Fermi paradox. J. Brit. Interplan. Soc., 54, 210–216.\n\nBeaugé, C., Callegari, N., Ferraz-Mello, S., and Michtchenko, T.A. (2005). Resonance and stability of extra-solar planetary systems. In Knež zević, Z. and Milani, A. (eds.), Dynamics of Populations of Planetary Systems. Proceedings of the IAU Colloquium No. 197, pp. 3–18 (Cambridge: Cambridge University Press).\n\nBenford, G. (1981). Extraterrestrial intelligence? Quarterly J. Royal Astron. Soc., 22, 217.\n\nBenford, G. (1984). Across the Sea of Suns (New York: Simon &Schuster).\n\nBenton, M.J. (2003). When Life Nearly Died: The Greatest Mass Extinction of All Time (London: Thames and Hudson).\n\nBostrom, N. (1999). The Doomsday Argument is alive and kicking. Mind, 108, 539–550.\n\nBostrom, N. (2000). When machines outsmart humans. Futures, 35, 759–764.\n\nBostrom, N. (2001). The Doomsday Argument, Adam &Eve, UN⁺⁺ and Quantum Joe. Synthese, 127, 359.\n\nBostrom, N. (2002a). Anthropic Bias: Observation Selection Effects in Science and Philosophy (New York: Routledge).\n\nBostrom, N. (2002b). Existential risks. J. Evol. Technol., http://www.jetpress.org/volume9 /risks.html\n\nBostrom, N. (2003). Are you living in a computer simulation? Philos. Quarterly, 53, 243–255.\n\nBrillouin, L. (1962). Science and Information Theory (New York: Academic Press).\n\nBrin, G.D. (1983). The Great Silence – the controversy concerning extraterrestrial intelligent life. Quarterly J. Royal Astron. Soc., 24, 283.\n\nBulgatz, J. (1992). Ponzi Schemes, Invaders from Mars and More Extraordinary Popular Delusions and the Madness of Crowds (New York: Harmony Books).\n\nCantril, H. (1947). The Invasion from Mars: A Study in the Psychology of Panic (Princeton, NJ: Princeton University Press).\n\nCarter, B. (1983). The anthropic principle and its implications for biological evolution. Philos. Trans. Royal Soc. London A, 310, 347–363.\n\nCaves, C. (2000). Predicting future duration from present age: a critical assessment. Contemp. Phys., 41, 143.\n\nCavicchioli, R. (2002). Extremophiles and the search for extraterrestrial life. Astrobiology, 2, 281 -292.\n\nChaitin, G.J. (1977). Algorithmic information theory. IBM J. Res. Develop., 21, 350.\n\nChernavskii, D.S. (2000). The origin of life and thinking from the viewpoint of modern physics. Physics-Uspekhi, 43, 151–176.\n\nĆirković, M.M. (2003). On the importance of SETI for transhumanism. J. Evol. Technol., 13. http://www.jetpress.org/volume13/cirkovic.html\n\nĆirković, M.M. (2004). On the temporal aspect of the Drake Equation and SETI. Astrobiology, 4, 225–231.\n\nĆirković, M.M. (2006). Too early? On the apparent conflict of astrobiology and cosmology. Biol. Philos., 21, 369–379.\n\nĆirković, M.M. (2007). Evolutionary catastrophes and the Goldilocks problem. Int. J. Astrobiol. 6, 325–329.\n\nĆirković, M.M. and Bradbury, R.J. (2006). Galactic gradients, postbiological evolution and the apparent failure of SETI. New Astronomy, 11, 628–639.\n\nĆirković, M.M., Dragićević, I., and Vukotic, B. (2008). Galactic punctuated equilibrium: how to undermine Carter’s anthropic argument in astrobiology. Astrobiology, in press.\n\nCohen, J. and Stewart, I. (2002). What Does a Martian Look Like? (Hoboken, NJ: John Wiley &Sons).\n\nDar, A., Laor, A., and Shaviv, N.J. (1998). Life extinctions by cosmic ray jets. Phys. Rev. Lett., 80, 5813–5816.\n\nDick, S.J. (2003) Cultural evolution, the postbiological universe and SETI. Int. J. Astrobiol., 2, 65–74.\n\nDreschhoff, G.A.M. and Laird, C.M. (2006). Evidence for a stratigraphic record of supernovae in polar ice. Adv. Space Res., 38, 1307–1311.\n\nEarth Impact Database. (2005). http://www.unb.ca/passc/ImpactDatabase/\n\nEgan, G. (2002). Schild’s Ladder (HarperCollins, New York).\n\nEhrenfreund, P. et al. (2002). Astrophysical and astrochemical insights into the origin of life. Rep. Prog. Phys., 65, 1427–1487.\n\nFogg, M.J. (1987). Temporal aspects of the interaction among the First Galactic Civilizations: The ‘Interdict Hypothesis’. Icarus, 69, 370–384.\n\nFreitas, R.A. Jr (1985). Observable characteristics of extraterrestrial technological civilizations. J. Brit. Interplanet. Soc., 38, 106–112.\n\nGonzalez, G., Brownlee, D., and Ward, P. (2001). The Galactic Habitable Zone: galactic chemical evolution. Icarus, 152, 185–200.\n\nGott, J.R. (1993). Implications of the Copernican principle for our future prospects. Nature, 363, 315–319.\n\nGould, S.J. (1987). SETI and the Wisdom of Casey Stengel. In The Flamingo’s Smile: Reflections in Natural History, pp. 403–413 (New York: W. W. Norton &Company).\n\nGreaves, J.S., Wyatt, M.C., Holland, W.S., and Dent, W.R.F. (2004). The debris disc around τ Ceti: a massive analogue to the Kuiper Belt. MNRAS, 351, L54-L58.\n\nGrinspoon, D. (2003). Lonely Planets: The Natural Philosophy of Alien Life (New York: HarperCollins).\n\nHanson, R. (1999). Great Filter. Preprint at http://hanson.berkeley.edu/greatfilter.html\n\nHanson, R. (2001). How to live in a simulation. J. Evol. Technol., 7, http://www.jetpress.org/volume7/simulation.html\n\nHunt, G.E. (1978). Possible climatic and biological impact of nearby supernovae. Nature, 271, 430–431.\n\nHut, P. and Rees, M.J. (1983). How stable is our vacuum? Nature, 302, 508–509.\n\nKorycansky, D.G., Laughlin, G., and Adams, F.C. (2001). Astronomical engineering: a strategy for modifying planetary orbits. Astrophys. Space Sci., 275, 349–366.\n\nKuiper, T.B.H. and Brin, G.D. (1989). Resource Letter ETC-1: Extraterrestrial civilization. Am. J. Phys., 57, 12–18.\n\nLahav, N., Nir, S., and Elitzur, A.C. (2001). The emergence of life on Earth. Prog. Biophys. Mol. Biol., 75, 75–120.\n\nLandauer, R. (1961). Irreversibility and heat generation in the computing process. IBM J. Res. Develop., 5, 183–191.\n\nLeslie, J. (1989), Risking the World’s End. Bull. Canadian Nucl. Soc., May, 10–15. Leslie, J. (1996). The End of the World: The Ethics and Science of Human Extinction (London: Routledge).\n\nLineweaver, C.H. (2001). An estimate of the age distribution of terrestrial planets in the Universe: quantifying metallicity as a selection effect. Icarus, 151, 307–313.\n\nLineweaver, C.H. and Davis, T.M. (2002). Does the rapid appearance of life on earth suggest that life is common in the Universe? Astrobiology, 2, 293–304.\n\nLineweaver, C.H., Fenner, Y., and Gibson, B.K. (2004). The Galactic Habitable Zone and the age distribution of complex life in the Milky Way. Science, 303, 59–62.\n\nLivio, M. (1999). How rare are extraterrestrial civilizations, and when did they emerge? Astrophys. J., 511, 429–431.\n\nMcGuire, B. (2002). A Guide to the End of the World: Everything You Never Wanted to Know (Oxford: Oxford University Press).\n\nMelott, A.L., Lieberman, B.S., Laird, C.M., Martin, L.D., Medvedev, M.V., Thomas, B.C., Cannizzo, J.K., Gehrels, N., and Jackman, C.H. (2004). Did a gamma-ray burst initiate the late Ordovician mass extinction? Int. J. Astrobiol., 3, 55–61.\n\nMojzsis, S.J., Arrhenius, G., McKeegan, K.D., Harrison, T.M., Nutman, A.P., and Friend, C.R.L. (1996). Evidence for life on Earth before 3800 million years ago. Nature, 384, 55–59.\n\nMoravec, H.P. (1988). Mind Children: The Future of Robot and Human Intelligence (Cambridge, MA: Harvard University Press).\n\nNapier, W.M. (2006). Evidence for cometary bombardment episodes. MNRAS, 366, 977–982.\n\nNoble, M., Musielak, Z.E., and Cuntz, M. (2002). Orbital stability of terrestrial planets inside the Habitable Zones of extrasolar planetary systems. Astrophys. J., 572, 1024–1030.\n\nOlum, K. (2002). The doomsday argument and the number of possible observers. Philos. Quarterly, 52, 164–184.\n\nOlum, K. (2004). Conflict between anthropic reasoning and observation. Analysis, 64, 1–8.\n\nRampino, M.R. (2002). Supereruptions as a threat to civilizations on earth-like planets. Icarus, 156, 562–569.\n\nRampino, M.R. and Self, S. (1992). Volcanic winter and accelerated glaciation following the Toba super-eruption. Nature, 359, 50–52.\n\nRoscoe, H.K. (2001). The risk of large volcanic eruptions and the impact of this risk on future ozone depletion. Nat. Haz., 23, 231–246.\n\nRose, C. and Wright, G. (2004). Inscribed matter as an energy-efficient means of communication with an extraterrestrial civilization. Nature, 431, 47–49.\n\nRuderman, M.A. (1974). Possible consequences of nearby supernova explosions for atmospheric ozone and terrestrial life. Science, 184, 1079–1081.\n\nSandberg, A. (1999). The physics of information processing superobjects: daily life among the Jupiter brains. J. Evol. Tech., 5, http://www.jetpress.org/volume5/Brains2.pdf\n\nScalo, J. andWheeler, J.C. (2002). Astrophysical and astrobiological implications of gamma-ray burst properties. Astrophys. J., 566, 723–737.\n\nSchaller, R.R. (1997). Moore’s law: past, present, and future. IEEE Spectrum, June, 53–59.\n\nSchindewolf, O. (1962). Neokatastrophismus? Deutsch Geologische Gesellschaf Zeitschrift Jahrgang, 114, 430–445.\n\nSchroeder, K. (2002). Permanence (New York: Tor Books).\n\nSchultz, P.H. et al. (2004). The Quaternary impact record from the Pampas, Argentina. Earth Planetary Sci. Lett., 219, 221–238.\n\nTipler, F.J. (1980). Extraterrestrial intelligent beings do not exist. Quarterly J. Royal. Astron. Soc., 21, 267–281.\n\nWard, P.D. and Brownlee, D. (2000). Rare Earth: Why Complex Life Is Uncommon in the Universe (New York: Springer).\n\nWebb, S. (2002). Where Is Everybody? Fifty Solutions to the Fermi’s Paradox (New York: Copernicus).\n\nWells, H.G. (1898). The War of the Worlds (London: Heinemann).\n\nWhite, R.V. (2002). Earth’s biggest ‘whodunnit’: unravelling the clues in the case of the end-Permian mass extinction. Philos. Trans. Royal Soc. Lond. A, 360, 2963–2985.\n\nWilliams, D.M., Kasting, J.F., and Wade, R.A. (1997). Habitable moons around extrasolar giant planets. Nature, 385, 234–236.\n\nWilson, P.A. (1994). Carter on Anthropic Principle Predictions. Brit. J. Phil. Sci., 45, 241–253.\n\nWoo, G. (1999). The Mathematics of Natural Catastrophes (Singapore: World Scientific).\n\n• 7 • Systems-based risk analysis\n\nYacov Y. Haimes\n\n7.1 Introduction\n\nRisk models provide the roadmaps that guide the analyst throughout the journey of risk assessment, if the adage ‘To manage risk, one must measure it’ constitutes the compass for risk management. The process of risk assessment and management may be viewed through many lenses, depending on the perspective, vision, values, and circumstances. This chapter addresses the complex problem of coping with catastrophic risks by taking a systems engineering perspective. Systems engineering is a multidisciplinary approach distinguished by a practical philosophy that advocates holism in cognition and decision making. The ultimate purposes of systems engineering are to (1) build an understanding of the system’s nature, functional behaviour, and interaction with its environment, (2) improve the decision-making process (e.g., in planning, design, development, operation, and management), and (3) identify, quantify, and evaluate risks, uncertainties, and variability within the decision-making process.\n\nEngineering systems are almost always designed, constructed, and operated under unavoidable conditions of risk and uncertainty and are often expected to achieve multiple and conflicting objectives. The overall process of identifying, quantifying, evaluating, and trading-off risks, benefits, and costs should be neither a separate, cosmetic afterthought nor a gratuitous add-on technical analysis. Rather, it should constitute an integral and explicit component of the overall managerial decision-making process. In risk assessment, the analyst often attempts to answer the following set of triplet questions (Kaplan and Garrick, 1981): ‘What can go wrong?’, ‘What is the likelihood that it would go wrong?’, and ‘What are the consequences?’ Answers to these questions help risk analysts identify, measure, quantify, and evaluate risks and their consequences and impacts.\n\nRisk management builds on the risk assessment process by seeking answers to a second set of three questions (Haimes, 1991): ‘What can be done and what options are available?’, ‘What are their associated trade-offs in terms of all costs, benefits, and risks?’, and ‘What are the impacts of current management decisions on future options?’ Note that the last question is the most critical one for any managerial decision-making. This is so because unless the negative and positive impacts of current decisions on future options are assessed and evaluated (to the extent possible), these policy decisions cannot be deemed to be ‘optimal’ in any sense of the word. Indeed, the assessment and management of risk is essentially a synthesis and amalgamation of the empirical and normative, the quantitative and qualitative, and the objective and subjective efforts. Total risk management can be realized only when these questions are addressed in the broader context of management, where all options and their associated trade-offs are considered within the hierarchical organizational structure. Evaluating the total trade-offs among all important and related system objectives in terms of costs, benefits, and risks cannot be done seriously and meaningfully in isolation from the modelling of the system and from considering the prospective resource allocations of the overall organization.\n\nTheory, methodology, and computational tools drawn primarily from systems engineering provide the technical foundations upon which the above two sets of triplet questions are addressed quantitatively. Good management must thus incorporate and address risk management within a holistic, systemic, and all-encompassing framework and address the following four sources of failure: hardware, software, organizational, and human. This set of sources is intended to be internally comprehensive (i.e., comprehensive within the system’s own internal environment. External sources of failure are not discussed here because they are commonly system-dependent.) However, the above four failure elements are not necessarily independent of each other. The distinction between software and hardware is not always straightforward, and separating human and organizational failure often is not an easy task. Nevertheless, these four categories provide a meaningful foundation upon which to build a total risk management framework.\n\nIn many respects, systems engineering and risk analysis are intertwined, and only together do they make a complete process. To paraphrase Albert Einstein’s comment about the laws of mathematics and reality, we say: ‘To the extent to which risk analysis is real, it is not precise; to the extent to which risk analysis is precise, it is not real’. The same can be applied to systems engineering, since modelling constitutes the foundations for both quantitative risk analysis and systems engineering, and the reality is that no single model can precisely represent large-scale and complex systems.\n\n7.2 Risk to interdependent infrastructure and sectors of the economy\n\nThe myriad economic, organizational, and institutional sectors, among others, that characterize countries in the developed world can be viewed as a complex large-scale system of systems. (In a similar way, albeit on an entirely different scale, this may apply to the terrorist networks and to the global socio-economic and political environment.) Each system is composed of numerous interconnected and interdependent cyber, physical, social, and organizational infrastructures (subsystems), whose relationships are dynamic (i.e., ever changing with time), non-linear (defeating a simplistic modelling schema), probabilistic (fraught with uncertainty), and spatially distributed (agents and infrastructures with possible overlapping characteristics are spread all over the continent(s)). These systems are managed or coordinated by multiple government agencies, corporate divisions, and decision makers, with diverse missions, resources, timetables, and agendas that are often in competition and conflict. Because of the above characteristics, failures due to human and organizational errors are common. Risks of extreme and catastrophic events facing this complex enterprise cannot and should not be addressed using the conventional expected value of risk as the sole measure for risk. Indeed, assessing and managing the myriad sources of risks facing many countries around the world will be fraught with difficult challenges. However, systems modelling can greatly enhance the likelihood of successfully managing such risks. Although we recognize the difficulties in developing, sustaining, and applying the necessary models in our quest to capture the essence of the multiple perspectives and aspects of these complex systems, there is no other viable alternative but to meet this worthy challenge.\n\nThe literature of risk analysis is replete with misleading definitions of vulnerability. Of particular concern is the definition of risk as the product of impact, vulnerability, and threat. This means that in the parlance of systems engineering we must rely on the building blocks of mathematical models, focusing on the use of state variables. For example, to control the production of steel, one must have an understanding of the states of the steel at any instant – its temperature and other physical and chemical properties. To know when to irrigate and fertilize a farm to maximize crop yield, a farmer must assess the soil moisture and the level of nutrients in the soil. To treat a patient, a physician must first know the temperature, blood pressure, and other states of the patient’s physical health.\n\nState variables, which constitute the building blocks for representing and measuring risk to infrastructure and economic systems, are used to define the following terms (Haimes, 2004, 2006, Haimes et al. 2002):\n\n1. Vulnerability is the manifestation of the inherent states of the system (e.g., physical, technical, organizational, cultural) that can be exploited to adversely affect (cause harm or damage to) that system.\n\n2. Intent is the desire or motivation to attack a target and cause adverse effects.\n\n3. Capability is the ability and capacity to attack a target and cause adverse effects.\n\n4. Threat is the intent and capability to adversely affect (cause harm or damage to) the system by adversely changing its states.\n\n5. Risk (when viewed from the perspectives of terrorism) can be considered qualitatively as the result of a threat with adverse effects to a vulnerable system. Quantitatively, however, risk is a measure of the probability and severity of adverse effects.\n\nThus, it is clear that modelling risk as the probability and severity of adverse effects requires knowledge of the vulnerabilities, intents, capabilities, and threats to the infrastructure system. Threats to a vulnerable system include terrorist networks whose purposes are to change some of the fundamental states of a country: from a stable to an unstable government, from operable to inoperable infrastructures, and from a trustworthy to an untrustworthy cyber system. These terrorist networks that threaten a country have the same goals as those commissioned to protect its safety, albeit in opposite directions – both want to control the states of the systems in order to achieve their objectives.\n\nNote that the vulnerability of a system is multidimensional, namely, a vector in mathematical terms. For example, suppose we consider the risk of hurricane to a major hospital. The states of the hospital, which represent vulnerabilities, are functionality/availability of the electric power, water supply, telecommunications, and intensive care and other emergency units, which are critical to the overall functionality of the hospital. Furthermore, each one of these state variables is not static in its operations and functionality – its levels of functionality change and evolve continuously. In addition, each is a system of its own and has its own sub-state variables. For example, the water supply system consists of the main pipes, distribution system, and pumps, among other elements, each with its own attributes. Therefore, to use or oversimplify the multidimensional vulnerability to a scalar quantity in representing risk could mask the underlying causes of risk and lead to results that are not useful.\n\nThe significance of understanding the systems-based nature of a system’s vulnerability through its essential state variables manifests itself in both the risk assessment process (the problem) and the risk management process (the remedy). As noted in Section 7 1, in risk assessment we ask: What can go wrong? What is the likelihood? What might be the consequences? (Kaplan and Garrick, 1981). In risk management we ask: What can be done and what options are available? What are the trade-offs in terms of all relevant costs, benefits, and risks? What are the impacts of current decisions on future options? (Haimes, 1991, 2004) This significance is also evident in the interplay between vulnerability and threat. (Recall that a threat to a vulnerable system, with adverse effects, yields risk.) Indeed, to answer the triplet questions in risk assessment, it is imperative to have knowledge of those states that represent the essence of the system under study and of their levels of functionality and security.\n\n7.3 Hierarchical holographic modelling and the theory of scenario structuring\n\n7.3.1 Philosophy and methodology of hierarchical holographic modelling\n\nHierarchical holographic modelling (HHM) is a holistic philosophy/ methodology aimed at capturing and representing the essence of the inherent diverse characteristics and attributes of a system – its multiple aspects, perspectives, facets, views, dimensions, and hierarchies. Central to the mathematical and systems basis of holographic modelling is the overlapping among various holographic models with respect to the objective functions, constraints, decision variables, and input-output relationships of the basic system. The term holographic refers to the desire to have a multi-view image of a system when identifying vulnerabilities (as opposed to a single view, or a flat image of the system). Views of risk can include but are not limited to (1) economic, (2) health, (3) technical, (4) political, and (5) social systems. In addition, risks can be geography related and time related. In order to capture a holographic outcome, the team that performs the analysis must provide a broad array of experience and knowledge.\n\nThe term hierarchical refers to the desire to understand what can go wrong at many different levels of the system hierarchy. HHM recognizes that for the risk assessment to be complete, one must recognize that there are macroscopic risks that are understood at the upper management level of an organization that are very different from the microscopic risks observed at lower levels. In a particular situation, a microscopic risk can become a critical factor in making things go wrong. To carry out a complete HHM analysis, the team that performs the analysis must include people who bring knowledge up and down the hierarchy.\n\nHHM has turned out to be particularly useful in modelling large-scale, complex, and hierarchical systems, such as defence and civilian infrastructure systems. The multiple visions and perspectives of HHM add strength to risk analysis. It has been extensively and successfully deployed to study risks for government agencies such as the President’s Commission on Critical Infrastructure Protection (PCCIP), the FBI, NASA, the Virginia Department of Transportation (VDOT), and the National Ground Intelligence Center, among others. (These cases are discussed as examples in Haimes [2004].) The HHM methodology/philosophy is grounded on the premise that in the process of modelling large-scale and complex systems, more than one mathematical or conceptual model is likely to emerge. Each of these models may adopt a specific point of view, yet all may be regarded as acceptable representations of the infrastructure system. Through HHM, multiple models can be developed and coordinated to capture the essence of many dimensions, visions, and perspectives of infrastructure systems.\n\n7.3.2 The definition of risk\n\nIn the first issue of Risk Analysis, Kaplan and Garrick (1981) set forth the following ‘set of triplets’ definition of risk, R:\n\n[Image]\n\nwhere Si here denotes the i-th ‘risk scenario’, L\\_(i) denotes the likelihood of that scenario, and Xi the ‘damage vector’ or resulting consequences. This definition has served the field of risk analysis well since then, and much early debate has been thoroughly resolved about how to quantify the Liand Xi, and the meaning of ‘probability’, ‘frequency’, and ‘probability of frequency’ in this connection (Kaplan, 1993, 1996).\n\nIn Kaplan and Garrick (1981) the Si themselves were defined, somewhat informally, as answers to the question, ‘What can go wrong?’ with the system or process being analysed. Subsequently, a subscript ‘c’ was added to the set of triplets by Kaplan (1991, 1993):\n\n[Image]\n\nThis denotes that the set of scenarios {S\\_(i)} should be ‘complete’, meaning it should include ‘all the possible scenarios, or at least all the important ones’.\n\n7.3.3 Historical perspectives\n\nAt about the same time that Kaplan and Garrick’s (1981) definition of risk was published, so too was the first article on HHM (Haimes, 1981, 2004). Central to the HHM method is a particular diagram (see, for example, Fig. 7.1). This is particularly useful for analysing systems with multiple, interacting (perhaps overlapping) subsystems, such as a regional transportation or water supply system. The different columns in the diagram reflect different ‘perspectives’ on the overall system.\n\nHHM can be seen as part of the theory of scenario structuring (TSS) and vice versa, that is, TSS as part of HHM. Under the sweeping generalization of the HHM method, the different methods of scenario structuring can lead to seemingly different sets of scenarios for the same underlying problem. This fact is a bit awkward from the standpoint of the ‘set of triplets’ definition of risk (Kaplan and Garrick, 1981).\n\n[Image]\n\nFig. 7.1 Hierarchical holographic modelling (HHM) framework for identification of sources of risk.\n\nThe HHM approach divides the continuum but does not necessarily partition it. In other words, it allows the set of subsets to be overlapping, that is, non-disjoint. It argues that disjointedness is required only when we are going to quantify the likelihood of the scenarios, and even then, only if we are going to add up these likelihoods (in which case the overlapping areas would end up counted twice). Thus, if the risk analysis seeks mainly to identify scenarios rather than to quantify their likelihood, the disjointedness requirement can be relaxed somewhat, so that it becomes a preference rather than a necessity.\n\nTo see how HHM and TSS fit within each other (Kaplan et al., 2001), one key idea is to view the HHM diagram as a depiction of the success scenario S\\_(0.) Each box in Fig. 7.1 may then be viewed as defining a set of actions or results required of the system, as part of the definition of’success’. Conversely then, each box also defines a set of risk scenarios in which there is failure to accomplish one or more of the actions or results defined by that box. The union of all these sets contains all possible risk scenarios and is then ‘complete’.\n\nThis completeness is, of course, a very desirable feature. However, the intersection of two of our risk scenario sets, corresponding to two different HHM boxes, may not be empty. In other words, our scenario sets may not be ‘disjoint’.\n\n7.4 Phantom system models for risk management of emergent multi-scale systems\n\nNo single model can capture all the dimensions necessary to adequately evaluate the efficacy of risk assessment and management activities of emergent multi-scale systems. This is because it is impossible to identify all relevant state variables and their sub-states that adequately represent large and multi-scale systems (Haimes, 1977, 1981, 2004). Indeed, there is a need for theory and methodology that will enable analysts to appropriately rationalize risk management decisions through a process that\n\n1. identifies existing and potential emergent risks systemically,\n\n2. evaluates, prioritizes, and filters these risks based on justifiable selection criteria,\n\n3. collects, integrates, and develops appropriate metrics and a collection of models to understand the critical aspects of regions,\n\n4. recognizes emergent risks that produce large impacts and risk management strategies that potentially reduce those impacts for various time frames,\n\n5. optimally learns from implementing risk management strategies, and\n\n6. adheres to an adaptive risk management process that is responsive to dynamic, internal, and external forced changes. To do so effectively, models must be developed to periodically quantify, to the extent possible, the efficacy of risk management options in terms of their costs, benefits, and remaining risks.\n\nA risk-based, multi-model, systems-driven approach can effectively address these emergent challenges. Such an approach must be capable of maximally utilizing what is known now and optimally learn, update, and adapt through time as decisions are made and more information becomes available at various regional levels. The methodology must quantify risks as well as measure the extent of learning to quantify adaptability. This learn-as-you-go tactic will result in re-evaluation and evolving/learning risk management over time.\n\nPhantom system models (PSMs) (Haimes, 2007) enable research teams to effectively analyse major forced (contextual) changes on the characteristics and performance of emergent multi-scale systems, such as cyber and physical infrastructure systems, or major socio-economic systems. The PSM is aimed at providing a reasoned virtual-to-real experimental modelling framework with which to explore and thus understand the relationships that characterize the nature of emergent multi-scale systems. The PSM philosophy rejects a dogmatic approach to problem-solving that relies on a modelling approach structured exclusively on a single school of thinking. Rather, PSM attempts to draw on a pluri-modelling schema that builds on the multiple perspectives gained through generating multiple models. This leads to the construction of appropriate complementary models on which to deduce logical conclusions for future actions in risk management and systems engineering. Thus, we shift from only deciding what is optimal, given what we know, to answering questions such as (1) What do we need to know? (2) What are the impacts of having more precise and updated knowledge about complex systems from a risk reduction standpoint? and (3) What knowledge is needed for acceptable risk management decision-making? Answering this mandates seeking the ‘truth’ about the unknowable complex nature of emergent systems; it requires intellectually bias-free modellers and thinkers who are empowered to experiment with a multitude of modelling and simulation approaches and to collaborate for appropriate solutions.\n\nThe PSM has three important functions: (1) identify the states that would characterize the system, (2) enable modellers and analysts to explore cause-and-effect relationships in virtual-to-real laboratory settings, and (3) develop modelling and analyses capabilities to assess irreversible extreme risks; anticipate and understand the likelihoods and consequences of the forced changes around and within these risks; and design and build reconfigured systems to be sufficiently resilient under forced changes, and at acceptable recovery time and costs.\n\nThe PSM builds on and incorporates input from HHM, and by doing so seeks to develop causal relationships through various modelling and simulation tools; it imbues life and realism into phantom ideas for emergent systems that otherwise would never have been realized. In other words, with different modelling and simulation tools, PSM legitimizes the exploration and experimentation of out-of-the-box and seemingly ‘crazy’ ideas and ultimately discovers insightful implications that otherwise would have been completely missed and dismissed.\n\n7.5 Risk of extreme and catastrophic events\n\n7.5.1 The limitations of the expected value of risk\n\nOne of the most dominant steps in the risk assessment process is the quantification of risk, yet the validity of the approach most commonly used to quantify risk – its expected value – has received neither the broad professional scrutiny it deserves nor the hoped-for wider mathematical challenge that it mandates. One of the few exceptions is the conditional expected value of the risk of extreme events (among other conditional expected values of risks) generated by the partitioned multi-objective risk method (PMRM) (Asbeck and Haimes, 1984; Haimes, 2004).\n\nLet p\\_(x) (x) denote the probability density function of the random variable X, where, for example, X is the concentration of the contaminant trichloroethylene (TCE) in a groundwater system, measured in parts per billion (ppb). The expected value of the concentration (the risk of the groundwater being contaminated by an average concentration of TCE), is E(X) ppb. If the probability density function is discretized to n regions over the entire universe of contaminant concentrations, then E(X) equals the sum of the product of p\\_(i) and x\\_(i), where p\\_(i) is the probability that the i -th segment of the probability regime has a TCE concentration of x\\_(i). Integration (instead of summation) can be used for the continuous case. Note, however, that the expected-value operation commensurates contaminations (events) of low concentration and high frequency with contaminations of high concentration and low frequency. For example, events x ₁ = 2 pbb and x ₂ = 20,000 ppb that have the probabilities p1 = 0.1 and p2 = 0.00001, respectively, yield the same contribution to the overall expected value: (0.1)(2) + (0.00001)(20, 000) = 0.2+0.2. However, to the decision maker in charge, the relatively low likelihood of a disastrous contamination of the groundwater system with 20,000 ppb of TCE cannot be equivalent to the contamination at a low concentration of 0.2 ppb, even with a very high likelihood of such contamination. Owing to the nature of mathematical smoothing, the averaging function of the contaminant concentration in this example does not lend itself to prudent management decisions. This is because the expected value of risk does not accentuate catastrophic events and their consequences, thus misrepresenting what would be perceived as an unacceptable risk.\n\n7.5.2 The partitioned multi-objective risk method\n\nBefore the partitioned multi-objective risk method (PMRM) was developed, problems with at least one random variable were solved by computing and minimizing the unconditional expectation of the random variable representing damage. In contrast, the PMRM isolates a number of damage ranges (by specifying so-called partitioning probabilities) and generates conditional expectations of damage, given that the damage falls within a particular range. A conditional expectation is defined as the expected value of a random variable, given that this value lies within some pre-specified probability range. Clearly, the values of conditional expectations depend on where the probability axis is partitioned. The analyst subjectively chooses where to partition in response to the extreme characteristics of the decision-making problem. For example, if the decision-maker is concerned about the once-in-a-million-years catastrophe, the partitioning should be such that the expected catastrophic risk is emphasized.\n\nThe ultimate aim of good risk assessment and management is to suggest some theoretically sound and defensible foundations for regulatory agency guidelines for the selection of probability distributions. Such guidelines should help incorporate meaningful decision criteria, accurate assessments of risk in regulatory problems, and reproducible and persuasive analyses. Since these risk evaluations are often tied to highly infrequent or low-probability catastrophic events, it is imperative that these guidelines consider and build on the statistics of extreme events. Selecting probability distributions to characterize the risk of extreme events is a subject of emerging studies in risk management (Bier and Abhichandani, 2003; Haimes, 2004; Lambert et al., 1994; Leemis, 1995).\n\nThere is abundant literature that reviews the methods of approximating probability distributions from empirical data. Goodness-of-fit tests determine whether hypothesized distributions should be rejected as representations of empirical data. Approaches such as the method of moments and maximum likelihood are used to estimate distribution parameters. The caveat in directly applying accepted methods to natural hazards and environmental scenarios is that most deal with selecting the best matches for the ‘entire’ distribution. The problem is that these assessments and decisions typically address worst-case scenarios on the tails of distributions. The differences in distribution tails can be very significant even if the parameters that characterize the central tendency of the distribution are similar. A normal and a uniform distribution that have similar expected values can markedly differ on the tails. The possibility of significantly misrepresenting the tails, which are potentially the most relevant portion of the distribution, highlights the importance of considering extreme events when selecting probability distributions (see also Chapter 8, this volume).\n\nMore time and effort should be spent to characterize the tails of distributions when modelling the entire distribution. Improved matching between extreme events and distribution tails provides policymakers with more accurate and relevant information. Major factors to consider when developing distributions that account for tail behaviours include (1) the availability of data, (2) the characteristics of the distribution tail, such as shape and rate of decay, and (3) the value of additional information in assessment.\n\nThe conditional expectations of a problem are found by partitioning the problem’s probability axis and mapping these partitions onto the damage axis. Consequently, the damage axis is partitioned into corresponding ranges. A conditional expectation is defined as the expected value of a random variable given that this value lies within some pre-specified probability range. Clearly, the values of conditional expectations are dependent on where the probability axis is partitioned. The choice of where to partition is made subjectively by the analyst in response to the extreme characteristics of the problem. If, for example, the analyst is concerned about the once-in-a-million-years catastrophe, the partitioning should be such that the expected catastrophic risk is emphasized. Although no general rule exists to guide the partitioning, Asbeck and Haimes (1984) suggest that if three damage ranges are considered for a normal distribution, then the +1s and +4s partitioning values provide an effective rule of thumb. These values correspond to partitioning the probability axis at 0.84 and 0.99968; that is, the low-damage range would contain 84% of the damage events, the intermediate range would contain just under 16%, and the catastrophic range would contain about 0.032% (probability of 0.00032). In the literature, catastrophic events are generally said to be those with a probability of exceedance of 10⁻⁵ (see, for instance, the National Research Council report on dam safety [National Research Council, 1985]). This probability corresponds to events exceeding +4s.\n\nA continuous random variable X of damages has a cumulative distribution function (cdf)P(x)and a probability density function (pdf)p(x), which are defined by the following relationships:\n\n[Image]\n\nThe cdf represents the non-exceedance probability of x. The exceedance probability of x is defined as the probability that X is observed to be greater than x and is equal to one minus the cdf evaluated at x. The expected value, average, or mean value of the random variable X is defined as\n\n[Image]\n\nFor the discrete case, where the universe of events (sample space) of the random variable X is discretized into I segments, the expected value of damage E [X ] can be written as\n\n[Image]\n\nwhere x\\_(i) is the i-th segment of the damage.\n\nIn the PMRM, the concept of the expected value of damage is extended to generate multiple conditional expected-value functions, each associated with a particular range of exceedance probabilities or their corresponding range of damage severities. The resulting conditional expected-value functions, in conjunction with the traditional expected value, provide a family of risk measures associated with a particular policy.\n\nLet 1 – α₁ and 1 – α₂ where 0 < α₁ < α ₂ < 1, denote exceedance probabilities that partition the domain of X into three ranges, as follows. On a plot of exceedance probability, there is a unique damage β₁ on the damage axis that corresponds to the exceedance probability 1 – α₁ on the probability axis. Similarly, there is a unique damage β₂ that corresponds to the exceedance probability 1 – α₂. Damages less than β₁ are considered to be of low severity, and damages greater than β ₂ of high severity. Similarly, damages of a magnitude between β₁ and β ₂ are considered to be of moderate severity. The partitioning of risk into three severity ranges is illustrated in Fig. 7.2. For example, if the partitioning probability α₁ is specified to be 0.05, then β₁ is the 5th exceedance percentile. Similarly, if α₂ is 0.95 (i.e., 1 – α₂ is equal to 0.05), then β₂ is the 95th exceedance percentile.\n\nFor each of the three ranges, the conditional expected damage (given that the damage is within that particular range) provides a measure of the risk associated with the range. These measures are obtained by defining the conditional expected value. Consequently, the new measures of risk are f₂ (·), of high exceedance probability and low severity; f₃ (·), of medium exceedance probability and moderate severity; and f₄ (·), of low exceedance probability and high severity. The function f₂ (·), is the conditional expected value of X, given that x is less than or equal to β₁:\n\n[Image]\n\n[Image]\n\nFig. 7.2 PDF of failure rate distributions for four designs.\n\n[Image]\n\nThus, for a particular policy option, there are three measures of risk, f2(·), f₃ (·), andf₄ (·), in addition to the traditional expected value denoted by f₅ (·). The function f₁ (·) is reserved for the cost associated with the management of risk. Note that\n\n[Image]\n\nsince the total probability of the sample space of X is necessarily equal to one. In the PMRM, all or some subset of these five measures are balanced in a multi-objective formulation. The details are made more explicit in the next two sections.\n\n7.5.3 Risk versus reliability analysis\n\nOver time, most, if not all, man-made products and structures ultimately fail. Reliability is commonly used to quantify this time-dependent failure of a system. Indeed, the concept of reliability plays a major role in engineering planning, design, development, construction, operation, maintenance, and replacement.\n\nThe distinction between reliability and risk is not merely a semantic issue; rather, it is a major element in resource allocation throughout the life cycle of a product (whether in design, construction, operation, maintenance, or replacement). The distinction between risk and safety, well articulated over two decades ago by Lowrance (1976), is vital when addressing the design, construction, and maintenance of physical systems, since by their nature such systems are built of materials that are susceptible to failure. The probability of such a failure and its associated consequences constitute the measure of risk. Safety manifests itself in the level of risk that is acceptable to those in charge of the system. For instance, the selected strength of chosen materials, and their resistance to the loads and demands placed on them, is a manifestation of the level of acceptable safety. The ability of materials to sustain loads and avoid failures is best viewed as a random process – a process characterized by two random variables: (1) the load (demand) and (2) the resistance (supply or capacity).\n\nUnreliability, as a measure of the probability that the system does not meet its intended functions, does not include the consequences of failures. On the other hand, as a measure of the probability (i.e., unreliability) and severity (consequences) of the adverse effects, risk is inclusive and thus more representative. Clearly, not all failures can justifiably be prevented at all costs. Thus, system reliability cannot constitute a viable metric for resource allocation unless an a priori level of reliability has been determined. This brings us to the duality between risk and reliability on the one hand, and multiple objectives and a single objective optimization on the other.\n\nIn the multiple-objective model, the level of acceptable reliability is associated with the corresponding consequences (i.e., constituting a risk measure) and is thus traded off with the associated cost that would reduce the risk (i.e., improve the reliability). In the simple-objective model, on the other hand, the level of acceptable reliability is not explicitly associated with the corresponding consequences; rather it is predetermined (or parametrically evaluated) and thus is considered as a constraint in the model.\n\nThere are, of course, both historical and evolutionary reasons for the more common use of reliability analysis rather than risk analysis, as well as substantive and functional justifications. Historically, engineers have always been concerned with strength of materials, durability of product, safety, surety, and operability of various systems. The concept of risk as a quantitative measure of both the probability and consequences (or an adverse effect) of a failure has evolved relatively recently. From the substantive-functional perspective, however, many engineers or decision-makers cannot relate to the amalgamation of two diverse concepts with different units – probabilities and consequences – into one concept termed risk. Nor do they accept the metric with which risk is commonly measured. The common metric for risk – the expected value of adverse outcome – essentially commensurates events of low probability and high consequences with those of high probability and low consequences. In this sense, one may find basic philosophical justifications for engineers to avoid using the risk metric and instead work with reliability. Furthermore and most important, dealing with reliability does not require the engineer to make explicit trade-offs between cost and the outcome resulting from product failure. Thus, design engineers isolate themselves from the social consequences that are by-products of the trade-offs between reliability and cost. The design of levees for flood protection may clarify this point.\n\nDesignating a ‘one-hundred-year return period’ means that the engineer will design a flood protection levee for a predetermined water level that on the average is not expected to be exceeded more than once every hundred years. Here, ignoring the socio-economic consequences, such as loss of lives and property damage due to a high water level that would most likely exceed the one-hundred-year return period, the design engineers shield themselves from the broader issues of consequences, that is, risk to the population’s social well-being. On the other hand, addressing the multi-objective dimension that the risk metric brings requires much closer interaction and coordination between the design engineers and the decision makers. In this case, an interactive process is required to reach acceptable levels of risks, costs, and benefits. In a nutshell, complex issues, especially those involving public policy with health and socio-economic dimensions, should not be addressed through overly simplified models and tools. As the demarcation line between hardware and software slowly but surely fades away, and with the ever-evolving and increasing role of design engineers and systems analysts in technology-based decision-making, a new paradigm shift is emerging. This shift is characterized by a strong overlapping of the responsibilities of engineers, executives, and less technically trained managers.\n\nThe likelihood of multiple or compound failure modes in infrastructure systems (as well as in other physical systems) adds another dimension to the limitations of a single reliability metric for such infrastructures (Park et al., 1998; Schneiter et al., 1996). Indeed, because the multiple reliabilities of a system must be addressed, the need for explicit trade-offs among risks and costs becomes more critical. Compound failure modes are defined as two or more paths to failure with consequences that depend on the occurrence of combinations of failure paths. Consider the following examples: (1) a water distribution system, which can fail to provide adequate pressure, flow volume, water quality, and other needs; (2) the navigation channel of an inland waterway, which can fail by exceeding the dredge capacity and by closure to barge traffic; and (3) highway bridges, where failure can occur from deterioration of the bridge deck, corrosion or fatigue of structural elements, or an external loading such as floodwater. None of these failure modes is independent of the others in probability or consequence. For example, in the case of the bridge, deck cracking can contribute to structural corrosion and structural deterioration in turn can increase the vulnerability of the bridge to floods. Nevertheless, the individual failure modes of bridges are typically analysed independent of one another. Acknowledging the need for multiple metrics of reliability of an infrastructure could markedly improve decisions regarding maintenance and rehabilitation, especially when these multiple reliabilities are augmented with risk metrics.\n\nSuggestions for further reading\n\nApgar, D. (2006). Risk Intelligence: Learning to Manage What We Don’t Know (Boston, MA: Harvard Business School Press). This book is to help business managers deal more effectively with risk.\n\nLevitt, S.D. and Dubner, S.J. (2005). Freakonomics: A Rogue Economist Explores the Hidden Side of Everything (New York: HarperCollins). A popular book that adapts insights from economics to understand a variety of everyday phenomena.\n\nTaleb, N.N. (2007). The Black Swan: The Impact of the Highly Improbable (Random House: New York). An engagingly written book, from the perspective of an investor, about risk, especially long-shot risk and how people fail to take them into account.\n\nReferences\n\nAsbeck, E.L. and Haimes, Y.Y. (1984). The partitioned multiobjective risk method (PMRM). Large Scale Syst., 6(1), 13–38.\n\nBier, V.M. and Abhichandani, V. (2003). Optimal allocation of resources for defense of simple series and parallel systems from determined adversaries. In Haimes, Y.Y., Moser, D.A., and Stakhiv, E.Z. (eds.), Risk-based Decision Making in Water Resources X (Reston, VA: ASCE).\n\nHaimes, Y.Y. (1977), Hierarchical Analyses of Water Resources Systems, Modeling &Optimization of Large Scale Systems, McGraw Hill, New York.\n\nHaimes, Y.Y. (1981). Hierarchical holographic modeling. IEEE Trans. Syst. Man Cybernet., 11(9), 606–617.\n\nHaimes, Y.Y. (1991). Total risk management. Risk Anal., 11(2), 169–171.\n\nHaimes, Y.Y. (2004). Risk Modeling, Assessment, and Management. 2nd edition (New York: John Wiley).\n\nHaimes, Y.Y. (2006). On the definition of vulnerabilities in measuring risks to infrastructures. Risk Anal., 26(2), 293–296.\n\nHaimes, Y.Y. (2007). Phantom system models for emergent multiscale systems. J. Infrastruct. Syst., 13, 81–87.\n\nHaimes, Y.Y., Kaplan, S., and Lambert, J.H. (2002). Risk filtering, ranking, and management framework using hierarchical holographic modeling. Risk Anal., 22(2), 383–397.\n\nKaplan, S. (1991). The general theory of quantitative risk assessment. In Haimes, Y., Moser, D., and Stakhiv, E. (eds.), Risk-based Decision Making in Water Resources V, pp. 11–39 (New York: American Society of Civil Engineers).\n\nKaplan, S. (1993). The general theory of quantitative risk assessment – its role in the regulation of agricultural pests. Proc. APHIS/NAPPO Int. Workshop Ident. Assess. Manag. Risks Exotic Agric. Pests, 11(1), 123–126.\n\nKaplan, S. (1996). An Introduction to TRIZ, The Russian Theory of Inventive Problem Solving (Southfield, MI: Ideation International).\n\nKaplan, S. and Garrick, B.J. (1981). On the quantitative definition of risk. Risk Anal., 1(1), 11 -27.\n\nKaplan, S., Haimes, Y.Y., and Garrick, B.J. (2001). Fitting hierarchical holographic modeling (HHM) into the theory of scenario structuring, and a refinement to the quantitative definition of risk. Risk Anal., 21(5), 807–819.\n\nKaplan, S., Zlotin, B., Zussman, A., and Vishnipolski, S. (1999). New Tools for Failure and Risk Analysis – Anticipatory Failure Determination and the Theory of Scenario Structuring (Southfield, MI: Ideation).\n\nLambert, J.H., Matalas, N.C., Ling, C.W., Haimes, Y.Y., and Li, D. (1994). Selection of probability distributions in characterizing risk of extreme events. Risk Anal., 149(5), 731–742.\n\nLeemis, M.L. (1995). Reliability: Probabilistic Models and Statistical Methods (Englewood Cliffs, NJ: Prentice-Hall).\n\nLowrance, W.W. (1976). Of Acceptable Risk (Los Altos, CA: William Kaufmann).\n\nNational Research Council (NRC), Committee on Safety Criteria for Dams. (1985). Safety of Dams – Flood and Earthquake Criteria (Washington, DC: National Academy Press).\n\nPark, J.I., Lambert, J.H., and Haimes, Y.Y. (1998). Hydraulic power capacity of water distribution networks in uncertain conditions of deterioration. Water Resources Res., 34(2), 3605–3614.\n\nSchneiter, C.D. Li, Haimes, Y.Y., and Lambert, J.H. (1996). Capacity reliability and optimum rehabilitation decision making for water distribution networks. Water Resources Res., 32(7), 2271–2278.\n\n• 8 • Catastrophes and insurance\n\nPeter Taylor\n\nThis chapter explores the way financial losses associated with catastrophes can be mitigated by insurance. It covers what insurers mean by catastrophe and risk, and how computer modelling techniques have tamed the problem of quantitative estimation of many hitherto intractable extreme risks. Having assessed where these techniques work well, it explains why they can be expected to fall short in describing emerging global catastrophic risks such as threats from biotechnology. The chapter ends with some pointers to new techniques, which offer some promise in assessing such emerging risks.\n\n8.1 Introduction\n\nCatastrophic risks annually cause tens of thousands of deaths and tens of billions of dollars worth of losses. The figures available from the insurance industry (see, for instance, the Swiss Re [2007] Sigma report) show that mortality has been fairly consistent, whilst the number of recognized catastrophic events, and even more, the size of financial losses, has increased. The excessive rise in financial losses, and with this the number of recognized ‘catastrophes’, primarily comes from the increase in asset values in areas exposed to natural catastrophe. However, the figures disguise the size of losses affecting those unable to buy insurance and the relative size of losses in developing countries. For instance, Swiss Re estimated that of the estimated $46 billion losses due to catastrophe in 2006, which was a very mild year for catastrophe losses, only some $16 billion was covered by insurance. In 2005, a much heavier year for losses, Swiss Re estimated catastrophe losses at $230 billion, of which $83 billion was insured. Of the $230 billion, Swiss Re estimated that $210 billion was due to natural catastrophes and, of this, some $173 billion was due to the US hurricanes, notably Katrina ($135 billion). The huge damage from the Pakistan earthquake, though, caused relatively low losses in monetary terms (around $5 billion mostly uninsured), reflecting the low asset values in less-developed countries.\n\nIn capitalist economies, insurance is the principal method of mitigating potential financial loss from external events in capitalist economies. However, in most cases, insurance does not directly mitigate the underlying causes and risks themselves, unlike, say, a flood prevention scheme. Huge losses in recent years from asbestos, from the collapse of share prices in 2000/2001, the 9/11 terrorist attack, and then the 2004/2005 US hurricanes have tested the global insurance industry to the limit. But disasters cause premiums to rise, and where premiums rise capital follows.\n\nLosses from hurricanes, though, pale besides the potential losses from risks that are now emerging in the world as technological, industrial, and social changes accelerate. Whether the well-publicized risks of global warming, the misunderstood risks of genetic engineering, the largely unrecognized risks of nanotechnology and machine intelligence, or the risks brought about by the fragility to shocks of our connected society, we are voyaging into a new era of risk management. Financial loss will, as ever, be an important consequence of these risks, and we can expect insurance to continue to play a role in mitigating these losses alongside capital markets and governments. Indeed, the responsiveness of the global insurance industry to rapid change in risks may well prove more effective than regulation, international cooperation, or legislation.\n\nInsurance against catastrophes has been available for many years – we need to only think of the San Francisco 1906 earthquake when Cuthbert Heath sent the telegram ‘Pay all our policyholders in full irrespective of the terms of their policies’ back to Lloyd’s of London, an act that created long-standing confidence in the insurance markets as providers of catastrophe cover. For much of this time, assessing the risks from natural hazards such as earthquakes and hurricanes was largely guesswork and based on market shares of historic worst losses rather than any independent assessment of the chance of a catastrophe and its financial consequence. In recent years, though, catastrophe risk management has come of age with major investments in computer-based modelling. Through the use of these models, the insurance industry now understands the effects of many natural catastrophe perils to within an order of magnitude. The recent book by Eric Banks (see Suggestions for further reading) offers a thorough, up-to-date reference on the insurance of property against natural catastrophe. Whatever doubts exist concerning the accuracy of these models – and many in the industry do have concerns as we shall see – there is no questioning that models are now an essential part of the armoury of any carrier of catastrophe risk.\n\nModels notwithstanding, there is still a swathe of risks that commercial insurers will not carry. They fall into two types (1) where the risk is uneconomic, such as houses on a flood plain and (2) where the uncertainty of the outcomes is too great, such as terrorism. In these cases, governments in developed countries may step in to underwrite the risk as we saw with TRIA (Terrorism Risk Insurance Act) in the United States following 9/11. An analysis¹ of uninsured risks revealed that in some cases risks remain uninsured for a further reason – that the government will bail them out! There are also cases where underwriters will carry the risk, but policyholders find them too expensive. In these cases, people will go without insurance even if insurance is a legal requirement, as with young male UK drivers.\n\nAnother concern is whether the insurance industry is able to cope with the sheer size of the catastrophes. Following the huge losses of 9/11 a major earthquake or windstorm would have caused collapse of many re-insurers and threatened the entire industry. However, this did not occur and some loss-free years built up balance sheets to a respectable level. But then we had the reminders of the multiple Florida hurricanes in 2004, and hurricane Katrina (and others!) in 2005, after which the high prices for hurricane insurance have attracted capital market money to bolster traditional re-insurance funds. So we are already seeing financial markets merging to underwrite these extreme risks – albeit ‘at a price’. With the doom-mongering of increased weather volatility due to global warming, we can expect to see inter-governmental action, such as the Ethiopian drought insurance bond, governments taking on the role of insurers of the last resort, as we saw with the UK Pool Re-arrangement, bearing the risk themselves through schemes, such as the US FEMA flood scheme, or indeed stepping in with relief when a disaster occurs.\n\n8.2 Catastrophes\n\nWhat are catastrophic events? A catastrophe to an individual is not necessarily a catastrophe to a company and thus unlikely to be a catastrophe for society. In insurance, for instance, a nominal threshold of $5 million is used by the Property Claims Service (PCS) in the United States to define a catastrophe. It would be a remarkable for a loss of $5 million to constitute a ‘global catastrophe’!\n\nWe can map the semantic minefield by characterizing three types of catastrophic risk as treated in insurance (see Table 8.1): physical catastrophes, such as windstorm and earthquake, whether due to natural hazards or man-made accidental or intentional cause; liability catastrophes, whether intentional such as terrorism or accidental such as asbestosis; and systemic underlying causes leading to large-scale losses, such as the dotcom stock market collapse.\n\nAlthough many of these catastrophes are insured today, some are not, notably emerging risks from technology and socio-economic collapse. These types of risk present huge challenges to insurers as they are potentially catastrophic losses and yet lack an evidential loss history.\n\nTable 8.1 Three Types of Catastrophic Risk as Treated in Insurance\n\n[Image]\n\nCatastrophe risks can occur in unrelated combination within a year or in clusters, such as a series of earthquakes and even the series of Florida hurricanes seen in 2004. Multiple catastrophic events in a year would seem to be exceptionally rare until we consider that the more extreme an event the more likely it is to trigger another event. This can happen, for example, in natural catastrophes where an earthquake could trigger a submarine slide, which causes a tsunami or triggers a landslip, which destroys a dam, which in turn floods a city. Such high-end correlations are particularly worrying when they might induce man-made catastrophes such as financial collapse, infrastructure failure, or terrorist attack. We return to this question of high-end correlations later in the chapter.\n\nYou might think that events are less predictable the more extreme they become. Bizarre as it is, this is not necessarily the case. It is known from statistics that a wide class of systems show, as we look at the extreme tail, a regular ‘extreme value’ behaviour. This has, understandably, been particularly important in Holland (de Haan, 1990), where tide level statistics along the Dutch coast since 1880 were used to set the dike height to a 1 in 10,000-year exceedance level. This compares to the general 1 in 30-year exceedance level for most New Orleans dikes prior to Hurricane Katrina (Kabat et al., 2005)!\n\nYou might also have thought that the more extreme an event is the more obvious must be its cause, but this does not seem to be true in general either. Earthquakes, stock market crashes, and avalanches all exhibit sudden large failures without clear ‘exogenous’ (external) causes. Indeed, it is characteristic of many complex systems to exhibit ‘endogenous’ failures following from their intrinsic structure (see, for instance, Sornette et al., 2003).\n\nIn a wider sense, there is the problem of predictability. Many large insurance losses have come from ‘nowhere’ – they simply were not recognized in advance as realistic threats. For instance, despite the UK experience with IRA bombing in the 1990s, and sporadic terrorist attacks around the world, no one in the insurance industry foresaw concerted attacks on the World Trade Center and the Pentagon on 11 September 2001.\n\nThen there is the problem of latency. Asbestos was considered for years to be a wonder material² whose benefits were thought to outweigh any health concerns. Although recognized early on, the ‘latent’ health hazards of asbestos did not receive serious attention until studies of its long-term consequences emerged in the 1970s. For drugs, we now have clinical trials to protect people from unforeseen consequences, yet material science is largely unregulated. Amongst the many new developments in nanotechnology, could there be latent modern versions of asbestosis?\n\n8.3 What the business world thinks\n\nYou would expect the business world to be keen to minimize financial adversity, so it is of interest to know what business sees as the big risks.\n\nA recent survey of perceived risk by Swiss Re (see Swiss Re, 2006, based on interviews in late 2005) of global corporate executives across a wide range of industries identified computer-based risk the highest priority risk in all major countries by level of concern and second in priority as an emerging risk. Also, perhaps surprisingly, terrorism came tenth, and even natural disasters only made seventh. However, the bulk of the recognized risks were well within the traditional zones of business discomfort such as corporate governance, regulatory regimes, and accounting rules.\n\nThe World Economic Forum (WEF) solicits expert opinion from business leaders, economists, and academics to maintain a finger on the pulse of risk and trends. For instance, the 2006 WEF Global Risks report (World Economic Forum, 2006) classified risks by likelihood and severity with the most severe risks being those with losses greater than $1 trillion or mortality greater than $1 million or adverse growth impact greater than 2%. They were as follows.\n\n1. US current account deficit was considered a severe threat to the world economy in both short (1-10% chance) and long term (<1% chance).\n\n2. Oil price shock was considered a short-term severe threat of low likelihood (<1%).\n\n3. Japan earthquake was rated as a 1–10% likelihood. No other natural hazards were considered sufficiently severe.\n\n4. Pandemics, with avian flu as an example, was rated as a 1–10% chance.\n\n5. Developing world disease: spread of HIV/AIDS and TB epidemics were similarly considered a severe and high likelihood threat (1 -20%).\n\n6. Organized crime counterfeiting was considered to offer severe outcomes (long term) due to vulnerability of IT networks, but rated low frequency (<1%).\n\n7. International terrorism considered potentially severe, through a conventional simultaneous attack (short term estimated at <1%) or a non-conventional attack on a major city in longer term (1 -10%).\n\nNo technological risks were considered severe, nor was climate change. Most of the risks classified as severe were considered of low likelihood (<1%) and all were based on subjective consensual estimates.\n\nThe more recent 2007 WEF Global Risks report (World Economic Forum, 2007) shows a somewhat different complexion with risk potential generally increased, most notably the uncertainty in the global economy from trade protectionism and over-inflated asset values (see Fig. 8.1). The report also takes a stronger line on the need for intergovernmental action and awareness.\n\nIt seems that many of the risks coming over the next 5–20 years from advances in biotechnology, nanotechnology, machine intelligence, the resurgence of nuclear, and socio-economic fragility, all sit beyond the radar of the business world today. Those that are in their sights, such as nanotechnology, are assessed subjectively and largely disregarded.\n\nAnd that is one of the key problems when looking at global catastrophic risk and business. These risks are too big and too remote to be treated seriously.\n\n8.4 Insurance\n\nInsurance is about one party taking on another’s financial risk. Given what we have just seen of our inability to predict losses, and given the potential for dispute over claims, it is remarkable that insurance even exists, yet it does! Through the protection offered by insurance, people can take on the risks of ownership of property and the creation of businesses. The principles of ownership and its financial protection that we have in the capitalist West, though, do not apply to many countries; so, for instance, commercial insurance did not exist in Soviet Russia. Groups with a common interest, such as farmers, can share their common risks either implicitly by membership of a collective, as in Soviet Russia, or explicitly by contributing premiums to a mutual fund. Although mutuals were historically of importance as they often initiated insurance companies, insurance is now almost entirely dominated by commercial risk-taking.\n\n[Image]\n\nFig. 8.1 World Economic Forum 2007 – The 23 core global risks: likelihood with severity by economic loss.\n\nThe principles of insurance were set down over 300 years ago in London by shipowners at the same time as the theory of probability was being formulated to respond to the financial demands of the Parisian gaming tables. Over these years a legal, accounting, regulatory, and expert infrastructure has built up to make insurance an efficient and effective form of financial risk transfer.\n\nTo see how insurance works, let us start with a person or company owning property or having a legal liability in respect of others. They may choose to take their chances of avoiding losses by luck but will generally prefer to protect against the consequences of any financial losses due to a peril such as fire or accident. In some cases, such as employer’s liability, governments require by law that insurance be bought. Looking to help out are insurers who promise (backed normally by capital or a pledge of capital) to pay for these losses in return for a payment of money called a ‘premium’. The way this deal is formulated is through a contract of insurance that describes what risks are covered. Insurers would only continue to stay in business over a period of years if premiums exceed claims plus expenses. Insurers will nonetheless try and run their businesses with as little capital as they can get away with, so government regulators exist to ensure they have sufficient funds. In recent years, regulators such as the Financial Services Authority in the United Kingdom have put in place stringent quantitative tests on the full range of risk within an insurer, which include underwriting risks, such as the chance of losing a lot of money in one year due to a catastrophe, financial risks such as risk of defaulting creditors, market risks such as failure of the market to provide profitable business, and operational risks from poor systems and controls.\n\nLet us take a simple example: your house. In deciding the premium to insure your house for a year, an underwriter will apply a ‘buildings rate’ for your type of house and location to the rebuild cost of the house, and then add on an amount for ‘contents rate’ for your home’s location and safety features against fire and burglary multiplied by the value of contents. The rate is the underwriter’s estimate of the chance of loss – in simple terms, a rate of 0.2% is equivalent to expecting a total loss once in 500 years. So, for example, the insurer might think you live in a particularly safe area and have good fire and burglary protection, and so charge you, say, 0.1% rate on buildings and 0.5% on contents. Thus, if your house’s rebuild cost was estimated at £500,000 and your contents at £100,000, then you would pay £500 for buildings and £500 for contents, a total of £1000 a year.\n\nMost insurance works this way. A set of’risk factors’ such as exposure to fire, subsidence, flood, or burglary are combined – typically by addition – in the construction of the premium. The rate for each of these factors comes primarily from claims experience – this type of property in this type of area has this proportion of losses over the years. That yields an average price. Insurers, though, need to guard against bad years and to do this they will try to underwrite enough of these types of risk, so that a ‘law of large numbers’ or ‘regression to the mean’ reduces the volatility of the losses in relation to the total premium received. Better still, they can diversify their portfolio of risks so that any correlations of losses within a particular class (e.g., a dry winter causes earth shrinkage and subsidence of properties built on clay soils) can be counteracted by uncorrelated classes.\n\nIf only all risks were like this, but they are not. There may be no decent claims history, the conditions in the future may not resemble the past, there may be a possibility of a few rare but extremely large losses, it may not be possible to reduce volatility by writing a lot of the same type of risk, and it may not be possible to diversify the risk portfolio. One or more of these circumstances can apply. For example, lines of business where we have low claims experience and doubt over the future include ‘political risks’ (protecting a financial asset against a political action such as confiscation). The examples most relevant to this chapter are ‘catastrophe’ risks, which typically have low claims experience, large losses, and limited ability to reduce volatility. To understand how underwriters deal with these, we need to revisit what the pricing of risk and indeed risk itself are all about.\n\n8.5 Pricing the risk\n\nThe primary challenge for underwriters is to set the premium to charge the customer – the price of the risk. In constructing this premium, an underwriter will usually consider the following elements:\n\n1. Loss costs, being the expected cost of claims to the policy.\n\n2. Acquisition costs, such as brokerage and profit commissions.\n\n3. Expenses, being what it costs to run the underwriting operation.\n\n4. Capital costs, being the cost of supplying the capital required by regulators to cover the possible losses according their criterion (e.g., the United Kingdom’s FSA currently requires capital to meet a 1 -in-200-year or more annual chance of loss).\n\n5. Uncertainty cost, being an additional subjective charge in respect of the uncertainty of this line of business. This can in some lines of business, such as political risk, be the dominant factor.\n\n6. Profit, being the profit margin required of the business. This can sometimes be set net of expected investment income from the cash flow of receiving premiums before having to pay out claims, which for ‘liability’ contracts can be many years.\n\nUsually the biggest element of price is the loss cost or ‘pure technical rate’. We saw above how this was set for household building cover. The traditional method is to model the history of claims, suitably adjusted to current prices, with frequency/severity probability distribution combinations, and then trend these in time into the future, essentially a model of the past playing forward. The claims can be those either on the particular contract or on a large set of contracts with similar characteristics.\n\nSetting prices from rates is – like much of the basic mathematics of insurance – essentially a linear model even though non-linearity appears pronounced when large losses happen. As an illustration of non-linearity, insurers made allowance for some inflation of rebuild/repair costs when underwriting for US windstorm, yet the actual ‘loss amplification’ in Hurricane Katrina was far greater than had been anticipated. Another popular use of linearity has been linear regression in modelling risk correlations. In extreme cases, though, this assumption, too, can fail. A recent and expensive example was when the dotcom bubble burst in April 2000. The huge loss of stock value to millions of Americans triggered allegations of impropriety and legal actions against investment banks, class actions against the directors and officers of many high technology companies whose stock price had collapsed, for example, the collapse of Enron and WorldCom and Global Crossing, and the demise of the accountants Arthur Andersen discredited when the Enron story came to light. Each of these events has led to massive claims against the insurance industry. Instead of linear correlations, we need now to deploy the mathematics of copulas. ³ This phenomenon is also familiar from physical damage where a damaged asset can in turn enhance the damage to another asset, either directly, such as when debris from a collapsed building creates havoc on its neighbours (’collateral damage’), or indirectly, such as when loss of power exacerbates communication functions and recovery efforts (’dependency damage’).\n\nAs well as pricing risk, underwriters have to guard against accumulation of risk. In catastrophe risks the simplest measure of accumulations is called the ‘aggregate’ – the cost of total ruin when everything is destroyed. Aggregates represent the worst possible outcome and are an upper limit on an underwriter’s exposure, but have unusual arithmetic properties. (As an example, the aggregate exposure of California is typically lower than the sum of the aggregate exposures in each of the Cresta zones into which California is divided for earthquake assessment. The reason for this is that many insurance policies cover property in more than one zone but have a limit of loss across the zones. Conversely, for fine-grained geographical partitions, such as postcodes, the sum across two postal codes can be higher than the aggregate of each. The reason for this is that risks typically have a per location [per policy when dealing with re-insurance] deductible!)\n\n8.6 Catastrophe loss models\n\nFor infrequent and large catastrophe perils such as earthquakes and severe windstorms, the claims history is sparse and, whilst useful for checking the results of models, offers insufficient data to support reliable claims analysis. Instead, underwriters have adopted computer-based catastrophe loss models, typically from proprietary expert suppliers such as RMS (Risk Management Solutions), AIR (Applied Insurance Research), and EQECAT.\n\nThe way these loss models work is well-described in several books and papers, such as the recent UK actuarial report on loss models (GIRO, 2006). From there we present Fig. 8.2, which shows the steps involved.\n\nQuoting directly from that report:\n\nCatastrophe models have a number of basic modules:\n\n• Event module iem\n\nA database of stochastic events (the event set) with each event defined by its physical parameters, location, and annual probability/frequency of occurrence.\n\n• Hazard module iem\n\nThis module determines the hazard of each event at each location. The hazard is the consequence of the event that causes damage – for a hurricane it is the wind at ground level, for an earthquake, the ground shaking.\n\n• Inventory (or exposure) module iem\n\nA detailed exposure database of the insured systems and structures. As well as location this will include further details such as age, occupancy, and construction.\n\n• Vulnerability module iem\n\nVulnerability can be defined as the degree of loss to a particular system or structure resulting from exposure to a given hazard (often expressed as a percentage of sum insured).\n\n• Financial analysis module iem\n\nThis module uses a database of policy conditions (limits, excess, sub limits, coverage terms) to translate this loss into an insured loss.\n\nOf these modules, two, the inventory and financial analysis modules, rely primarily on data input by the user of the models. The other three modules represent the engine of the catastrophe model, with the event and hazard modules being based on seismological and meteorological assessment and the vulnerability module on engineering assessment. (GIRO, 2006, p. 6)\n\n[Image]\n\nFig. 8.2 Generic components of a loss model.\n\n[Image]\n\nFig. 8.3 Exceedance probability loss curve.\n\nThe model simulates a catastrophic event such as a hurricane by giving it a geographical extent and peril characteristics so that it ‘damages’ – as would a real hurricane – the buildings according to ‘damageability’ profiles for occupancy, construction, and location. This causes losses, which are then applied to the insurance policies in order to calculate the accumulated loss to the insurer. The aim is to produce an estimate of the probability of loss in a year called the occurrence exceedance probability (OEP), which estimates the chance of exceeding a given level of loss in any one year, as shown in Fig. 8.3. When the probability is with respect to all possible losses in a given year, then the graph is called an aggregate exceedance probability curve (AEP).\n\nYou will have worked out that just calculating the losses on a set of events does not yield a smooth curve. You might also have asked yourself how the ‘annual’ bit gets in. You might even have wondered how the damage is chosen because surely in real life there is a range of damage even for otherwise similar buildings. Well, it turns out that different loss modelling companies have different ways of choosing the damage percentages and of combining these events, which determine the way the exceedance probability distributions are calculated. ⁴ Whatever their particular solutions, though, we end up with a two-dimensional estimate of risk through the ‘exceedance probability (EP) curve’.\n\n8.7 What is risk?\n\n[R]isk is either a condition of, or a measure of, exposure to misfortune – more concretely, exposure to unpredictable losses. However, as a measure, risk is not one-dimensional – it has three distinct aspects or ‘facets’ related to the anticipated values of unpredictable losses. The three facets are Expected Loss, Variability of Loss Values, and Uncertainty about the Accuracy of Mental Models intended to predict losses.\n\nTed Yellman, 2000\n\nAlthough none of us can be sure whether tomorrow will be like the past or whether a particular insurable interest will respond to a peril as a representative of its type, the assumption of such external consistencies underlies the construction of rating models used by insurers. The parameters of these models can be influenced by past claims, by additional information on safety factors and construction, and by views on future medical costs and court judgments. Put together, these are big assumptions to make, and with catastrophe risks the level of uncertainty about the chance and size of loss is of primary importance, to the extent that such risks can be deemed uninsurable. Is there a way to represent this further level of uncertainty? How does it relate to the ‘EP curves’ we have just seen?\n\nKaplan and Garrick (1981) defined quantitative risk in terms of three elements – probability for likelihood, evaluation measure for consequence, and ‘level 2 risk’ for the uncertainty in the curves representing the first two elements. Yellman (see quote in this section) has taken this further by elaborating the ‘level 2 risk’ as uncertainty of the likelihood and adversity relationships.\n\nWe might represent these ideas by the EP curve in Fig. 8.4. When dealing with insurance, ‘Likelihood’ is taken as probability density and ‘Adversity’ as loss. Jumping further up the abstraction scale, these ideas can be extended to qualitative assessments, where instead of defined numerical measures of probability and loss we look at categoric (low, medium, high) measures of Likelihood and Adversity. The loss curves now look like those shown in Figs. 8.5 and 8.6. Putting these ideas together, we can represent these elements of risk in terms of fuzzy exceedance probability curves as shown in Fig. 8.7.\n\nAnother related distinction is made in many texts on risk (e.g., see Woo, 1999) between intrinsic or ‘aleatory’ (from the Greek for dice) uncertainty and avoidable or ‘epistemic’ (implying it follows from our lack of knowledge) risk. The classification of risk we are following looks at the way models predict outcomes in the form of a relationship between chance and loss. We can have many different parameterizations of a model and, indeed, many different models. The latter types of risk are known in insurance as ‘process risk’ and ‘model risk’, respectively.\n\n[Image]\n\nFig. 8.4 Qualitative loss curve.\n\n[Image]\n\nFig. 8.5 Qualitative risk assessment chart – Treasury Board of Canada.\n\nThese distinctions chime very much with the way underwriters in practice perceive risk and set premiums. There is a saying in catastrophe re-insurance that ‘nothing is less than 1 on line’, meaning the vagaries of life are such that you should never price high-level risk at less than the chance of a total loss once in a hundred years (1%). So, whatever the computer models might tell the underwriter, the underwriter will typically allow for the ‘uncertainty’ dimension of risk. In commercial property insurance this add-on factor has taken on a pseudo-scientific flavour, which well illustrates how intuition may find an expression with whatever tools are available.\n\n[Image]\n\nFig. 8.6 Qualitative risk assessment chart – World Economic Forum 2006.\n\n[Image]\n\nFig. 8.7 Illustrative qualitative loss curves\n\n8.8 Price and probability\n\nArmed with a recognition of the three dimensions of risk – chance, loss, and uncertainty – the question arises as to whether the price of an insurance contract, or indeed some other financial instrument related to the future, is indicative of the probability of a particular outcome. In insurance it is common to ‘layer’ risks as ‘excess of loss’ to demarcate the various parts of the EP curve. When this is done, then we can indeed generally say that the element of price due to loss costs (see aforementioned) represents the mean of the losses to that layer and that, for a given shape of curve, tells us the probability under the curve. The problem is whether that separation into ‘pure technical’ price can be made, and generally it cannot be as we move into the extreme tail because the third dimension – uncertainty – dominates the price. For some financial instruments such as weather futures, this probability prediction is much easier to make as the price is directly related to the chance of exceedance of some measure (such as degree days). For commodity prices, though, the relationship is generally too opaque to draw any such direct relationships of price to probability of event.\n\n8.9 The age of uncertainty\n\nWe have seen that catastrophe insurance is expressed in a firmly probabilistic way through the EP curve, yet we have also seen that this misses many of the most important aspects of uncertainty.\n\nChoice of model and choice of parameters can make a big difference to the probabilistic predictions of loss we use in insurance. In a game of chance, the only risk is process risk, so that the uncertainty resides solely with the probability distribution describing the process. It is often thought that insurance is like this, but it is not: it deals with the vagaries of the real world. We attempt to approach an understanding of that real world with models, and so for insurance there is additional uncertainty from incorrect or incorrectly configured models.\n\nIn practice, though, incorporating uncertainty will not be that easy to achieve. Modellers may not wish to move from the certainties of a single EP curve to the demands of sensitivity testing and the subjectivities of qualitative risk assessment. Underwriters in turn may find adding further levels of explicit uncertainty uncomfortable. Regulators, too, may not wish to have the apparent scientific purity of the loss curve cast into more doubt, giving insurers more not less latitude! On the plus side, though, this approach will align the tradition of expert underwriting, which allows for many risk factors and uncertainties, with the rigour of analytical models such as modern catastrophe loss models.\n\nOne way to deal with ‘process’ risk is to find the dependence of the model on the source assumptions of damage and cost, and the chance of events. Sensitivity testing and subjective parameterizations would allow for a diffuse but more realistic EP curve.\n\nThis leaves ‘model’ risk – what can we do about this? The common solution is to try multiple models and compare the results to get a feel for the spread caused by assumptions. The other way is to make an adjustment to reflect our opinion of the adequacy or coverage of the model, but this is today largely a subjective assessment.\n\nThere is a way we can consider treating parameter and model risk, and that is to construct adjusted EP curves to represent the parameter and model risk. Suppose that we could run several different models and got several different EP curves? Suppose, moreover, that we could rank these different models with different weightings. Well, that would allow us to create a revised EP curve, which is the ‘convolution’ of the various models.\n\nIn the areas of emerging risk, parameter and model risk, not process risk, play a central role in the risk assessment as we have little or no evidential basis on which to decide between models or parameterizations.\n\nBut are we going far enough? Can we be so sure the future will be a repeat of the present? What about factors outside our domain of experience? Is it possible that for many risks we are unable to produce a probability distribution even allowing for model and parameter risk? Is insurance really faced with ‘black swan’ phenomena (’black swan’ refers to the failure of the inductive principle that all swans were white when black swans were discovered in Australia), where factors outside our models are the prime driver of risk?\n\nWhat techniques can we call upon to deal with these further levels of uncertainty?\n\n8.10 New techniques\n\nWe have some tools at our disposal to deal with these challenges.\n\n8.10.1 Qualitative risk assessment\n\nQualitative risk assessment, as shown in the figures in the chapter, is the primary way in which most risks are initially assessed. Connecting these qualitative tools to probability and loss estimates is a way in which we can couple the intuitions and judgements of everyday sense with the analytical techniques used in probabilistic loss modelling.\n\n8.10.2 Complexity science\n\nComplexity science is revealing surprising order in what was hitherto the most intractable of systems. Consider, for instance, wildfires in California, which have caused big losses to the insurance industry in recent years. An analysis of wildfires in different parts of the world (Malamud et al., 1998) shows several remarkable phenomena at work: first, that wildfires exhibit negative linear behaviour on a log-log graph of frequency and severity; second, that quite different parts of the world have comparable gradients for these lines; and third, that where humans interfere, they can create unintended consequences and actually increase the risk, as it appears that forest management by stamping out small fires has actually made large fires more severe in southern California. Such log-log negative linear plots correspond to inverse power probability density functions (pdfs) (Sornette, 2004), and this behaviour is quite typical of many complex systems as popularized in the book Ubiquity by Mark Buchanan (see Suggestions for further reading).\n\n8.10.3 Extreme value statistics\n\nIn extreme value statistics similar regularities have emerged in the most surprising of areas – the extreme values we might have historically treated as awkward outliers. Can it be coincidence that complexity theory predicts inverse power law behaviour, extreme value theory predicts an inverse power pdf, and that empirically we find physical extremes of tides, rainfall, wind, and large losses in insurance showing pareto (inverse power pdf) distribution behaviour?\n\n8.11 Conclusion: against the gods?\n\nGlobal catastrophic risks are extensive, severe, and unprecedented. Insurance and business generally are not geared up to handling risks of this scale or type. Insurance can handle natural catastrophes such as earthquakes and windstorms, financial catastrophes such as stock market failures to some extent, and political catastrophes to a marginal extent. Insurance is best when there is an evidential basis and precedent for legal coverage. Business is best when the capital available matches the capital at risk and the return reflects the risk of loss of this capital. Global catastrophic risks unfortunately fail to meet any of these criteria. Nonetheless, the loss modelling techniques developed for the insurance industry coupled with our deeper understanding of uncertainty and new techniques give good reason to suppose we can deal with these risks as we have with others in the past. Do we believe the fatalist cliché that ‘risk is the currency of the gods’ or can we go ‘against the gods’ by thinking the causes and consequences of these emerging risks through, and then estimating their chances, magnitudes, and uncertainties? The history of insurance indicates that we should have a go!\n\nAcknowledgement\n\nI thank Ian Nicol for his careful reading of the text and identification and correction of many errors.\n\nSuggestions for further reading\n\nBanks, E. (2006). Catastrophic Risk (New York: John Wiley). Wiley Finance Series. This is a thorough and up-to-date text on the insurance and re-insurance of catastrophic risk. It explains clearly and simply the way computer models generate exceedance probability curves to estimate the chance of loss for such risks.\n\nBuchanan, M. (2001). Ubiquity (London: Phoenix). This is a popular account – one of several now available including the same author’s Small Worlds – of the ‘inverse power’ regularities somewhat surprisingly found to exist widely in complex systems. This is of particular interest to insurers as the long-tail probability distribution most often found for catastrophe risks is the pareto distribution which is ‘inverse power’.\n\nGIRO (2006). Report of the Catastrophe Modelling Working Party (London: Institute of Actuaries). This specialist publication provides a critical survey of the modelling methodology and commercially available models used in the insurance industry.\n\nReferences\n\nDe Haan, L. (1990). Fighting the arch enemy with mathematics. Statistica Neerlandica, 44, 45–68.\n\nKabat, P., van Vierssen, W., Veraart, J., Vellinga, P., and Aerts, J. (2005). Climate proofing the Netherlands. Nature, 438, 283–284.\n\nKaplan, S. and Garrick, B.J. (1981). On the quantitative definition of risk. Risk Anal., 1(1), 11.\n\nMalamud, B.D., Morein, G., and Turcotte, D.L. (1998). Forest fires – an example of self-organised critical behaviour. Science, 281, 1840–1842.\n\nSornette, D. (2004). Critical Phenomena in Natural Sciences – Chaos, Fractals, Self organization and Disorder: Concepts and Tools, 2nd edition (Berlin: Springer).\n\nSornette, D., Malevergne, Y., and Muzy, J.F. (2003). Volatility fingerprints of large shocks: endogeneous versus exogeneous. Risk Magazine.\n\nSwiss Re. (2006). Swiss Re corporate survey 2006 report. Zurich: Swiss Re.\n\nSwiss Re. (2007). Natural catastrophes and man-made disasters 2006. Sigma report no 2/2007. Zurich: Swiss Re.\n\nWoo, G. (1999). The Mathematics of Natural Catastrophes (London: Imperial College Press).\n\nWorld Economic Forum. Global Risks 2006 (Geneva: World Economic Forum). World Economic Forum. Global Risks 2007 (Geneva: World Economic Forum). Yellman, T.W. (2000). The three facets of risk (Boeing Commercial Airplane Group, Seattle, WA) AIAA-2000-5594 2000. In World Aviation Conference, San Diego, CA, 10–12 October, 2000.\n\n• 9 • Public policy towards catastrophe\n\nRichard A. Posner\n\nThe Indian Ocean tsunami of December 2004 focused attention on a type of disaster to which policymakers pay too little attention – a disaster that has a very low or unknown probability of occurring, but that if it does occur creates enormous losses. The flooding of New Orleans in the late summer of 2005 was a comparable event, although the probability of the event was known to be high; the Corps of Engineers estimated its annual probability as 0.33% (Schleifstein and McQuaid, 2002), which implies a cumulative probability of almost 10% over a 30-year span. The particular significance of the New Orleans flood for catastrophic-risk analysis lies in showing that an event can inflict enormous loss even if the death toll is small – approximately 1/250 of the death toll from the tsunami.\n\nGreat as that toll was, together with the physical and emotional suffering of survivors, and property damage, even greater losses could be inflicted by other disasters of low (but not negligible) or unknown probability. The asteroid that exploded above Siberia in 1908 with the force of a hydrogen bomb might have killed millions of people had it exploded above a major city. Yetthat asteroid was only about 200 feet in diameter, and a much larger one (among the thousands of dangerously large asteroids in orbits that intersect the earth’s orbit) could strike the earth and cause the total extinction of the human race through a combination of shock waves, fire, tsunamis, and blockage of sunlight, wherever it struck. ¹ Another catastrophic risk is that of abrupt global warming, discussed later in this chapter.\n\nOddly, with the exception of global warming (and hence the New Orleans flood, to which global warming may have contributed, along with man-made destruction of wetlands and barrier islands that formerly provided some protection for New Orleans against hurricane winds), none of the catastrophes mentioned above, including the tsunami, is generally considered an ‘environmental’ catastrophe. This is odd, since, for example, abrupt catastrophic global change would be a likely consequence of a major asteroid strike. The reason non-asteroid-induced global warming is classified as an environmental disaster but the other disasters are not is that environmentalists are concerned with human activities that cause environmental harm but not with natural activities that do so. This is an arbitrary separation because the analytical issues presented by natural and human-induced environmental catastrophes are very similar.\n\nTo begin the policy analysis, suppose that a tsunami as destructive as the Indian Ocean one occurs on average once a century and kills 250,000 people. That is an average of 2500 deaths per year. Even without attempting a sophisticated estimate of the value of life to the people exposed to the risk, one can say with some confidence that if an annual death toll of 2500 could be substantially reduced at moderate cost, the investment would be worthwhile. A combination of educating the residents of low-lying coastal areas about the warning signs of a tsunami (tremors and a sudden recession in the ocean), establishing a warning system involving emergency broadcasts, telephoned warnings, and air-raid-type sirens, and improving emergency response systems would have saved many of the people killed by the Indian Ocean tsunami, probably at a total cost less than any reasonable estimate of the average losses that can be expected from tsunamis. Relocating people away from coasts would be even more efficacious, but except in the most vulnerable areas or in areas in which residential or commercial uses have only marginal value, the costs would probably exceed the benefits – for annual costs of protection must be matched with annual, not total, expected costs of tsunamis. In contrast, the New Orleans flood might have been prevented by flood-control measures such as strengthening the levees that protect the city from the waters of the Mississippi River and the Gulf of Mexico, and in any event, the costs inflicted by the flood could have been reduced at little cost simply by a better evacuation plan.\n\nThe basic tool for analysing efficient policy towards catastrophe is cost-benefit analysis. Where, as in the case of the New Orleans flood, the main costs, both of catastrophe and of avoiding catastrophe, are fairly readily monetizable and the probability of the catastrophe if avoidance measures are not taken is known with reasonable confidence, analysis is straightforward In the case of the tsunami, however, and of many other possible catastrophes, the main costs are not readily monetizable and the probability of the catastrophe may not be calculable. ² Regarding the first problem, however, there is now a substantial economic literature inferring the value of life from the costs people are willing to incur to avoid small risks of death; if from behaviour towards risk one infers that a person would pay $70 to avoid a 1 in 100,000 risk of death, his value of life would be estimated at $7 million ($70/.00001), which is in fact the median estimate of the value of life of a ‘prime-aged US workers’ today (Viscusi and Aldy, 2003, pp. 18, 63). ³ Because value of life is positively correlated with income, this figure cannot be used to estimate the value of life of most of the people killed by the Indian Ocean tsunami. A further complication is that the studies may not be robust with respect to risks of death much smaller than the 1 in 10,000 to 1 in 100,000 range of most of the studies (Posner, 2004, pp. 165–171); we do not know what the risk of death from a tsunami was to the people killed. Additional complications come from the fact that the deaths were only a part of the cost inflicted by the disaster – injuries, suffering, and property damage also need to be estimated, along with the efficacy and expense of precautionary measures that would have been feasible. The risks of smaller but still destructive tsunamis that such measures might protect against must also be factored in; nor can there be much confidence about the “once a century’ risk estimate. Nevertheless, it is apparent that the total cost of the recent tsunami was high enough to indicate that precautionary measures would have been cost-justified, even though they would have been of limited benefit because, unlike the New Orleans flood, there was no possible measure for preventing the tsunami.\n\nSo why were not such measures taken in anticipation of a tsunami on the scale that occurred? Tsunamis are a common consequence of earthquakes, which themselves are common; and tsunamis can have other causes besides earthquakes – a major asteroid strike in an ocean would create a tsunami that could dwarf the Indian Ocean one. A combination of factors provides a plausible answer. First, although a once-in-a-century event is as likely to occur at the beginning of the century as at any other time, it is much less likely to occur in the first decade of the century than later. That is, probability is relative to the span over which it is computed; if the annual probability of some event is 1%, the probability that it will occur in 10 years is justa shade under 10%. Politicians with limited terms of office and thus foreshortened political horizons are likely to discount low-risk disaster possibilities, since the risk of damage to their careers from failing to take precautionary measures is truncated. Second, to the extent that effective precautions require governmental action, the fact that government is a centralized system of control makes it difficult for officials to respond to the full spectrum of possible risks against which cost-justified measures might be taken. The officials, given the variety of matters to which they must attend, are likely to have a high threshold of attention below which risks are simply ignored. Third, where risks are regional or global rather than local, many national governments, especially in the poorer and smaller countries, may drag their heels in the hope of taking a free ride on the larger and richer countries. Knowing this, the latter countries may be reluctant to take precautionary measures and by doing so reward and thus encourage free riding. (Of course, if the large countries are adamant, this tactic will fail.) Fourth, often countries are poor because of weak, inefficient, or corrupt government, characteristics that may disable poor nations from taking cost-justified precautions. Fifth, because of the positive relation between value of life and per capita income, even well-governed poor countries will spend less per capita on disaster avoidance than rich countries will.\n\nAn even more dramatic example of neglect of low-probability/high-cost risks concerns the asteroid menace, which is analytically similar to the menace of tsunamis. NASA, with an annual budget of more than $10 billion, spends only $4 million a year on mapping dangerously close large asteroids, and at that rate may not complete the task for another decade, even though such mapping is the key to an asteroid defence because it may give us years of warning. Deflecting an asteroid from its orbit when it is still millions of miles from the earth appears to be a feasible undertaking. Although asteroid strikes are less frequent than tsunamis, there have been enough of them to enable the annual probabilities of various magnitudes of such strikes to be estimated, and from these estimates, an expected cost of asteroid damage can be calculated (Posner, 2004, pp. 24–29, 180).\n\nAs in the case of tsunamis, if there are measures beyond those being taken already that can reduce the expected cost of asteroid damage at a lower cost, thus yielding a net benefit, the measures should be taken, or at least seriously considered.\n\nOften it is not possible to estimate the probability or magnitude of a possible catastrophe, and so the question arises whether or how cost-benefit analysis, or other techniques of economic analysis, can be helpful in devising responses to such a possibility. One answer is what can be called ‘inverse cost-benefit analysis’ (Posner, 2004, pp. 176–184). Analogous to extracting probability estimates from insurance premiums, it involves dividing what the government is spending to prevent a particular catastrophic risk from materializing by what the social cost of the catastrophe would be if it did materialize. The result is an approximation of the implied probability of the catastrophe. Expected cost is the product of probability and consequence (loss): C= PI. If P and L are known, C can be calculated. If instead C and L are known, P can be calculated: if $1 billion (C) is being spentto avert a disaster, which, if it occurs, will impose a loss (L) of $100 billion, then P = C/L = .01.\n\nIf P so calculated diverges sharply from independent estimates of it, this is a clue that society may be spending too much or too little on avoiding L. It is just a clue, because of the distinction between marginal and total costs and benefits. The optimal expenditure on a measure is the expenditure that equates marginal cost to marginal benefit. Suppose we happen to know that P is not. 01 but .1, so that the expected cost of the catastrophe is not $1 billion but $10 billion. It does not follow that we should be spending $10 billion, or indeed anything more than $1 billion, to avert the catastrophe. Maybe spending just $1 billion would reduce the expected cost of catastrophe from $10 billion all the way down to $500 million and no further expenditure would bring about a further reduction, or at least a cost-justified reduction. For example, if spending another $1 billion would reduce the expected cost from $500 million to zero, that would be a bad investment, at least if risk aversion is ignored.\n\nThe federal government is spending about $2 billion a year to prevent a bioterrorist attack (raised to $2.5 billion for 2005, however, under the rubric of ‘Project BioShield’) (U.S. Department of Homeland Security, 2004; U.S. Office of Management and Budget, 2003). The goal is to protect Americans, so in assessing the benefits of this expenditure casualties in other countries can be ignored. Suppose the most destructive biological attack that seems reasonably possible on the basis of what little we now know about terrorist intentions and capabilities would kill 100 million Americans. We know that value-of-life estimates may have to be radically discounted when the probability of death is exceedingly slight. However, there is no convincing reason for supposing the probability of such an attack less than, say, one in 100,000; and the value of life that is derived by dividing the cost that Americans will incur to avoid a risk of death of that magnitude by the risk is about $7 million. Then if the attack occurred, the total costs would be $700 trillion – and that is actually too low an estimate because the death of a third of the population would have all sorts of collateral consequences, mainly negative. Let us, still conservatively however, refigure the total costs as $1 quadrillion. The result of dividing the money being spent to prevent such an attack, $2 billion, by $1 quadrillion is 1/500,000. Is there only a 1 in 500,000 probability of a bioterrorist attack of that magnitude in the next year? One does not know, but the figure seems too low.\n\nIt does not follow that $2 billion a year is too little to be spending to prevent a bioterrorist attack; one must not forget the distinction between total and marginal costs. Suppose that the $2 billion expenditure reduces the probability of such an attack from.01 to .0001. The expected cost of the attack would still be very high – $1 quadrillion multiplied by .0001 is $100 billion – but spending more than $2 billion might not reduce the residual probability of .0001 at all. For there might be no feasible further measures to take to combat bioterrorism, especially when we remember that increasing the number of people involved in defending against bioterrorism, including not only scientific and technical personnel but also security guards in laboratories where lethal pathogens are stored, also increases the number of people capable, alone or in conjunction with others, of mounting biological attacks. But there are other response measures that should be considered seriously, such as investing in developing and stockpiling broad-spectrum vaccines, establishing international controls over biological research, and limiting publication of bioterror ‘recipes’. One must also bear in mind that expenditures on combating bioterrorism do more than prevent mega-attacks; the lesser attacks, which would still be very costly both singly and cumulatively, would also be prevented.\n\nCosts, moreover, tend to be inverse to time. It would cost a great deal more to build an asteroid defence in 1 year than in 10 years because of the extra costs that would be required for a hasty reallocation of the required labour and capital from the current projects in which they are employed; so would other crash efforts to prevent catastrophes. Placing a lid on current expenditures would have the incidental benefit of enabling additional expenditures to be deferred to a time when, because more will be known about both the catastrophic risks and the optimal responses to them, considerable cost savings may be possible. The case for such a ceiling derives from comparing marginal benefits to marginal costs; the latter may be sharply increasing in the short run. ⁴\n\nA couple of examples will help to show the utility of cost-benefit analytical techniques even under conditions of profound uncertainty. The first example involves the Relativistic Heavy Ion Collider (RHIC), an advanced research particle accelerator that went into operation at Brookhaven National Laboratory in Long Island in 2000. As explained by the distinguished English physicist Sir Martin Rees (2003, pp. 120–121), the collisions in RHIC might conceivably produce a shower of quarks that would ‘reassemble themselves into a very compressed object called a strangelet. … A strangelet could, by contagion, convert anything else it encountered into a strange new form of matter. … A hypothetical strangelet disaster could transform the entire planet Earth into an inert hyperdense sphere about one hundred metres across’. Rees (2003, p. 125) considers this ‘hypothetical scenario’ exceedingly unlikely, yet points out that even an annual probability of 1 in 500 million is not wholly negligible when the result, should the improbable materialize, would be so total a disaster.\n\nConcern with such a possibility led John Marburger, the director of the Brookhaven National Laboratory and now the President’s science advisor, to commission a risk assessment by a committee of physicists chaired by Robert Jaffe before authorizing RHIC to begin operating. Jaffe’s committee concluded that the risk was slight, but did not conduct a cost-benefit analysis.\n\nRHIC cost $600 million to build and its annual operating costs were expected to be $130 million. No attempt was made to monetize the benefits that the experiments conducted in it were expected to yield but we can get the analysis going by making a wild guess (to be examined critically later) that the benefits can be valued at $250 million per year. An extremely conservative estimate, which biases the analysis in favour of RHIC’s passing a cost-benefit test, of the cost of the extinction of the human race is $600 trillion. ⁵ The final estimate needed to conduct a cost-benefit analysis is the annual probability of a strangelet disaster in RHIC: here a ‘best guess’ is 1 in 10 million. (See also Chapter 16 in this volume.)\n\nGranted, this really is a guess. The physicist Arnon Dar and his colleagues estimated the probability of a strangelet disaster during RHIC’s planned period of 10-year life as no more than 1 in 50 million, which on an annual basis would mean roughly 1 in 500 million. Robert Jaffe and his colleagues, the official risk-assessment team for RHIC, offered a series of upper-bound estimates, including a 1 in 500,000 probability of a strangelet disaster over the 10-year period, which translates into an annual probability of such a disaster of approximately 1 in 5 million.\n\nA 1 in 10 million estimate yields an annual expected extinction cost of $60 million for 10 years to add to the $130 million in annual operating costs and the initial investment of $600 million – and with the addition of that expected cost, it is easily shown that the total costs of the project exceed its benefits if the benefits are only $250 million a year. Of course this conclusion could easily be reversed by raising the estimate of the project’s benefits above my ‘wild guess’ figure of $250 million. But probably the estimate should be lowered rather than raised. For, from the standpoint of economic policy, it is unclear whether RHIC could be expected to yield any social benefits and whether, if it did, the federal government should subsidize particle-accelerator research. The purpose of RHIC is not to produce useful products, as earlier such research undoubtedly did, but to yield insights into the earliest history of the universe. In other words, the purpose is to quench scientific curiosity. Obviously, that is a benefit to scientists, or at least to high-energy physicists. But it is unclear why it should be thought a benefit to society as a whole, or in any event why it should be paid for by the taxpayer, rather than financed by the universities that employ the physicists who are interested in conducting such research. The same question can be asked concerning other government subsidies for other types of purely academic research but with less urgency for research that is harmless. If there is not good answer to the general question, the fact that particular research poses even a slight risk of global catastrophe becomes a compelling argument against its continued subsidization.\n\nThe second example, which will occupy much of the remaining part of this chapter, involves global warming. The Kyoto Protocol, which recently came into effect by its terms when Russia signed it, though the United States has not, requires the signatory nations to reduce their carbon dioxide emissions to a level 7–10% below what they were in the late 1990s, but exempts developing countries, such as China, a large and growing emitter, and Brazil, which is destroying large reaches of the Amazon rain forest, much of it by burning. The effect of carbon dioxide emissions on the atmospheric concentration of the gas is cumulative, because carbon dioxide leaves the atmosphere (by being absorbed into the oceans) at a much lower rate than it enters it, and therefore the concentration will continue to grow even if the annual rate of emission is cut down substantially. Between this phenomenon and the exemptions, it is feared that the Kyoto Protocol will have only a slight effect in arresting global warming. Yet the tax or other regulatory measures required to reduce emissions below their level of 6 years ago will be very costly.\n\nThe Protocol’s supporters are content to slow the rate of global warming by encouraging, through heavy taxes (e.g., on gasoline or coal) or other measures (such as quotas) that will make fossil fuels more expensive to consumers, conservation measures such as driving less or driving more fuel-efficient cars that will reduce the consumption of these fuels. This is either too much or too little. It is too much if, as most scientists believe, global warming will continue to be a gradual process, producing really serious effects – the destruction of tropical agriculture, the spread of tropical diseases such as malaria to currently temperate zones, dramatic increases in violent storm activity (increased atmospheric temperatures, by increasing the amount of water vapour in the atmosphere, increase precipitation), ⁶ and a rise in sea levels (eventually to the point of inundating most coastal cities) – only towards the end of the century. For by that time science, without prodding by governments, is likely to have developed economical ‘clean’ substitutes for fossil fuels (we already have a clean substitute – nuclear power) and even economic technology for either preventing carbon dioxide from being emitted into the atmosphere by the burning of fossil fuels or for removing it from the atmosphere. ⁷ However, the Protocol, at least without the participation of the United States and China, the two largest emitters, is too limited a response to global warming if the focus is changed from gradual to abrupt global warming. Because of the cumulative effect of carbon-dioxide emissions on the atmospheric concentration of the gas, a modest reduction in emissions will not reduce that concentration, but merely modestly reduce its rate of growth.\n\nAt various times in the earth’s history, drastic temperature changes have occurred in the course of just a few years. In the most recent of these periods, which geologists call the ‘Younger Dryas’ and date to about 11,000 years ago, shortly after the end of the last ice age, global temperatures soared by about 14°F in about a decade (Mithin, 2003). Because the earth was still cool from the ice age, the effect of the increased warmth on the human population was positive. However, a similar increase in a modern decade would have devastating effects on agriculture and on coastal cities, and might even cause a shift in the Gulf Stream that would result in giving all of Europe a Siberian climate. Recent dramatic shrinking of the north polar icecap, ferocious hurricane activity, and a small westward shift of the Gulf Stream are convincing many scientists that global warming is proceeding much more rapidly than expected just a few years ago.\n\nBecause of the enormous complexity of the forces that determine climate, and the historically unprecedented magnitude of human effects on the concentration of greenhouse gases, the possibility that continued growth in that concentration could precipitate – and within the near rather than the distant future – a sudden warming similar to that of the Younger Dryas cannot be excluded. Indeed, no probability, high or low, can be assigned to such a catastrophe. But it may be significant that, while dissent continues, many climate scientists are now predicting dramatic effects from global warming within the next 20–40 years, rather than just by the end of the century (Lempinen, 2005). ⁸ It may be prudent, therefore, to try to stimulate the rate at which economical substitutes for fossil fuels, and technology both for limiting the emission of carbon dioxide by those fuels when they are burned in internal-combustion engines or electrical generating plants, and for removing carbon dioxide from the atmosphere, are developed.\n\nSwitching focus from gradual to abrupt global warming has two advantages from the standpoint of analytical tractability. The first is that, given the rapid pace of scientific progress, if disastrous effects from global warming can safely be assumed to lie at least 50 years in the future, it makes sense not to incur heavy costs now but instead to wait for science to offer a low-cost solution of the problem. Second, to compare the costs of remote future harms with the costs of remedial measures taken in the present presents baffling issues concerning the choice of a discount rate. Baffling need not mean insoluble; the ‘time horizons’ approach to discounting offers a possible solution (Fearnside, 2002). A discounted present value can be equated to an undiscounted present value simply by shortening the time horizon for the consideration of costs and benefits. For example, the present value of an infinite stream of costs discounted at 4% is equal to the undiscounted sum of those costs for 25 years, while the present value of an infinite stream of costs discounted at 1% is equal to the undiscounted sum of those costs for 100 years. The formula for the present value of $1 per year forever is $1/r, where r is the discount rate. So if r is 4%, the present value is $25, and this is equal to an undiscounted stream of $1 per year for 25 years. If r is 1%, the undiscounted equivalent is 100 years.\n\nOne way to argue for the 4% rate (i.e., for truncating our concern for future welfare at 25 years) is to say that people are willing to weight the welfare of the next generation as heavily as our own welfare but that’s the extent of our regard for the future. One way to argue for the 1% rate is to say that they are willing to give equal weight to the welfare of everyone living in this century, which will include us, our children, and our grandchildren, but beyond that we do not care. Looking at future welfare in this way, one may be inclined towards the lower rate – which would have dramatic implications for willingness to invest today in limiting gradual global warming. The lower rate could even be regarded as a ceiling. Most people have some regard for human welfare, or at least the survival of some human civilization, in future centuries. We are grateful that the Romans did not exterminate the human race in chagrin at the impending collapse of their empire.\n\nAnother way to bring future consequences into focus without conventional discounting is by aggregating risks over time rather than expressing them in annualized terms. If we are concerned about what may happen over the next century, then instead of asking what the annual probability of a collision with a 10 km asteroid is, we might ask what the probability is that such a collision will occur within the next 100 years. An annual probability of 1 in 75 million translates into a century probability of roughly 1 in 750,000. That may be high enough – considering the consequences if the risk materializes – to justify spending several hundred million dollars, perhaps even several billion dollars to avert it.\n\nThe choice of a discount rate can be elided altogether if the focus of concern is abrupt global warming, which could happen at any time and thus constitutes a present rather than merely a remote future danger. Because it is a present danger, gradual changes in energy use that promise merely to reduce the rate of emissions are not an adequate response. What is needed is some way of accelerating the search for a technological response that will drive the annual emissions to zero or even below. Yet the Kyoto Protocol might actually do this by impelling the signatory nations to impose stiff taxes on carbon dioxide emissions in order to bring themselves into compliance with the Protocol. The taxes would give the energy industries, along with business customers of them such as airlines and manufacturers of motor vehicles, a strong incentive to finance R&D designed to create economical clean substitutes for such fuels and devices to ‘trap’ emissions at the source, before they enter the atmosphere, or even to remove carbon dioxide from the atmosphere. Given the technological predominance of the United States, it is important that these taxes be imposed on US firms, which they would be if the United States ratified the Kyoto Protocol and by doing so became bound by it.\n\nOne advantage of the technology-forcing tax approach over public subsidies for R&D is that the government would not be in the business of picking winners – the affected industries would decide what R&D to support – and another is that the brunt of the taxes could be partly offset by reducing other taxes, since emission taxes would raise revenue as well as inducing greater R&D expenditures.\n\nItmightseem that subsidies would be necessary for technologies that would have no market, such as technologies for removing carbon dioxide from the atmosphere. There would be no private demand for such technologies because, in contrast to ones that reduce emissions, technologies that remove already emitted carbon dioxide from the atmosphere would not reduce any emitter’s tax burden. This problem is, however, easily solved by making the tax a tax on net emissions. Then an electrical generating plant or other emitter could reduce its tax burden by removing carbon dioxide from the atmosphere as well as by reducing its own emissions of carbon dioxide into the atmosphere.\n\nThe conventional assumption about the way that taxes, tradable permits, or other methods of capping emissions of greenhouse gases work is that they induce substitution away from activities that burn fossil fuels and encourage more economical use of such fuels. To examine this assumption, imagine (unrealistically) that the demand for fossil fuels is completely inelastic in the short run. ⁹ Then even a very heavy tax on carbon dioxide emissions would have no short-run effect on the level of emissions, and one’s first reaction is likely to be that, if so, the tax would be ineffectual. Actually it would be a highly efficient tax from the standpoint of generating government revenues (the basic function of taxation); it would not distort the allocation of resources, and therefore its imposition could be coupled with a reduction in less efficient taxes without reducing government revenues, although the substitution would be unlikely to be complete because, by reducing taxpayer resistance, more efficient taxes facilitate the expansion of government.\n\nMore important, such a tax might – paradoxically – have an even greater impact on emissions, precisely because of the inelasticity of short-run demand, than a tax that induced substitution away from activities involving the burning of fossil fuels or that induced a more economical use of such fuels. With immediate substitution of alternative fuels impossible and the price of fossil fuels soaring because of the tax, there would be powerful market pressures both to speed the development of economical alternatives to fossil fuels as energy sources and to reduce emissions, and the atmospheric concentration, of carbon dioxide directly.\n\nFrom this standpoint a tax on emissions would be superior to a tax on the fossil fuels themselves (e.g., a gasoline tax, or a gas on B.T.U. content). Although an energy tax is cheaper to enforce because there is no need to monitor emissions, only an emissions tax would be effective in inducing carbon sequestration, because sequestration reduces the amount of atmospheric carbon dioxide without curtailing the demand for fossil fuels. A tax on gasoline will reduce the demand for gasoline but will not induce efforts to prevent the carbon dioxide emitted by the burning of the gasoline that continues to be produced from entering the atmosphere.\n\nDramatic long-run declines in emissions are likely to result only from technological breakthroughs that steeply reduce the cost of both clean fuels and carbon sequestration, rather than from insulation, less driving, lower thermostat settings, and other energy-economizing moves; and it is dramatic declines that we need. Even if the short-run elasticity of demand for activities that produce carbon dioxide emissions were – 1 (i.e., if a small increase in the price of the activity resulted in a proportionately equal reduction in the scale of the activity), a 20% tax on emissions would reduce their amount by only 20% (this is on the assumption that emissions are produced in fixed proportions with the activities generating them). Because of the cumulative effect of emissions on atmospheric concentrations of greenhouse gases, those concentrations would continue to grow, albeit at a 20% lower rate; thus although emissions might be elastic with respect to the tax, the actual atmospheric concentrations, which are the ultimate concern, would not be. In contrast, a stiff emissions tax might precipitate within a decade or two technological breakthroughs that would enable a drastic reduction of emissions, perhaps to zero. If so, the effect of the tax would be much greater than would be implied by estimates of the elasticity of demand that ignored such possibilities. The possibilities are masked by the fact that because greenhouse-gas emissions are not taxed (or classified as pollutants), the private incentives to reduce them are meagre.\n\nSubsidizing research on measures to control global warming might seem more efficient than a technology-forcing tax because it would create a direct rather than merely an indirect incentive to develop new technology. But the money to finance the subsidy would have to come out of tax revenues, and the tax (whether an explicit tax, or inflation, which is a tax on cash balances) that generated these revenues might be less efficient than a tax on emissions if the latter taxed less elastic activities, as it might. A subsidy, moreover, might induce overinvestment. A problem may be serious and amenable to solution through an expenditure of resources, but above a certain level additional expenditures may contribute less to the solution than they cost. An emissions tax set equal to the social cost of emissions will not induce overinvestment, as industry will have no incentive to incur a greater cost to avoid the tax. If the social cost of emitting a specified quantity of carbon dioxide is $1 and the tax therefore is $1, industry will spend up to $1, but not more, to avoid the tax. If it can avoid the tax only by spending $1.01 on emission-reduction measures, it will forgo the expenditure and pay the tax.\n\nFurthermore, although new technology is likely to be the ultimate solution to the problem of global warming, methods for reducing carbon dioxide emissions that do not depend on new technology, such as switching to more fuel-efficient cars, may have a significant role to play, and the use of such methods would be encouraged by a tax on emissions but not by a subsidy for novel technologies, at least until those technologies yielded cheap clean fuels.\n\nThe case for subsidy would be compelling only if inventors of new technologies for combating global emissions could not appropriate the benefits of the technologies and therefore lacked incentives to develop them. But given patents, trade secrets, trademarks, the learning curve (which implies that the first firm in a new market will have lower production costs than latecomers), and other methods of internalizing the benefits of inventions, appropriability should not be a serious problem, with the exception of basic research, including research in climate science.\n\nA superficially appealing alternative to the Kyoto Protocol would be to adopt a ‘wait and see’ approach – the approach of doing nothing at all about greenhouse-gas emissions in the hope that a few more years of normal (as distinct from tax-impelled) research in climatology will clarify the true nature and dimensions of the threat of global warming, and then we can decide what if any measures to take to reduce emissions. This probably would be the right approach were it not for the practically irreversible effect of greenhouse-gas emissions on the atmospheric concentration of those gases. Because of that irreversibility, stabilizing the atmospheric concentration of greenhouse gases at some future date might require far deeper cuts in emissions then than if the process of stabilization begins now. Making shallower cuts now can be thought of as purchasing an option to enable global warming to be stopped or slowed at some future time at a lower cost. Should further research show that the problem of global warming is not a serious one, the option would not be exercised.\n\nTo illustrate, suppose there is a 70% probability that in 2024 global warming will cause a social loss of $1 trillion (present value) and a 30% probability that it will cause no loss, and that the possible loss can be averted by imposing emission controls now that will cost the society $500 billion (for simplicity’s sake, the entire cost is assumed to be borne this year). In the simplest form of cost-benefit analysis, since the discounted loss from global warming in 2024 is $700 billion, imposing the emission controls now is cost-justified. But suppose that in 2014 we will learn for certain whether there is going to be the bad ($1 trillion) outcome in 2024. Suppose further that if we postpone imposing the emission controls until 2014, we can still avert the $1 trillion loss. Then clearly we should wait, not only for the obvious reason that the present value of $500 billion to be spent in 10 years is less than $500 billion (at a discount rate of 3% it is approximately $425 billion) but also and more interestingly because there is a 30% chance that we will not have to incur any cost of emission controls. As a result, the expected cost of the postponed controls is not $425 billion, but only 70% of that amount, or $297.5 billion, which is a lot less than $500 billion. The difference is the value of waiting.\n\nNow suppose that if today emission controls are imposed that cost society $100 billion, this will, by forcing the pace of technological advance (assume for simplicity that this is their only effect – that there is no effect in reducing emissions), reduce the cost of averting in 2014 the global-warming loss of $1 trillion in 2024 from $500 billion to $250 billion. After discounting to present value at 3% and by 70% to reflect the 30% probability that we will learn in 2014 that emission controls are not needed, the $250 billion figure shrinks to $170 billion. This is $127.5 billion less than the superficially attractive pure wait-and-see approach ($297.5 billion minus $170 billion). Of course, there is a price for the modified wait-and-see option – $100 billion. But the value is greater than the price.\n\nThis is an example of how imposing today emissions limits more modest than those of the Kyoto Protocol might be a cost-justified measure even if the limits had no direct effect on atmospheric concentrations of greenhouse gases. Global warming could be abrupt without being catastrophic and catastrophic without being abrupt. But abrupt global warming is more likely to be catastrophic than gradual global warming, because it would deny or curtail opportunities for adaptive responses, such as switching to heat-resistant agriculture or relocating population away from coastal regions. The numerical example shows that the option approach is attractive even if the possibility of abrupt global warming is ignored; in the example, we know that we are safe until 2024. However, the possibility of abrupt warming should not be ignored. Suppose there is some unknown but not wholly negligible probability that the $1 trillion global-warming loss will hit in 2014 and that it will be too late then to do anything to avert it. That would be a ground for imposing stringent emissions controls earlier even though by doing so we would lose the opportunity to avoid their cost by waiting to see whether they would actually be needed. Since we do not know the point at which atmospheric concentrations of greenhouse gases would trigger abrupt global warming, the imposition of emissions limits now may, given risk aversion, be an attractive insurance policy. An emissions tax that did not bring about an immediate reduction in the level of emissions might still be beneficial by accelerating technological breakthroughs that would result in zero emissions before the trigger point was reached.\n\nThe risk of abrupt global warming is not only an important consideration in deciding what to do about global warming; unless it is given significant weight, the political prospects for strong controls on greenhouse-gas emissions are poor. The reason can be seen in a graph that has been used without much success to galvanize public concern about global warming (IPCC, 2001; Fig. 9.1). The shaded area is the distribution of predictions of global temperature changes over the course of the century, and is at first glance alarming. However, a closer look reveals that the highest curve, which is based on the assumption that nothing will be done to curb global warming, shows a temperature increase of only about 10° Fahrenheit over the course of the century. Such an increase would be catastrophic if it occurred in a decade, but it is much less alarming when spread out over a century, as that is plenty of time for a combination of clean fuels and cheap carbon-sequestration methods to reduce carbon dioxide emissions to zero or even (through carbon sequestration) below zero without prodding by governments. Given such an outlook, convincing governments to incur heavy costs now to reduce the century increase from 10 to say 5 degrees is distinctly an uphill fight. There is also a natural scepticism about any attempt to predict what is going to happen a hundred years in the future, and a belief that since future generations will be wealthier than our generation they will find it less burdensome to incur large costs to deal with serious environmental problems.\n\nNevertheless, once abrupt global warming is brought into the picture, any complacency induced by the graph is quickly dispelled. For we then understand that the band of curves in the graph is arbitrarily truncated; that we could have a vertical takeoff say in 2020 that within a decade would bring us to the highest point in the graph. Moreover, against that risk, a technology-forcing tax on emissions might well be effective even if only the major emitting countries imposed substantial emission taxes. If manufacturers of automobiles sold in North America, the European Union, and Japan were hit with a heavy tax on carbon dioxide emissions from their automobiles, the fact that China was not taxing automobiles sold in its country would not substantially erode the incentive of the worldwide automobile industry to develop effective methods for reducing the carbon dioxide produced by their automobiles.\n\nIt is tempting to suppose that measures to deal with long-run catastrophic threats can safely be deferred to the future because the world will be richer and therefore abler to afford costly measures to deal with catastrophe. However, such complacency is unwarranted. Catastrophes can strike at any time and if they are major could make the world significantly poorer. Abrupt climate change is a perfect example. Change on the order of the Younger Dryas might make future generations markedly poorer than we are rather than wealthier, as might nuclear or biological attacks, cosmic impacts, or super-volcanic eruptions. These possibilities might actually argue for using a negative rather than positive discount rate to determine the present-value cost of a future climate disaster. ¹⁰\n\n[Image]\n\nFig. 9.1 The global climate of the twenty-first century will depend on natural changes and the response of the climate system to human activities.\n\nCredit: IPCC, 2001: Climate Change 2001: Scientific Basis. Contribution of Working Group I to the Third Assessment Report of the Intergovernmental Panel on Climate Change [Houghton, J.T., Y. Ding, D.J. Griggs, M. Noguer, P.J. van der Linden, X. Dai, K. Maskell, and C.A. Johnson (eds.)]. Figure 5, p 14. Cambridge University Press, Cambridge, United Kingdom and New York, NY, USA.\n\nAcknowledgement\n\nI thank Meghan Maloney for her very helpful research assistance and Nick Bostrom and Milan Cirkovic for their very helpful comments on a previous draft.\n\nReferences\n\nChesley, S.R. and Ward, S.N. (2006). A quantitative assessment of the human hazard from impact-generated tsunami. J. Nat. Haz., 38, 355–374.\n\nEmanuel, K. (2005). Increasing destructiveness of tropical cyclones over the past 30 years. Nature, 436, 686–688.\n\nFearnside, P.M. (2002). Time preference in global warming calculations: a proposal for a unified index. Ecol. Econ., 41, 21–31.\n\nHassol, S.J. (2004). Impacts of a Warming Arctic: Arctic Climate Impact Assessment (Cambridge: Cambridge University Press). Available online at http://amap.no/acia/\n\nIPCC (Houghton, J.T., Ding, Y., Griggs, D.J., Noguer, M., van der Linden, P.J., and Xiaosu, D. (eds.)) (2001). Climate Change 2001: The Scientific Basis. Contribution of Working Group I to the Third Assessment Report of the Intergovernmental Panel on Climate Change (IPCC) (Cambridge: Cambridge University Press).\n\nLempinen, E.W. (2005). Scientists on AAAS panel warn that ocean warming is having dramatic impact (AAAS news release 17 Feb 2005) http://www.aaas.org/news/releases/2005/0217warmingwarning.shtml\n\nMithin, S. (2003). After the Ice: A Global Human History, 20,000–5,000 BC (Cambridge, MA: Harvard University Press).\n\nPosner, R.A. (2004). Catastrophe: Risk and Response (New York: Oxford University Press).\n\nRees, M.J. (2003). Our Final Hour: A Scientist’s Warning; How Terror, Error, and Environmental Disaster Threaten Humankind’s Future in this Century – on Earth and Beyond (New York: Basic Books).\n\nSchleifstein, M. and McQuaid, J. (3 July 2002). The big easy is unprepared for the big one, experts say. Newhouse News Service. http://www.newhouse.com/archive/story1b070502.html\n\nSocolow, R.H. (July 2005). Can we bury global warming? Scientific Am., 293, 49–55. Trenberth, K. (2005). Uncertainty in hurricanes and global warming. Science, 308, 1753–1754.\n\nU.S. Department of Homeland Security (2004). Fact sheet: Department of Homeland Security Appropriations Act of 2005 (Press release 18 October 2004) http://www.dhs.gov/dhspublic/interapp/press\\_release/press\\_release\\_0541\n\nU.S. Office of Management and Budget (2003). 2003 report to Congress on combating terrorism (O.M.B. report Sept 2003). http://www.whitehouse.gov/omb/inforeg/2003\\_combat\\_terr.pdf\n\nViscusi, W.K. and Aldy, J.E. (2003). The value of a statistical life: a critical review of market estimates throughout the world. J. Risk Uncertainty, 27, 5–76.\n\nWard, S.N. and Asphaug, E. (2000). Asteroid impact tsunami: a probabilistic hazard assessment. Icarus, 145, 64–78.\n\nPART II Risks from nature\n\n• 10 • Super-volcanism and other geophysical processes of catastrophic import\n\nMichael R. Rampino\n\n10.1Introduction\n\nIn order to classify volcanic eruptions and their potential effects on the atmosphere, Newhall and Self (1982) proposed a scale of explosive magnitude, the Volcanic Explosivity Index (VEI), based mainly on the volume of the erupted products (and the height of the volcanic eruption column). VEI’s range varies from VEI = 0 (for strictly non-explosive eruptions) to VEI = 8 (for explosive eruptions producing ~10¹² m³ bulk volume of tephra). Eruption rates for VEI = 8 eruptions may be greater than 10⁶ m³s⁻¹ (Ninkovich et al., 1978a, 1978b).\n\nEruptions also differ in the amounts of sulphur-rich gases released to form stratospheric aerosols. Therefore, the sulphur content of the magma, the efficiency of degassing, and the heights reached by the eruption column are important factors in the climatic effects of eruptions (Palais and Sigurdsson, 1989; Rampino and Self, 1984). Historic eruptions of VEI ranging from 3 to 6 (volume of ejecta from <1 km³ to a few tens of km³) have produced stratospheric aerosol clouds up to a few tens of Mt. These eruptions, including Tambora 1815 and Krakatau 1883, have caused cooling of the Earth’s global climate of a few tenths of a degree Centigrade (Rampino and Self, 1984). The most recent example is the Pinatubo (Philippines) eruption of 1991 (Graf et al., 1993; Hansen et al., 1996).\n\nVolcanic super-eruptions are defined as eruptions that are tens to hundreds of times larger than historic eruptions, attaining a VEI of 8 (Mason et al., 2004; Rampino, 2002; Rampino et al., 1988; Sparks et al., 2005). Super-eruptions are usually caldera-forming events and more than 20 super-eruption sites for the last 2 million years have been identified in North America, South America, Italy, Indonesia, the Philippines, Japan, Kamchatka, and New Zealand. No doubt additional super-eruption sites for the last few million years exist (Sparks et al., 2005).\n\nThe Late Pleistocene eruption of Toba in Sumatra, Indonesia was one of the greatest known volcanic events in the geologic record (Ninkovich et al., 1978a, 1978b; Rampino and Self, 1993a; Rose and Chesner, 1990). The relatively recent age and the exceptional size of the Toba eruption make it an important test case of the possible effects of explosive volcanism on the global atmosphere and climate (Oppenheimer, 2002; Rampino and Self, 1992, 1993a; Rampino et al., 1988; Sparks et al., 2005). For the Toba event, we have data on intercaldera fill, outflow sheets produced by pyroclastic flows and tephra fallout. Recent information on the environmental effects of super-eruptions supports the exceptional climatic impact of the Toba eruption, with significant effects on the environment and human population.\n\n10.2 Atmospheric impact of a super-eruption\n\nThe Toba eruption has been dated by various methods K/Ar method at 73,500 ± 3500 yr BP (Chesner et al., 1991). The Toba ash layer occurs in deep-sea cores from the Indian Ocean and South China Sea (Huang et al., 2001; Shultz et al., 2002; Song et al., 2000). The widespread ash layer has a dense rock equivalent volume (DRE) of approximately 800 km³ (Chesner et al., 1991). The pyroclastic flow deposits on Sumatra have a volume of approximately 2000 km³DRE (Chesner et al., 1991; Rose and Chesner, 1990), for a total eruption volume of approximately 2800 km³ (DRE). Woods and Wohletz (1991) estimated Toba eruption cloud heights of 32 ± 5 km, and the duration of continuous fallout of Toba ash over the Indian Ocean has been estimated at two weeks or less (Ledbetter and Sparks, 1979).\n\nRelease of sulphur volatiles is especially important for the climatic impact of an eruption, as these form sulphuric acid aerosols in the stratosphere (Rampino and Self, 1984). Although the intrinsic sulphur content of rhyolite magmas is generally low, the great volume erupted is sufficient to give an enormous volatile release. Based on studies of the sulphur content of the Toba deposits, Rose and Chesner (1990) estimated that approximately 3 × 10¹⁵ g of H₂S/SO₂ (equivalentto ~ 1 × 10¹⁶ g of H₂SO₄ aerosols) could have been released from the erupted magma. The amounts of fine ash and sulphuric acid aerosols that could have been generated by Toba was estimated independently using data from smaller historical rhyolitic eruptions (Rampino and Self, 1992). By this simple extrapolation, the Toba super-eruption could have produced up to 2 × 10¹⁶ g of fine (<2 μ) dust and approximately 1.5 × 10¹⁵ g of sulphuric acid aerosols.\n\nPhysical and chemical processes in dense aerosol clouds may act in a ‘self-limiting’ manner, significantly reducing the amount of long-lived H₂SO₄ aerosols (Rampino and Self, 1982; Pinto et al., 1989). Using one-dimensional aerosol microphysical and photochemical models, Pinto and others (1989) showed that for an aerosol cloud of approximately 10¹⁴ g of SO₂, condensation and coagulation are important in producing larger-sized particles, which have a smaller optical effect per unit mass, and settle out of the stratosphere faster than smaller particles. However, the maximum sulphur volatile emission that they modelled was 2 × 10¹⁴ g of SO₂, and no data exist on the behaviour of H₂SO₄ aerosols in more than 10 times denser clouds.\n\nAnother possible limitation on aerosol loading is the amount of water in the stratosphere available to convert SO₂ to H₂SO₄. Stothers et al. (1986) calculated that approximately 4 × 10¹⁵ g of water might be available in the ambient stratosphere, and injection into the stratosphere of up to 5.4 × 10¹⁷ g of H₂O from Toba is possible (Rose and Chesner, 1990), more than enough water to convert the sulphur gases emitted by Toba into H₂SO₄ aerosols.\n\nThe exceptional magnitude of the Toba eruption makes it a natural target in the studies of large volcanic events preserved in polar ice cores. Work on the GISP2 ice core from Summit, Greenland, revealed an approximately 6-year long period of enhanced volcanic sulphate dated at 71, 100 ± 5000 years ago identified with the Toba eruption (Zielinski et al., 1996a, 1996b). The magnitude of this sulphate signal is the largest in the entire 110,000 years of the GISP2 record.\n\nZielinski and others (1996a) estimated that the total atmospheric loading of H₂SO₄ for the approximately 6-year period of the ice-core peak ranged from approximately 0.7 to 4.4 × 10¹⁵ g, in general agreement with the above estimates derived from volcano-logical techniques and scaling from smaller eruptions (Rampino and Self, 1992, 1993a; Rose and Chesner, 1990). Estimates of aerosol loadings range from approximately 150 to 1000 Mt per year, over the approximately 6-year period of the ice-core peak.\n\nThe SO²⁻₄ signal identified with Toba coincides with the beginning of an approximately 1000-year cooling event seen in the ice-core record between brief warm periods (interstadials), but is separated from the most recent major approximately 9000-year glacial period by the approximately 2000-year-long warmer period. A similar cool pulse between interstadials is seen in the pollen record of the Grande Pile in northeastern France, dated as approximately 70,000 years BP (Woillard and Mook, 1982).\n\nThus, the ice-core evidence suggests that the Toba signal occurred during the transition from a warm interglacial climate and was preceded and followed by abrupt climate oscillations that preceded the start of the most recent major early glaciation (Zielinski et al., 1996a, 1996b).\n\n10.3 Volcanic winter\n\nSince Toba is a low-latitude volcano, dust and volatiles would have been injected efficiently into both Northern and Southern Hemispheres (Rampino et al., 1988), although the season of the eruption is unknown. These estimated aerosol optical effects are roughly equivalent in visible opacity to smoke-clouds (Turco et al., 1990), which is within the range used in nuclear-winter scenarios of massive emissions of soot emanating from burning urban and industrial areas in the aftermath of nuclear war.\n\nAlthough the climate conditions and duration of a nuclear winter have been much debated, simulations by Turco and others (1990) predicted that land temperatures in the 30°-70°N latitude zone could range from approximately 5°C to approximately 15°C colder than normal, with freezing events in mid-latitudes during the first few months. At lower latitudes, model simulations suggest cooling of 10°C or more, with drastic decreases in precipitation in the first few months. Ocean-surface cooling of approximately 2–6°C might extend for several years, and persistence of significant soot for 1–3 years might lead to longer term (decadal) climatic cooling, primarily through climate feedbacks including increased snow cover and sea ice, changes in land surface albedo, and perturbed sea-surface temperatures (Rampino and Ambrose, 2000).\n\nThe injection of massive amounts of volcanic dust into the stratosphere by a super-eruption such as Toba might be expected to lead to similar immediate surface cooling, creating a ‘volcanic winter’ (Rampino and Self, 1992; Rampino et al., 1988). Volcanic dust probably has a relatively shorter residence time in the atmosphere (3-6 months) than soot (Turco et al., 1990) and spreads from a point source, but volcanic dust is injected much higher into the stratosphere, and hence Toba ash could have had a wide global coverage despite its short lifetime. Evidence of the wide dispersal of the dust and ash from Toba can be seen from lake deposits in India, where the reworked Toba ash forms a layer up to 3 m thick, and from the widespread ash layer in the Indian Ocean and South China Sea (Acharya and Basu, 1993; Huang et al., 2001; Shane et al., 1995).\n\nEvidence for rapid and severe cooling from the direct effects of volcanic ash clouds comes from the aftermath of the 1815 Tambora eruption. Madras, India experienced a dramatic cooling during the last week of April 1815, a time when the relatively fresh ash and aerosol cloud from Tambora (10-11 April) would have been overhead. Morning temperatures dropped from 11°C on Monday to – 3°C on Friday (Stothers, 1984a). A similar, but much smaller effect, occurred as the dust cloud from the 1980 Mt St Helens eruption passed over downwind areas (Robock and Mass, 1982).\n\nThe stratospheric injection of sulphur volatiles (>10¹⁵ g), and the time required for the formation and spread of volcanic H₂SO₄ aerosols in the stratosphere should lead to an extended period of increased atmospheric opacity and surface cooling. The ice-core record, however, indicates stratospheric loadings of 10¹⁴ to 10¹⁵ g of H₂SO₄ aerosols for up to 6 years after the eruption (Zielinski et al., 1996a).\n\nThis agrees with model calculations by Pope and others (1994) that predict oxidation lifetimes (time required to convert a given mass of sulphur into H₂SO₄ aerosols) of between 4 and 17 years, and diffusion lifetimes (time required to remove unoxidized SO₂ by diffusion to the troposphere) of between 4 and 7 years for total sulphur masses between 10¹⁵ and 10¹⁶ g. For atmospheric injection in this range, the diffusion lifetime is the effective lifetime of the cloud because the SO₂ reservoir is depleted before oxidation is completed.\n\nIf the relationship between Northern Hemisphere cooling and aerosol loading from large eruptions is approximately linear, then scaling up from the 1815 AD. Tambora eruption would lead to an approximately 3.5°C hemispheric cooling after Toba (Rampino and Self, 1993a). Similarly, empirical relationships between SO₂ released and climate response (Palais and Sigurdsson, 1989) suggested a hemispheric surface-temperature decrease of about 4 ± 1°C. The eruption clouds of individual historic eruptions have been too short-lived to drive lower tropospheric temperatures to their steady-state values (Pollack et al., 1993), but the apparently long-lasting Toba aerosols may mean that the temperature changes in the troposphere attained a larger fraction of their steady-state values. Huang et al. (2001) were able to correlate the Toba ash in the South China Sea with a 1°C cooling of surface waters that lasted about 1000 years.\n\nConsidering a somewhat smaller super-eruption, the Campanian eruption of approximately 37,000 cal yr BP in Italy (150 km³ of magma discharged) was coincident with Late Pleistocene bio-cultural changes that occurred within and outside the Mediterranean region. These included the Middle to Upper Paleolithic cultural transition and the replacement of Neanderthals by ‘modern’ Homo sapiens (Fedele et al., 2002).\n\n10.4 Possible environmental effects of a super-eruption\n\nThe climatic and environmental impacts of the Toba super-eruption are potentially so much greater than that of recent historical eruptions (e.g., Hansen et al., 1992; Stothers, 1996) that instrumental records, anecdotal information, and climate-model studies of the effects of these eruptions may not be relevant in scaling up to the unique Toba event (Rampino and Self, 1993a; Rampino et al., 1988). Various studies on the effects of extremes of atmospheric opacity and climate cooling on the environment and life have been carried out, however, in connection with studies of nuclear winter and the effects of asteroid impacts on the earth (e.g., Green et al., 1985; Harwell, 1984; Tinus and Roddy, 1990), and some of these may be relevant to the Toba situation.\n\nTwo major effects on plant life from high atmospheric opacity are reduction of light levels and cold temperatures. Reduction in light levels expected from the Toba eruption would range from dim-sun conditions (~75% sunlight transmitted) like those seen after the 1815 Tambora eruption, to that of an overcast day (~10% sunlight transmitted). Experiments with young grass plants have shown how net photosynthesis varies with light intensity. For a decrease to 10% of the noon value for a sunny summer day, photosynthesis was reduced by about 85% (van Kuelan et al., 1975), and photosynthesis also drops with decreasing temperatures (Redman, 1974).\n\nResistance of plants to unusual cold conditions varies somewhat. Conditions in the tropical zone are most relevant to possible impacts on early human populations in Africa. Tropical forests are very vulnerable to chilling, and Harwell and others (1985) argue that for freezing events in evergreen tropical forests, essentially all aboveground plant tissues would be killed rapidly.\n\nAverage surface temperatures in the tropics today range from approximately 16–24^(°)C. Nuclear winter scenarios predict prolonged temperature decreases of 3–7°C in Equatorial Africa, and short-term temperature decreases of up to 10^(°)C. Many tropical plants are severely damaged by chilling to below 10–15°C for a few days (Greene et al., 1985; Leavitt, 1980). Most tropical forest plants have limited seed banks, and the seeds typically lack a dormant phase. Furthermore, regrowth tends to produce forests of limited diversity, capable of supporting much less biomass (Harwell et al., 1985).\n\nEven for temperate forests, destruction could be very severe (Harwell, 1984; Harwell et al., 1985). In general, the ability of well-adapted trees to withstand low temperatures (cold hardiness) is much greater than that needed at any single time of the year, but forests can be severely damaged by unusual or sustained low temperatures during certain times of the year. A simulation of a 10^(°)C decrease in temperatures during winter shows a minimal effect on the cold-hardy and dormant trees, whereas a similar 10^(°)C drop in temperature during the growing season (when cold hardiness is decreased) leads to a 50% dieback, and severe damage to surviving trees, resulting in the loss of at least a year’s growth.\n\nThe situation for deciduous forest trees would be even worse than that for the evergreens, as their entire foliage would be new and therefore lost. For example, Larcher and Bauer (1981) determined that cold limits of photosynthesis of various temperate zone plants range from –1.3 to –3.9°C, approximately the same range as the tissue-freezing temperatures for these plants. Lacking adequate food reserves, most temperate forest trees would not be able to cold harden in a timely manner, and would die or suffer additional damage during early freezes in the Fall (Tinus and Roddy, 1990).\n\nThe effect of the Toba super-eruption on the oceans is more difficult to estimate. Regionally, the effect on ocean biota of the fallout of approximately 4 g/cm² of Toba ash over an area of 5 × 10⁶ km² in the Indian Ocean must have been considerable. Deposition rates of N, organic C, and CaCO₃ all rise sharply in the first few centimetres of the Toba ash layer, indicating that the ash fallout swept the water column of most of its particulate organic carbon and calcium carbonate (Gilmour et al., 1990).\n\nAnother possible effect of a dense aerosol cloud is decreased ocean productivity. For example, satellite observations after the 1982 El Chichón eruption showed high aerosol concentrations over the Arabian Sea, and these values were associated with low surface productivity (as indicated by phytoplankton concentrations) from May through October of that year (Strong, 1993). Brock and McClain (1992) suggested that the low productivity was related to weaker-than-normal monsoon winds, and independent evidence suggests that the southwest monsoon in the area arrived later and withdrew earlier than usual, and that the wind-driven Somali current was anomalously weak. Conversely, Genin and others (1995) reported enhanced vertical mixing of cooled surface waters in weakly stratified areas of the Red Sea following the Pinatubo eruption, which resulted in algal and phytoplankton blooms that precipitated widespread coral death.\n\nStudies following the 1991 Pinatubo eruption provide evidence that aerosol-induced cooling of the southwestern Pacific could lead to significant weakening of Hadley Cell circulation and rainfall, and might precipitate long-term El Niño-like anomalies with extensive drought in many tropical areas (Gagan and Chivas, 1995). Some climate-model simulations predict significant drought in tropical areas from weakening of the trade winds/Hadley circulation and from reduction in the strength of the summer monsoon (e.g., Pittock et al., 1986, 1989; Turco et al., 1990). For example, Pittock and others (1989) presented GCM results that showed a 50% reduction in convective rainfall in the tropics and monsoonal regions.\n\n10.5 Super-eruptions and human population\n\nRecent debate about the origin of modern humans has focused on two competing hypotheses: (1) the ‘multiregional’ hypothesis, in which the major subdivisions of our species evolved slowly and in situ, with gene flow accounting for the similarities now observed among groups, and (2) the ‘replacement’ hypothesis, in which earlier populations were replaced 30,000 to 100,000 years ago by modern humans that originated in Africa (Hewitt, 2000; Rogers and Joude, 1995).\n\nGenetic studies have been used in attempts to test these two hypotheses. Studies of nuclear and mitochondrial DNA from present human populations led to the conclusion that the modern populations originated in Africa and spread to the rest of the Old World approximately 50,000 ± 20,000 years ago (Harpending et al., 1993; Jones and Rouhani, 1986; Wainscoat et al., 1986). This population explosion apparently followed a severe population bottleneck, estimated by Harpending and others (1993) to have reduced the human population to approximately 500 breeding females, or a total population as small as 4000 for approximately 20,000 years. At the same time, Neanderthals who were probably better adapted for cold climate moved into the Levant region when modern humans vacated it (Hewitt, 2000).\n\nHarpending and others (1993) proposed that the evidence may fit an intermediate ‘Weak Garden of Eden’ hypothesis that a small ancestral human population separated into partially isolated groups about 100,000 years ago, and about 30,000 years later these populations underwent either simultaneous bottlenecks or simultaneous expansions in size. Sherry and others (1994) estimated mean population expansions times ranging from approximately 65,000 to 30,000 years ago, with the African expansion possibly being the earliest.\n\nAmbrose (1998, 2003; see Gibbons, 1993) pointed out that the timing of the Toba super-eruption roughly matched the inferred timing of the bottleneck and release, and surmised that the environmental after-effects of the Toba eruption might have been so severe as to lead to a precipitous decline in the population of human ancestors (but see Gathorne-Hardy and Harcourt-Smith, 2003 for opposing views). Rampino and Self (1993a) and Rampino and Ambrose (2000) concurred that the climatic effects of Toba could have constituted a true ‘volcanic winter’, and could have caused severe environmental damage. It may be significant that analysis of mtDNA of Eastern Chimpanzee (Pan troglodytes schweinfurthii) shows a similar pattern to human DNA, suggesting a severe reduction in population at about the same time as in the human population (see Rogers and Jorde, 1995).\n\n10.6 Frequency of super-eruptions\n\nDecker (1990) proposed that if all magnitude 8 eruptions in the recent past left caldera structures that have been recognized, then the frequency of VEI 8 eruptions would be approximately 2 × 10 ^(– 5) eruptions per year, or roughly one VEI 8 eruption every 50,000 years.\n\nThe timing and magnitude of volcanic eruptions, however, are difficult to predict. Prediction strategies have included (1) recognition of patterns of eruptions at specific volcanoes (e.g., Godano and Civetta, 1996; Klein, 1982), (2) precursor activity of various kinds (e.g., Chouet, 1996; Nazzaro, 1998), (3) regional and global distribution of eruptions in space and time (Carr, 1977; Mason et al., 2004; Pyle, 1995), and (4) theoretical\n\npredictions based on behaviour of materials (Voight, 1988; Voight and Cornelius, 1991). Although significant progress has been made in short-term prediction of eruptions, no method has proven successful in consistently predicting the timing, and more importantly, the magnitude of the resulting eruption or its magmatic sulphur content and release characteristics. State-of-the-art technologies involving continuous satellite monitoring of gas emissions, thermal anomalies and ground deformation (e.g., Alexander, 1991; Walter, 1990) promise improved forecasting and specific prediction of volcanic events, but these technologies are thus far largely unproven.\n\nFor example, although we have 2000 years of observations for the Italian volcano Vesuvius (Nazzaro, 1998), and a long history of monitoring and scientific study, prediction of the timing and magnitude of the next Vesuvian eruption remains a problem (Dobran et al., 1994; Lirer et al., 1997). For large caldera-forming super-eruptions, which have not taken place in historic times, we have little in the way of meaningful observations on which to base prediction or even long-range forecasts.\n\n10.7 Effects of a super-eruptions on civilization\n\nThe regional and global effects of the ash fallout and aerosol clouds on climate, agriculture, health, and transportation would present a severe challenge to modern civilization. The major effect on civilization would be through collapse of agriculture as a result of the loss of one or more growing seasons (Toon et al., 1997). This would be followed by famine, the spread of infectious diseases, breakdown of infrastructure, social and political unrest, and conflict. Volcanic winter predictions are for global cooling of 3–5°C for several years, and regional cooling up to 15°C (Rampino and Self, 1992; Rampino and Ambrose, 2000). This could devastate the major food-growing areas of the world. For example, the Asian rice crop could be destroyed by a single night of below-freezing temperatures during the growing season. In the temperate grain-growing areas, similar drastic effects could occur. In Canada, a 2–3^(°)C average local temperature drop would destroy wheat production, and 3–4^(°)C would halt all Canadian grain production. Crops in the American Midwest and the Ukraine could be severely injured by a 3–4°C temperature decrease (Harwell and Hutchinson, 1985; Pittock et al., 1986). Severe climate would also interfere with global transportation of foodstuffs and other goods. Thus, a super-eruption could compromise global agriculture, leading to famine and possible disease pandemics (Stothers, 2000).\n\nFurthermore, large volcanic eruptions might lead to longer term climatic change through positive feedback effects on climate such as cooling the surface oceans, formation of sea-ice, or increased land ice (Rampino and Self, 1992, 1993a, 1993b), prolonging recovery from the ‘volcanic winter’. The result could be widespread starvation, famine, disease, social unrest, financial collapse, and severe damage to the underpinnings of civilization (Sagan and Turco, 1990; Sparks et al., 2005).\n\nThe location of a super-eruption can also be an important factor in its regional and global effects. Eruptions from the Yellowstone Caldera over the last 2 million years have included three super-eruptions. Each of these produced thick ash deposits over the western and central United States (compacted ash thicknesses of 0.2 m occur ~1500 km from the source; Wood and Kienle, 1990).\n\nOne mitigation strategy could involve the stockpiling of global food reserves. In considering the vagaries of normal climatic change, when grain stocks dip below about 15% of utilization, local scarcities, worldwide price jumps and sporadic famine were more likely to occur. Thus a minimum world level of accessible grain stocks near 15% of global utilization should be maintained as a hedge against year-to-year production fluctuations due to climatic and socioeconomic disruptions. This does not take into account social and economic factors that could severely limit rapid and complete distribution of food reserves.\n\nAt present, a global stockpile equivalent to a 2-month global supply of grain exists, which is about 15% of annual consumption. For a super-volcanic catastrophe, however, several years of growing season might be curtailed, and hence a much larger stockpile of grain and other foodstuffs would have to be maintained, along with the means for rapid global distribution.\n\n10.8 Super-eruptions and life in the universe\n\nThe chances for communicative intelligence in the Galaxy is commonly represented by a combination of the relevant factors called the Drake Equation, which can be written as\n\n[Image]\n\nwhere N is the number of intelligent communicative civilizations in the Galaxy; R\\* is the rate of star formation averaged over the lifetime of the Galaxy; f\\_(p) is the fraction of stars with planetary systems; n\\_(e) is the mean number of planets within such systems that are suitable for life; f\\_(l) is the fraction of such planets on which life actually occurs; f\\_(i) is the fraction of planets on which intelligence of arises; f\\_(c) is the fraction of planets on which intelligent life develops a communicative phase; and L is the mean lifetime of such technological civilizations (Sagan, 1973).\n\nAlthough the Drake Equation is useful in organizing the factors that are thought to be important for the occurrence of extraterrestrial intelligence, the actual assessment of the values of the terms in the equation is difficult. The only well-known number is R\\*, which is commonly taken as 10 yr ^(– 1). Estimates for N have varied widely from approximately 0 to >108 civilizations (Sagan, 1973).\n\nIt has been pointed out recently that f\\_(c) and L are limited in part by the occurrence of asteroid and comet impacts that could prove catastrophic to technological civilizations (Sagan and Ostro, 1994; Chyba, 1997). Present human civilization, dependent largely on annual crop yields, is vulnerable to an ‘impact winter’ that would result from dust lofted into the stratosphere by the impact of objects ≥1 km in diameter (Chapman and Morrison, 1994; Toon et al., 1997). Such an impact would release approximately 10⁵-10⁶ Mt (TNT equivalent) of energy, produce a crater approximately 20–40 km in diameter, and is calculated to generate a global cloud consisting of approximately 1000 Mt of submicron dust (Toon et al., 1997). Covey et al. (1990) performed 3-D climate-model simulations for a global dust cloud containing submicron particles with a mass corresponding that that produced by an impact of 6 × 10⁵ Mt (TNT). In this model, global temperatures dropped by approximately 8 C during the first few weeks. Chapman and Morrison (1994) estimated that an impact of this size would kill more than 1.5 billion people through direct and non-direct effects.\n\nImpacts of this magnitude are expected to occur on average about every 100,000 years (Chapman and Morrison, 1994). Thus, a civilization must develop science and technology sufficient to detect and deflect such threatening asteroids and comets on a time scale shorter than the typical times between catastrophic impacts. Recent awareness of the impact threat to civilization has led to investigations of the possibilities of detection, and deflection or destruction of asteroids and comets that threaten the Earth (e.g., Gehrels, 1994; Remo, 1997). Planetary protection technology has been described as essential for the long-term survival of human civilization on the Earth.\n\nThe drastic climatic and ecological effects predicted for explosive super-eruptions leads to the question of the consequences for civilization here on Earth, and on other earth-like planets that might harbour intelligent life (Rampino, 2002; Sparks et al., 2005). Chapman and Morrison (1994) suggested that the global climatic effects of super-eruptions such as Toba might be equivalent to the effects of an approximately 1 km diameter asteroid. Fine volcanic dust and sulphuric acid aerosols have optical properties similar to the submicron dust produced by impacts (Toon et al., 1997), and the effects’ on atmospheric opacity should be similar. Volcanic aerosols, however, have a longer residence time of several years (Bekki et al., 1996) compared to a few months for fine dust, so a huge eruption might be expected to have a longer lasting effect on global climate than an impact producing a comparable amount of atmospheric loading.\n\nEstimates of the frequency of large volcanic eruptions that could cause ‘volcanic winter’ conditions suggest that they should occur about once every 50,000 years. This is approximately a factor of two more frequent than asteroid or comet collisions that might cause climate cooling of similar severity (Rampino, 2002). Moreover, predicting or preventing a volcanic climatic disaster might be more difficult than tracking and diverting incoming asteroids and comets. These considerations suggest that volcanic super-eruptions pose a real threat to civilization, and efforts to predict and mitigate volcanic climatic disasters should be contemplated seriously (Rampino, 2002; Sparks et al., 2005).\n\nAcknowledgement\n\nI thank S. Ambrose, S. Self, R. Stothers, and G. Zielinski for the information provided.\n\nSuggestions for further reading\n\nBindeman, I.N. (2006). The Secrets of Supervolcanoes. Scientific American Magazine (June 2006). A well-written popular introduction in the rapidly expanding field of super-volcanism.\n\nMason, B.G., Pyle, D.M., and Oppenheimer, C. (2004). The size and frequency of the largest explosive eruptions on Earth. Bull. Volcanol., 66, 735–748. The best modern treatment of statistics of potential globally catastrophic volcanic eruptions. It includes a comparison of impact and super-volcanism threats and concludes that super-eruptions present a significantly higher risk per unit energy yield.\n\nRampino, M.R. (2002). Super-eruptions as a threatto civilizations on Earth-like planets. Icarus, 156, 562–569. Puts super-volcanism into a broader context of evolution of intelligence in the universe.\n\nRampino, M.R., Self, S., and Stothers, R.B. (1988). Volcanic winters. Annu. Rev. Earth Planet. Sci., 16, 73–99. Detailed discussion of climatic consequences of volcanism and other potentially catastrophic geophysical processes.\n\nReferences\n\nAcharya, S.K. and Basu, P.K. (1993). Toba ash on the Indian subcontinent and its implications for correlations of Late Pleistocene alluvium. Quat. Res., 40, 10–19.\n\nAlexander, D. (1991). Information technology in real-time for monitoring and managing natural disasters. Prog. Human Geogr., 15, 238–260.\n\nAmbrose, S.H. (1998). Late Pleistocene human population bottlenecks, volcanic winter, and the differentiation of modern humans. J. Human Evol., 34, 623–651.\n\nAmbrose, S.H. (2003). Did the super-eruption of Toba cause a human population bottleneck? Reply to Gathorne-Hardy and Harcourt-Smith. J. Human Evol., 45, 231237.\n\nBekki, S., Pype, J.A., Zhong, W., Toumi, R., Haigh, J.D., and Pyle, D.M. (1996). The role of microphysical and chemical processes in prolonging the climate forcing of the Toba eruption. Geophys. Res. Lett., 23, 2669–2672.\n\nBischoff, J.L., Solar, N., Maroto, J., and Julia, R. (1989). Abrupt Mousterian-Aurignacian boundaries at c. 40 ka bp: Accelerator¹⁴C dates from l’Arbreda Cave (Catalunya, Spain). J. Archaeol. Sci., 16, 563–576.\n\nBrock, J.C. and McClain, C.R. (1992). Interannual variability in phytoplankton blooms observed in the northwestern Arabian Sea during the southwest monsoon. J. Geophys. Res., 97, 733–750.\n\nCarr, M.J. (1977). Volcanic activity and great earthquakes at convergent plate margins. Science, 197, 655–657.\n\nChapman, C.R. and Morrison, D. (1994). Impacts on the Earth by asteroids and comets: assessing the hazards. Nature, 367, 33–40.\n\nChesner, C.A., Rose, W.I., Deino, A., Drake, R. and Westgate, J.A. (1991). Eruptive history of the earth’s largest Quaternary caldera (Toba, Indonesia) clarified. Geology, 19, 200–203.\n\nChouet, B.A. (1996). Long-period volcano seismicity: Its source and use in eruption forecasting. Nature, 380, 316.\n\nChyba, C.F. (1997). Catastrophic impacts and the Drake Equation. In Cosmovici, C.B., Bowyer, S. and Werthimer, D. (eds.), Astronomical and Biochemical Origins and the Search for Life in the Universe, pp. 157–164 (Bologna: Editrice Compositori).\n\nCovey, C., Ghan, S.J., Walton, J.J., and Weissman, P.R. (1990). Global environmental effects of impact-generated aerosols: Results from a general circulation model. Geol. Soc. Am. Spl. Paper, 247, 263–270.\n\nDe La Cruz-reyna, S. (1991). Poisson-distributed patterns of explosive eruptive activity. Bull. Volcanol., 54, 57–67.\n\nDecker, R.W. (1990). How often does a Minoan eruption occur? In Hardy, D.A. (ed.), Thera and the Aegean World III 2, pp. 444–452 (London: Thera Foundation).\n\nDobran, F., Neri, A., and Tedesco, M. (1994). Assessing the pyroclastic flow hazard at Vesuvius. Nature, 367, 551–554.\n\nFedele, F.G., Giaccio, B., Isaia, R., and Orsi, G. (2002). Ecosystem impact of the Campanian ignimbrite eruption in Late Pleistocene Europe. Quat. Res., 57, 420–424.\n\nGagan, M.K. and Chivas, A.R. (1995). Oxygen isotopes in western Australian coral reveal Pinatubo aerosol-induced cooling in the Western Pacific Warm Pool. Geophys. Res. Lett., 22, 1069–1072.\n\nGathorne-Hardy, F.J. and Harcourt-Smith, W.E.H. (2003). The super-eruption of Toba, did it cause a human population bottleneck? J. Human Evol., 45, 227–230.\n\nGenin, A., Lazar, B., and Brenner, S. (1995). Vertical mixing and coral death in the Red Sea following the eruption of Mount Pinatubo. Nature, 377, 507–510.\n\nGehrels, T. (ed.) (1994). Hazards Due to Comets &Asteroids, 1300 p (Tucson: University of Arizona Press).\n\nGibbons, A. (1993). Pleistocene population explosions. Science, 262, 27–28.\n\nGilmour, I., Wolbach, W.S., and Anders, E. (1990). Early environmental effects of the terminal Cretaceous impact. Geol. Soc. Am. Spl. Paper, 247, 383–390.\n\nGodano, C. and Civetta, L. (1996). Multifractal analysis of Vesuvius volcano eruptions. Geophys. Res. Lett., 23, 1167–1170.\n\nGraf, H.-F., Kirschner, I., Robbock, A., and Schult, I. (1993). Pinatubo eruption winter climate effects: Model versus observations. Clim. Dynam., 9, 81–93.\n\nGreen, O., Percival, I., and Ridge, I. (1985). Nuclear Winter, the Evidence and the Risks, 216 p (Cambridge: Polity Press).\n\nHansen, J., Lacis, A., Ruedy, R., and Sato, M. (1992). Potential climate impact of the Mount Pinatubo eruption. Geophys. Res. Lett., 19, 215–218.\n\nHansen, J.E., Sato, M., Ruedy, R., Lacis, A., Asamoah, K, Borenstein, S., Brown, E., Cairns, B., Caliri, G., Campbell. M., Curran, B., De Castrow, S., Druyan, L., Fox, M., Johnson, C., Lerner, J., Mscormick, M.P., Miller, R., Minnis, P., Morrison, A., Palndolfo, L., Ramberran, I., Zaucker, F., Robinson, M., Russell, P., Shah, K., Stone, P., Tegen, I., Thomason, L., Wilder, J., and Wilson, H. (1996). A Pinatubo modeling investigation. In Fiocco, G., Fua, D., and Visconti, G. (eds.), The Mount Pinatubo Eruption: Effects on the Atmosphere and Climate NATO ASI Series Volume 142, pp. 233–272 (Heidelberg: Springer Verlag).\n\nHarpending, H.C., Sherry, S.T., Rogers, A.L., and Stoneking, M. (1993). The genetic structure of ancient human populations. Curr. Anthropol., 34, 483–496.\n\nHarwell, M.A. (1984). The Human and Environmental Consequences of Nuclear War, 179 p (New York: Springer-Verlag).\n\nHarwell, M.A. and Hutchinson, T.C. (eds.) (1985). Environmental Consequences of Nuclear War, Volume II Ecological and Agricultural Effects, 523 p (New York: Wiley).\n\nHarwell, M.A., Hutchinson, T.C., Cropper, W.P., Jr, and Harwell, C.C. (1985). Vulnerability of ecological systems to climatic effects of nuclear war. In Harwell, M.A. and Hutchinson, T.C. (eds.), Environmental Consequences of Nuclear War, Volume II Ecological and Agricultural Effects, pp. 81–171 (New York: Wiley).\n\nHewitt, G. (2000). The genetic legacy of the Quaternary ice ages. Nature, 405, 907–913. Huang, C.-H., Zhao, M., Wang, C.-C., and Wei, G. (2001). Cooling of the South China Sea by the Toba eruption and other proxies ~71,000 years ago. Geophys. Res. Lett., 28, 3915–3918.\n\nJones, J.S. and Rouhani, S. (1986). How small was the bottleneck? Nature, 319, 449–450.\n\nKlein, F.W. (1982). Patterns of historical eruptions at Hawaiian volcanoes. J. Volcanol. Geotherm. Res., 12, 1–35.\n\nLarcher, W. and Bauer, H. (1981). Ecological significance of resistance to low temperature. In Lange, O.S., Nobel, P.S., Osmond, C.B., and Zeigler, H. (eds.), Encyclopedia of Plant Physiology, Volume 12A: Physiological Plant Ecology I, pp. 403–37 (Berlin: Springer-Verlag). Leavitt, J. (1980). Responses of Plants to Environmental Stresses. I. Chilling, Freezing and High Temperature Stresses, 2nd edition (New York: Academic Press). Ledbetter, M.T. and Sparks, R.S.J. (1979). Duration of large-magnitude explosive eruptions deduced from graded bedding in deep-sea ash layers. Geology, 7, 240–244.\n\nLirer, L., Munno, R., Postiglione, I., Vinci, A., and Vitelli, L. (1997). The A.D. 79 eruption as a future explosive scenario in the Vesuvian area: Evaluation of associated risk. Bull. Volcanol., 59, 112–124.\n\nMason, B.G., Pyle, D.M., and Oppenheimer, C. (2004). The size and frequency of the largest explosive eruptions on Earth. Bull. Volcanol., 66, 735–748.\n\nNazzaro, A. (1998). Some considerations on the state of Vesuvius in the Middle Ages and the precursors of the 1631 eruption. Annali di Geofísica, 41, 555–565.\n\nNewhall, C.A. and Self, S. (1982). The volcanic explosivity index (VEI): an estimate of the explosive magnitude for historical volcanism. J. Geophys. Res., 87, 1231–1238.\n\nNinkovich, D., Shackleton, N.J., Abdel-Monem, A.A., Obradovich, J.A., and Izett, G. (1978a). K-Ar age of the late Pleistocene eruption of Toba, north Sumatra. Nature, 276, 574–577.\n\nNinkovich, D., Sparks, R.S.J., and Ledbetter, M.T. (1978b). The exceptional magnitude and intensity of the Toba eruption: an example of the use of deep-sea tephra layers as a geological tool. Bull. Volcanol., 41, 1–13.\n\nOppenheimer, C. (2002). Limited global change due to the largest known Quaternary eruption, Toba = 74 kyr BP? Quat. Sci. Rev., 21, 1593–1609.\n\nPalais, J.M. and Sigurdsson, H. (1989). Petrologic evidence of volatile emissions from major historic and pre-historic volcanic eruptions. Am. Geophys. Union, Geophys. Monogr., 52, 32–53.\n\nPinto, J.P., Turco, R.P., and Toon, O.B. (1989). Self-limiting physical and chemical effects in volcanic eruption clouds. J. Geophys. Res., 94, 11165–11174.\n\nPittock, A.B., Ackerman, T.P., Crutzen, P.J., MacCracken, M.C., Shapiro, C.S., and Turco, R.P. (eds.) (1986). Environmental Consequences of Nuclear war, Volume I Physical and Atmospheric Effects, 359 p (New York: Wiley).\n\nPittock, A.B., Walsh, K., and Frederiksen, J.S. (1989). General circulation model simulation of mild nuclear winter effects. Clim. Dynam., 3, 191–206.\n\nPollack, J.B., Rind, D., Lacis, A., Hansen, J.E., Sato, M., and Ruedy, R. (1993). GCM simulations of volcanic aerosol forcing. Part 1: climate changes induced by steady-state perturbations. J. Clim., 6, 1719–1742.\n\nPope, K.O., Baines, K.H., Ocampo, A.C., and Ivanov, B.A. (1994). Impact winter and the Cretaceous/Tertiary extinctions: Results of a Chicxulub asteroid impact model. Earth Planet. Sci. Lett., 128, 719–725.\n\nPyle, D.M. (1995). Mass and energy budgets of explosive volcanic eruptions. Geophys. Res. Lett., 22, 563–566.\n\nRampino, M.R. (2002). Supereruptions as a threat to civilizations on Earth-like planets. Icarus, 156, 562–569.\n\nRampino, M.R. and Ambrose, S.H. (2000). Volcanic winter in the Garden of Eden: the Toba super-eruption and the Late Pleistocene human population crash. In McCoy, F.W. and Heiken, G. (eds.), Volcanic Hazards and Disasters in Human Antiquity. Special paper 345, pp. 71–82 (Boulder, CO: Geological Society of America).\n\nRampino, M.R. and Self, S. (1982). Historic eruptions of Tambora (1815), Krakatau (1883), and Agung (1963), their stratospheric aerosols and climatic impact. Quat. Res., 18, 127–143.\n\nRampino, M.R. and Self, S. (1984). Sulphur-rich volcanism and stratospheric aerosols. Nature, 310, 677–679.\n\nRampino, M.R. and Self, S. (1992). Volcanic winter and accelerated glaciation following the Toba super-eruption. Nature, 359, 50–52.\n\nRampino, M.R. and Self, S. (1993a). Climate-volcanism feedback and the Toba eruption of ~74,000 years ago. Quat. Res., 40, 269–280.\n\nRampino, M.R. and Self, S. (1993b). Bottleneck in human evolution and the Toba eruption: Science, 262, 1955.\n\nRampino, M.R., Self, S., and Stothers, R.B. (1988). Volcanic winters. Annu. Rev. Earth Planet. Sci., 16, 73–99.\n\nRedman, R.E. (1974). Photosynthesis, plant respiration, and soil respiration measured with controlled environmental chambers in the field: Canadian Committee, IBP Technical Report 49 (Saskatoon: University of Saskatchewan).\n\nRemo, J.L. (ed.) (1997). Near Earth Objects: The United Nations International Conference, Volume 822, 623 p (New York Academy of Sciences Annals). New York\n\nRobock, A. and Mass, C. (1982). The Mount St. Helens volcanic eruption of 18 May 1980: large short-term surface temperature effects. Science, 216, 628–630.\n\nRogers, A.R. and Jorde, L.B. (1995). Genetic evidence on modern human origins. Human Biol., 67, 1–36.\n\nRose, W.I. and Chesner, C.A. (1990). Worldwide dispersal of ash and gases from earth’s largest known eruption: Toba, Sumatra, 75 Ka. Global Planet. Change, 89, 269–275.\n\nSagan, C. (ed.) (1973). Communication with Extraterrestrial Intelligence (Cambridge, MA: MIT Press).\n\nSagan, C. and Ostro, S. (1994). Long-range consequences of interplanetary collisions. Issues Sci. Technol., 10, 67–72.\n\nSagan C. and Turco, R. (1990). A Path Where No Man Thought, 499 p (New York: Random House).\n\nShane, P., Westgate, J., Williams, M., and Korisettar, R. (1995). New geochemical evidence for the Youngest Toba Tuff in India. Quat. Res., 44, 200–204.\n\nSherry, S.T., Rogers, A.R., Harpending, H., Soodyall, H., Jenkins, T., and Stoneking, M. (1994). Mismatch distributions of mtDNA reveal recent human population expansions. Human Biol., 66, 761 -775.\n\nShultz, H., Emeis, K.-C., Erlenkeuser, H., von Rad, U., and Rolf, C. (2002). The Toba volcanic event and interstadial/stadial climates at the marine isotopic stage 5 to 4 transition in the northern Indian Ocean. Quat. Res., 57, 22–31.\n\nSong, S.-R., Chen, C.-H., Lee, M.-Y., Yang, T.F., and Wei, K.-Y. (2000). Newly discovered eastern dispersal of the youngest Toba tuff. Marine Geol., 167, 303–312.\n\nSparks, S., Self, S., Grattan, J., Oppenheimer, C., Pyle, D., and Rymer, H. (2005). Super-eruptions global effects and future threats. Report of a Geological Society of London Working Group, The Geological Society, London, pp. 1–25.\n\nStothers, R.B. (1984a). The greatTambora eruption of 1815 and its aftermath. Science, 224, 1191–1198.\n\nStothers, R.B. (1984b). The mystery cloud of AD 536. Nature, 307, 344–345. Stothers, R.B. (1996). Major optical depth perturbations to the stratosphere from volcanic eruptions: Pyrheliometric period, 1881–1960. J. Geophys. Res., 101, 3901–3920.\n\nStothers, R.B. (2000). Climatic and demographic consequences of the massive volcanic eruption of 1258. Clim. Change, 45, 361–374.\n\nStrong, A.E. (1993). A note on the possible connection between the El Chichón eruption and ocean production in the northwest Arabian Sea during 1982. J. Geophys. Res., 98, 985–987.\n\nTinus, R.W. and Roddy, D.J. (1990). Effects of global atmospheric perturbations on forest ecosystems in the Northern Temperate Zone; Predictions of seasonal depressed-temperature kill mechanisms, biomass production, and wildfire soot emissions. Geol. Soc. Am. Spl. Paper, 247, 77–86.\n\nToon, O.B., Turco, R.P., and Covey, C. (1997). Environmental perturbations caused by the impacts of asteroids and comets. Rev. Geophys., 35, 41–78.\n\nTurco, R.P., Toon, O.B., Ackerman, T.P., Pollack, J.B., and Sagan, C., (1990). Climate and smoke: An appraisal of nuclear winter. Science, 247, 166–176.\n\nvan Keulan, H., Lowerse, W., Sibma, L., and Alberda, M. (1975). Crop simulation and experimental evaluation – a case study, In Cooper, J.P. (ed.), Photosynthesis and Productivity in Different Environments, pp. 623–643 (Cambridge: Cambridge University Press).\n\nVoight, B. (1988). A method for prediction of volcanic eruptions. Nature, 332, 125–130. Voight, B. and Cornelius, R.R. (1991). Prospects for eruption prediction in near realtime. Nature, 350, 695–697.\n\nWainscoat, J.S., Hill, A.V.S., Thein, S.L., Clegg, J.J. (1986). Evolutionary relationships of human populations from an analysis of nuclear DNA polymorphisms. Nature, 319, 491 -493.\n\nWalter, L.S. (1990). The uses of satellite technology in disaster management. Disasters, 14, 20–35.\n\nWoillard, G. and Mook, W.G. (1982). Carbon-14 dates at Grande Pile: Correlation of land and sea chronologies. Science, 215, 159–161.\n\nWood, C.A. and Kienle, J. (eds.) (1990). Volcanoes of North America, 354 p (Cambridge: Cambridge University Press).\n\nWoods, A.W.M. and Wohletz, K.H. (1991). Dimensions and dynamics of co-ignimbrite eruption columns. Nature, 350, 225–227.\n\nZielinski, G.A., Mayewski, P.A., Meeker, L.D., Whitlow, S., Twickler, M.S., and Taylor, K., (1996a). Potential atmospheric impact of the Toba mega-eruption ~71,000 years ago. Geophys. Res. Lett., 23, 837–840.\n\nZielinski, G.A., Mayewski, P.A., Meeker, L.D., Whitlow, S., and Twickler, M.S. (1996b). An 110, 000-year record of explosive volcanism from the GISP2 (Greenland) ice core. Quat. Res., 45, 109–118.\n\n• 11 • Hazards from comets and asteroids\n\nWilliam Napier\n\nThere are risks everywhere. Even heaven is a stage of risk.\n\n                            Wilson Harris, Carnival (1985)\n\n11.1 Something like a huge mountain\n\nThe first angel sounded his trumpet, and there came hail and fire mixed with blood, and it was hurled down upon the earth. A third of the earth was burned up, a third of the trees were burned up, and all the green grass was burned up. The second angel sounded his trumpet, and something like a huge mountain, all ablaze, was thrown into the sea … The third angel sounded his trumpet, and a great star, blazing like a torch, fell from the sky on a third of the rivers … a third of the sun was struck, a third of the moon, and a third of the stars, so that a third of them turned dark … and I saw a star that had fallen from the sky to the earth. The star was given the key to the shaft of the Abyss. When he opened the Abyss, smoke rose from it like the smoke from a gigantic furnace. The sun and sky were darkened by the smoke from the Abyss ….\n\nThe Revelation of St John was probably written around 100 AD, but is part of a very much older ‘Star Wars’ literature, going back to the very earliest writings and probably based on pre-literate oral traditions. Common threads in these tales are often hot blast, hurricane winds, flattened forests, tsunami and cataclysmic floods, associated with blazing thunderbolts from the sky, a darkened sun, a great, red-tailed comet and what appears to be a meteor storm. Even without benefit of the twentieth century Tunguska impact, which destroyed 2000 square kilometres of Siberian forest in 1908, classical scholars have long regarded the stories as descriptions of a cosmic impact. Myth was a vehicle for transmitting astronomical and cosmological information through the generations, and it is surely a seductive proposition to see these tales of celestial catastrophe – which are found worldwide – as prehistoric descriptions of cosmic cataclysm, one-off or recurrent, local or global. Inevitably, this is a contentious area – only qualitative statements can be made, and one individual’s unifying hypothesis is another’s Velikovskian fantasy.\n\nA great earthquake or tsunami may take 100,000 lives; a great impact could take 1000 or 10,000 times as many, and bring civilization to an abrupt halt. In assessing the hazard posed by the stray celestial bodies in our celestial environment, we want to get it right! To do this we must leave the realm of myth and enter that of science. Three quantitative lines of attack are available – impact crater studies, telescopic searches, and dynamical analysis. Each throws light on different aspects of the problem.\n\n11.2 How often are we struck?\n\n11.2.1 Impact craters\n\nThe number of impact craters known on the surface of the Earth has increased substantially over the last few decades. Along with this, improved dating techniques have allowed the ages of many of them to be quite well determined. By the end of 2004, about 170 terrestrial impact structures were known. Of these, only 40 are useful for statistical purposes: they have diameters over 3 km, are less than 250 million years old, and have been dated with precision better than 10 million years. Most of the 40 are dated to precision better than 5 million years, and about half to better than one million years. This is a small database, but it is just enough to search for trends, periodicities, impact episodes, and the like.\n\nTheir age distribution is shown in Fig. 11.1. Structure appears on several levels. First, the record gives the appearance that, going into the increasingly remote past, there has been a steep decline in the impact cratering rate. This can only be an illusion, due to the cumulative overlaying of impact craters by sediments, or their erosion by the elements. Even if all craters say less than 10 million years old have been found – an unlikely assumption – the figure reveals that only 40% of impact craters formed 100 million years ago have been discovered, and only about 10% of those formed 200 million years ago.\n\nA closer examination of Fig. 11.1 gives the impression that some craters are bunched in time, and statistical scrutiny bears this out (Napier, 2006). The largest craters all occur within these bombardment episodes: the mean diameter of craters inside an episode is about 50 km, that of craters outside is approximately 20 km. The history of bombardment, then, is not one of random arrivals. Rather, it seems to have the character of ‘impact epochs’, during which the Earth is heavily bombarded, interspersed with relatively quiescent periods.\n\nA third feature of the cratering record is not obvious to the eye, but can be teased out with detailed analysis: there is a weak periodicity. Strong random surges are also present which make it impossible to determine which of several possible periodic solutions is real, and which are harmonics (Fig. 11.1). The best we can do is say that over the last 250 Myr periodic cratering episodes have recurred at intervals of 24, 30, 36, or 42 million years, interspersed with a handful of random episodes of comparable strength. The peak-to-trough ratios are uncertain, but may be in the range 2:1 to 5:1.\n\nIt follows that crater counts on ancient surfaces are of limited use in inferring the contemporary impact rate. The lunar cratering record in particular has been used to infer that an impact capable of yielding a 100 km diameter terrestrial crater (of species-destroying energy) happens once per 27 million years (Neukum and Ivanov, 1994). However the Moon’s surface is ancient and fluctuations in impact rates on it cannot be determined with resolving power much better than a billion years. Figure 11.1 implies that the current impact rate may be several times higher than estimates based on the lunar record, and there are indications that coherent structure exists at even higher temporal resolution (Steel et al., 1990).\n\n[Image]\n\nFig. 11.1 The age distribution of 40 impact craters 3 km or more in diameter, with ages less than 250 Myr, known to precision better than 10 Myr. A rectangular window of width 8 Myr corresponding roughly to the mean uncertainty in ages has been passed over the data, and the smooth curve is a fit to the overall trend. The impression that impacts occur in discrete episodes of bombardment and is confirmed by detailed statistical analysis (Napier 2005).\n\nOne conceivable source of bombardment episodes is the collisional breakup of a large asteroid in the main asteroid belt, followed by the feeding of its debris into Earth-crossing orbits. This process may well cause the Earth’s cratering rate to fluctuate by about an order of magnitude over 0.1-1 million years timescales, in the case of fragments about a kilometre across (Menichella et al., 1996). However the largest craters require the disruption of correspondingly large asteroids, and these break up too infrequently by a factor of 10–100 to supply the extinction-level surges of bombardment (Napier, 2006).\n\nProbably, the surges are mostly due to disturbances of the Oort comet cloud. This is a major reservoir of about 100 billion long-period comets, comprising a roughly spherical swarm orbiting the sun out to 50,000 astronomical units, a quarter of the way to the nearest star. The influx of long-period comets from the Oort cloud is mainly due to the perturbing action of the Galactic tide on their orbits. This tide varies cyclically due to the out-of-plane oscillations of the Sun as it orbits the Galaxy, and a periodicity in the long-period comet flux in the range 35–42 million years is predicted (Clube and Napier, 1996; Matese et al., 1995). The sun passed through the plane of the Galaxy 3 or 4 million years ago, and since the infall time of a long-period comet is 2 or 3 million years, we should be close to the peak of a bombardment episode now. There is thus a pleasing consistency with the cratering data (Fig.11.1). On this evidence the largest impactors at least are more likely to be comets than asteroids, and their current impact rate is likely to be higher than the long-term average.\n\nCrudely speaking, we can think of a cosmic impactor as generating a crater about 20 times its own size. Given the cosmic velocities involved-approximately 20 km per second for an asteroid impact and 55 km per second for a comet – the energies involved in creating the biggest terrestrial craters are characteristically equivalent to the explosion of 100 million megatons TNT, or about 10 atomic bombs of Hiroshima size on each square kilometre of the Earth’s surface (McCrea, 1981). This enormous energy is released in a fraction of a second and spreads around the globe on a timescale of an hour or so.\n\nEstimates based on the mean impact cratering rate indicate that, on the long-term, a 1 km impactor might be expected every half a million years or so. Again, modelling uncertainties to do with both excavation mechanics and the erratic replenishment of the near-Earth object (NEO) population yield an overall uncertainty of a factor of a few. A rate of one such impact every 100,000 years cannot be excluded by the cratering evidence.\n\nOn the sub-kilometre bolide scale the impact cratering record does not strongly constrain the current rate. The Earth’s atmosphere acts as a barrier, tending to break up bodies much less than 100 or 200 m across. On airless bodies such as the Moon or the Galilean satellites the situation is unclear since the small crater population appears to be dominated by secondary impacts, arising when hunks of native terrain are thrown out by large impactors (Bierhaus et al., 2005). On this scale – important for assessing the tsunami hazard – we must either extrapolate from the size distribution of larger craters or turn to telescopic surveys.\n\n11.2.2 Near-Earth object searches\n\nUntil the 1970s, only a handful of Earth-crossers were known. The subject was of little interest to most astronomers, whose telescopes were (and still are) mainly directed towards stellar, galactic and extragalactic realms. However following the pioneering work of Helin and Shoemaker (1979), who searched for Earth-crossing bodies using a small, wide-angled telescope on Mount Palomar, it became clear that there is indeed a significant impact hazard out there. This was becoming clear, too, from the increasing number of terrestrial impact craters being discovered. Search programmes got underway in the early 1990s and small bodies in hazardous, Earth-crossing orbits (NEOs) began to be found in serious numbers.\n\nThe rate of discovery of Earth-crossers has been impressive, going from 350 in 1995 to 3400 a decade later – of which about 800 are thought to be a kilometre or more in diameter. Most of these small bodies have orbital periods of a few years. It is generally thought that the total population of near-Earth asteroids over a kilometre across is about 1100. If correct, this leads to an expected impact frequency of about one such body every 500,000 years. There is thought to be a sharp threshold between regional and global effects around this size range: civilization ends with an impactor bigger than a kilometre or two! But there is a caveat: extremely dark objects would go undiscovered and not be entered in the inventory of global hazards. The population of sub-kilometre bodies is almost entirely unexplored; but it is this population which may give damaging tsunamis and possible short-lived climatic coolings on timescales of historical interest.\n\n11.2.3 Dynamical analysis\n\nThe known population of Earth-crossing objects is transient, with a median lifetime of only about 2 million years. Without replenishment, it would rapidly vanish, most NEOs falling into the sun. This short-lived population must therefore be supplied from other sources. Both asteroidal and cometary reservoirs are available, and the supply from both is likely to be erratic.\n\nA comet is a conglomerate of ice and dust, which, on approaching the sun to within about the orbit of Mars, may grow one or more tails, that can be tens or hundreds of millions of kilometres long. It will die when its volatiles are exhausted. There are several documented cases of comets whose activity has died, leaving a dark, inert body of asteroidal appearance. It is plausible to think that an accumulation of dust on the surface eventually chokes off or insulates underlying ice from solar heating. Equally, numerous comets have been seen to split and a few have disintegrated altogether.\n\nA typical orbital period of a comet arriving from the Oort cloud is a few million years. Recent years have seen the discovery of other major cometary reservoirs on the fringes of the planetary system and these probably help to replenish the Earth-crossing comets. In their active form, long-period comets may amount to only about a percent of the total impact hazard. About one in a hundred, however, are perturbed by the giant planets into Halley-type orbits (high-eccentricity orbits with periods of less than 200 years), whence a single such comet has typically a thousand opportunities to strike the Earth before it falls into the Sun or is ejected from the solar system. This ought to make them a substantial risk, and one, furthermore, difficult to handle because of the high speeds and short warning times involved. There is, however, a paradox: we don’t see them! Knowing the rate at which bright comets arrive from the Oort cloud, and the fraction which are captured into the Halley system, it turns out that there should be about 3000 active comets over 5 km or so across in such orbits. And yet only a couple of dozen are observed.\n\nIt could be that, after their first passage or two through the inner planetary system, active comets simply become dormant, fading into dark, asteroidlike bodies (Emel’yanenko and Bailey, 1998). The hazard posed by these unseen, dormant bodies would be comparable with that of the observed near-Earth asteroids, in line with other investigators who have likewise concluded that active and dormant comets together ‘yield a large, perhaps dominant, contribution to kilometre-sized terrestrial impactors’ (Rickman et al., 2001; see also Nurmi et al., 2001). The problem is that, even although very dark, about 400 such bodies – in highly eccentric orbits, with orbital periods up to 200 years – should by now have been discovered, whereas only about 25 are currently known, forming a system known as Damocloids. This is assuming reflectivities 0.04 comparable with the surfaces of the known dormant comets.\n\nAnother possibility is that comets thrown into Halley-type orbits disintegrate altogether, turning completely to dust after one or two perihelion passages (Levison et al., 2002). The hypothesis was adopted by NASA’s Near-Earth Object Science Definition Team (Stokes et al., 2003) and is the basis of their claim that comets, active and dormant together, constitute no more than 1% of the impact hazard. However, it turns out that this hypothesis also has problems (Napier et al., 2004; Rickman; 2005). For example, for the process to work, almost 99% of incoming Halley-type comets would have to disintegrate in this way. But such complete and rapid disintegration does not seem to be the normal fate of comets: nearly all of the strongest annual meteor showers have, orbiting within them, either an active comet or a large dormant body, presumably a defunct parent (Table 11.1).\n\nOne may question the adopted reflectivity of 0.04 for dormant comets, based on the commonly observed surface properties of active comets. A flyby of Comet Borrelly revealed the presence of spots with reflectivities 0.008 (Nelson et al., 2004): if all comet surfaces darken to this extent as they become inactive, then the paradox is solved. Standard radiation theory reveals that a comet which becomes completely inactive, losing its icy interstitial volatiles and leaving a ‘bird’s nest’ structure of organic grains on its surface, may indeed develop a vanishingly small reflectivity (Napier et al., 2004). But if that is the resolution of the fading paradox, then there is a significant population of highspeed hazards, undiscoverable because they are too dark and spend about 99% of their time beyond the orbit of Mars.\n\nTable 11.1 Major Annual Meteor Streams\n\n[Image]\n\nAbout 25 Damocloids are known at the time of writing. Their average radius is 8 km, which would yield an impact of 60 million megatons, with a mean impact speed of 58 km per second. The reflectivities of six Damocloids have been measured to date, and they are amongst the darkest known objects in the solar system. In general, the more comet-like the orbit of an asteroid, the darker its surface is found to be (Fernandez et al., 2005).\n\nWhether small, dark Damocloids, for example, of 1 km diameter exist in abundance is unknown – they are in essence undiscoverable with current search programmes. The magnitude of the hazard they present is likewise unknown; it could be negligible, it could more than double the risk assessments based on the objects we see. Crater counts are again of little help, since even the youngest surfaces – such as that of the icy satellite Europa which orbits Jupiter – are older than the probable duration of a cometary bombardment episode and do not strongly constrain contemporary impact rates. The best chance for discovery of such bodies would be through their thermal radiation around perihelion, using infrared instrumentation on the ground (Rivkin et al., 2005) or in satellites.\n\nFor a threat object discovered in a short period, Earth-crossing orbit, decades or centuries of advance warning will probably be available. For a comet, the warning time is measured in months. In the case of a dark Damocloid, there will generally be no warning at all.\n\n11.3 The effects of impact\n\nThe Tunguska impact of 30 June 1908, in the central Siberian plateau, was an airburst with an energy approximately 10 to 30 megatons, that of a very large hydrogen bomb. It destroyed approximately 2000 km² of forest, knocking trees over and charring the barks of trees on one side. Such impacts are local in effect (unless, perhaps, mistaken for a hydrogen bomb explosion in time of crisis). Estimates of their recurrence time range from 200 to about 2000 years.\n\nAt 10,000 megatons – comparable to the energy unleashed in a full-scale nuclear war – the area of devastation approaches 100,000 km² (Table 11.2). Flying shards of glass in urban areas would cause substantial injury far beyond this area. The rising fireball from such an impact could cause serious burns and extensive conflagration along its line of sight, while earthquakes at the extreme end of human experience occur within a few 100 km of the impact site. There is uncertainty, too, about the recurrence time of impacts in this energy range, estimates ranging from typically 10,000 years to an order of magnitude more.\n\nTable 11.2 Possible Impact Effects\n\n[Image]\n\nThere is some disagreement about the likely effects of an ocean impact in this energy range. A large earthquake-generated tsunami will carry an energy of perhaps 5 megatons, and even an inefficient coupling of say a 10,000 megaton impact to wave energy clearly has the potential to cause an immensely damaging tsunami. However, the huge waves generated in the water crater may be so steep that they break up in the open ocean. If, as suggested by some analyses, they generate tsunamis a few metres high on reaching land, then tsunami damage around ocean rims is likely the greatest single hazard of these small, relatively common impacts. A 10,000-megaton Pacific impact would on the more pessimistic analyses lead to waves 4–7 m high all around the rim, presumably with the loss of millions of lives (over 100 million people live within 20 m of sea level and 2 km from the ocean). In other studies, the wave energy dissipates relatively harmlessly before it reaches distant shores. If the pessimistic studies are correct, ocean impacts may peak, in terms of loss of life, at about this level, representing a trade-off between frequency of impact and extent of inundation of coastal lands from the resulting tsunami. The giant wave of the Hollywood movie is thus less to be feared than the more frequent few-metre wave which runs inland for a few kilometres. In terms of species extinction, however, these small impacts are not a problem. Impacts approaching end-times ferocity probably begin at a million or 2 megatons TNT equivalent (Chapman and Morrison, 1994), corresponding to the impact of bodies a kilometre or so across. Blast and earthquake devastation are now at least continental in scale. While direct radiation from the rising fireball, peaking at 100 km altitude, is limited to a range of 1000 km by the curvature of the Earth, ballistic energy would throw hot ash to the top of the atmosphere, whence it would spread globally. Sunlight would be cut off, and food chains would collapse. The settling time of fine dust is measured in years, and commercial agriculture could not be sustained (Engvild, 2003). Lacking sunlight, continental temperatures would plummet, and heat would flow from the warmer oceans onto the cooled land masses, resulting in violent, freezing winds blowing from sea to land as long as the imbalance persisted. At these higher energies, an ocean impact yields water waves whose dimensions are comparable with the span of underwater earthquakes, and so the transport of the wave energy over global distances seems more assured, as does the hydraulic bore which could create a deep and catastrophic inundation of land.\n\nFrom 10 million megatons upwards, we may be approaching the mass extinctions of species from a cocktail of prompt and prolonged effects. A land impact of this order could conceivably exterminate humanity and would surely leave signatures in the evolutionary record for future intelligent species to detect. Regionally, the local atmosphere might simply be blown into space. A rain of perhaps 10 million boulders, metre sized and upwards, would be expected over at least continental dimensions if analogy with the Martian impact crater distribution holds (McEwen et al., 2005). Major global effects include wildfires through the incinerating effect of dust thrown around the Earth; poisoning of the atmosphere and ocean by dioxins, acid rain, sulphates and heavy metals; global warming due to water and carbon dioxide injections; followed some years later by global cooling through drastically reduced insolation, all of this happening in pitch black. The dust settling process might last a year to a decade with catastrophic effects on the land and sea food chains (Alvarez et al., 1980; Napier and Clube 1979; Toon et al. 1990). At these extreme energies, multiple bombardments may be involved over several hundred thousand years or more, and in addition prolonged trauma are likely due to dustings from large, disintegrating comets. However this aspect of the hazard is less well understood and the timescales involved are more of geological than societal concern.\n\n11.4 The role of dust\n\nCollisions between asteroids in the main belt may on occasion produce an upsurge of dust on to the Earth (Parkin, 1985), and it has been suggested that climatic and biological effects would follow (Kortenkamp and Dermott, 1998).\n\nThere is evidence from seafloor sediments that dust showers of duration about 1.5 million years occurred about 8 and 36 million years ago (Farley et al., 2006). The former event is coincident in time with the known break-up of a large asteroid in the main belt, but the enhancement of dust was modest. The provenance of the 36 Myr dust shower is uncertain: no asteroid breakup capable of yielding this bombardment episode has been identified in the main\n\nbelt.\n\nBrief episodes (millennia rather than megayears) of cosmic dusting must occur and at some level play a role in modifying the terrestrial climate. The most massive objects to enter the near-Earth environment are rare, giant comets, 100–200 km across, at characteristic intervals of 100,000 years during a bombardment episode. Thrown into a short period, Earth-crossing orbit, such a body will disintegrate under the influence of sunlight and may generate a mass of dust equal that from the simultaneous disintegration of 10,000 Halley comets. Its emplacement could increase the annual flux of cometary dust into the Earth’s atmosphere from its present 40,000 tonnes per annum to a million or more tonnes per annum, over its active lifetime of a few millennia. Meteoroids swept up by the atmosphere will ablate to smoke in the form of micron-sized particles (Klekociuk et al., 2005), which are efficient scatterers of sunlight, and whose settling time is characteristically 3–10 years. The atmospheric disintegration of incoming meteoroids into micron-sized aerosols will yield a significant reduction of sunlight reaching the surface of the Earth. Climatic effects would seem unavoidable (Clube et al 1996; Hoyle and Wickramasinghe, 1978; Napier, 2001). Thus mass extinction need not be a single, large impact: multiple bombardments and a series of sharp cooling episodes also provide a reasonable astronomical framework.\n\nThe fossil remains of a past large comet can still be discerned in the inner interplanetary environment: it has long been recognised that the progenitor of Comet Encke, and the associated Taurid meteor stream, was such a body (Whipple, 1967). From reconstruction of the initial orbits (Steel and Asher, 1998) it appears that the original comet must have been at least 10 km in diameter, and may have come to the end of its principal activity some 10,000 years ago, with continuing significant activity until about 5000 years BP. With an orbital period of 3.3 ± 0.2 years, this large, disintegrating comet must have been a brilliant object in the Neolithic night sky. Orbital precession would ensure that, at intervals of about 2500 years, the Earth would be subjected to close encounters and annual meteor storms of an intensity out with modern experience. There may after all be a scientific underpinning to the myths of celestial apocalypse! This Taurid progenitor may have been the offshoot of a much larger body: the zodiacal cloud, a disc of interplanetary dust particles which includes the Taurids, is about two orders of magnitude too massive in relation to current replenishing sources (Hughes, 1996). A recent injection of mass amounting to at least 2000 times the present mass of Comet Halley is implied, corresponding to a comet 150 km in diameter (Hughes, 1996).\n\nThe question arises whether material within this complex still constitutes a global risk significantly above background. That there are still concentrations of material within it is established by occasional fireball swarms, by mediaeval records and by seismically detected impacts on the Moon (Table 11.1). Over the period 1971–1975 during which lunar seismometers operated, strong boulder swarms were detected, of a few days duration, coincident with the maxima of the daytime β Taurids, the Perseids, the Leonids and the Geminids. But (see Table 11.1) also close to the peaks of these annual showers were the 1908 Tunguska impact (30 June), a 1930 fall in the megaton range in the Amazon forest (13 August) and a similarly energetic impact in British Guiana in 1935 (11 December 1935). It seems to be stretching coincidence too far to assume that these are chance juxtapositions. Whether meteoroids in say the 1000-megaton range might be concentrated within meteor showers in significant numbers is as yet an unanswered question.\n\nThe existence of this material has led to the suggestion that rapid, short lived, otherwise unexplained coolings of the Earth, taking place at characteristic intervals of a few millennia, might be due to interactions with sub-kilometre meteoroids within the Taurid complex. The most recent of these occurred over 536–545 AD. Tree ring data point to a sudden climatic change, apparently of global reach, accompanied by a dry fog of duration 12–18 months. No chemical signature diagnostic of a volcano is recorded in ice cores over this period. This event – and a similar one at 2345 BC – suggested to Baillie (1994, 1999) that a comet impact was involved. This was modelled by Rigby et al. (2004) as an airburst, wherein vapourized comet material is ejected as a plume and falls back on to the top of the atmosphere as small, condensed particles. They found that a comet of radius of 300 m could reduce sunlight by about 4% which, they considered, might account for both the historical descriptions and the sudden global cooling. The effects of such a cosmic winter can be summarized as crop failures followed by famine. This would be most immediate in the third world, but a cooling event lasting more than a year would also hit the developed countries (Engvild, 2003).\n\n11.5 Ground truth?\n\nThere is as yet no clear consensus on the relative importance of dark, dormant comets versus stony asteroids in the overall impact rate. Neither, lacking full exploration of the sub-kilometre regime, can one be confident about the total contemporary hazard presented by bodies in this range. Attempts have been made to correct for the discovery bias against very dark objects, but almost by definition such corrections are most uncertain where they are most needed.\n\nSuch models have been used to suggest that impacts of10 megatons upwards occur on Earth every 2000–3000 years (Stuart and Binzel, 2004), and yet the Tunguska impact took place only 100 years ago. Likewise an impact of energy 1000 megatons or more was predicted to occur every 60,000 years (Morbidelli et al., 2002), and yet within 2 years a 1000 megaton asteroid was discovered which will pass within six Earth radii in 2029. In the same vein active comets more than 7 km across are sometimes estimated to hit the Earth once every 3 billion years, and yet a low-activity comet of this size, IRAS-Araki-Alcock, passed within 750 Earth radii in 1983, consistent with an impact rate 200 times higher. Hughes (2003) examined the distribution of near-miss distances of known NEOs passing the Earth, carrying out this exercise for all known 2002 close encounters. He found that impacts in the Tunguska class are expected every 300 years, and in the 1000-megaton range upwards every 500–5000 years. This ‘ground truth’ is based on a small number of recent close encounters but does indicate that there is still considerable uncertainty in estimates of impact rate (Asher et al., 2005).\n\nThe ultimate ground truth is, of course, to be found in the ground. Courty et al. (2005), in a series of detailed sedimentological studies, finds evidence for a widespread spray of hot, fine impact ejecta throughout tropical Africa, the near East and West Asia, which she dates to around 2600–2300 BC and associates with an abrupt environmental change at that period. Abbott et al. (2005), in a study of West Antarctic Siple Dome ice cores, obtained a variety of data which they find to be consistent with impact ejecta from a 24 km crater, Mahuika, on the southern New Zealand shelf. These latter anomalies are dated at around 1443 AD and a large impact at so recent a date seems very unlikely, if only because its effects would be widely felt; on the other hand deposits from a 130 m high megatsunami in Jervis Bay, Australia, have been dated to 1450 ± 50 AD. These lines of work are quite new and still have to go through the full critical mill; if they continue to hold up, the case for a high current risk will in effect have been made.\n\n11.6 Uncertainties\n\nMultiplying the low probability of an impact by its large consequences, one finds that the per capita impact hazard is at the level associated with the hazards of air travel and the like. Unlike these more mundane risks, however, the impact hazard is unbounded: a big one could end civilization. The effects of cosmic dusting are much less well understood and have hardly been studied; they lack the drama of a great, incinerating impact, and are difficult to assess and handle; but it is not obvious that in terms of frequency or consequences they matter less. Although the concept of the Earth as a bombarded planet is very old, it is only in the last 20 years that it has become anything like ‘mainstream’, and there remain qualitative uncertainties and areas of disagreement. We do not know where the bulk of the mass coming into the Halley system from the Oort cloud is going and it remains possible that a large population of unstoppable dark bodies is being missed. A significant population of such ‘stealth comets’ would be difficult to map out since the bodies are in high eccentricity orbits and spend most of their time beyond Mars. There are uncertainties too about the role of the residual debris from the large, short-period comet which was active as recently as 5000 yr BP. This debris contains concentrations of material which the Earth encounters on various timescales from annual to millennial; whether they include bodies large enough to constitute, say, a significant tsunami hazard, or a short-lived global cooling, cannot be determined from the available evidence.\n\nProgress in this field has been impressively rapid since the 1970s, however, and will no doubt continue apace. There is little doubt that, especially with the mapping out of interplanetary bodies down to the sub-kilometre level, many of these uncertainties will be reduced or disappear.\n\nSuggestions for further reading\n\nBaillie, M.G.L. (1999). Exodus to Arthur (London: Batsford). A popular-level discussion of dendrochronological evidence for impacts in recent geological and historical times.\n\nClube, S.V.M. and Napier, W.M. (1990). The Cosmic Winter (Oxford: Basil Blackwell Ltd.). A popular exposition of the theory of coherent catastrophism, arguably the ‘worst case’ scenario from the point of view of this book.\n\nGehrels, T. (ed.) (1994). Hazards Due to Comets and Asteroids (Tucson: University of Arizona Press). The most general treatment of the impact hazard thus far. While a bit out of date, this comprehensive volume is still the best point-of-entry for the present chapter’s concerns.\n\nKneevic, Z. and Milani, A. (eds.) (2005). Dynamics of Populations of Planetary Systems, Proc. IAU colloq. 197 (Cambridge: Cambridge University Press). A recent overview of the dynamical side of the impactor populations.\n\nReferences\n\nAbbott, D., Biscaye, P., Cole-Dai, J., and Breger, D. (2005). Evidence from an ice core of a large impact circa 1443 AD. EOS Trans. AGU, 86, 52, Fall Meet. Suppl., Abstract PP31C-05.\n\nAlvarez, L.W., Alvarez, W., Asaro, F., and Michel, H.V. (1980). Extraterrestrial cause for the Cretaceous-Tertiary extinction. Science, 208, 1095–1108.\n\nAsher, D.J., Bailey, M.E., Emel’yanenko, V., and Napier, W.M. (2005). Earth in the cosmic shooting gallery. The Observatory, 125, 319–322.\n\nBaillie, M.G.L. (1994). Dendrochronology raises questions about the nature of the a.d. 536 dust-veil event. Holocene, 4, 212–217.\n\nBaillie, M.G.L. (1999). Exodus to Arthur (London: Batsford).\n\nBierhaus, E.B., Chapman, C.R., and Merline, W.J. (2005). Secondary craters on Europa and implications for crater surfaces. Nature, 437, 1125–1127.\n\nChapman, C. and Morrison, D. (1994). Impacts on the Earth by asteroids and Comets. Assessing the hazard. Nature, 367, 33–39.\n\nClube, S.V.M., Hoyle, F., Napier, W.M., and Wickramasinghe, N.C. (1996). Giant comets, evolution and civilisation. Astrophys. Space Sci., 245, 43–60.\n\nClube, S.V.M. and Napier, W.M. (1996). Galactic dark matter and terrestrial periodicities. Quarterly J. Royal Astron. Soc., 37, 617–642.\n\nCourty, M.-A., et al. (2005). Sequels on humans, lands and climate of the 4-kyr BP impact across the Near East. Presented at European Geosciences Union, Symposium. CL18, Vienna, Austria.\n\nEmel’yanenko, V.V. and Bailey, M.E. (1998). Capture of Halley-type comets from the near-parabolic flux. MNRAS, 298, 212–222.\n\nEngvild, K.C. (2003). A review of the risks of sudden global cooling and its effects on agriculture. Agric. Forest Meteorol., 115, 127–137.\n\nFarley, K.A., Vokrouhlicky, D., Bottke, W.F., and Nesvorny, D. (2006). A late Miocene dust shower from the break-up of an asteroid in the main belt. Nature, 439, 295–297.\n\nFernandez, Y.R., Jewitt, D.C., and Sheppard, S.S. (2005). Albedos of asteroids in Cometlike orbits. Astrophys. J., 130, 308–318.\n\nHelin, E.F. and Shoemaker, E.M. (1979). Palomar planet-crossing asteroid survey, 1973–1978. Icarus, 40, 321–328.\n\nHoyle, F. and Wickramasinghe, N.C. (1978). Comets, ice ages, and ecological catastrophes. Astrophys. Space Sci., 53, 523–526.\n\nHughes, D.W. (1996). The size, mass and evolution of the Solar System dust cloud. Quarterly J. Royal Astron Soc., 37, 593–604.\n\nHughes, D.W. (2003). The approximate ratios between the diameters of terrestrial impact craters and the causative incident asteroids. MNRAS, 338, 999–1003.\n\nKlekociuk, A.R., Brown, P.G., Pack, D.W., Revelle, D.O., Edwards, W.N., Spalding, R.E., Tagliaferri, E., Yoo, B.B., and Zagevi, J. (2005). Meteoritic dust from the atmospheric disintegration of a large meteoroid. Nature, 436, 1132–1135.\n\nKortenkamp, S.J. and Dermott, S.F.A. (1998). A 100, 000-year periodicity in the accretion rate of interplanetary dust. Science, 280, 874–876.\n\nLevison, H.F. et al. (2002). The mass disruption of Oort cloud comets. Science, 296, 2212–2215.\n\nMatese, J.J. et al. (1995). Periodic modulation of the Oort cloud comet flux by the adiabatically changing tide. Icarus, 116, 255–268.\n\nMcCrea, W.H. (1981). Long time-scale fluctuations in the evolution of the Earth. Proc. Royal Soc. London, A375, 1–41.\n\nMcEwen, A.S., Preblich, B.S., Turke, E.P., Artemieva, N.A., Golombek, M.P., Hurst, M., Kirk, R.L., Burr, D.M., and Christensen, P.R. (2005). The rayed crater Zunil and interpretations of small impact craters on Mars. Icarus, 176, 351–381.\n\nMenichella, M., Paolicci, P., and Farinella, P. (1996). The main belt as a source of near-Earth asteroids. Earth, Moon, Planets, 72, 133–149.\n\nMorbidelli, A., Jedicke, R., Bottke, W.F., Michel, P., and Tedesco, E.F. (2002). From magnitudes to diameters: the albedo distribution of near Earth objects and the Earth collision hazard. Icarus, 158, 329–342.\n\nNapier, W.M. (2001). Temporal variation of the zodiacal dust cloud. MNRAS, 321, 463.\n\nNapier, W.M. (2006). Evidence for cometary bombardment episodes. MNRAS, 366(3), 977–982.\n\nNapier, W.M. and Clube, S.V.M. (1979). A theory of terrestrial catastrophism. Nature, 282, 455–459.\n\nNapier, W.M., Wickramasinghe, J.T., and Wickramasinghe, N.C. (2004). Extreme albedo comets and the impact hazard. MNRAS, 355, 191–195.\n\nNelson, R.M., Soderblom, L.A., and Hapke, B.W. (2004). Are the circular, dark features on Comet Borrelly’s surface albedo variations or pits? Icarus, 167, 37–44.\n\nNeukum, G. and Ivanov, B.A. (1994). Crater size distributions and impact probabilities on Earth from lunar, terrestrial-planet, and asteroid cratering data. In Gehrels, T. (ed.), Hazards Due to Comets and Asteroids, p. 359 (Tucson: University of Arizona Press).\n\nNurmi, P., Valtonen, M.J., and Zheng, J.Q. (2001). Periodic variation of Oort Cloud flux and cometary impacts on the Earth and Jupiter. MNRAS, 327, 1367–1376.\n\nParkin, D.W. (1985). Cosmic spherules, asteroidal collisions and the possibility of detecting changes in the solar constant. Geophys. J., 83, 683–698.\n\nRickman, H. (2005). Transport of comets to the inner solar system. In Knesevic, Z. and Milani, A. (eds.), Dynamics of Populations of Planetary Systems, Proc. IAU colloq. no. 197.\n\nRickman, H., et al. (2001). The cometary contribution to planetary impact rates. In Marov, M. and Rickman, H. (eds.), Collisional Processes in the Solar System (Kluwer).\n\nRigby, E., Symonds, M., and Ward-Thompson, D. (2004). A comet impact in AD 536? Astron. Geophys., 45, 23–26.\n\nRivkin, A.S., Binzel, R.P., and Bus, S.J. (2005). Constraining near-Earth object albedos using neo-infrared spectroscopy. Icarus, 175, 175–180.\n\nSteel, D.I., Asher, D.J., Napier, W.M., and Clube, S.V.M. (1990). Are impacts correlated in time? In Gehrels, T. (ed.), Hazards Due to Comets and Asteroids, p.463 (Tucson: University of Arizona).\n\nSteel, D.I., Asher, D.J. 1998: On the possible relation between the Tunguska bolide &comet encke, in: planetary &space science, 46, pp 205–211.\n\nStokes, G.H. et. al. (2003). Report of the Near-Earth Object Science Definition Team. NASA, http://neo.jpl.nasa.gov/neo/neoreport030825.pdf\n\nStuart, J.S. and Binzel, R.P. (2004). NEO impact risk overrated? Tunguska events once every 2000–3000 years? Icarus, 170, 295–311.\n\nToon, O.B. et al. (1990). Environmental perturbations caused by asteroid impacts. In Gehrels, T. (ed.), Hazards Due to Comets and Asteroids, p.791 (Tucson: University of Arizona).\n\nWhipple, F.L. (1967). The Zodiacal Light and the Interplanetary Medium. NASA SP-150, p. 409, Washington, DC.\n\n• 12 • Influence of Supernovae, gamma-ray bursts, solar flares, and cosmic rays on the terrestrial environment\n\nArnon Dar\n\n12.1 Introduction\n\nChanges in the solar neighbourhood due to the motion of the sun in the Galaxy, solar evolution, and Galactic stellar evolution influence the terrestrial environment and expose life on the Earth to cosmic hazards. Such cosmic hazards include impact of near-Earth objects (NEOs), global climatic changes due to variations in solar activity and exposure of the Earth to very large fluxes of radiations and cosmic rays from Galactic supernova (SN) explosions and gamma-ray bursts (GRBs). Such cosmic hazards are of low probability, but their influence on the terrestrial environment and their catastrophic consequences, as evident from geological records, justify their detailed study, and the development of rational strategies, which may minimize their threat to life and to the survival of the human race on this planet. In this chapter I shall concentrate on threats to life from increased levels of radiation and cosmic ray (CR) flux that reach the atmosphere as a result of (1) changes in solar luminosity, (2) changes in the solar environment owing to the motion of the sun around the Galactic centre and in particular, owing to its passage through the spiral arms of the Galaxy, (3) the oscillatory displacement of the solar system perpendicular to the Galactic plane, (4) solar activity, (5) Galactic SN explosions, (6) GRBs, and (7) cosmic ray bursts (CRBs). The credibility of various cosmic threats will be tested by examining whether such events could have caused some of the major mass extinctions that took place on planet Earth and were documented relatively well in the geological records of the past 500 million years (Myr).\n\n12.2 Radiation threats\n\n12.2.1 Credible threats\n\nA credible claim of a global threat to life from a change in global irradiation must first demonstrate that the anticipated change is larger than the periodical changes in irradiation caused by the motions of the Earth, to which terrestrial life has adjusted itself. Most of the energy of the sun is radiated in the visible range. The atmosphere is highly transparent to this visible light but is very opaque to almost all other bands of the electromagnetic spectrum except radio waves, whose production by the sun is rather small. The atmosphere protects the biota at ground level from over-exposure to high fluxes of extraterrestrial gamma-rays, X-ray and UV light. Because of this atmospheric protection, life has not developed immunity to these radiations (except species that perhaps were exposed elsewhere during their evolution to different conditions, such as Deinoccocus radiodurance), but has adapted itself to the normal flux levels of radiations that penetrate the atmosphere. In particular, it has adapted itself to the ground level solar irradiance, whose latitudinal and seasonal redistribution undergoes long-term quasi-periodical changes, the so-called Milankovitch cycles, due to quasi-periodical variations in the motions and orientation of the Earth. These include variation in the Earth’s eccentricity, tilt of the Earth’s axis relative to the normal to the plane of the ecliptic and precession of the Earth’s axis. Milutin Milankovitch, the Serbian astronomer, is generally credited with calculating their magnitude and the times of increased or decreased solar radiation, which directly influence the Earth’s climate system, thus impacting the advance and retreat of the Earth’s glaciers. The climate change, and subsequent periods of glaciation resulting from these variables is not due to the total amount of solar energy reaching Earth. The three Milankovitch Cycles impact the seasonality and location of solar energy around the Earth, thus affecting contrasts between the seasons. These are important only because the Earth has an asymmetric distribution of land masses, with virtually all (except Antarctica) located in/near the Northern Hemisphere.\n\n[Image]\n\nFig. 12.1 The intense solar flare of 4 November 2003. A giant sun spot region lashed out with an intense solar flare followed by a large coronal mass ejection (CME) on 4 November 2003. The flare itself is seen here at the lower right in an extreme ultraviolet image from the sun-staring SOHO spacecraft’s camera. This giant flare was among the most powerfulever recorded since the 1970s, the third such historic blast from AR10486 within two weeks. The energetic particle radiation from the flare did cause substantial radio interference.\n\nCredit: SOHO-EIT Consortium, ESA, NASA\n\nEven when all of the orbital parameters favour glaciation, the increase in winter snowfall and decrease in summer melt would barely suffice to trigger glaciation. Snow and ice have a much larger albedo (i.e., the ratio of reflected to incident electromagnetic radiation) than ground and vegetation (if the Earth was covered in ice like a giant snowball, its albedo would be approximately 0.84). Snow cover and ice masses tend to reflect more radiation back into space, thus cooling the climate and allowing glaciers to expand. Likewise supernovae (SNe), GRBs, solar flares, and cosmic rays had large influence on the terrestrial environment.\n\n[Image]\n\nFig. 12.2 Comet Shoemaker-Levy 9 Collision with Jupiter. From 16 through 22 July 1994, pieces of the Comet Shoemaker-Levy 9 collided with Jupiter. The comet consisted of at least 21 discernable fragments with diameters estimated at up to 2 kilometres. The four frames show the impact of the first of the 20 odd fragments of Comet Shoemaker-Levy 9 into Jupiter. The upper left frame shows Jupiter just before impact. The bright object to the right is its closest satellite Io, and the fainter oval structure in the southern hemisphere is the Great Red Spot. The polar caps appear bright at the wavelength of the observations, 2.3 μm, which was selected to maximize contrast between the fireball and the jovian atmosphere. In the second frame, the fireball appears above the southeast (lower left) limb of the planet. The fireball flared to maximum brightness within a few minutes, at which time its flux surpassed that of 10. The final frame shows Jupiter approximately 20 minutes later when the impact zone had faded somewhat. Credit: Dr. David R. Williams, NASA Goddard Space Flight Center.\n\n[Image]\n\nFig. 12.3 The supernova remnant Cassiopeia A. Cas A is the 300-year-old remnant created by the SN explosion of a massive star. Each Great Observatory image highlights different characteristics of the remnant. Spitzer Space Telescope reveals warm dust in the outer shell with temperatures of about 10° C(50°F), and Hubble Space Telescope sees the delicate filamentary structures of warmer gases about 10,000°C. Chandra X-ray observatory shows hot gases at about 10 million degrees Celsius. This hot gas was created when ejected material from the SN smashed into surrounding gas and dust at speeds of about 10 million miles per hour.\n\nCredit: NASA/CXC/MIT/UMass Amherst/M.D.Stage et al.\n\nThe 1912 Milankovitch theory of glaciation cycles is widely accepted since paleoclimatic archives contain strong spectral components that match the Milankovitch cycles. However, it was recently argued that high precision paleoclimatic data have revealed serious discrepancies with the Milankovitch model that fundamentally challenge its validity and reopen the question of what causes the glacial cycles. For instance, Kirkby et al. (2004) proposed that the ice ages are initially driven not by insolation cycles but by cosmic ray changes, probably through their effect on clouds. Even if the cause of the glacial cycles is still debated, changes in global irradiation of astronomical origin must be larger than the orbital modulation of the solar irradiation in order to pose a credible threat to terrestrial life.\n\n[Image]\n\nFig. 12.4 The after glow of the gamma-ray burst (GRB) 030329: Images of the fading optical after glow of the GRB 030329 that took place on 29 March 2003 taken by the very large telescope (VLT) of the European Southern Observatory (ESO) in Chile on 3 April 2003 and 1 May 2003. The image taken on 1 May is dominated by an underlying supernova that produced the GRB. The discovery of the underlying supernova SN203dh convinced the majority of the astrophysicists community that ‘long-duration’ GRBs are produced by highly relativistic jets as long advocated by the Cannon ball model of GRBs. The underlying supernova was first discovered spectroscopically in the fading after glow of GRB 0302329 10 days after the GRB took place, as predicted by Dado et al. (2003) from their study of the early after glow of GRB 030329.\n\nCredit: European Southern Observatory.\n\n12.2.2 Solar flares\n\nSolar flares are the most energetic explosions in the solar system. They occur in the solar atmosphere. The first solar flare recorded in astronomical literature, by the British astronomer Richard C. Carrington, occurred on 1 September 1859. Solar flares lead to the emission of electromagnetic radiation, energetic electrons, protons and atomic nuclei (solar cosmic rays) and a magnetized plasma from a localized region on the sun. A solar flare occurs when magnetic energy that has built up in the solar atmosphere is suddenly released. The emitted electromagnetic radiation is spread across the entire electromagnetic spectrum, from radio waves at the long wavelength end, through optical emission to X-rays and gamma-rays at the short wavelength end. The energies of solar cosmic rays reach a few giga electron volts = 10⁹ev [1 ev = 1 6021753(14).10⁻¹³ J]. The frequency of solar flares varies, from several per day when the sun is particularly active to less than one per week when the sun is quiet. Solar flares may take several hours or even days to build up, but the actual flare takes only a matter of minutes to release its energy.\n\nThe total energy released during a flare is typically of the order 10²⁷ erg s⁻¹. Large flares can emit up to 10³² erg. This energy is less than one-tenth of the total energy emitted by the sun every second (I. = 3.84 × 10³³ erg s⁻¹). In the unlikely event that all the magnetic field energy in the solar atmosphere is radiated in a single solar flare, the solar flare energy cannot exceed ~ B²R³/12 ~ 1.4 × 10³³ erg where B ~ 50 Gauss is the strength of the sun’s dipole surface magnetic field and R. MSS for symbol.] = 7x 10¹⁰ cm is the solar radius. Even this energy is only approximately one-third of the total energy emitted by the sun every second. Thus, individual solar flares are not energetic enough to cause global catastrophes on planet Earth. However, solar flares and associated coronal mass ejections strongly influence our local space weather. They produce streams of highly energetic particles in the solar wind and the Earth’s magnetosphere that can present radiation hazards to spacecraft and astronauts. The soft X-ray flux from solar flares increases the ionization of the upper atmosphere, which can interfere with short-wave radio communication, and can increase the drag on low orbiting satellites, leading to orbital decay. Cosmic rays that pass through living bodies do biochemical damage. The large number of solar cosmic rays and the magnetic storms that are produced by large solar flares are hazardous to unprotected astronauts in interplanetary space. The Earth’s atmosphere and magnetosphere protect people on the ground.\n\n12.2.3 Solar activity and global warming\n\nGlobal temperature has increased over the twentieth century by approximately 0.75°C relative to the period 1860–1900. Land and sea measurements independently show much the same warming since 1860. During this period, the concentration of CO₂ in the Earth’s atmosphere has increased by approximately 27% from 290 to 370 parts per million (ppm). This level is considerably higher than at any time during the last 800,000 years, the period for which reliable data has been extracted from ice cores. This increase in the CO₂ concentration in the atmosphere is widely believed to be anthropogenic in origin, that is, derived from human activities, mainly fossil fuel burning and deforestation. Recently, the Intergovernmental Panel on Climate Change (IPCC) concluded that ‘most of the observed increase in globally averaged temperatures since the mid-twentieth century is very likely due to the observed increase in anthropogenic greenhouse gas concentrations’ via the greenhouse effect (the process in which the emission of infrared radiation by the atmosphere warms a planet’s surface, such as that of Earth, Mars and especially Venus, which was discovered by Joseph Fourier in 1829). Relying on climate models, scientists project that global surface temperatures are likely to increase by 1.1-6.4°C until 2100. An increase in global temperatures is expected to cause other changes, including sea level rise, increased intensity of extreme weather events, and changes in the amount and pattern of precipitation. Other effects of global warming include changes in agricultural yields, glacier retreat, species extinctions and increases in the ranges of disease vectors.\n\nHowever, although scientists essentially agree that mankind should drastically reduce the emission of greenhouse gases and other pollutants, there are scientists who disagree with the IPCC conclusion that there is substantial evidence to prove that the increase of CO₂ concentration in the atmosphere from anthropogenic sources and of other greenhouse gases are the primary cause for global warming. They point out that half of the increase in global temperature over the twentieth century took place in the beginning of the century, long before the bulk of the human influence took place. Moreover, the Earth has experienced pre-human large warming and cooling many times in the past as inferred from geological records and global temperature proxies (variables used to infer past global temperature), such as the concentration of heavy water molecules (D₂O and H₂¹⁸O) in ice cores: The relative rate of evaporation of these molecules from seawater compared to light water molecules (H₂O), increases with temperature. This increases the concentration of the heavy water molecules in precipitation, which solidify into ice over the north and south poles of the Earth. In particular, the ice cores of the Vostok and Epicaant arctic sites, which date back by 740,000 years, reveal eight previous glacial cycles with strong variation in temperature, up to a decrease by -8°C (relative to the present temperature) during the coldest ice ages and an increase by +3°C during the warmest periods. The change in the concentration of atmospheric CO₂ has followed closely the change in global temperature. Supporters of the anthropogenic origin of global warming argue that dramatic increase in greenhouse gases from natural sources was responsible for past global warming, while others suggest that the release of large quantities of CO₂ by the oceans was caused by the increase in their temperature by global warming. Unfortunately, so far no precise data have been presented that allow to determine which event preceded the other, the increase in atmospheric CO₂ or the global warming.\n\nGlobal warming remains an active field of research, although the scientific consensus is that greenhouse gases produced by human activity are responsible for it. However, a consensus is not a proper substitute for a scientific proof. Other hypotheses have been suggested to explain the observed increase in mean global temperature and should be examined scientifically. Perhaps, the one that looks most credible (to the author) is the hypothesis that the current global warming is largely the result of the reduction in the flux of cosmic rays that reach the atmosphere by an increased solar activity (e.g., Shaviv, 2005; Svens mark, 1998). This possibility is discussed shortly in Section 12.3.\n\n12.2.4 Solar extinction\n\nThe sun is about 4.5 billion years old. It will continue to shine for another 5 billion years. But, it will start to run out of hydrogen fuel in its core in less than 2 billion years from now. Then its core will contract and become hot enough for helium fusion to occur in it and hydrogen fusion in a shell around its growing helium core. Owing to the growing radiation pressure in the burning shell, the sun will begin expanding into a red giant. This fast phase will last less than 10 million years. When the sun becomes a red giant, Mercury and Venus will be swallowed up by the sun and perhaps the Earth will be too. Even if the Earth will not be swallowed up, conditions on its surface will make it impossible for life to exist. The sun’s increased luminosity will heat the Earth’s surface so much that the water of oceans and atmosphere will evaporate away. In fact, in only 1 or 2 billion years, prior to the red giant phase, the energy output of the sun will increase to a point where the Earth will probably become too hot to support life.\n\n12.2.5 Radiation from supernova explosions\n\nThe most violent events likely to have occurred in the solar neighbourhood during geologic and biological history are SN explosions. Such explosions are the violent death of either massive stars following gravitational collapse of their core (core-collapse supernovae) or white dwarfs in binary systems whose mass increases by accretion beyond the Chandrasekhar mass limit (thermonuclear SN).\n\nCore-collapse supernova takes place when the nuclear fuel in the core of a massive star of more than eight solar masses (M > 8M) has been exhausted and can no longer produce the thermal pressure that balances the gravitational pressure of the outlying layers. Then the core collapses into a neutron star or a stellar black hole and releases a huge amount of gravitational energy (~3 × 10⁵³ erg), most of which is converted to neutrinos and only a few percent into kinetic energy of the ejected stellar envelope, which contains radio isotopes whose decay powers most of its radiation.\n\nThermonuclear supernovae involve the thermonuclear explosion of a white dwarf star in a binary star system. A white dwarf is the end point in the evolution for stars with mass less than eight solar masses (M < 8M). It is usually made of carbon or oxygen. Its mass cannot exceed 1.4 times the mass of the sun. A white dwarf in a binary star system can accrete material off its companion star if they are close to each other because of its strong gravitational pull. The in-falling matter from the companion star causes the white dwarf to cross the 1.4 solar-mass limit (a mass called the Chandrasekhar limit after its discoverer) and collapse gravitationally. The gravitational energy release increases the temperature to a level where the carbon and oxygen nuclei fuse uncontrollably. This results in a thermonuclear explosion that disrupts the entire star.\n\nIf a supernova explosion occurred sufficiently close to the Earth it could have dramatic effects on the biosphere. Potential implications of a nearby SN explosion for the Earth’s biosphere have been considered by a number of authors (Ellis and Schramm, 1995; Ellis et al., 1996; Ruderman, 1979) and later work has suggested that the most important effects might be induced by their cosmic rays. In particular, their possible role in destroying the Earth’s ozone layer and opening the biosphere to the extent of irradiation by solar UV radiation has been emphasized (Ellis and Schramm, 1995; Ellis et al., 1996). We shall first consider the direct radiation threats from SN explosions.\n\nAmong the new elements produced in core-collapse and thermonuclear SN explosions is radioactive nickel, which liberates huge amounts of energy inside the debris. Most of this energy is absorbed within the debris and radiated as visible light. However, the SN light does not constitute a high risk hazard. The brightest supernovae reach a peak luminosity of approximately 10⁴³ erg s⁻¹ within a couple of weeks after the explosion, which then declines roughly exponentially with a half-life time of 77 days (the half-life time of the radioactive cobalt that is produced by the decay of nickel). Such a luminosity at a distance of 5 parsecs from the Earth over a couple of weeks adds approximately 1% to the solar radiation that reaches the Earth and has no catastrophic consequences whatsoever. Moreover, the mean rate of Galactic SN explosions is approximately 1 in 50 years (van den Bergh and Tammann, 1991). Most of the SN explosions occur at distances from the Galactic centre much smaller than the radius of the solar orbit. Using the observed distribution of Galactic SN remnants, and the mean rate of SN explosions, the chance probability that during the next 2 billion years (before the red giant phase of the sun) the solar system in its Galactic motion will pass within 15 light years (LY) from an SN explosion is less than 10⁻².\n\nThe direct threats to life on the Earth from the UV, X-ray and gamma-ray emission from SN explosions and their remnants are even smaller because the atmosphere is opaque to these radiations. The only significant threat is from the possible stripping of the Earth’s ozone layer followed by the penetration of UV radiation and absorption of visible sunlight by NO₂ in the atmosphere. However, the threat from supernovae more distant than 30 LY is not larger than that from solar flares. The ozone layer has been frequently damaged by large solar flares and apparently has recovered in relatively short times.\n\n12.2.6 Gamma-ray bursts\n\nGamma-ray bursts are short-duration flares of MeV gamma-rays that occur in the observable universe at a mean rate of approximately 2–3 per day (e.g., Meegan and Fishman, 1995). They divide into two distinct classes. Nearly 75% are long-duration soft spectrum bursts which last more than 2 seconds; the rest are short-duration hard-spectrum bursts (SHBs) that last less than 2 seconds. There is mounting evidence from observations of the optical after-glows of long-duration GRBs that long bursts are produced by highly relativistic jets ejected during the death of massive stars in SN explosions (e.g., Dar, 2004 and references therein). The origin of SHBs is only partially known. They are not produced in SN explosion of any known type and their energy is typically three orders of magnitude smaller.\n\nThorsett (1995) was the first to discuss the potential effects on the atmosphere of the Earth and the damage to the biota from the hard X-rays and gamma-rays from a Galactic GRB pointing towards the Earth, while Dar et al. (1998) suggested that the main damage from Galactic GRBs arises from the cosmic rays accelerated by the jets that produce the GRBs (Shaviv and Dar, 1995). Whereas the fluxes of gamma-rays and X-rays from Galactic GRBs that illuminate the Earth and their frequency can be reliably estimated from GRB observations and their association with SN explosions, this is not the case for cosmic rays whose radiation must be estimated from debatable models. Consequently, although the effects of cosmic ray illumination may be much more devastating than those of the gamma-rays and X-rays from the same event, other authors (e.g., Galante and Horvath, 2005; Melott et al., 2004; Scalo and Wheeler, 2002; Smith et al., 2004; Thomas et al., 2005) preferred to concentrate mainly on the effects of gamma-ray and X-ray illumination.\n\nThe Galactic distribution of SN explosions is known from the distribution of their remnants. Most of these SN explosions take place in the Galactic disk at Galactocentric distances much shorter than the distance of the Earth from the Galactic centre. Their mean distance from the Earth is roughly 25,000 LY. From the measured energy fluence of GRBs (energy that reaches Earth per unit area) of known red shift it was found that the mean radiation energy emitted in long GRBs is approximately 5 × 10⁵³/A£2/4;r erg, where Aí2 is the solid angle illuminated by the GRB (the beaming angle). The radiation energy per solid angle of short GRBs is smaller by approximately two to three orders of magnitude.\n\nIf GRBs in our Galaxy and in external galaxies are not different, then the ratio of their fluences scale like the inverse of their distance ratio squared. Should a typical Galactic GRB at a distance of d = 25,000 LY point in the direction of the Earth, its hemisphere facing the GRB will be illuminated by gamma-rays with a total fluence F\\_(y) ~ 5 × 10⁵³/4n d² ~ 4 × 10⁷ erg s⁻¹ within typically 30s. The gamma-rays’ energy and momentum will be deposited within typically 70 gcm⁻² of the upper atmosphere (the total column density of the atmosphere at sea level is ~1000 gcm⁻²). Such fluxes would destroy the ozone layer and create enormous shocks going through the atmosphere, provoke giant global storms, and ignite huge fires. Smith et al. (2004) estimated that a fraction between 2 × 10⁻³ and 4 × 10⁻²of the Gamma-ray fluence will be converted in the atmosphere to an UV fluence at ground level. The UV radiation is mainly harmful to DNA and RNA molecules that absorb this radiation. The lethal dose for a UV exposure, approximately 10⁴ erg cm⁻², makes a GRB at 25,000 LY potentially highly lethal (e.g., Galante and Horvath, 2005) to the hemisphere illuminated by the GRB. But, protection can be provided by habitat (underwater, underground, under-roof, shaded areas) or by effective skin covers such as furs of animals or clothing of human beings. The short duration of GRBs and the lack of a nearly warning signal, however, make protection by moving into the shade or a shelter, or by covering up quickly, unrealistic for most species.\n\nIt should be noted that the MeV gamma-ray emission from GRBs may be accompanied by a short burst of very high energy gamma-rays, which, so far, could not be detected, either by the gamma-ray and X-ray satellites (CGRO, BeppoSAX, HETE, Chandra, XMMNewton, Integral, SWIFT and the interplanetary network), or by ground-level high energy gamma-ray telescopes such as HESS and Magic (due to timelag in response). Such bursts of GeV and TeV gamma-rays, if produced by GRBs, might be detected by the Gamma-ray Large Area Space Telescope (GLAST), which will be launched into space in 16.V.2008. GeV-TeV gamma-rays from relatively nearby Galactic GRBs may produce lethal doses of atmospheric muons.\n\n12.3 Cosmic ray threats\n\nThe mean energy density of Galactic cosmic rays is similar to that of star light, the cosmic microwave background radiation and the Galactic magnetic field, which all happen to be of the order of approximately 1 eV cm⁻³. This energy density is approximately eight orders of magnitude smaller than that of solar light at a distance of one astronomical unit, that is, that of the Earth, from the sun. Moreover, cosmic rays interact at the top of the atmosphere and their energy is converted to atmospheric showers. Most of the particles and gamma-rays in the atmospheric showers are stopped in the atmosphere before reaching ground level, and almost only secondary muons and neutrinos, which carry a small fraction of their energy, reach the ground. Thus, at first it appears that Galactic cosmic rays cannot affect life on the Earth significantly. However, this is not the case. There is accumulating evidence that even moderate variations in the flux of cosmic rays that reach the atmosphere have significant climatic effects, despite their low energy density. The evidence comes mainly from two sources:\n\n1. The interaction of cosmic rays with nuclei in the upper atmosphere generates showers of secondary particles, some of which produce the radioisotopes¹⁴C and¹⁰Be that reach the surface of the Earth either via the carbon cycle (¹⁴CO₂) or in rain and snow (¹⁰Be). Since this is the only terrestrial source, their concentration in tree rings, ice cores, and marine sediments provides a good record of the intensity of Galactic cosmic rays that have reached the atmosphere in the past. They show clear correlation between climate changes and variation of the cosmic ray flux in the Holocene era.\n\n2. The ions produced in cosmic ray showers increase the production of low altitude clouds (e.g., Carslaw et al. 2002). In data collected in the past 20 years, by satellites and neutron monitors, there is a clear correlation between global cloud cover and cosmic ray flux above 10 GeV that penetrate the geomagnetic field. Cloud cover reduces ground level radiation by a global average of 30 Wm⁻², which are 13% of the ground level solar irradiance. An increased flux of Galactic cosmic rays is associated with an increase in low cloud cover, which increases the reflectivity of the atmosphere and produces a cooler temperature.\n\nCosmic rays affect life in other ways:\n\n1. Cosmic ray-produced atmospheric showers of ionized particles trigger lightening discharges in the atmosphere (Gurevich and Zybin, 2005). These showers produce NO and NO₂ by direct ionization of molecules, which destroy ozone at a rate faster than its production in the discharges. The depletion of the ozone in the atmosphere leads to an increased UV flux at the surface.\n\n2. The decay of secondary mesons produced in showers yields high energy penetrating muons, which reach the ground and penetrate deep underground and deep underwater. A small fraction of energetic protons and neutrons from the shower that increases with the energy of the primary cosmic ray particle also reaches the surface. Overall, the very penetrating secondary muons are responsible for approximately 85% of the total equivalent dose delivered by cosmic rays at ground level. Their interactions, and the interactions of their products, with electrons and nuclei in living cells, ionize atoms and break molecules and damage DNA and RNA by displacing electrons, atoms and nuclei from their sites. The total energy deposition dose from penetrating muons resulting in 50% mortality in 30 days is between 2.5 and 3.0 Gy (1 Gy = 10⁴, erg g⁻¹). A cosmic ray muon deposits approximately 4 Mev g⁻¹ in living cells, and thus the lethal flux of cosmic ray muons is 5 × 10⁹ cm⁻² if delivered in a short time (less than a month). In order to deliver such a dose within a month, the normal cosmic ray flux has to increase by nearly a factor of a thousand during a whole month).\n\nA large increase in cosmic ray flux over extended periods may produce global climatic catastrophes and expose life on the ground, underground, and underwater to hazardous levels of radiation, which result in cancer and leukaemia. However, the bulk of Galactic cosmic rays have energies below 10 GeV. Such cosmic rays that enter the heliosphere are deflected by the magnetic field of the solar wind before they reach the Earth’s neighbourhood and by its geomagnetic field before they reach the Earth’s atmosphere. Consequently, the Galactic cosmic ray flux that reaches the Earth’s atmosphere is modulated by variations in the solar wind and in the Earth’s magnetic field.\n\nLife on the Earth has adjusted itself to the normal cosmic ray flux, which reaches its atmosphere. Perhaps, cosmic ray-induced mutations in living cells played a major role in the evolution and diversification of life from a single cell to the present millions of species. Any credible claim for a global threat to life from increasing fluxes of cosmic rays must demonstrate that the anticipated increase is larger than the periodical changes in the cosmic ray flux that reaches the Earth, which result from the periodic changes in solar activity, in the geomagnetic field and in the motions of the Earth, and to which terrestrial life has adjusted itself.\n\n12.3.1 Earth magnetic field reversals\n\nThe Earth’s magnetic field reverses polarity every few hundred thousand years, and is almost non-existent for perhaps a century during the transition. The last reversal was 780 Ky ago, and the magnetic field’s strength decreased by 5% during the twentieth century! During the reversals, the ozone layer becomes unprotected from charged solar particles, which weakens its ability to protect humans from UV radiation. However, past reversals were not associated with any major extinction according to the fossil record, and thus are not likely to affect humanity in a catastrophic way.\n\n12.3.2 Solar activity, cosmic rays, and global warming\n\nCosmic rays are the main physical mechanism controlling the amount of ionization in the troposphere (the lower 10 km or so of the atmosphere). The amount of ionization affects the formation of condensation nuclei required for the formation of clouds in clean marine environment. The solar wind – the outflow of energetic particles and entangled magnetic field from the sun – is stronger and reaches larger distances during strong magnetic solar activity. The magnetic field carried by the wind deflects the Galactic cosmic rays and prevents the bulk of them from reaching the Earth’s atmosphere. A more active sun therefore inhibits the formation of condensation nuclei, and the resulting low-altitude marine clouds have larger drops, which are less reflective and live shorter. This decrease in cloud coverage and in cloud reflectivity reduces the Earth’s albedo. Consequently, more solar light reaches the surface and warms it.\n\nCosmic ray collisions in the atmosphere produce¹⁴C, which is converted to¹⁴CO₂ and incorporated into the tree rings as they form; the year of growth can be precisely determined from dendrochronology. Production of¹⁴C is high during periods of low solar magnetic activity and low during high magnetic activity. This has been used to reconstruct solar activity during the past 8000 years, after verifying that it correctly reproduces the number of sunspots during the past 400 years (Solanki et al., 2004). The number of such spots, which are areas of intense magnetic field in the solar photosphere, is proportional to the solar activity. Their construction demonstrates that the current episode of high sunspot number and very high average level of solar activity, which has lasted for the past 70 years, has been the most intense and has had the longest duration of any in the past 8000 years. Moreover, the solar activity correlates very well with palaeoclimate data, supporting a major solar activity effect on the global climate.\n\nUsing historic variations in climate and the cosmic ray flux, Shaviv (2005) could actually quantify empirically the relation between cosmic ray flux variations and global temperature change, and estimated that the solar contribution to the twentieth century warming has been 0.50 ± 0.20°C out of the observed increase of 0.75 ± 0.15°C, suggesting that approximately two-thirds of global warming resulted from solar activity while perhaps only approximately one-third came from the greenhouse effect. Moreover, it is quite possible that solar activity coupled with the emission of greenhouse gases is a stronger driver of global warming than just the sum of these two climatic drivers.\n\n12.3.3 Passage through the Galactic spiral arms\n\nRadio emission from the Galactic spiral arms provides evidence for their enhanced CR density. Diffuse radio emission from the interstellar medium is mainly synchrotron radiation emitted by CR electrons moving in the interstellar magnetic fields. High contrasts in radio emission are observed between the spiral arms and the discs of external spiral galaxies. Assuming equipartition between the CR energy density and the magnetic field density, as observed in many astrophysical systems, CR energy density in the spiral arms should be higher than in the disk by a factor of a few. Indeed, there is mounting evidence from radio and X-ray observations that low energy CRs are accelerated by the debris of core-collapse SNe. Most of the supernovae in spiral galaxies like our own are core-collapse SNe. They predominantly occur in spiral arms where most massive stars are born and shortly thereafter die. Thus, Shaviv (2002) has proposed that when the solar system passes through the Galactic spiral arms, the heliosphere is exposed to a much higher cosmic ray flux, which increases the average low-altitude cloud cover and reduces the average global temperature. Coupled with the periodic variations in the geomagnetic field and in the motion of the Earth around the sun, this may have caused the extended periods of glaciations and ice ages, which, in the Phanerozoicera, have typically lasted approximately 30 Myr with a mean separation of approximately 140 Myr. Indeed, Shaviv (2002) has presented supportive evidence from geological and meteoritic records for a correlation between the extended ice ages and the periods of an increased cosmic ray flux. Also, the duration of the extended ice ages is in agreement with the typical crossing time of spiral arms (typical width of 100 LY divided by a relative velocity of approximately 10 km s⁻¹yields approximately 30 Myr crossing time). Note also that passage of the heliosphere through spiral arms, which contain a larger density of dust grains produced by SN explosions, can enter the heliosphere, reach the atmosphere, scatter away sunlight, reduce the surface temperature and cause an ice age.\n\n12.3.4 Cosmic rays from nearby supernovae\n\nThe cosmic ray flux from a supernova remnant (SNR) has been estimated to produce a fluence F ~ 7.4 × 10⁶ (30 LY/d)² erg cm⁻² at a distance d from the remnant. The active acceleration time of an SNR is roughly 10⁴years. The ambient CR flux near the Earth is 9 × 10⁴ erg cm⁻² year⁻¹. Thus, at a distance of 300 LY approximately, the ambient flux level would increase approximately by a negligible 0.1 % during a period of 10⁴ years approximately. To have a significant effect, the supernova has to explode within 30 LY from the sun. Taking into consideration the estimated SN rate and distribution of Galactic SNRs, the rate of SN explosions within 30 LY from the sun is approximately 3 × 10⁻¹⁰ year⁻¹. However, at such a distance, the SN debris can blow away the Earth’s atmosphere and produce a major mass extinction.\n\n12.3.5 Cosmic rays from gamma-ray bursts\n\nRadio, optical, and X-ray observations with high spatial resolution indicate that relativistic jets, which are fired by quasar and micro-quasars, are made of a sequence of plasmoids (cannonballs) of ordinary matter whose initial expansion (presumably with an expansion velocity similar to the speed of sound in a relativistic gas) stops shortly after launch (e.g., Dar and De Rujula, 2004 and references therein). The photometric and spectroscopic detection of SNe in the fading after glows of nearby GRBs and various other properties of GRBs and their after glows provide decisive evidence that long GRBs are produced by highly relativistic jets of plasmoids of ordinary matter ejected in SN explosions, as long advocated by the Cannonball (CB) Model of GRBs (see, for example, Dar, 2004, Dar &A. De Rujula 2004, “Magnetic field in galaxies, galaxy clusters, &intergalactic space in: Physical Review D 72, 123002123006; Dar and De Rujula, 2004). These jets of plasmoids (cannonballs) produce an arrow beam of high energy cosmic rays by magnetic scattering of the ionized particles of the interstellar medium (ISM) in front of them. Such CR beams from Galactic GRBs may reach large Galactic distances and can be much more lethal than their gamma-rays (Dar and De Rujula, 2001; Dar et al., 1998).\n\nLet v = β3 c be the velocity of a highly relativistic CB and [Image] be its Lorentz factor. For long GRBs, typically, γ ~ 10³ (v ~ 0.999999c!). Because of the highly relativistic motion of the CBs, the ISM particles that are swept by the CBs enter them with a Lorentz factor γ ~ 10³ in the CBs’ rest frame. These particles are isotropized and accelerated by the turbulent magnetic fields in the CBs (by a mechanism proposed by Enrico Fermi) before they escape back into the ISM. The highly relativistic motion of the CBs, boosts further their energy by an average factor γ through the Doppler effect and collimates their isotropic distribution into an arrow conical beam of an opening angle θ ~ 1/γ around the direction of motion of the CB in the ISM. This relativistic beaming depends only on the CB’s Lorentz factor but not on the mass of the scattered particles or their energy.\n\nThe ambient interstellar gas is nearly transparent to the cosmic ray beam because the Coulomb and hadronic cross-sections are rather small with respect to typical Galactic column densities. The energetic CR beam follows a ballistic motion rather than being deflected by the ISM magnetic field whose typical value is B ~ 3 × 10⁻⁶ Gauss. This is because the magnetic field energy swept up by the collimated CR beam over typical distances to Galactic supernovae is much smaller than the kinetic energy of the beam. Thus, the CR beam sweeps away the magnetic field along its way and follows a straight ballistic trajectory through the interstellar medium. (The corresponding argument, when applied to the distant cosmological GRBs, leads to the opposite conclusion: no CR beams from distant GRBs accompany the arrival of their beamed gamma-rays.)\n\nThe fluence of the collimated beam of high energy cosmic rays at a distance from a GRB that is predicted by the CB model of GRBs is given approximately by F ~ E\\_(k)γ²/4π d² ~ 10²⁰ (LY/d²) erg cm⁻², where the typical values of the kinetic energy of the jet of CBs, E\\_(k) ~ 10⁵¹ erg and γ ~ 10³, were obtained from the CB analysis of the observational data on long GRBs. Observations of GRB after glows indicate that it typically takes a day or two for a CB to lose approximately 50% of its initial kinetic energy, that is, for its Lorentz factor to decrease to half its initial value. This energy is converted to CRs with a typical Lorentz factor γCR ~ γ² whose arrival time from a Galactic distance is delayed relative to the arrival of the afterglow photons by a negligible time, Δt ~ d/cγ\\_(CR). Consequently, the arrival of the bulk of the CR energy practically coincides with the arrival of the afterglow photons.\n\nThus, for a typical long GRB at a Galactic distance d = 25,000 LY, which is viewed at a typical angle θ ~ 1/γ ~ 10⁻³ radians, the energy deposition in the atmosphere by the CR beam is F ~ 10¹¹ erg cm⁻² while that deposited by gamma-rays is smaller by about 3 order of magnitude (the kinetic energy of the electrons in the jet is converted to a conical beam of gamma-rays while the bulk of the kinetic energy that is carried by protons is converted to a conical beam of CRs with approximately the same opening angle). The beam of energetic cosmic rays accompanying a Galactic GRB is deadly for life on Earthlike planets. When high energy CRs with energy Ep collide with the atmosphere at a zenith angle θ\\_(Z), they produce high energy muons whose number is given approximately by N\\_(μ) (E > 25 GeV) ~ 9.14[E\\_(p)/TeV]^(0.757)/cos θ\\_(z) (Drees et al., 1989). Consequently, a typical GRB produced by a jet with EK ~ 10⁵¹ erg at a Galactic distance of 25,000 LY, which is viewed at the typical viewing angle θ ~ 1/γ ~ 10⁻³, is followed by a muon fluence at ground level that is given by F\\_(μ)(E > 25 GeV) ~ 3 × 10¹¹cm⁻². Thus, the energy deposition rate at ground level in biological materials, due to exposure to atmospheric muons produced by an average GRB near the centre of the Galaxy, is 1.4 × 10¹² MeVg⁻¹. This is approximately 75 times the lethal dose for human beings. The lethal dosages for other vertebrates and insects can be a few times or as much as a factor 7 larger, respectively. Hence, CRs from galactic GRBs can produce a lethal dose of atmospheric muons for most animal species on the Earth. Because of the large range of muons (~4[E\\_(μ)/GeV]m) in water, their flux is lethal, even hundreds of metres under water and underground, for CRs arriving from well above the horizon. Thus, unlike other suggested extraterrestrial extinction mechanisms, the CRs of galactic GRBs canal so generate massive extinctions deep under water and underground. Although half of the planet is in the shade of the CR beam, its rotation exposes a larger fraction of its surface to the CRs, half of which will arrive within over approximately 2 days after the gamma-rays. Additional effects that will increase the lethality of the CRs over the whole planet include:\n\n1. Evaporation of a significant fraction of the atmosphere by the CR energy deposition.\n\n2. Global fires resulting from heating of the atmosphere and the shock waves produced by the CR energy deposition in the atmosphere.\n\n3. Environmental pollution by radioactive nuclei, produced by spallation of atmospheric and ground nuclei by the particles of the CR-induced showers that reach the ground.\n\n4. Depletion of stratospheric ozone, which reacts with the nitric oxide generated by the CR-produced electrons (massive destruction of stratospheric ozone has been observed during large solar flares, which generate energetic protons).\n\n5. Extensive damage to the food chain by radioactive pollution and massive extinction of vegetation by ionizing radiation (the lethal radiation dosages for trees and plants are slightly higher than those for animals, but still less than the flux estimated above for all but the most resilient species).\n\nIn conclusion, the CR beam from a Galactic SN/GRB event pointing in our direction, which arrives promptly after the GRB, can kill, in a relatively short time (within months), the majority of the species alive on the planet.\n\n12.4 Origin of the major mass extinctions\n\nGeological records testify that life on Earth has developed and adapted itself to its rather slowly changing conditions. However, good quality geological records, which extend up to approximately 500 Myr ago, indicate that the exponential diversification of marine and continental life on the Earth over that period was interrupted by many extinctions (e.g., Benton 1995; Erwin 1996, 1997; Raup and Sepkoski, 1986), with the major ones exterminating more than 50% of the species on land and sea, and occurring on average, once every 100 Myr. The five greatest events were those of the final Ordovician period (some 435 Myr ago), the late Devonian (357 Myr ago), the final Permian (251 Myr ago), the late Triassic (198 Myr ago) and the final Cretaceous (65 Myr ago). With, perhaps, the exception of the Cretaceous-Tertiary mass extinction, it is not well known what caused other mass extinctions. The leading hypotheses are:\n\n• Meteoritic Impact : The impact of a sufficiently large asteroid or comet could create mega-tsunamis, global forest fires, and simulate nuclear winter from the dust it puts in the atmosphere, which perhaps are sufficiently severe as to disrupt the global ecosystem and cause mass extinctions. A large meteoritic impact was invoked (Alvarez et al., 1980) in order to explain their idium anomaly and the mass extinction that killed the dinosaurs and 47% of all species around the K/T boundary, 65 Myr ago. Indeed, a 180 km wide crater was later discovered, buried under 1 km of Cenozoic sediments, dated back 65 Myr ago and apparently created by the impact of a 10 km diameter meteorite or comet near Chicxulub, in the Yucatan (e.g., Hildebrand, 1990; Morgan et al., 1997; Sharpton and Marin, 1997). However, only for the End Cretaceous extinction is there compelling evidence of such an impact. Circumstantial evidence was also claimed for the End Permian, End Ordovician, End Jurassic and End Eocene extinctions.\n\n• Volcanism: The huge Deccan basalt floods in India occurred around the K/T boundary 65 Myr ago when the dinosaurs were finally extinct. The Permian/Triassic (P/T) extinction, which killed between 80% and 95% of the species, is the largest known is the history of life; occurred 251 Myr ago, around the time of the gigantic Siberian basalt flood. The outflow of millions of cubic kilometres of lava in a short time could have poisoned the atmosphere and oceans in a way that may have caused mass extinctions. It has been suggested that huge volcanic eruptions caused the End Cretaceous, End Permian, End Triassic, and End Jurassic mass extinctions (e.g., Courtillot, 1988; Courtillot et al., 1990; Officer and Page, 1996; Officer et al., 1987).\n\n• Drastic Climate Changes: Rapid transitions in climate may be capable of stressing the environment to the point of making life extinct, though geological evidence on the recent cycles of ice ages indicate they had only very mild impacts on biodiversity. Extinctions suggested to have this cause include: End Ordovician, End Permian, and Late Devonian.\n\nPaleontologists have been debating fiercely which one of the above mechanisms was responsible for the major mass extinctions. But, the geological records indicate that different combinations of such events, that is, impacts of large meteorites or comets, gigantic volcanic eruptions, drastic changes in global climate and huge sea regressions/sea rise seem to have taken place around the time of the major mass extinctions. Can there be a common cause for such events?\n\nThe orbits of comets indicate that they reside in an immense spherical cloud (‘the Oort cloud’), that surrounds the planetary with a typical radius of R ~ 100,000 AU. The statistics imply that it may contain as many as 10¹² comets with a total mass perhaps larger than that of Jupiter. The large radius implies that the comets share very small binding energies and mean velocities of v < 100 ms⁻¹. Relatively small gravitational perturbations due to neighbouring stars are believed to disturb their orbits, unbind some of them, and put others into orbits that cross the inner solar system. The passage of the solar system through the spiral arms of the Galaxy, where the density of stars is higher, could have caused such perturbations, and consequently, the bombardment of the Earth with a meteorite barrage of comets over an extended period longer than the free fall time. It has been claimed by some authors that the major extinctions were correlated with passage times of the solar system through the Galactic spiral arms. However, these claims were challenged. Other authors suggested that biodiversity and extinction events may be influenced by cyclic processes. Raup and Sepkoski (1986) claimed a 26–30 Myr cycle in extinctions. Although this period is not much different from the 31 Myr period of the solar system crossing the Galactic plane, there is no correlation between the crossing time and the expected times of extinction. More recently, Rohde and Muller (2005) have suggested that biodiversity has a 62 ±3 Myr cycle. But, the minimum in diversity is reached only once during a full cycle when the solar system is farthest away in the northern hemisphere from the Galactic plane.\n\nCould Galactic GRBs generate the major mass extinction, and can they explain the correlation between mass extinctions, meteoritic impacts, volcano eruptions, climate changes and sea regressions, or can they only explain the volcanic-quiet and impact-free extinctions?\n\nPassage of the GRB jet through the Oort cloud, sweeping up the interstellar matter on its way, could also have generated perturbations, sending some comets into a collision course with the Earth.\n\nThe impact of such comets and meteorites may have triggered the huge volcanic eruptions, perhaps by focusing shock waves from the impact at an opposite point near the surface on the other side of the Earth, and creating the observed basalt floods, timed within 1−2 Myr around the K/T and P/T boundaries. Global climatic changes, drastic cooling, glaciation and sea regression could have followed from the drastic increase in the cosmic ray flux incident on the atmosphere and from the injection of large quantities of light-blocking materials into the atmosphere from the cometary impacts and the volcanic eruptions. The estimated rate of GRBs from observations is approximately 10³ year⁻¹. The sky density of galaxies brighter than magnitude 25 (the observed mean magnitude of the host galaxies of the GRBs with known red-shifts) in the Hubble telescope deep field is approximately 2 × 10⁻⁵ per square degree. Thus, the rate of observed GRBs, per galaxy with luminosity similar to that of the Milky Way, is R\\_(GRB) ~ 1.2 × 10⁻⁷ year⁻¹. To translate this result into the number of GRBs born in our own galaxy, pointing towards us, and occurring in recent cosmic times, one must take into account that the GRB rate is proportional to the star formation rate, which increases with red-shift z like (1 + z)⁴ for z < 1and remains constant up to z ~ 6. The mean red-shift of GRBs with known red-shift, which were detected by SWIFT, is ~2.8, that is, most of the GRBs were produced at a rate approximately 16 times larger than that in the present universe. The probability of a GRB pointing towards us within a certain angle is independent of distance. Therefore, the mean rate of GRBs pointing towards us in our galaxy is roughly R\\_(GRB) /(1 + z)⁴ ~ 0.75x 10⁻⁸ year⁻¹, or once every 130 Myr. If most of these GRBs take place not much farther away than the distance to the galactic centre, their effect is lethal, and their rate is consistent with the rate of the major mass extinctions on our planet in the past 500 Myr.\n\n12.5 The Fermi paradox and mass extinctions\n\nThe observation of planets orbiting nearby stars has become almost a routine. Although current observations/techniques cannot detect yet planets with masses comparable to the Earth near other stars, they do suggest their existence. Future space-based observatories to detect Earth-like planets are being planned. Terrestrial planets orbiting in the habitable neighbourhood of stars, where planetary surface conditions are compatible with the presence of liquid water, might have global environments similar to ours, and harbour life. But, our solar system is billions of years younger than most of the stars in the Milky Way and life on extra solar planets could have preceded life on the Earth by billions of years, allowing for civilizations much more advanced than ours. Thus Fermi’s famous question, ‘where are they?’, that is, why did they not visit us or send signals to us? One of the possible answers is provided by cosmic mass extinction: even if advanced civilizations are not self-destructive, they are subject to a similar violent cosmic environment that may have generated the big mass extinctions on this planet. Consequently, there may be no nearby aliens who have evolved long enough to be capable of communicating with us, or pay us a visit.\n\n12.6 Conclusions\n\n• Solar flares do not comprise a major threat to life on the Earth. The Earth’s atmosphere and the magnetosphere provide adequate protection to life on its surface, under water and underground.\n\n• Global warming is a fact. It has drastic effects on agricultural yields, cause glacier retreat, species extinctions and increases in the ranges of disease vectors. Independent of whether or not global warming is of anthropogenic origin, human kind must conserve energy, burn less fossil fuel, and use and develop alternative non-polluting energy sources.\n\n• The current global warming may be driven by enhanced solar activity. On the basis of the length of past large enhancements in solar activity, the probability that the enhanced activity will continue until the end of the twenty-first century is quite low (1%). (However, if the global warming is mainly driven by enhanced solar activity, it is hard to predict the time when global warming will turn into global cooling.\n\n• Within 1–2 billion years, the energy output of the sun will increase to a point where the Earth will probably become too hot to support life.\n\n• Passage of the sun through the Galactic spiral arms once in 140 Myr approximately will continue to produce major, approximately 30 Myr long, ice ages.\n\n• Our knowledge of the origin of major mass extinctions is still very limited. Their mean frequency is extremely small, once every 100 Myr. Any initiative/decision beyond expanding research on their origin is premature.\n\n• Impacts between near-Earth Objects and the Earth are very infrequent but their magnitude can be far greater than any other natural disaster. Such impacts that are capable of causing major mass extinctions are extremely infrequent as evident from the frequency of past major mass extinctions. At present, modern astronomy cannot predict or detect early enough such an imminent disaster and society does not have either the capability or the knowledge to deflect such objects from a collision course with the Earth.\n\n• A SN would have to be within few tens of light years from the Earth for its radiation to endanger creatures living at the bottom of the Earth’s atmosphere. There is no nearby massive star that will undergo a SN explosion close enough to endanger the Earth in the next few million years. The probability of such an event is negligible, less than once in 10⁹years.\n\n• The probability of a cosmic ray beam or a gamma-ray beam from Galactic sources (SN explosions, mergers of neutron stars, phase transitions in neutron stars or quark stars, and micro-quasarejections) pointing in our direction and causing a major mass extinction is rather small and it is strongly constrained by the frequency of past mass extinctions – once every 100Myr.\n\n• No source in our Galaxy is known to threaten life on the Earth in the foreseeable future.\n\nReferences\n\nAlvarez, L.W., Alvarez, W., Asaro, F., and Michel, H.V. (1980). Extraterrestrial cause for the Cretaceous tertiary extinction, Science, 208, 1095–1101.\n\nBenton, M.J. (1995). Diversification and extinction in the history of life, Science, 268, 52–58.\n\nCarslaw, K.S., Harrison, R.G., and Kirkby, J. (2002). Cosmic rays, clouds, and climate. Science, 298, 1732–1737.\n\nCourtillot, V. (1990). A Volcanic Eruption, Scientific American, 263, October 1990, pp. 85–92.\n\nCourtillot, V., Feraud, G., Maluski, H., Vandamme, D., Moreau, M.G., and Besse, J. (1998). Deccan Flood Basalts and the Cretaceous/Tertiary Boundary. Nature, 333, 843–860.\n\nDado, S., Dar, A., and De Rújula, A. (2002). On the optical and X-ray afterglows of gamma ray bursts. Astron. Astrophys., 388, 1079–1105.\n\nDado, S., Dar, A., and De Rújula, A. (2003). The supernova associated with GRB 030329. Astrophys. J., 594, L89.\n\nDar, A. (2004). The GRB/XRF-SN Association, arXiv astro-ph/0405386.\n\nDar, A. and De Rújula, A. (2002). The threat to life from Eta Carinae and gamma ray bursts. In Morselli, A. and Picozza, P. (eds.) Astrophysics and Gamma Ray Physics in Space (Frascati Physics Series Vol. XXIV, pp. 513–523 (astro-ph/0110162).\n\nDar, A. and De Rújula, A. (2004). Towards a complete theory of gamma-ray bursts. Phys. Rep., 405, 203–278.\n\nDar, A., Laor, A., and Shaviv, N. (1998). Life extinctions by cosmic ray jets, Phys. Rev. Lett., 80, 5813–5816.\n\nDrees, M., Halzen, F., and Hikasa, K. (1989). Muons in gamma showers, Phys. Rev., D39, 1310–1317.\n\nDu and De Rujula, A. (2004). Magnetic field in galaxies, galaxy dusters, and intergalactic space in: Physical Review D 72, 123002–123006.\n\nEllis, J., Fields, B.D., and Schramm, D.N. (1996). Geological isotope anomalies as signatures of nearby supernovae. Astrophys. J., 470, 1227–1236.\n\nEllis, J. and Schramm, D.N. (1995). Could a nearby supernova explosion have caused a mass extinction?, Proc. Nat. Acad. Sci., 92, 235–238.\n\nErwin, D.H. (1996). The mother of mass extinctions, Scientific American, 275, July 1996, p. 56–62.\n\nErwin, D.H. (1997). The Permo-Triassic extinction. Nature, 367, 231–236.\n\nFields, B.D. and Ellis, J. (1999). On deep-ocean⁶⁰Fe as a fossilofa near-earth supernova. New Astron., 4, 419–430.\n\nGalante, D. and Horvath, J.E. (2005). Biological effects of gamma ray bursts: distances for severe damage on the biota. Int. J. Astrobiology, 6, 19–26.\n\nGurevich, A.V. and Zybin, K.P. (2005). Runaway breakdown and the mysteries of lightning. Phys. Today, 58, 37–43.\n\nHildebrand, A.R. (1990). Mexican site for K/T Impact Crater?, Mexico, Eos, 71, 1425.\n\nKirkby, J., Mangini, A., and Muller, R.A. (2004). Variations of galactic cosmic rays and the earth’s climate. In Frisch, P.C. (ed.), Solar Journey: The Significance of Our Galactic Environment for the Heliosphere and Earth (Netherlands: Springer) pp. 349397 (arXivphysics/0407005).\n\nMeegan, C.A. and Fishman, G.J. (1995). Gamma ray bursts. Ann. Rev. Astron. Astrophys., 33, 415–458.\n\nMelott, A., Lieberman, B., Laird, C., Martin, L., Medvedev, M., Thomas, B., Cannizzo, J., Gehrels, N., and Jackman, C. (2004). Did a gamma-ray burst initiate the late Ordovician mass extinction? Int. J. Astrobiol., 3, 55–61.\n\nMorgan, J., Warner, M., and Chicxulub Working Group. (1997). Size and morphology of the Chixulub Impact Crater. Nature, 390, 472–476.\n\nOfficer, C.B., Hallan, A., Drake, C.L., and Devine, J.D. (1987). Global fire at the Cretaceous-Tertiary boundary. Nature, 326, 143–149.\n\nOfficer, C.B. and Page, J. (1996). The Great Dinosaurs Controversy (Reading, MA: Addison-Wesley Pub. Com.).\n\nRaup, D. and Sepkoski, J. (1986). Periodic extinction of families and genera. Science, 231, 833–836.\n\nRohde, R.A. and Muller, R.A. (2005). Cycles in fossil diversity. Nature, 434, 208–210. Ruderman, M.A. (1974). Possible consequences of nearby supernova explosions for atmospheric ozone and terrestrial life. Science, 184, 1079–1081.\n\nScalo, J. and Wheeler, J.C. (2002). Did a gamma-ray burst initiate the late Ordovician mass extinction? Astrophys. J., 566, 723–737.\n\nSepkoski, J.J. (1986). Is the periodicity of extinctions a taxonomic artefact? In Raup, D.M. and Jablonski, D. (eds.), Patterns and Processes in the History of Life. pp. 277–295 (Berlin: Springer-Verlag).\n\nSharpton, V.L. and Marin, L.E. (1997). The Cretaceous-Tertiary impact crater. Ann. NY Acad. Sci., 822, 353–380.\n\nShaviv, N. (2002). The spiral structure of the Milky Way, cosmic rays, and ice age epochs on earth. New Astron., 8, 39–77.\n\nShaviv, N. and Dar, A. (1995). Gamma ray bursts from Minijets. Astrophys. J., 447, 863–873.\n\nSmith, D.S., Scalo, J., and Wheeler, J.C. (2004). Importance of biologically active Aurora-like ultraviolet emission: stochastic irradiation of earth and mars by flares and explosions. Origins Life Evol. Bios., 34, 513–532.\n\nSolanki, S.K., Usoskin, I.G., Kromer, B., Sch,ússler, M., and Bear, J. (2004). Unusual activity of the sun during recent decades compared to the previous 11000 Years. Nature, 431, 1084–1087.\n\nSvensmark, H. (1998). Influence of Cosmic rays on earths climate. Phys. Rev. Lett., 81, 5027–5030.\n\nThomas, B.C., Jackman, C.H., Melott, A.L., Laird, C.M., Stolarski, R.S., Gehrels, N., Cannizzo, J.K., and Hogan, D.P. (2005). Terrestrial ozone depletion due to a milky way gamma-ray burst, Astrophys. J., 622, L153-L156.\n\nThorsett, S.E. (1995). Terrestrial implications of cosmological gamma-ray burst models. Astrophys. J. Lett., 444, L53-L55.\n\nvan den Bergh, S. and Tammann, G.A. (1991). Galactic and extragalactic supernova rates. Ann. Rev. Astron. Astrophys., 29, 363–407.\n\nPART III Risks from unintended consequences\n\n• 13 • Climate change and global risk\n\nDavid Frame and Myles R. Allen\n\n13.1 Introduction\n\nClimate change is among the most talked about and investigated global risks. No other environmental issue receives quite as much attention in the popular press, even though the impacts of pandemics and asteroid strikes, for instance, may be much more severe. Since the first Intergovernmental Panel on Climate Change (IPCC) report in 1990, significant progress has been made in terms of (1) establishing the reality of anthropogenic climate change and (2) understanding enough about the scale of the problem to establish that it warrants a public policy response. However, considerable scientific uncertainty remains. In particular scientists have been unable to narrow the range of the uncertainty in the global mean temperature response to a doubling of carbon dioxide from pre-industrial levels, although we do have a better understanding of why this is the case. Advances in science have, in some ways, made us more uncertain, or at least aware of the uncertainties generated by previously unexamined processes. To a considerable extent these new processes, as well as familiar processes that will be stressed in new ways by the speed of twenty-first century climate change, underpin recent heightened concerns about the possibility of catastrophic climate change.\n\nDiscussion of ‘tipping points’ in the Earth system (for instance Kemp, 2005; Lenton, 2007) has raised awareness of the possibility that climate change might be considerably worse than we have previously thought, and that some of the worst impacts might be triggered well before they come to pass, essentially suggesting the alarming image of the current generation having lit the very long, slow-burning fuse on a climate bomb that will cause great devastation to future generations. Possible mechanisms through which such catastrophes could play out have been developed by scientists in the last 15 years, as a natural output of increased scientific interest in Earth system science and, in particular, further investigation of the deep history of climate. Although scientific discussion of such possibilities has usually been characteristically guarded and responsible, the same probably cannot be said for the public debate around such notions. Indeed, many scientists regard these hypotheses and images as premature, even alarming. Mike Hulme, the Director of the United Kingdom’s Tyndall Centre recently complained that:\n\nThe IPCC scenarios of future climate change – warming somewhere between 1.4 and 5.8° Celsius by 2100 – are significant enough without invoking catastrophe and chaos as unguided weapons with which forlornly to threaten society into behavioural change. […] The discourse of catastrophe is in danger of tipping society onto a negative, depressive and reactionary trajectory.\n\nThis chapter aims to explain the issue of catastrophic climate change by first explaining the mainstream scientific (and policy) position: that twenty-first century climate change is likely to be essentially linear, though with the possibility of some non-linearity towards the top end of the possible temperature range. This chapter begins with a brief introduction to climate modelling, along with the concept of climate forcing. Possible ways in which things could be considerably more alarming are discussed in a section on limits to our current knowledge, which concludes with a discussion of uncertainty in the context of palaeoclimate studies. We then discuss impacts, defining the concept of ‘dangerous anthropogenic interference’ in the climate system, and some regional impacts are discussed alongside possible adaptation strategies. The chapter then addresses mitigation policy-policies that seek to reduce the atmospheric loading of greenhouse gases (GHG) – in light of the preceeding treatment of linear and non-linear climate change. We conclude with a brief discussion of some of the main points and problems.\n\n13.2 Modelling climate change\n\nScientists attempting to understand climate try to represent the underlying physics of the climate system by building various sorts of climate models. These can either be top down, as in the case of simple models that treat the Earth essentially as a closed system possessing certain simple thermodynamic properties, or they can be bottom up, as in the case of general circulation models (GCMs), which attempt to mimic climate processes (such as cloud formation, radiative transfer, and weather system dynamics). The range of models, and the range of processes they contain, is large: the model we use to discuss climate change below can be written in one line, and contains two physical parameters; the latest generation of GCMs comprise well over a million lines of computer code, and contain thousands of physical variables and parameters. In between there lies a range of Earth system models of intermediate complexity (EMICs) (Claussen et al., 2002; Lenton et al., 2006) which aim at resolving some range of physical processes between the global scale represented by EBMs and the more comprehensive scales represented by GCMs. EMICs are often used to investigate long-term phenomena, such as the millennial scale response to Milankovitch cycles, Dansger-Oeschgaard events or other such episodes and periods that it would be prohibitively expensive to investigate with a full GCM. In the following section we introduce and use a simple EBM to illustrate the global response to various sorts of forcings, and discuss the source of current and past climate forcings.\n\n13.3 A simple model of climate change\n\nA very simple model for the response of the global mean temperature to a specified climate forcing is given in the equation below. This model uses a single physical constraint – energy balance – to consider the effects of various drivers on global mean temperature. Though this is a very simple and impressionistic model of climate change, it does a reasonable job of capturing the aggregate climate response to fluctuations in forcing. Perturbations to the Earth’s energy budget can be approximated by the following equation (Hansen et al., 1985):\n\n[Image]\n\nin which c\\_(e)ff is the effective heat capacity of the system, governed mainly (Levitus et al., 2005) by the ocean, X is a feedback parameter, and AT is a global temperature anomaly. The rate of change is governed by the thermal inertia of the system, while the equilibrium response is governed by the feedback parameter alone (since the term on the left hand side of the equation tends to zero as the system equilibrates). The forcing, F, is essentially the perturbation to the Earth’s energy budget (in W/m²) which is driving the temperature response (ΔT). Climate forcing can arise from various sources, such as changes in composition of the atmosphere (volcanic aerosols; GHG) or changes in insolation. An estimate of current forcings is displayed in Fig. 13.1, and an estimate of a possible range of twenty-first century responses is shown in Fig.13.2. It canbe seen that the bulk of current forcing comes from elevated levels of carbondioxide (CO₂), though other agents are also significant. Historically, three main forcing mechanisms are evident in the temperature record: solar forcing, volcanic forcing and, more recently, GHG forcing. The range of responses in Fig. 13.2 is mainly governed by (1) choice of future GHG scenario and (2) uncertainty in the climate response, governed in our simple model (which can essentially replicate Fig. 13.3) by uncertainty in the parameters c\\_(e)ff and X. The scenarios considered are listed in the figure, along with corresponding grey bands representing climate response uncertainty for each scenario (at right in grey bands).\n\n[Image]\n\nFig. 13.1 Global average radiative forcing (RF) estimates and ranges in 2005 for anthropogenic carbon dioxide (CO₂), methane (CH₄), nitrous oxide (N₂O), and other important agents and mechanisms, together with the typical geographical extent (spatial scale) of the forcing and the assessed level of scientific understanding (LOSU). The net anthropogenic radiative forcing and its range are also shown. These require summing asymmetric uncertainty estimates from the component terms and cannot be obtained by simple addition. Additional forcing factors not included here are considered to have a very low LOSU. Volcanic aerosols contribute an additional natural forcing but are not included in this figure due to their episodic nature. The range for linear contrails does not include other possible effects of aviation on cloudiness. Reprinted with permission from Solomon et al. (2007). Climatic Change 2007: The physical Science Basis.\n\n13.3.1 Solar forcing\n\nAmong the most obvious ways in which the energy balance can be altered is if the amount of solar forcing changes. This is in fact what happens as the earth wobbles on its axis in three separate ways on times cales of tens to hundreds of thousands of years. The earth’s axis precesses with a period of 15,000 years, the obliquity of the Earth’s orbit oscillates with a period of around 41,000 years, and its eccentricity varies on multiple times cales (95,000, 125,000, and 400,000 years).\n\nThese wobbles provide the source of the earth’s periodic ice ages, by varying the amount of insolation at the Earth’s surface. In particular, the Earth’s climate is highly sensitive to the amount of solar radiation reaching latitudes north of about 60°N. Essentially, if the amount of summer radiation is insufficient to melt the ice that accumulates over winter, the depth of ice thickens, reflecting more back. Eventually, this slow accrual of highly reflective and very cold ice acts to reduce the Earth’s global mean temperature, and the Earth enters an ice age.\n\n[Image]\n\nFig. 13.2 Forcing and temperature response curves using the simple models presented in this chapter. In the top panel the thin line corresponds to historical forcings over the twentieth century; the solid line corresponds to GHG-only forcings, and these are projected forward under an extended version of the IPCC B1 scenario (SRES, 1992). In the bottom panel is the base model [Equation (13.1)] response to the B1 scenario with best guess parameters from Frame et al. (2006). Also shown (dashed line) is the response with the arbitrary disruption represented by Equation (13.1b). SRES: N. Nakicenovic et al., IPCC Special Report on Emission Scenarios, IPCC, Cambridge University Press, 2000. Frame, D.J., D.A. Stone, P.A. Stoll, M.R. Allen, Alternatives to stabilization scenarios, Geophysical Research Letters, 33, L14707.\n\nCurrent solar forcing is thought to be +0.12 Wm⁻², around one- eighth of the total current forcing (above pre-industrial levels). There has been some advocacy for a solar role in current climate change, but it seems unlikely that this is the case, since solar trends over some of the strongest parts of the climate change signal appear to be either insignificant or even out of phase with the climate response (Lockwood and Frohlich, 2007). Detection and attribution studies of recent climate change have consistently been able to detect a climate change signal attributable to elevated levels of GHG, but have been unable to detect a signal attributable to solar activity (Hegerl et al., 2007). Though dominant historically over millennial and geological timescales, the sun apparently is not a significant contributor to current climate change.\n\n13.3.2 Volcanic forcing\n\nAnother form of climate forcing comes from the explosive release of aerosols into the atmosphere through volcanoes. This is a significant forcing when aerosols can make it into the stratosphere where the residence time for aerosols is several years. During this time, they act to cool the planet by back-scattering incoming solar radiation. Though peak forcings due to large volcanoes are massive, the stratospheric aerosol residence time is short (~4 years) in comparison to that of the principal anthropogenic GHG, CO₂ (~150 years) so volcanic forcings tend to be much more dramatic and spikier (Fig. 13.3) than those associated with GHG. Peak forcing from the recent El Chichon (1982) and Mt Pinatubo (1991) volcanoes was on the order of – 3Wm⁻². Though it is harder to infer the forcing from volcanoes that pre-date the satellite record, estimates suggest that volcanic forcing from Karakatau and Tambora in the nineteenth century are likely to have been larger.\n\n[Image]\n\nFig. 13.3 Solid lines are multi-model global averages of surface warming (relative to 1980–1999) for the scenarios A2, A1B, and B1, shown as continuations of the twentieth century simulations. Shading denotes the ±1 standard deviation range of individual model annual averages. The orange line is for the experiment where concentrations were held constant at year 2000 values. The grey bars at right indicate the best estimate (solid line within each bar) and the likely range assessed for the six SRES marker scenarios. The assessment of the best estimate and likely ranges in the grey bars includes the AOGCMs in the left part of the figure, as well as results from a hierarchy of independent models and observational constraints. Reprinted with permission from Solomonet al. (2007). Climatic Change 2007: The physical Science Basis.\n\nEven bigger are supervolcanoes, the most extreme class of volcanic events seen on Earth. Supervolcanoes eject over a 1000 km³ of material into the atmosphere (compared with around 25 km³ for Pinatubo). They are highly uncommon, occurring less than once every 10,000 years, with some of the largest events being increasingly rare. Examples include the formation of the La Garita Caldera, possibly the largest eruption in history, which occurred some 28 million years ago, ejecting around 5000 km³ of matter and, more recently, the Toba explosion, which occurred about 71,000 years ago and ejected around 2800 km³ of material into the atmosphere. These very large events occur with a frequency in the vicinity of 1.4-22 events per million years (Mason et al., 2004), and the fact that the frequency of the most extreme events tends to fall away quickly with the magnitude of the event suggests that there is some sort of upper limit to the size of volcanic events (Mason et al., 2004).\n\nIn the last 1000 years, massive volcanic eruptions, responsible for climate forcings on the order of 10, occurred around 1258, probably in the tropics (Crowley, 2000). This medieval explosion was enormous, releasing around eight times the sulphate of the Krakatau explosion. However, correlations between the amount of sulphate released and observed global mean temperatures show that the temperature response for extremely large volcanoes does not scale simply from Krakatau-sized volcanoes (and smaller). Pinto et al. (1989) suggest that this is because beyond a certain level, increases in sulphate loading act to increase the size of stratospheric aerosols, rather than increasing their number. In essence, the radiative forcing per unit mass decreases at very large stratospheric sulphate loadings, and this moderates the cooling effects of very large volcanoes.\n\nThe expectations for the coming decades are that the odds of a volcanically induced radiative forcing of – 1.0 Wm ^(– 2) or larger occurring in a single decade are in the range 35–40%. The odds of two such eruptions in a single decade are around 15%, while the odds of three or more such eruptions is something like 5%. The odds of getting lucky volcanically (though perhaps not climatically) and experiencing no such eruptions is approximately 44%. Considering larger volcanic events, there is a 20% likelihood of a Pinatubo-scale eruption in the next decade and a 25% chance for an El Chichón-scale eruption (Hyde and Crowley, 2000). Although events of this magnitude would certainly act to cool the climate, and lead to some deleterious impacts (especially in the vicinity of the volcano) they would represent essentially temporary perturbations to the expected warming trend.\n\n13.3.3 Anthropogenic forcing\n\nA simple model for the accumulation of GHG (simplified here to carbon dioxide), its conversion to radiative forcing, and the subsequent effect on global mean temperatures, can be given by the following two equations:\n\n[Image]\n\nHere, CO₂ (t) represents the atmospheric concentration of carbon dioxide over time, which is taken to be the sum of emissions from natural (e\\_(nat)) and anthropogenic (e\\_(ant)) sources. We can further specify a relationship between carbon dioxide concentration and forcing:\n\n[Image]\n\nin which F2\\_(X)CO₂ is the forcing corresponding to a doubling of CO₂, CO₂ refers to the atmospheric concentration of CO₂ and CO\\_(2pre) is the (constant) pre-industrial concentration of CO₂. There is some uncertainty surrounding F\\_(2x) CO₂, but most general circulation models (GCMs) diagnose it to be in the range 3.5−4.0 W/m². Current forcings are displayed in Fig. 13.1. This shows that the dominant forcing is due to elevated levels (above pre-industrial) of carbon dioxide, with the heating effects of other GHG and the cooling effects of anthropogenic sulphates happening to largely cancel each other out.\n\nIt suffers from certain limitations but mimics the global mean temperature response of much more sophisticated state of the art GCMs adequately enough, especially with regard to relatively smooth forcing series. Both the simple model described above and the climate research community’s best-guess GCMs predict twenty-first century warmings, in response to the kinds of elevated levels of GHG we expect to see this century, of between about 1.5°C and 5.8°C.\n\nThe key point to emphasize in the context of catastrophic risk is that this warming is expected to be quite smooth and stable. No existing best guess state-of-the-art model predicts any sort of surprising non-linear disruption in terms of global mean temperature. In recent years, climate scientists have attempted to quantify the parameters in the above model by utilizing combinations of detuned models, simple (Forest et al., 2002) or complex (Murphy et al., 2004) constrained by recent observations (and sometimes using additional information from longer term climate data (Hegerl et al., 2006; Schneider von Deimling et al., 2006). These studies have reported a range of distributions for climate sensitivity, and while the lower bound and median values tend to be similar across studies there is considerable disagreement regarding the upper bound.\n\nSuch probabilistic estimates are dependent on the choice of data used to constrain the model, the weightings given to different data streams, the sampling strategies and statistical details of the experiment, and even the functional forms of the models themselves. Although estimation of climate sensitivity remains a contested arena (Knutti et al., 2007) the global mean temperature response of the models used in probabilistic climate forecasting can be reasonably well approximated by manipulation of the parameters c\\_(e)ff and, especially, X, in Equation(13.1) of this chapter. Essentially, Equation (13.1) can be tuned to mimic virtually any climate model in which atmospheric feedbacks scale linearly with surface warming and in which effective oceanic heat capacity is approximately constant under twentieth century climate forcing. This includes the atmospheric and oceanic components of virtually all current best-guess GCMs.\n\n13.4 Limits to current knowledge\n\nHowever, we know there are plenty of ways in which our best-guess models may be wrong. Feedbacks within the carbon cycle, in particular, have been the source of much recent scientific discussion. Amazon dieback, release of methane hydrates from the ocean floor, and release of methane from melting permafrost are among mechanisms thought to have the potential to add nasty surprises to the linear picture of anthropogenic climate change we see in our best models. There are essentially two ways in which we could be wrong about the evolution of the system towards the target, even if we knew the climate sensitivity S:\n\n• Forcing could be more temperature-dependent than we thought. That is: [Image].\n\n• Feedbacks could be more temperature-dependent than we thought. In this case: [Image].\n\nIn Fig. 13.2, we plot the latter sort of variant: in the dotted line in the bottom panel of Fig. 13.2 the forcing remains the same as in the base case, but the response is different, reflecting an example of a disruptive temperature dependence of X, which is triggered as the global mean temperature anomaly passes 2.5°C. This is a purely illustrative example, to show how the smooth rise in global mean temperature could receive a kick from some as yet un-triggered feedback in the system. In the last decade, scientists have investigated several mechanisms that might make our base equation more like (1) or (2). These include the following:\n\n• Methane hydrate release (F) in which methane, currently locked up in the ocean in the form of hydrates can be released rapidly if ocean temperatures rise beyond some threshold value (Kennett et al., 2000). Since methane sublimes (goes straight from a solid phase to a gaseous phase) this would lead to a very rapid rise in atmospheric GHG loadings. Maslin et al. (2004) claim that there is evidence for the hypothesis from Dansgaard-Oeschger interstadials; Benton and Twitchett (2003) claim that the sudden release of methane gas from methane hydrates (sometimes called the ‘clathrate gun hypothesis’) may have been responsible for the mass extinction at the end of the Permian, the ‘biggest crisis on Earth in the past 500 Myr’. If this is so, they claim with considerable understatement, ‘it is […] worth exploring further’.\n\n• Methane release from melting permafrost (F) is related to the oceanic clathrate gun hypothesis. In this case, methane is trapped in permafrost through the annual cycle of summer thawing and winter freezing. Vegetable matter and methane gas from the rotting vegetable material get sequestered into the top layers of permafrost; the reaction with water forms methane hydrates (Dallimore and Collet, 1995). Increased temperatures in permafrost regions may give rise to sufficient melting to unlock the methane currently sequestered, providing an amplification of current forcing (Sazonova et al., 2004).\n\n• Tropical forest (especially Amazonian) dieback (F) is perhaps the best known amplification to expected climate forcing. Unlike the previous two examples which are additional sources of GHG, tropical forest dieback is a reduction in the strength of a carbon sink (Cox et al., 2004). As temperatures rise, Amazonia dries and warms, initiating the loss of forest, leading to a reduction in the uptake of carbondioxide (Cox et al., 2000).\n\n• Ocean acidification(F)is an oceanic analogue to forest dieback from a carbon cycle perspective in that it is the reduction in the efficacy of a sink. Carbon dioxide is scrubbed out of the system by phytoplankton. As carbon dioxide builds up the ocean acidifies, rendering the phytoplankton less able to perform this role, weakening the ocean carbon cycle (Orr et al., 2005).\n\n• Thermohaline circulation disruption (λ), while perhaps less famous than Amazonian dieback, can at least claim the dubious credit of having inspired a fully fledged disaster movie. In this scenario, cool fresh water flowing from rapid disintegration of the Greenland ice sheet acts to reduce the strength of the (density-driven) vertical circulation in the North Atlantic (sometimes known as the ‘thermohaline circulation’, but better described as the Atlantic meridional overturning circulation. There is strong evidence that this sort of process has occurred in the Earth’s recent past, the effect of which was to plunge Europe into an ice age for another 1000 years.\n\n• The iris effect (λ) would be a happy surprise. This is a probable mechanism through which the Earth could self-regulate its temperature. The argument, developed in Lindzen et al. (2001) is that tropical cloud cover could conceivably increase in such a way as to reduce the shortwave radiation received at the surface. On this view, the expected (and worrying warming) will not happen or will largely be offset by reductions in insolation. Thus, while the Earth becomes less efficient at getting rid of thermal radiation, it also becomes better at reflecting sunlight. Unlike the other mechanisms discussed here, it would cause us to undershoot, rather than overshoot, our target (the ball breaks left, rather than right).\n\n• Unexpected enhancements to water vapour or cloud feedbacks (λ). In this case warming triggers new cloud or water vapour feedbacks that amplify the expected warming.\n\nThose marked (F) are disruptions to the earth’s carbon cycle, the others, marked (λ), can be seen as temperature-dependent disruptions to feedbacks. In reality, many of the processes that affect feedbacks would likely trigger at least some changes in carbon sources and sinks. In practice, the separation between forcing surprises and feedback surprises is a little artificial, but it is a convenient first order way to organize potential surprises. All are temperature-dependent, and this temperature-dependence goes beyond the simple picture sketched out in the section above. Some could be triggered – committed to – well in advance of their actual coming to pass. Most disturbingly, none of these surprises are well-quantified. We regard them as unlikely, but find it hard to know just how unlikely. We are also unsure of the amplitude of these effects. Given the current state of the climate system, the current global mean temperature and the expected temperature rise, we do not know much about just how big an effect they will have. In spite of possibly being responsible for some dramatic effects in previous climates, it is hard to know how big an effect surprises like these will have in the future. Quantifying these remote and in many cases troubling possibilities is extremely difficult. Scientists write down models that they think give a justifiable and physically meaningful description of the process under study, and then attempt to use (usually palaeoclimate) data to constrain the magnitude and likelihood of the event. This is a difficult process: our knowledge of past climates essentially involves an inverse problem, in which we build up a picture of three-dimensional global climates from surface data proxies, which are often either biological data (tree-ring patterns, animal middens, etc.) or cryospheric (ice cores). Climate models are often run in under palaeo conditions in order to provide an independent check on the adequacy of the models, which have generally been built to simulate today’s conditions. Several sources of uncertainty are present in this process:\n\n1. Data uncertainty. Generally, the deeper we look into the past, the less sure we are that our data are a good representation of global climate. The oceans, in particular, are something of a data void, and there are also issues about the behaviour of clouds in previous climates.\n\n2. Forcing uncertainty. Uncertainty in forcing is quite large for the pre-satellite era generally, and again generally grows as one moves back in time. In very different climate regimes, such as ice ages, there are difficult questions regarding the size and reflectivity of ice sheets, for instance. Knowing that we have the right inputs when we model past climates is a significant challenge.\n\n3. Model uncertainty. Climate models, while often providing reasonable simulations over continental scales, struggle to do a good job of modelling smaller-scale processes (such as cloud formation). Model imperfections thus add another layer of complexity to the problem, and thus far at least, GCMs simulating previous climates such as the last glacial maximum have had little success in constraining even global mean properties (Crucifix, 2006).\n\nIn the presence of multiple scientific uncertainties, it is hard to know how to reliably quantify possibilities such as methane hydrate release or even meridional overturning circulation disruption. These things remain possibilities, and live areas of research, but in the absence of reliable quantification it is difficult to know how to incorporate such possibilities into our thinking about the more immediate and obvious problem of twenty-first century climate change.\n\n13.5 Defining dangerous climate change\n\nWhile this book has drawn the distinction between terminal and endurable risks (see Chapter 1), limits to acceptable climate change risk have usually been couched in terms of ‘dangerous’ risk. The United Nations Framework Convention on Climate Change (UNFCCC), outlines a framework through which nations might address climate change. Under the Convention, governments share data and try to develop national strategies for reducing GHG emissions while also adapting to expected impacts. Central to the convention, in article 2, is the intention to stabilize GHG concentrations (and hence climate):\n\nThe ultimate objective of this Convention and any related legal instruments that the Conference of the Parties may adopt is to achieve, in accordance with the relevant provisions of the Convention, stabilization of greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic interference with the climate system.\n\nArticle 2 introduced the problematic but perhaps inevitable concept of ‘dangerous anthropogenic interference’ (DAI) in the climate system into the scientific and policy communities. DAI has been much discussed, with many quite simple studies presenting it, to first order, in terms of possible global mean temperature thresholds (O’Neill and Oppenheimer, 2002; Mastrandrea and Schneider, 2004), or equilibrium responses to some indefinite atmospheric concentration of CO₂ (Nordahus, 2005), although some commentators provide more complete discussions of regional or not-always-temperature based metrics of DAI (Keller et al., 2005; Oppenheimer, 2005).\n\nIt has become usual to distinguish between two, ideally complementary ways of dealing with the threats posed by climate change: mitigation and adaptation.\n\nThe UNFCCC’s glossary ¹ describes mitigationas follows:\n\nIn the context of climate change, a human intervention to reduce the sources or enhance the sinks of greenhouse gases. Examples include using fossil fuels more efficiently for industrial processes or electricity generation, switching to solar energy or wind power, improving the insulation of buildings, and expanding forests and other ‘sinks’ to remove greater amounts of carbon dioxide from the atmosphere.\n\nWhile adaptationis:\n\nAdjustment in natural or human systems in response to actual or expected climatic stimuli or their effects, which moderates harm or exploits beneficial opportunities.\n\nInternationally, the focus of most climate policy has been on establishing targets for mitigation. The Kyoto Protocol, for instance, set ‘binding’ targets for emissions reductions for those industrialized countries which chose to ratify it. This is changing as nations, communities, and businesses (1) come to appreciate the difficulty of the mitigation coordination problem and (2) realize that they are beginning to need to adapt their practices to a changing climate.\n\nIn the general case, what is dangerous depends on who one is, and what one is concerned about. Local thresholds in temperature (for coral reefs, say), integrated warming (polar bears), or sea level (for small island states) rise, to take a few examples, may constitute compelling metrics for regional or highly localized DAI, but it is not obvious that they warrant a great deal of global consideration. Moreover, the inertia in the system response implies that we are already committed to a certain level of warming: the climate of the decade between 2020 and 2030 is essentially determined by the recent accumulation of GHG and no realistic scenario for the mitigation will alter that. Thus, as well as mitigating against the worst aspects of centennial timescale temperature rise, we also need to learn to adapt to climate change. Adaptation is inherently a regional or localized response, since the effects of climate change – though both most discernable and best constrained at the global level – actually affect people’s lives at regional scales. Although mitigation is usually thought of as a genuinely global problem requiring a global scale response, adaptation is offers more opportunity for nation states and smaller communities to exercise effective climate policy. Adaptation alone is not expected to be sufficient to deal with climate change in the next century, and is best seen as part of a combined policy approach (along with mitigation) that allows us to avoid the very worst impacts of climate change, while adapting to those aspects that we are either already committed to, or for which the costs of mitigation would be too high.\n\n13.6 Regional climate risk under anthropogenic change\n\nThe IPCC’s Fourth Assessment Report highlights a number of expected changes at regional scales. These include the following:\n\n• Extra-tropical storm tracks are expected to move polewards.\n\n• Hot extremes are expected to increase in frequency.\n\n• Hurricanes are expected to become more frequent, with more intense precipitation.\n\n• Warming is expected over land areas, especially over the continents, and is also expected to be strong in the Arctic.\n\n• Snow cover and sea-ice are expected to decrease.\n\n• Patterns of precipitation are generally expected to intensify.\n\nThese findings are qualitatively similar to those highlighted in the IPCC’s Third Assessment Report, though some of the details have changed, and in many cases scientists are more confident of the predicted impacts.\n\nImpacts generally become more uncertain as one moves to smaller spatio-temporal scales. However, many large-scale processes, such as El Nino Southern Oscillation and the North Atlantic Oscillation, are crucially dependent on small-scale features such as convective systems over the maritime continent, and patterns Atlantic sea-surface temperature, respectively. Given that the El Nino Southern Oscillation and the North Atlantic Oscillation are key determinants of variability and climate-related impacts on seasonal-to-decadal timescales, good representation of these features will become an essential part of many regions’ adaptation strategies.\n\nAdaptation strategies can take many forms. They can be based on strategies that take different sorts of stakeholder relationships with the impacts into account. Depending on the nature of one’s exposure, risk profile, and preferences, one may opt to base one’s strategy on one, or a mix, of several different principles, including: some reading of the precautionary principle, traditional cost-benefit analysis (usually where one is confident of the numbers underpinning the analysis) or robustness against vulnerability. Other adaptive principles have also been discussed, including novel solutions such as legal remedies through climate detection and attribution studies, which is like cost-benefit analysis, though backward looking (at actual attributable harm done) and done via the legal system rather than through economics (Allen et al., 2007). As the choice of principle, tools and policies tend to be highly situation specific, there tends to be a great diversity in adaptation strategies, which contrasts with the surprisingly narrow band of mitigation policies that have entered the public domain.\n\nApart from a range of possible principles guiding the adaptation strategy, the range of possible responses is also very large. Technological responses include improved sea and flood defences, as well as improved air-conditioning systems; policy responses include changes in zoning and planning practices (to avoid building in increasingly fire- or flood-prone areas, for instance); managerial responses include changes in firm behaviour – such as planting new crops or shifts in production- in anticipation of the impacts associated with climate change; and behavioural responses include changes in patterns of work and leisure and changes in food consumption.\n\nIn traditional risk analysis, the risk of an event is defined as the costs associated with its happening multiplied by the probability of the event. However, identifying the costs associated with adapting to climate change is deeply problematic, since (1) the damages associated with climate change are extremely hard to quantify in the absence of compelling regional models and (2) the economic future of the community implementing the adaptive strategy is highly uncertain. As the IPCC AR4 states:\n\nAt present we do not have a clear picture of the limits to adaptation, or the cost, partly because effective adaptation measures are highly dependent on specific, geographical and climate risk factors as well as institutional, political and financial constraints. p. 19\n\nAn alternative strategy, and one that seems to be gaining popularity with the adaptation community, is the development of ‘robust’ strategies that seek to minimize the vulnerability of a community to either climate change, or even natural climate variability. Because of the difficulty in quantifying the costs of adaptation and likely development pathways, strategies emphasizing resilience are gaining in popularity.\n\n13.7 Climate risk and mitigation policy\n\nThe default science position from which to think about climate mitigation policy has been the simple linear model of anthropogenic warming along with the expected solar and volcanic forcings, which remains basically the same (in the long-run) even in the presence of large volcanoes that we might expect only once in every 1000 years. This is usually coupled with a cost-benefit analysis of some kind to think about what sort of policy we should adopt. However, many of the hard to quantify possibilities that would disrupt this linear picture of climate change are often thought to lead to dramatic changes in our preferred policy responses to twenty-first century climate change.\n\nThe basic problem of anthropogenic climate change is that we are releasing too much carbondioxide ² into the atmosphere. The principal aspect of the system we can control is the amount of CO₂ we emit.³ This is related to the temperature response (a reasonable proxy for the climate change we are interested in) by the chain emissions concentrations forcings temperature response, or in terms of the equations we introduced above, (13.2) – (13.3) – (13.1). Various aspects of this chainare subject to considerable scientific uncertainty. The mapping between emissions and concentrations is subject to carbon cycle uncertainties on the order of 30% of the emissions themselves, for a doubling of CO₂ (Huntingford and Jones, 2007); and though the concentrations to forcings step seems reasonably well behaved the forcing-response step is subject to large uncertainties surrounding the values of the parameters in Equation (13.1) (Hegerl et al., 2006). So eventhough decision makers are interested in avoiding certain aspects of the response, they are stuck with a series of highly uncertain inferences in order to calculate the response for a given emissions path.\n\nIn order to control CO₂ emissions, policy makers can prefer price controls (taxes) or quantity controls (permits). Quantity controls directly control the amount of a pollutant released into a system, whereas price controls do not. In the presence of uncertainty, if one fixes the price of a pollutant, one is uncertain about the quantity released; whereas if one fixes the quantity, one is uncertain about how to price the pollutant. Weitzman (1974) showed how, in the presence of uncertainty surrounding the costs and benefits of a good, a flat marginal benefit curve, compared to the marginal cost curve, leads to a preference for price regulation over quantity regulation. If anthropogenic climate change is as linear as it seems under the simple model mentioned above, as most studies have quite reasonably assumed to be the case, then a classical optimal cost-benefit strategy suggests that we ought to prefer price controls (Nordhaus, 1994; Roughgarden and Schneider, 1997, for instance). Intuitively, this makes sense: if the marginal benefits of abatement are a weaker function of quantity than the marginal costs are, then it suggests getting the price right is more important than specifying the quantity, since there is no reason to try to hit some given quantity regardless of the cost. If, however, the marginal benefits of abatement are a steep function of quantity, then it suggests that specifying the quantity is more important, since we would rather pay a sub-optimal amount than miss our environmental target.\n\nThis sort of intuition has implied that our best-guess, linear model of climate change leads to a preference for price controls. At the same time, if it were shown that catastrophic disruptions to the climate system were a real threat, it would lead us to opt instead for quantity controls. Ideally, we would like to work out exactly how likely catastrophic processes are, factor these into our climate model, and thus get a better idea of what sort of mitigation policy instrument to choose. However, catastrophic surprises are difficult to incorporate into our thinking about climate change policy in the absence of strong scientific evidence for the reliable calculation of their likelihood. Our most comprehensive models provide scant evidence to support the idea that anthropogenic climate change will provide dramatic disruptions to the reasonably linear picture of climate change represented by the simple EBM presented above (Meehl et al., 2007). At the same time, we know that there are plenty of parts of the Earth System that could, at least in theory, present catastrophic risks to human kind. Scientists attempt to model these possibilities, but this task is not always well-posed, because (1) in a system as complex as the Earth system it is can be hard to know if the model structure is representative of the phenomena one is purporting to describe and (2) data constraints are often extremely weak. Subjective estimates of the reduction in strength of the Atlantic meridional overturning circulation in response to a 4° C global mean temperature rise put the probability of an overturning circulation collapse at anywhere between about 0% and 60% (Zickfeld et al., 2007). Worse, the ocean circulation is reasonably well understood in comparison to many of the other processes we considered above in Section 13.4: we have very few constraints to help us quantify the odds of a clathrate gun event, or to know just how much tropical forest dieback or permafrost methane release to expect. Attempting to quantify the expected frequency or magnitude of some of the more extreme possibilities even to several magnitudes is difficult, in the presence of multiple uncertainties.\n\nTherefore, while the science behind our best expectations of anthropogenic climate change suggests a comparatively linear response, warranting a traditional sort of cost-benefit, price control-based strategy, catastrophe avoidance suggests a quite different, more cautious approach. However, we find it hard to work out just how effective our catastrophe avoidance policy will be, since we know so little about the relative likelihood of catastrophes with or without our catastrophe avoidance policy.\n\n13.8 Discussion and conclusions\n\nDifferent policy positions are suggested depending on whether twenty-first century climate change continues to be essentially linear, in line with recent temperature change and our best-guess expectations, or disruptive in some new and possibly catastrophic way. In the first case, as we have seen, price controls are preferable since there’s no particular reason to need to avoid any given quantity of atmospheric GHG; in the second, we are trying to remain under some threshold, and allow prices to adjust in response. Ideally, science would tell us what that threshold is, but as we have seen, this is a difficult, and elusive, goal. Furthermore, quantity controls remain indirect. The thing we wish to avoid is dangerous anthropogenic interference in the climate system. Yet we cannot control temperature, we can control only anthropogenic emissions of GHG, and these map to temperature increases through lagged and uncertain processes. This matters because it affects the way we construct policy. European policy makers, who have been at the forefront of designing policy responses to the threat of anthropogenic warming, generally back-infer acceptable ‘emissions pathways’ by inverting, starting with an arbitrarily chosen temperature target, and then attempting to invert the equations in this chapter. This bases short- and medium-term climate policy on the inversion of a series of long-term relationships, each of which is uncertain.\n\nDangerous, possibly even catastrophic, climate change is a risk, but it is exceptionally hard to quantify. There is evidence that the climate has changed radically and disruptively in the past. While its true that a global mean temperature rise of 4^(°)C, would probably constitute dangerous climate change on most metrics, it would not necessarily be catastrophic in the sense of being a terminal threat. The challenges they suggest our descendents will face look tough, but endurable.\n\nIn terms of global climate, potential catastrophes and putative tipping points retain a sort of mythic aspect: they are part of a useful way of thinking about potentially rapid surprises in a system we do not fully understand, but we find it hard to know just how to incorporate these suspicions into our strategies. We cannot reliably factor them into our decision algorithms because we have such a poorly quantified understanding of their probability. We need to investigate the mechanisms more thoroughly to get a better handle on their amplitude, triggering mechanisms, and likelihood. There is no unique and uncontroversial way to bring these concerns within the realm of climate forecasting, nor, especially, our decision-making processes.\n\nSuggestions for further reading\n\nPizer, W.A. http://www.ss.org.ss/Documents/RII-DP-03-31.pdg. Climate Change Catastrophes, Discussion Paper 03–31, Resources for the Future, Washington, DC, 2003. Provides a nice summary of the ways in which catastrophic risks can change the choice of climate mitigation policy instrument.\n\n• Stern Review. The most comprehensive economic review yet undertaken into the http://www.hm-treasury.gov.uk/independant\\_reviews/stern\\_review\\_economics\\_climate\\_change/stern\\_review\\_report.cfm problem of climate change. Although controversial in some parts, the Stern review is probably the most important document on climate change produced in the last few years: it has moved the discourse surrounding climate change away from scientific issues towards policy responses.\n\n• IPCC AR4 Summary for Policy Makers. WG1 provides a comprehensive review of the state of climate change science. To a large extent it echoes the previous assessment (2001).\n\n• S.H. Schneider, A. Rosencranz, J.O. Niles 2002, climate change policy: A A survey Island Press. Schneider et al. on climate policy. Provides a nice one-volume synthesis of the physical and impacts science behind climate change alongside possible policy responses. Chapters solicited from a wide range of experts.\n\n• Schellnhuber reference discusses the state of science and some policy thinking regarding our ability to meet the aims of the UNFCCC and avoid dangerous climate change. Provides a nice selection of papers and think pieces by many of the leading commentators in the field.\n\n• Brooks. Something of an underappreciated gem, this paper combines the intriguing uncertainty of palaeoclimate research with a fascinating – if necessarily speculative – discussion of the developments of early civilizations around the globe. An interesting scientific and social look at how social responses to climate change can succeed or fail.\n\nReferences\n\nAndronova, N., Schlesinger, M., Hulme, M., Desai, S., and Li, B. (2006). The concept of climate sensitivity: history and development. In Schlesinger, M.E., Kheshgi, H.S., Joel Smith, dela Chesnaye, F.C., Reilly, J.R., Wilson, T, Kolstad, C., (eds.), Human-Induced Climate Change: An Interdisciplinary Assessment (Cambridge: Cambridge University Press).\n\nBenton, M.J. and Twitchett, R.J. (2003). How to kill (almost) all life: the end-Permian extinction event. Trend Ecol. Evol., 18(7), 358–365.\n\nBrooks, N. (2006). Cultural responses to aridity in the Middle Holocene and increased social complexity. Quat. Int., 151, 29–49.\n\nClaussen, M., Mysak, L.A., Weaver, A.J., Crucifix, M., Fichefet, T., Loutre, M.-F., Weber, S.L., Alcamo, J., Alexeev, V.A., Berger, A., Calov, R., Ganopolski, A., Goosse, H., Lohman, G., Lunkeit, F., Mokhov, Petoukhov, V., Stone, P., and Wang, Zh. (2002). Earth system models of intermediate complexity: closing the gap in the spectrum of climate models. Clim. Dynam., 18, 579–586.\n\nCox, P.M., Richard A. Betts, Chris D. Jones, Steven A. Spall and Ian J. Totterdell (9 November, 2000). Acceleration of global warming due to carbon cycle feedbacks in a coupled climate model. Nature, 408(6809), 184–187.\n\nCox, P.M., Betts, R.A., Collins, M., Harris, P.P., Huntingford, C., and Jones, C.D. (2004). Amazonian forest dieback under climate-carbon cycle projections for the 21st century. Theor. Appl. Climatol., 78, 137–156.\n\nCrowley, T.J. (2000). Causes of climate change over the past 1000 years. Science, 289, 270–277.\n\nCrucifix, M. (2006). Does the last glacial maximum constrain climate sensitivity? Geophys. Res. Lett., 33(18), L18701-L18705.\n\nCrutzen, P.J. (2006). Albedo enhancement by stratospheric sulphur injections: a contribution to resolve a policy dilemma? Clim. Change, 77, 211–220.\n\nDallimore, S.R. and Collett, T.S. (1995). Intrapermafrost gas hydrates from a deep corehole in the Mackenzie Delta, Northwest-Territories, Canada. Geology, 23(6), 527–530, 1995.\n\nDiamond, J. (2005). Collapse (Penguin).\n\nFlannery, T. (1994) The Future Eaters (Grove Press).\n\nForest, C.E., Stone, P.H., Sokolov, A.P., Allen, M.R., and Webster, M.D. (2002). Quantifying uncertainties in climate system properties with the use of recent climate observations. Science, 295, 113–117.\n\nGovindasamy, B. and Caldeira, K. (2000). Geoengineering Earth’s Radiation Balance to Mitigate CO₂-Induced Climate Change. Geophys. Res. Lett., 27, 2141–2144.\n\nHansen, J., Russell, G., Lacis, A., Fung, I., and Rind, D. (1985). Climate response times: dependence on climate sensitivity and ocean mixing. Science, 229, 857–859.\n\nHegerl, G.C., Zwiers, F.W., Braconnot, P., Gillett, N.P., Luo, Y., Marengo Orsini, J.A., Nicholls, N., Penner, J.E., and Stott, P.A. (2007): Understanding and Attributing Climate Change. In: Climate Change 2007: The Physical Science Basis. Contribution of Working Group I to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change [Solomon, S., Qin, D., Manning, M., Chen, Z., Marquis, M., Averyt, K.B., Tignor, M., and Miller, H.L. (eds.)], Cambridge University Press, United Kingdom and New York, NY, USA.\n\nHegerl, G.C., Crowley, T.J., Hyde, W.T. and Frame, D.J. (2006). Climate sensitivity constrained by temperature reconstructions over the past seven centuries. Nature, 440, 1029–1032.\n\nHoughton, J.T., Ding, Y., Griggs, D.J., Noguer, M., van der Linden, P.J., Dai, X., Maskell, K., and Johnson, C.A., (eds.). (2001). Climate Change 2001: The Science of Climate Change (Cambridge: Cambridge University Press).\n\nHyde, W.T. and Crowley, T.J. (2000). Probability of future climatically significant volcanic eruptions. J. Clim., 13, 1445–1450.\n\nKeith, D.W. (2007). Engineering the planet. In Schneider, S. and Mastrandrea, M. (eds.), Climate Change Science and Policy (Island Press).\n\nKeith, D.W. (2001). Geoengineering. Nature, 409, 420.\n\nKiehl, J.T. (2006). Geoengineering climate change: treating the symptom over the cause? Clim. Change, 77, 227–228. Kemp, M. (2005). Science in culture: inventing an icon. Nature, 437, 1238.\n\nKeller, K., Hall, M., Kim, S., Bradford, D.F., and Oppenheimer, M. (2005). Avoiding dangerous anthropogenic interference with the climate system. Clim. Change, 73, 227–238.\n\nKennett, J.P., Cannariato, K.G., Hendy, I.L., and Behl, R.J. (2000). Carbonisotopic evidence for methane hydrate instability during Quaternary interstadials. Science, 288, 128–133.\n\nLevitus, S., Antonov, J., and Boyer, T. (2005). Warming of the world ocean, 1955–2003. Geophys. Res. Lett., 32, L02604.\n\nLenton, T.M. et al. (2006). The GENIE team, Millennial timescale carbon cycle and climate change inanefficient Earth system model. Clim. Dynam., 26, 687–711.\n\nLenton, T.M., Held, H., Hall, J.W., Kriegler, E., Ludd, W., Rahmstorf, S., and Schellnhuber, H.J. (2007). Tipping Elements in the Earth System. Proc. Natl. Acad. Sci., 1.\n\nLindzen, R.S., Chou, M.-D., and Hou, A.Y. (2001). Does the Earth have an adaptive iris? Bull. Am. Meteorol. Soc., 82, 417–432.\n\nLockwood, M. and Frôhlich, C. (2007). Recent oppositely directed trends in solar climate forcings and the global mean surface air temperature. Proc. R. Soc. A., 10.1098/rspa.2007.1880.\n\nMaslin, M., Owen, M., Day, S., and Long, D. (2004). Linking continental-slope failures and climate change: testing the clathrate gun hypothesis. Geology, 32, 53–56.\n\nMason, B.G., Pyle, D.M., and Oppenheimer, C. (2004). The size and frequency of the largest explosive eruptions on Earth. Bull. Volcanol., 66(8), 735–748.\n\nMastrandrea, M. and Schneider, S. (2004). Probabilistic integrated assessment of “dangerous” climate change. Science, 304, 571–575.\n\nMastrandrea, M. and Schneider, S. (2005). Probabilistic assessment of dangerous climate change and emission scenarios. In Schellnhuber, H.-J. et al. (eds.), Avoiding Dangerous Climate Change (DEFRA).\n\nMeehl, G.A., Stocker, T.F., Collins, W., Friedlingstein, P., Gaye, A., Gregory, J., Kitoh, A., Knutti, R., Murphy, J., Noda, A., Raper, S., Watterson, I., Weaver, A., and Zhao, Z.-C. (2007). Global climate projections Climate Change 2007: The Physical Science Basis. Contribution of Working Group I to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change, S. Solomon, D. Qin, and M. Manning, Eds., Cambridge University Press.\n\nMeinshausen, M. (2005). On the risk of overshooting 2° C. In Schelln huber, H.-J. et al. (eds.), Avoiding Dangerous Climate Change (DEFRA).\n\nMurphy, J., David M.H. Sexton, David N. Barnett, Gareth S. Jones, Mark J. Webb, Matthew Collins &David A. Stainforth, (2004). Quantification of modelling uncertainties in a large ensemble of climate change simulations. Nature, 430, 768–772.\n\nNordhaus, W.D. (1994). Managing the Global Commons. The Economics of Climate Change, MIT Press, Cambridge, MA.\n\nNordhaus, W. (2005). Life After Kyoto: Alternative Approaches to Global Warming Policies, National Bureau of Economic Research, Working Paper 11889.\n\nO’Neill, B.C. and Oppenheimer, M. (2004). Climate change impacts are sensitive to the concentration stabilization path. Proc. Natl. Acad. Sci., 101, 16411–16416.\n\nOppenheimer, M. (2005). Defining dangerous anthropogenic interference: the role of science, the limits of science. Risk Anal., 25, 1–9.\n\nOrr, J.C., Victoria J. Fabry, Olivier Aumont, Laurent Bopp, Scott C. Doney, Richard A. Feely, Anand Gnanadesikan, Nicolas Gruber, Akio Ishida, Fortunat Joos, Robert M. Key, Keith Lindsay, Ernst Maier-Reimer, Richard Matear, Patrick Monfray, Anne Mouchet, Raymond G. Najjar, Gian-Kasper Plattner, Keith B. Rodgers, Christopher L. Sabine, Jorge L. Sarmiento, Reiner Schlitzer, Richard D. Slater, Ian J. Totterdell, Marie-France Weirig, Yasuhiro Yamanaka and Andrew Yool, (2005). Anthropogenic ocean acidification over the twenty-first century and its impact on calcifying organisms. Nature, 437, 681–686.\n\nPinto, J.P., Turco, R.P., and Toon, O.B. (1989) Self-limiting physical and chemical effects in volcanic eruption clouds. J. Geophys. Res., 94, 11165–11174.\n\nPizer, W.A. (2003). Climate change catastrophes. Discussion Paper 03–31, Resources for the Future, Washington, DC.\n\nRoughgarden, T. and Schneider, S.H. (1997). Climate change policy: quantifying uncertainties for damage and optimal carbon taxes. Energy Policy, 27(7), 371 -434.\n\nSazonova, T.S., Romanovsky, V.E., Walsh, J.E., and Sergueev, D.O. (2004). Permafrost dynamics in the 20th and 21st centuries along the East Siberian transect. J. Geophys. Res.-Atmos., 109, D1.\n\nSchellnhuber, H.-J. (ed.). (2006). Avoiding Dangerous Climate Change (Cambridge, UK: Cambridge University Press).\n\nSchneider von Deimling, T., Held, H., Ganopolski, A., and Rahmstorf, S. (2006). Climate sensitivity estimated from ensemble simulations of glacial climate. Clim. Dynam., 27, 149.\n\nSolomon, S. et al. (2007). IPCC: Summary for Policymakers. In Solomon, S., Qin, D., Manning, M., Chen, Z., Marquis, M., Averyt, K.B., Tignor, M., and Miller, H.L. (eds.), Climate Change 2007: The Physical Science Basis. Contribution of Working Group I to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change (Cambridge, UK: Cambridge University Press and New York, NY, USA).\n\nWeitzman, M.L., (1974). Prices vs. Quantities in: Review of Economic Studies, 41(4), 477–491.\n\nZickfeld, K., Levermann, A., Morgan, M.G., Kuhlbrodt, T., Rahmstorf, S., and Keith, D. Expert judgements on the response of the Atlantic meridional overturning circulation to climate change. Clim. Change, accepted.\n\n• 14 • Plagues and pandemics: past, present, and future\n\nEdwin Dennis Kilbourne\n\n14.1 Introduction\n\nThis chapter is about pandemics, a somewhat ambiguous term, defined in the Oxford English Dictionary as ‘a disease prevalent throughout a country, a continent, or the world’. In present modern usage the term takes greater cognizance of its original Greek derivation and is largely restricted to global prevalence (pan demos) – all people. The same source tells us that plague has a broader meaning, implying a sudden unexpected event that is not necessarily a disease, but introducing the concept of acute, lethal, and sudden danger-characteristics that are connoted but not specifically denoted by the term ‘pandemic’.\n\nIt will become apparent that glimpses of the future must consider the emergence of new pathogens, the re-emergence of old ones, the anthropogenic fabrication of novel agents, and changes in the environment and in human behaviour. In other words ‘the problem’ in addressing infectious disease threats is not one but many separable problems, each of which must be isolated in traditional scientific fashion and separately evaluated as components of what I like to call ‘holistic epidemiology’. This emerging discipline comprises microbial and human genetics, human behaviour, global ecology, toxicology, and environmental change.\n\n14.2 The baseline: the chronic and persisting burden of infectious disease\n\nAs we leave our mothers’ wombs and enter this vale of tears (and sometimes before) we are invaded by microbes that may become our lifelong companions, profiting from this intimate relationship by the food and shelter that our bodies offer. They, in turn, often provide us with nutrients or vitamins derived from their own metabolic processes and may even immunize us against future assaults by related but less kindly microbes. In other words, we and they (usually) coexist in a state of armed neutrality and equilibrium.\n\nBut humans bear a chronic burden of infectious diseases. Included in this burden are some diseases that have demonstrated a capacity to break out in pandemic form, depending on the circumstances that are defined later. The less overt contributors to human misery will be briefly reviewed before discussing the nature of the acute aberrations that comprise the more dramatic pandemics and plagues that suddenly burst forth in a catastrophic manner.\n\nBeginning at the end of the nineteenth century and culminating in the middle of the twentieth, the battle with infections seemed allied with the recognition of their microbial cause and the consequent development of vaccines, environmental sanitation, and later, antimicrobial drugs. In high income regions the accustomed childhood infections became a rarity with the development of vaccines for diphtheria, pertussis, measles, rubella, varicella, mumps, and poliomyelitis. It is paradoxical that poliomyelitis, or ‘infantile paralysis’, emerged as a consequence of the improved sanitation of food and water that postponed infection of infants and young children to later life when susceptibility to paralysis increases (Horstmann, 1955).\n\nImportant recent studies in which population census and income level of different regions have been determined have documented the expected, that is, that the price of poverty is an increased burden of infection – particularly of the intestinal tract – and that even diseases such as tuberculosis and measles for which there are drugs and vaccines continue to burden a large part of the world. The childhood mortality from measles in Africa is expected to reach half a million in 2006 and accounts for 4% of the total deaths of children each year, and yet the disease is a current and logical target for eradication because, as was the case with smallpox, the virus has no venue other than humans, and the present vaccine is highly effective.\n\nTuberculosis, an ancient disease never adequately suppressed by vaccine or antibiotics, is now resurgent, aided by the new, cryptic, and terrible challenge: HIV/AIDS. In regions of poor environmental sanitation, which include much of sub-Saharan Africa, diarrhoeal diseases persist, killing millions each year, mostly children.\n\nWe are not writing here about abstruse, unsolved scientific and medical problems but obvious economic ones. This is nowhere better documented than in the rates of perinatal mortality in low and high income groups; a disparity of more than 12-fold (according to WHO statistics for 2001). The unglamorous but nonetheless deadly infantile diarrhoeas and ‘lower respiratory infections’ are among the commonplace diseases that encumber humankind in poverty-stricken areas of the globe. The leading infectious causes of death in 2002 (comprising almost 20% of all causes) are shown in Table 14.1. About 75% of all deaths from infectious diseases are geographically localized to Southeast Asia and sub-Saharan Africa.\n\nTable 14.1 Leading Global Causes of Deaths Due to Infectious Diseases¹\n\n[Image]\n\n14.3 The causation of pandemics\n\nAlthough the proximate causes of pandemics are microbial or viral, there are ancillary causes of almost equal importance. These include human behaviour, season of the year and other environmental conditions, and the state of induced or innate immunity. These will be discussed in the context of specific disease paradigms.\n\nIn the short list of causes of past plagues identified in Table 14.2 are the viruses of smallpox, influenza and yellow fever, the bacterial causes of plague, cholera and syphilis, and the protozoan cause of malaria.\n\n14.4 The nature and source of the parasites\n\nAdhering to this ‘short list’ pro temps, the sources of these pathogens in nature are illustrative of the variety of mechanisms by which they survive, and often flourish. Only one, the cholera Vibrio, is capable of independently living freely in the environment. The plague bacillus (Yersinia pestis) has its primary home in rodents, in which its spread is facilitated through the bites of rat fleas, which may seek food and shelter in humans when the infection has killed their rodent hosts. The Plasmodia of malaria have a complicated life cycle in which their multiplication in both vertebrates (including humans) and mosquitoes is required. The arbovirus cause of yellow fever is transmitted from one to another (in the case of urban yellow fever in humans) by mosquitoes that act essentially as flying hypodermic needles, or, in the case of jungle yellow fever, from animals (principally monkeys) via mosquitoes.\n\nTable 14.2 Some Examples of Historically Significant Pandemics and Epidemics\n\n[Image]\n\n14.5 Modes of microbial and viral transmission\n\nMicrobes (bacteria and protozoa) and viruses can enter the human body by every conceivable route: through gastrointestinal, genitourinary, and respiratory tract orifices, and through either intact or injured skin. In what is known as vertical transmission the unborn infant can be infected by the mother through the placenta.\n\nTable 14.3 A Comparison of Prototype Pandemic Agents\n\n[Image]\n\nOf these routes, dispersal from the respiratory tract has the greatest potential for rapid and effective transmission of the infecting agent. Small droplet nuclei nebulized in the tubular bronchioles of the lung can remain suspended in the air for hours before their inhalation and are not easily blocked by conventional gauze masks. Also, the interior of the lung presents an enormous number of receptors as targets for the entering virus. For these reasons, influenza virus currently heads the list of pandemic threats (Table 14.3).\n\n14.6 Nature of the disease impact: high morbidity, high mortality, or both\n\nIf a disease is literally pandemic it is implicit that it is attended by high morbidity, that is, many people become infected, and of these most become ill – usually within a short period of time. Even if symptoms are not severe, the sheer load of many people ill at the same time can become incapacitating for the function of a community and taxing for its resources. If a newly induced infection has a very high mortality rate, as often occurs with infections with alien agents from wild animal sources, it literally reaches a ‘dead end’: the death of the human victim.\n\nSmallpox virus, as an obligate parasite of humans, when it moved into susceptible populations, was attended by both a high morbidity and a high mortality rate, but it was sufficiently stable in the environment to be transmitted by inanimate objects (fomites) such as blankets. (Schulman and Kilbourne, 1963).\n\nInfluenza, in the terrible pandemic of 1918, killed more than 20 million people but the overall mortality rate rarely exceeded 23% of those who were sick.\n\n14.7 Environmental factors\n\nEven before the birth of microbiology the adverse effects of filthy surroundings on health, as in ‘The Great Stink of Paris’ and of ‘miasms’, were assumed. But even late in the nineteenth century the general public did not fully appreciate the connection between ‘germs and filth’ (CDC, 2005). The nature of the environment has potential and separable effects on host, parasite, vector (if any), and their interactions. The environment includes season, the components of weather (temperature and humidity), and population density. Many instances of such effects could be cited but recently published examples dealing with malaria and plague are relevant. A resurgence of malaria in the East African highlands has been shown to be related to progressive increases in environmental temperature, which in turn have increased the mosquito population (Oldstone, 1998; Saha et al., 2006). In central Asia plague dynamics are driven by variations in climate as rising temperature affects the prevalence of Yersinia pestis in the great gerbil, the local rodent carrier. It is interesting that ‘climatic conditions favoring plague apparently existed in this region at the onset of the Black Death as well as when the most recent plague (epidemic) arose in the same region (Ashburn, 1947).\n\nDifferences in relative humidity can affect the survival of airborne pathogens, with high relative humidity reducing survival of influenza virus and low relative humidity (indoor conditions in winter) favouring its survival in aerosols (Simpson, 1954). But this effect is virus dependent. The reverse effect is demonstrable with the increased stability of picornaviruses in the high humidity of summer.\n\n14.8 Human behaviour\n\nOne has no choice in the inadvertent, unwitting contraction of most infections, but this is usually not true of sexually transmitted diseases (STD). Of course one never chooses to contract a venereal infection, but the strongest of human drives that ensures the propagation of the species leads to the deliberate taking of risks – often at considerable cost, as biographer Boswell could ruefully testify. Ignorant behaviour, too, can endanger the lives of innocent people, for example when parents eschew vaccination in misguided attempts to spare their children harm. On the other hand, the deliberate exposure of young girls to rubella (German measles) before they reached the child-bearing age was in retrospect a prudent public health move to prevent congenital anomalies in the days before a specific vaccine was available.\n\n14.9 Infectious diseases as contributors to other natural catastrophes\n\nSudden epidemics of infection may follow in the wake of non-infectious catastrophes such as earthquakes and floods. They are reminders of the dormant and often unapparent infectious agents that lurk in the environment and are temporarily suppressed by the continual maintenance of environmental sanitation or medical care in civilized communities. But developing nations carry an awesome and chronic load of infections that are now uncommonly seen in developed parts of the world and are often thought of as diseases of the past. These diseases are hardly exotic but include malaria and a number of other parasitic diseases, as well as tuberculosis, diphtheria, and pneumococcal infections, and their daily effects, particularly on the young, are a continuing challenge to public health workers.\n\nThe toll of epidemics vastly exceeds that of other more acute and sudden catastrophic events. To the extent that earthquakes, tsunamis, hurricanes, and floods breach the integrity of modern sanitation and water supply systems, they open the door to water-borne infections such as cholera and typhoid fever. Sometimes these illnesses can be more deadly than the original disaster. However, recent tsunamis and hurricanes have not been followed by expected major outbreaks of infectious disease, perhaps because most of such outbreaks in the past occurred after the concentration of refugees in crowded and unsanitary refugee camps. Hurricane Katrina, which flooded New Orleans in 2005, left the unusual sequelae of non-cholerogenic Vibrio infections, often with skin involvement as many victims were partially immersed in contaminated water (McNeill, 1976). When true cholera infections do occur, mortality can be sharply reduced if provisions are made for the rapid treatment of victims with fluid and electrolyte replacement. Azithromycin, a new antibiotic, is also highly effective (Crosby, 1976a).\n\n14.10 Past Plagues and pandemics and their impact on history\n\nThe course of history itself has often been shaped by plagues or pandemics.¹ Smallpox aided the greatly outnumbered forces of Cortez in the conquest of the Aztecs (Burnet, 1946; Crosby, 1976b; Gage, 1998; Kilbourne, 1981; Wikipedia), and the Black Death (bubonic plague) lingered in Europe for three centuries (Harley et al., 1999), with a lasting impact on the development of the economy and cultural evolution. Yellow fever slowed the construction of the Panama Canal (Benenson, 1982). Although smallpox and its virus have been eradicated, the plague bacillus continues to cause sporadic deaths in rodents, in the American southwest, Africa, Asia, and South America (Esposito et al., 2006). Yellow fever is still a threat but currently is partially suppressed by mosquito control and vaccine, and cholera is always in the wings, waiting – sometimes literally – for a turn in the tide. Malaria has waxed and waned as a threat, but with the development of insecticide resistance to its mosquito carriers and increasing resistance of the parasite to chemoprophylaxis and therapy, the threat remains very much with us (Table 14.1).\n\nOn the other hand, smallpox virus (Variola) is an obligate human parasite that depends in nature on a chain of direct human-to-human infection for its survival. In this respect it is similar to the viral causes of poliomyelitis and measles. Such viruses, which have no other substrates in which to multiply, are prime candidates for eradication. When the number of human susceptibles has been exhausted through vaccination or by natural immunization through infection, these viruses have no other place to go.\n\nInfluenza virus is different from Variola on several counts: as an RNA virus it is more mutable by three orders of magnitude; it evolves more rapidly under the selective pressure of increasing human immunity; and, most important, it can effect rapid changes by genetic re-assortment with animal influenza viruses to recruit new surface antigens not previously encountered by humans to aid its survival in human populations (Kilbourne, 1981). Strangely, the most notorious pandemic of the twentieth century was for a time almost forgotten because of its concomitance with World War I (Jones).\n\n14.11 Plagues of historical note\n\n14.11.1 Bubonic plague: the Black Death\n\nThe word ‘plague’ has both specific and general meanings. In its specific denotation plague is an acute infection caused by the bacterium Yersinia pestis, which in humans induces the formation of characteristic lymph node swellings called ‘buboes’ -hence ‘bubonic plague’. Accounts suggestive of plague go back millennia, but by historical consensus, pandemics of plague were first clearly described with the Plague of Justinian in AD 541 in the city of Constantinople. Probably imported with rats and their s in ships bearing grain from either Ethiopia or Egypt, the disease killed an estimated 40% of the city’s population and spread through the eastern Mediterranean with almost equal effect. Later (AD 588) the disease reached Europe, where its virulence was still manifest and its death toll equally high. The Black Death is estimated to have killed between a third and two-thirds of Europe’s population. The total number of deaths worldwide due to the pandemic is estimated at 75 million people, where of an estimated 20 million deaths occurred in Europe. Centuries later, a third pandemic began in China in 1855 and spread to all continents in a true pandemic manner. The disease persists in principally enzootic form in wild rodents and is responsible for occasional human cases in North and South America, Africa, and Asia. The WHO reports a total of 1000–3000 cases a year.\n\n14.11.2 Cholera\n\nCholera, the most lethal of past pandemics, kills its victims rapidly and in great numbers, but is the most easily prevented and cured – given the availability of appropriate resources and treatment. As is the case with smallpox, poliomyelitis, and measles, it is restricted to human hosts and infects no other species. Man is the sole victim of Vibrio cholerae. But unlike the viral causes of smallpox and measles, Vibrio can survive for long periods in the free-living state before its ingestion in water or contaminated food.\n\nCholera is probably the first of the pandemics, originating in the Ganges Delta from multitudes of pilgrims bathing in the Ganges river. It spread thereafter throughout the globe in a series of seven pandemics covering four centuries, with the last beginning in 1961 and terminating with the first known introduction of the disease into Africa. Africa is now a principal site of endemic cholera.\n\nThe pathogenesis of this deadly illness is remarkably simple: it kills through acute dehydration by damaging cells of the small and large intestine and impairing the reabsorption of water and vital minerals. Prompt replacement of fluid and electrolytes orally or by intravenous infusion is all that is required for rapid cure of almost all patients. A single dose of a new antibiotic, azithro mycin, can further mitigate symptoms.\n\n14.11.3 Malaria\n\nIt has been stated that ‘no other single infectious disease has had the impact on humans … [that] malaria has had’ (Harley et al., 1999). The validity of this statement may be arguable, but it seems certain that malaria is a truly ancient disease, perhaps 4000–5000 years old, attended by significant mortality, especially in children less than five years of age.\n\nThe disease developed with the beginnings of agriculture (Benenson, 1982), as humans became less dependent on hunting and gathering and lived together in closer association and near swamps and standing water – the breeding sites of mosquitoes. Caused by any of four Plasmodia species of protozoa, the disease in humans is transmitted by the Anopheles mosquito in which part of the parasite’s replicative cycle occurs.\n\nThus, with its pervasive and enduring effects, malaria did not carry the threatening stigma of an acute cause of pandemics but was an old, unwelcome acquaintance that was part of life in ancient times. The recent change in this picture will be described in a section that follows.\n\n14.11.4 Smallpox\n\nThere is no ambiguity about the diagnosis of smallpox. There are few, if any, asymptomatic cases of smallpox (Benenson, 1982) with its pustular skin lesions and subsequent scarring that are unmistakable, even to the layman. There is also no ambiguity about its lethal effects. For these reasons, of all the old pandemics, smallpox can be most surely identified in retrospect. Perhaps most dramatic was the decimation of Native Americans that followed the first colonization attempts in the New World. A number of historians have noted the devastating effects of the disease following the arrival of Cortez and his tiny army of 500 and have surmised that the civilized, organized Aztecs were defeated not by muskets and cross bows but by viruses, most notably smallpox, carried by the Spanish. Subsequent European incursions in North America were followed by similar massive mortality in the immunologically naïve and vulnerable Native Americans. Yet a more complete historical record presents a more complex picture. High mortality rates were also seen in some groups of colonizing Europeans (Gage, 1998). It was also observed, even that long ago, that smallpox virus probably comprised both virulent and relatively avirulent strains, which also might account for differences in mortality among epidemics. Modern molecular biology has identified three ‘clades’ or families of virus with genomic differences among the few viral genomes still available for study (Esposito et al., 2006). Other confounding and contradictory factors in evaluating the ‘Amerindian’ epidemics was the increasing use of vaccination in those populations (Ashburn, 1947), and such debilitating factors as poverty and stress (Jones).\n\n14.11.5 Tuberculosis\n\nThat traditional repository of medical palaeo-archeology, ‘an Egyptian mummy’, in this case dated at 2400 BC, showed characteristic signs of tuberculosis of the spine (Musser, 1994). More recently, the DNA of Mycobacterium tuberculosis was recovered from a 1000-year-old Peruvian mummy (Musser, 1994).\n\nThe more devastating a disease the more it seems to inspire the poetic. In seventeenth century England, John Bunyan referred to ‘consumption’ (tuberculosis in its terminal wasting stages) as ‘the captain of all these men of death’ (Comstock, 1982). Observers in the past had no way of knowing that consumption (wasting) was a stealthy plague, in which the majority of those infected when in good health neverbecame ill. Hippocrates’ mistaken conclusion that tuberculosis killed nearly everyone it infected was based on observation of far advanced clinically apparent cases.\n\nThe ‘White Plague’ of past centuries is still very much with us. It is one of the leading causes of death due to an infectious agent worldwide (Musser, 1994). Transmitted as a respiratory tract pathogen through coughs and aerosol spread and with a long incubation period, tuberculosis is indeed a pernicious and stealthy plague.\n\n14.11.6 Syphilis as a paradigm of sexually transmitted infections\n\nIf tuberculosis is stealthy at its inception, there is no subtlety to the initial acquisition of Treponema pallidum and the resultant genital ulcerative lesions (chancres) that follow sexual intercourse with the infected, or the florid skin rash that may follow. But the subsequent clinical course of untreated syphilis is stealthy indeed, lurking in the brain and spinal cord and in the aorta as a potential cause of aneurysm. Accurate figures on morbidity and mortality rates are hard to come by, despite two notorious studies in which any treatment available at the time was withheld after diagnosis of the initial acute stages (Gjestland, 1955; Kampmeir, 1974). The tertiary manifestations of the disease, general paresis tabes dorsalis, cardiovascular and other organ involvement by ‘the great imitator’, occurred in some 30% of those infected decades after initial infection.\n\nBefore the development of precise diagnostic technology, syphilis was often confused with other venereal diseases and leprosy so that its impact as a past cause of illness and mortality is difficult to ascertain.\n\nIt is commonly believed that just as other acute infections were brought into the New World by the Europeans, so was syphilis by Columbus’s crew to the Old World. However, there are strong advocates of the theory that the disease was exported from Europe rather than imported from America. Both propositions are thoroughly reviewed by Ashburn (1947).\n\n14.11.7 Influenza\n\nInfluenza is an acute, temporarily incapacitating, febrile illness characterized by generalized aching (arthralgia and myalgia) and a short course of three to seven days in more than 90% of cases. This serious disease, which kills hundreds of thousands every year and infects millions, has always been regarded lightly until its emergence in pandemic form, which happened only thrice in the twentieth century, including the notorious pandemic of 19181919 that killed 20–50 million people (Kilbourne, 2006a). It is a disease that spreads rapidly and widely among all human populations. In its milder, regional, yearly manifestations it is often confused with other more trivial infections of the respiratory tract, including the common cold. In the words of the late comedian Rodney Dangerfield, ‘it gets no respect’. But the damage its virus inflicts on the respiratory tract can pave the way for secondary bacterial infection, often leading to pneumonia. Although vaccines have been available for more than 50 years (Kilbourne, 1996), the capacity of the virus for continual mutation warrants annual or biannual reformulation of the vaccines.\n\n14.12 Contemporary plagues and pandemics\n\n14.12.1 HIV/AIDS\n\nTowards the end of the twentieth century a novel and truly dreadful plague was recognized when acquired immunodeficiency disease syndrome (AIDS) was first described and its cause established as the human immunodeficiency virus (HIV). This retrovirus (later definitively subcategorized as a Lenti [slow] virus) initially seemed to be restricted to a limited number of homosexual men, but its pervasive and worldwide effects on both sexes and young and old alike are all too evident in the present century.\n\nInitial recognition of HIV/AIDS in 1981 began with reports of an unusual pneumonia in homosexual men caused by Pneumocystis carinii and previously seen almost exclusively in immunocompromised subjects. In an editorial in Science (Fauci, 2006), Anthony Fauci writes, ‘Twenty five years later, the human immunodeficiency virus (HIV) … has reached virtually every corner of the globe, infecting more than 65 million people. Of these, 25 million have died’. Much has been learned in the past 25 years. The origin of the virus is most probably chimpanzees (Heeney et al., 2006), who carry asymptomatically a closely related virus, SIV (S for simian). The disease is no longer restricted to homosexuals and intravenous drug users but indeed, particularly in poor countries, is a growing hazard of heterosexual intercourse. In this rapidly increasing, true pandemic, perinatal infection can occur, and the effective battery of antiretroviral drugs that have been developed for mitigation of the disease are available to few in impoverished areas of the world. It is a tragedy that AIDS is easily prevented by the use of condoms or by circumcision, means that in many places are either not available or not condoned by social mores or cultural habits. The roles of sexual practices and of the social dominance of men over women emphasize the importance of human behaviour and economics in the perpetuation of disease.\n\nOther viruses have left jungle hosts to infect humans (e.g., Marburg virus) (Bausch et al., 2006) but in so doing have not modified their high mortality rate in a new species in order to survive and be effectively transmitted among members of the new host species. But, early on, most of those who died with AIDS did not die of AIDS. They died from the definitive effects of a diabolically structured virus that attacked cells of the immune system, striking down defences and leaving its victims as vulnerable to bacterial invaders as are the pitiable, genetically immunocompromised children in hospital isolation tents.\n\n14.12.2 Influenza\n\nInfluenza continues to threaten future pandemics as human-virulent mutant avian influenza viruses, such as the currently epizootic H5N1 virus, or by recombination of present ‘human’ viruses with those of avian species. At the time of writing (June 2007) the H5N1 virus remains almost exclusively epizootic in domestic fowl, and in humans, the customary yearly regional epidemics of H₃N₂ and H 1 N1 ‘human’ subtypes continue their prevalence. Meanwhile, vaccines utilizing reverse genetics technology and capable of growing in cell culture are undergoing improvements that still have to be demonstrated in the field. Similarly, antiviral agents are in continued development but are as yet unproven in mass prophylaxis.\n\n14.12.3 HIV and tuberculosis: the double impact of new and ancient threats\n\nThe ancient plague of tuberculosis has never really left us, even with the advent of multiple drug therapy. The principal effect of antimicrobial drugs has been seen in the richer nations, but to a much lesser extent in the economically deprived, in which the drugs are less available and medical care and facilities are scanty. However, in the United States, a progressive decline in cases was reversed in the 1980s (after the first appearance of AIDS) when tuberculosis cases increased by 20%. Of the excess, at least 30% were attributed to AIDS-related cases. Worldwide, tuberculosis is the most common opportunistic infection in HIV-infected persons and the most common cause of death in patients with AIDS.\n\nThe two infections may have reciprocal enhancing effects. The risk of rapid progression of pre-existing tuberculosis infection is much greater among those with HIV infection and the pathogenesis of the infection is altered, with an increase in non-pulmonary manifestation of tuberculosis. At the same time, the immune activation induced by response to tuberculosis may paradoxically be associated with an increase in the viral load and accelerated progression of HIV infection. The mechanism is not understood.\n\n14.13 Plagues and pandemics of the future\n\n14.13.1 Microbes that threaten without infection: the microbial toxins\n\nCertain microbial species produce toxins that can severely damage or kill the host. The bacterial endotoxins, as the name implies, are an integral part of the microbial cell and assist in the process of infection. Others, the exotoxins (of anthrax, botulism), are elaborated and can produce their harmful effects, as do other prefabricated, non-microbial chemical poisons. These microbial poisons by themselves seem unlikely candidates as pandemic agents. Anthrax spores through the mail caused 17 illnesses and 5 deaths in the United States in 2001 (Elias, 2006). Accordingly, a one billion dollar contract was awarded by the U.S. Department of Health and Human Services for an improved vaccine. Development was beset with problems, and delivery is not expected until 2008 (Elias, 2006). In any case, as non-propagating agents, microbial toxins do not seem to offer a significant pandemic threat.\n\n14.13.2 Iatrogenic diseases\n\nIatrogenic diseases are those unintentionally induced by physicians and the altruism of the dead – or, ‘The way to [health] is paved with good intentions’. An unfortunate result of medical progress can be the unwitting induction of disease and disability as new treatments are tried for the first time. Therefore, it will not be surprising if the accelerated and imaginative devising of new technologies in the future proves threatening at times. Transplantation of whole intact vital organs, including heart, kidney, and even liver, has seen a dramatic advance, although as an alien tissue, rejection by the immune system of the patient has been a continuing problem.\n\nReliance on xenotransplantation of non-human organs and tissues such as porcine heart valves does not seem to have much future because they may carry dangerous retroviruses. In this connection, Robin Weiss proposes that ‘we need a Hippocratic oath for public health that would minimize harm to the community resulting from the treatment of Individuals’ (Weiss, 2004).\n\nAll these procedures have introduced discussion of quality of life (and death) values, which will and should continue in the future. Based on present evidence, I do not see these procedures as instigators of pandemics unless potentially pandemic agents are amplified or mutated to virulence in the immunosuppressed recipients of this bodily largesse.\n\nHow complicated can things get? A totally unforeseen complication of the successful restoration of immunologic function by the treatment of AIDS with antiviral drugs has been the activation of dormant leprosy as a consequence (McNeil, 2006; Visco-Comandini et al., 2004).\n\n14.13.3 The homogenization of peoples and cultures\n\nThere is evidence from studies of isolated populations that such populations, because of their smaller gene pool, are less well equipped to deal with initial exposure to unaccustomed infectious agents introduced by genetically and racially different humans. This is suggested by modern experience with measles that demonstrated ‘intensified reactions to [live] measles vaccine in [previously unexposed] populations of American Indians’ (Black et al., 1971). This work and studies of genetic antigen markers in the blood have led Francis Black to propose, ‘[P]eople of the New World are unusually susceptible to the diseases of the Old not just because they lack any [specific] resistance but primarily because, as populations, they lack genetic heterogeneity (Black, 1992, 1994). They are susceptible because agents of disease can adapt to each population as a whole and cause unusual damage’ (Black, 1992, 1994). If I may extrapolate from Black’s conclusions, a population with greater genetic heterogeneity would fare better with an ‘alien’ microbial or viral invasion.\n\nAlthough present-day conflicts, warlike, political, and otherwise, seem to fly in the face of attaining genetic or cultural uniformity and the ‘one world’ ideal, in fact, increasing genetic and cultural homogeneity is a fact of life in many parts of the world. Furthermore, barriers to communication are breached by the World Wide Web, the universal e-mail post office, by rapid and frequent travel, and the ascendancy of the English language as an international tongue that is linking continents and ideas as never before.\n\nWe have already seen the rapid emergence of the respiratory virus, SARS, in humans and its transport from China to Canada. We also have learned that, unlike influenza, close and sustained contact with patients was required for the further perpetuation of the epidemic (Kilbourne, 2006b). This experience serves to emphasize that viruses can differ in their epidemic pattern of infection even if their target and site of infection are the same.\n\nWith this caveat, let us recall the rapid and effective transmission of influenza viruses by aerosols but the highly variable experience of isolated population groups in the pandemic of 1918. Groups sequestered from the outside world (and in close contact in small groups) prior to that epidemic suffered higher morbidity and mortality, suggesting that prior more frequent experience with non-pandemic influenza A viruses had at least partially protected those in more open societies. The more important point is that such ‘hot houses’ could favour the emergence of even more transmissible strains of virus than those initially introduced. Such hot houses or hotbeds in sequestered societies would be lacking in our homogenized world of the future.\n\nTo consider briefly the more prosaic, but no less important aspects of our increasingly homogeneous society, the mass production of food and behavioural fads concerning their consumption has led to the ‘one rotten apple’ syndrome. If one contaminated item, apple, egg, or most recently spinach leaf, carries a billion bacteria – not an unreasonable estimate – and it enters a pool of cake mix constituents, and is then packaged and sent to millions of customers nationwide, a bewildering epidemic may ensue.\n\n14.13.4 Man-made viruses\n\nAlthough the production and dangers of factitious infectious agents are considered elsewhere in this book (see Chapter 20), the present chapter would be incomplete without some brief consideration of such potential sources of plagues and pandemics of the future.\n\nNo one has yet mixed up a cocktail of off-the-shelf nucleotides to truly ‘make’ a new virus. The genome of the extinct influenza virus of 1918 has been painstakingly resurrected piece by piece from preserved human lung tissue (Taubenberger et al., 2000). This remarkable accomplishment augurs well for the palaeo-archeology of other extinct viral ‘dodos’, but whether new or old, viruses cannot truly exist without host cellular substrates on which to replicate, as does the resurrected 1918 virus, which can multiply, and indeed, kill animals. The implications are chilling indeed. Even confined to the laboratory, these or truly novel agents will become part of a global gene pool that will lie dormant as a potential threat to the future.\n\nPredictive principles for the epidemiology of such human (or non-human) creations can perhaps be derived from the epidemiology of presently familiar agents, for example, pathogenesis (Kilbourne, 1985).\n\n14.14 Discussion and conclusions\n\nIf sins of omission have been committed here, it is with the recognition that Pandora’s Box was full indeed and there is space in these pages to discuss only those infectious agents that have demonstrated a past or present capacity for creating plagues or pandemics, or which now appear to be emerging as serious threats in the future. I now quote from an earlier work.\n\nIn anticipating the future, we must appreciate the complexity of microbial strategies for survival. The emergence of drug-resistant mutants is easily understood as a consequence of Darwinian selection. Less well appreciated is the fact that genes for drug resistance are themselves transmissible to still other bacteria or viruses. In other instances, new infectious agents may arise through the genetic recombination of bacteria or of viruses which individually may not be pathogenic. By alteration of their environment, we have abetted the creation of such new pathogens by the promiscuous overuse or misuse of antimicrobial drugs. The traditional epidemiology of individual infectious agents has been superseded by a molecular epidemiology of their genes. (Kilbourne, 2000, p. 91)\n\nMany of my colleagues like to make predictions. ‘An influenza pandemic is inevitable’, ‘The mortality rate will be 50%’, etc. This makes good press copy, attracts TV cameras, and raises grant funding. Are influenza pandemics likely? Possibly, except for the preposterous mortality rate that has been proposed. Inevitable? No, not with global warming and increasing humidity, improved animal husbandry, better epizootic control, and improving vaccines. This does not have the inevitability of shifting tectonic plates or volcanic eruptions.\n\nPandemics, if they occur, will be primarily from respiratory tract pathogens capable of airborne spread. They can be quickly blunted by vaccines, if administrative problems associated with their production, distribution, and administration are promptly addressed and adequately funded. (A lot of ‘ifs’ but maybe we can start learning from experience!)\n\nBarring extreme mutations of the infective agent or changed methods of spread, all of them can be controlled (and some eradicated) with presently known methods. The problem, of course, is an economic one, but such organizations as the Global Health Foundation and the Bill and Melinda Gates Foundation offer hope for the future by their organized and carefully considered programs, which have identified and targeted specific diseases in specific developing regions of the world.\n\nIn dealing with the novel and the unforeseen – the unconventional prions of Bovine Spongio form Encephalitis that threatened British beef (WHO, 2002) and the exotic imports such as the lethal Marburg virus that did not come from Marburg (Bausch et al., 2006) – we must be guided by the lessons of the past, so it is essential that we reach a consensus on what these lessons are. Of these, prompt and continued epidemiological surveillance for the odd and unexpected and use of the techniques of molecular biology are of paramount importance (admirably reviewed by King et al. [2006]). For those diseases not amenable to environmental control, vaccines, the ultimate personal suits of armour that will protect the wearer in all climes and places, must be provided.\n\nShould we fear the future? If we promptly address and properly respond to the problems of the present (most of which bear the seeds of the future), we should not fear the future. In the meantime we should not cry ‘Wolf!’, or even ‘Fowl!’, but maintain vigilance.\n\nSuggestions for further reading\n\nAll suggestions are accessible to the general intelligent reader except for the Burnet book, which is ‘intermediate’ in difficulty level.\n\nBurnet, F.M. (1946). Virus as Organism: Evolutionary and Ecological Aspects of Some Human Virus Diseases (Cambridge, MA: Harvard University Press). A fascinating glimpse into a highly original mind grappling with the burgeoning but incomplete knowledge of virus diseases shortly before mid-twentieth century, and striving for a synthesis of general principles in a series of lectures given at Harvard University; all the more remarkable because Burnet won a shared Nobel Prize later for fundamental work on immunology.\n\nCrosby, A.W. (1976). Epidemic and Peace, 1918 (Westport, CT: Greenwood). This pioneering book on the re-exploration of the notorious 1918 influenza pandemic had been surprisingly neglected by earlier historians and the general public.\n\nDubos, R. (1966). Man Adapting (New Haven, CT: Yale University Press). This work may be used to gain a definitive understanding of the critical inter-relationship of microbes, environment, and humans. This is a classic work by a great scientist-philosopher must be read. Dubos’ work with antimicrobial substances from soil immediately preceded the development of antibiotics.\n\nKilbourne, E.D. (1983). Are new diseases really new? Natural History, 12, 28. An early essay for the general public on the now popular concept of ‘emerging diseases’, in which the prevalence of paralytic poliomyelitis as the price paid for improved sanitation, Legionnaire’s Disease as the price of air conditioning, and the triggering of epileptic seizures by the flashing lights of video games are all considered.\n\nMcNeill, W.H. (1977). Plagues and Peoples (Garden City, NY: Anchor Books). Although others had written earlier on the impact of certain infectious diseases on the course of history, McNeill recognized that there was no aspect of history that was untouched by plagues and pandemics. His book has had a significant influence on how we now view both infection and history.\n\nPorter, K.A. (1990). Pale Horse, Pale Rider (New York: Harcourt Brace &Company). Katherine Anne Porter was a brilliant writer and her evocation of the whole tragedy of the 1918 influenza pandemic with this simple, tragic love story tells us more than a thousand statistics.\n\nReferences\n\nAshburn, P.A. (1947). The Ranks of Death – A Medical History of the Conquest of America (New York: Coward-McCann).\n\nBarnes, D.S. (2006). The Great Stink of Paris and the Nineteenth Century Struggle against Filth and Germs (Baltimore, MD: Johns Hopkins University Press\n\nBausch, D.G. and Nichol, S.T., Muymembe-Tamfum, J.J. (2006). Marburg Hemorrhagic Fever Associated with Multiple Genetic Lineages of Virus. N. Engl. J. Med. 355, 909–919.\n\nBenenson, A.S. (1982). Smallpox. In Evans, A.S. (ed.), Viral Infections of Humans. 2nd edition, p. 542 (New York: Plenum Medical Book Company).\n\nBlack, F.L. (1992). Why did they die? Science, 258, 1739–1740.\n\nBlack, F.L. (1994). An explanation of high death rates among New World peoples when in contact with Old World diseases. Perspect. Biol. Med., 37(2), 292–303.\n\nBlack, F.L., Hierholzer, W., Woodall, J.P., and Pinhiero, F. (1971). Intensified reactions to measles vaccine in unexposed populations of American Indians. J. Inf. Dis., 124, 306–317.\n\nBoth, G.W., Shi, C.H., and Kilbourne, E.D. (1983). Hemagglutinin of swine influenza virus: a single amino acid change pleotropically affects viral antigenicity and replication. Proc. Natl. Acad. Sci. USA, 80, 6996–7000.\n\nBurnet, F.M. (1946). Virus as Organism (Cambridge, MA: Harvard University Press). CDC. (2005). Vibrio illnesses after hurricane Katrina – multiple states. Morbidity Mortality Weekly Report, 54, 928–931.\n\nComstock, G.W. (1982). Tuberculosis. In Evans, A.S. and Feldman, H.A. (eds.), Bacterial Infections of Humans, p.605 (New York: Plenum Medical Book Company).\n\nCrosby, A.W., Jr. (1976). Virgin soil epidemics as a factor in the depopulation in America. William Mary Q., 33, pp. 289–299.\n\nElias, P. (2006). Anthrax dispute suggests Bioshield woes. Washington Post, 6 October, 2006.\n\nEsposito, J.J., Sammons, S.A., Frace, A.M., Osborne, J.D., Melissa Olsen-Rasmussen, Ming Zhang, Dhwani Govil, Inger K. Damon, Richard Kline, Miriam Laker, Yu Li, Geoffrey L. Smith, Hermann Meyer, James W. LeDuc, Robert M. Wohlhueter (2006). Genome sequence diversity and clues to the evolution of Variola (smallpox) virus. Science, 313, 807–812.\n\nFauci, A.S. (2006). Twenty-five years of HIV/AIDS. Science, 313, 409.\n\nGage, K.L. (1998). In Collier, L., Balows, A., Sussman, M., and Hausles, W.J. (eds.), Topley and Wilson’s Microbiology and Microbiological Infections, Vol. 3, pp. 885–903 (London: Edward Arnold).\n\nGambaryan, A.S., Matrosovich, M.N., Bender, C.A., and Kilbourne, E.D. (1998). Differences in the biological phenotype of low-yielding (L) and high-yielding (H) variants of swine influenza virus A/NJ/11/76 are associated with their different receptor-binding activity. Virology, 247, 223.\n\nGjestland, T. (1955). The Oslo study of untreated syphilis – an epidemiologic investigation of the natural course of syphilistic infection as based on a re-study of the Boeck-Bruusgaard material. Acta Derm. Venereol., 35, 1.\n\nHarley, J., Klein, D., Lansing, P. (1999). Microbiology, pp. 824–826 (Boston: McGraw-Hill).\n\nHeeney, J.L., Dalgleish, A.G., and Weiss, R.A. (2006). Origins of HIV and the evolution of resistance to AIDS. Science, 313, 462–466.\n\nHorstmann, D.M. (1955). Poliomyelitis: severity and type of disease in different age groups. Ann. N. Y. Acad. Sci., 61, 956–967.\n\nJones, (2003). D.S. Virgin soils revisited. William Mary Q., 60, pp. 703–742. Kampmeir, R.H. (1974). Final report on the ‘Tuskegee Syphilis Study’. South Med. J., 67, 1349–1353.\n\nKilbourne, E.D. (1981). Segmented genome viruses and the evolutionary potential of asymmetrical sex. Perspect. Biol. Med., 25, 66–77.\n\nKilbourne, E.D. (1985). Epidemiology of viruses genetically altered by man – predictive principles. In Fields, B., Martin, M., and Potter, C.W. (eds.), Banbury Report 22: Genetically Altered Viruses and the Environment, pp. 103–117 (Cold Spring Harbor, NY: Cold Spring Harbor Laboratories).\n\nKilbourne, E.D. (1996). A race with evolution – a history of influenza vaccines. In Plotkin, S. and Fantini, B. (eds.). Vaccinia, Vaccination and Vaccinology: Jenner, Pasteur and Their Successors, pp. 183–188 (Paris: Elsevier).\n\nKilbourne, E.D. (2000). Communication and communicable diseases: cause and control in the 21st century. In Haidemenakis, E.D. (ed.), The Sixth Olympiad of the Mind, ‘The Next Communication Civilization’, November 2000, St. Georges, Paris, p. 91 (International S.T.E.P.S. Foundation).\n\nKilbourne, E.D. (2006a). Influenza pandemics of the 20th century. Emerg. Infect. Dis., 12(1), 9.\n\nKilbourne, E.D. (2006b). SARS in China: prelude to pandemic? JAMA, 295, 1712–1713. Book review.\n\nKilbourne, E.D.\n\nKing, D.A., Peckham, C., Waage, J.K., Brownlie, M.E., and Woolhouse, M.E.G. (2006). Infectious diseases: preparing for the future. Science, 313, 1392–1393.\n\nMcNeil, D.G., Jr. (2006). Worrisome new link: AIDS drugs and leprosy. The New York Times, pp. F1, F6, 24 October, 2006.\n\nMcNeill, W.H. (1976). Plagues and Peoples, p. 21 (Garden City, NY: Anchor Books). Musser, J.M. (1994). Is Mycobacterium tuberculosis, 15,000 years old? J. Infect. Dis., 170, 1348–1349.\n\nOldstone, M.B.A. (1998). Viruses, Plagues and History, pp. 31–32 (Oxford: Oxford University Press).\n\nPascual, M., Ahumada, J.A., Chaves, L.F., Rodo, X., and Bouma, M. (2006). Malaria resurgence in the East African highlands: temperature trends revisited. Proc. Natl. Acad. Sci. USA, 103, 5829–5834.\n\nPatz, J.A. and Olson, S.H. (2006). Malaria risk and temperature: influences from global climate change and local land use practices. Proc. Natl. Acad. Sci. USA, 103, 5635–5636.\n\nSaha, D., Karim, M.M., Khan, W.A., Ahmed, S., Salam, M.A., and Bennish, M.I. (2006). Single dose Azithromycin for the treatment of cholera in adults. N. Engl. J. Med., 354, 354, 2452–2462.\n\nSchulman, J.L. and Kilbourne, E.D. (1966). Seasonal variations in the transmission of influenza virus infection in mice. In Biometeorology II, Proceedings of the Third International Biometeorological Congress, Pau, France, 1963, pp. 83–87 (Oxford: Pergamon).\n\nSimpson, H.N. (1954). The impact of disease on American history. N. Engl. J. Med., 250, 680.\n\nStearn, E.W. and Stearn, A.E. (1945). The Effect of Smallpox on the Destiny of the Amerindian, pp. 44–45 (Boston: Bruce Humphries).\n\nStenseth, N.C., Samia, N.I., Hildegunn Viljugrein, Kyrre Linné Kausrud, Mike Begon, Stephen Davis, Herwig Leirs, V.M. Dubyanskiy, Jan Esper, Vladimir S. Ageyev, Nikolay L. Klassovskiy, Sergey B. Pole, and Kung-Sik Chan (2006). Plague dynamics are driven by climate variation. Proc. Natl. Acad. Sci. USA, 103, 13110–13115.\n\nTaubenberger, J.K., Reid, A.H., and Fanning, T.G. (2000). The 1918 influenza virus: a killer comes into view. Virology, 274, 241–245.\n\nTumpey, T.M., Maines, T.R., Van Hoeven, N., Glaser, L., Solorzano, A., Pappas, C., Cox, N.J., Swayne, D.E., Palese, P., Katz, J.M., and Garcia-Sastre, A. (2007). A two-amino acid change in the hemagglutinin of the 1918 influenza virus abolishes transmission. Science, 315, 655–659.\n\nVisco-Comandini, U., Longo, B., Cozzi, T., Paglia, M.G., Antonucci, G. (2004). Tuberculoid leprosy in a patient with AIDS: a manifestation of immune restoration syndrome. Scand. J. Inf. Dis., 36, 881–883.\n\nWeiss, R.A. (2004). Circe, Cassandra, and the Trojan Pigs: xenotransplantation. Am. Phil. Soc., 148, 281–295.\n\nWikipedia. http://en.wikipedia.org//Bubonic\\_plague\n\nWHO. (2002). WHO fact sheet no. 113 (Geneva: WHO).\n\nWHO. (2004). Bull. WHO, 82, 1–81.\n\n• 15 • Artificial Intelligence as a positive and negative factor in global risk\n\nEliezer Yudkowsky\n\n15.1 Introduction\n\nBy far the greatest danger of Artificial Intelligence (AI) is that people conclude too early that they understand it. Of course, this problem is not limited to the field of AI. Jacques Monod wrote: ‘A curious aspect of the theory of evolution is that everybody thinks he understands it’ (Monod, 1974). The problem seems to be unusually acute in Artificial Intelligence. The field of AI has a reputation for making huge promises and then failing to deliver on them. Most observers conclude that AI is hard, as indeed it is. But the embarrassment does not stem from the difficulty. It is difficult to build a star from hydrogen, but the field of stellar astronomy does not have a terrible reputation for promising to build stars and then failing. The critical inference is not that AI is hard, but that, for some reason, it is very easy for people to think they know far more about AI than they actually do.\n\nIt may be tempting to ignore Artificial Intelligence because, of all the global risks discussed in this book, AI is probably hardest to discuss. We cannot consult actuarial statistics to assign small annual probabilities of catastrophe, as with asteroid strikes. We cannot use calculations from a precise, precisely confirmed model to rule out events or place infinitesimal upper bounds on their probability, as with proposed physics disasters. But this makes AI catastrophes more worrisome, not less.\n\nThe effect of many cognitive biases has been found to increase with time pressure, cognitive busyness, or sparse information. Which is to say that the more difficult the analytic challenge, the more important it is to avoid or reduce bias. Therefore I strongly recommend reading my other chapter (Chapter 5) in this book before continuing with this chapter.\n\n15.2 Anthropomorphic bias\n\nWhen something is universal enough in our everyday lives, we take it for granted to the point of forgetting it exists.\n\nImagine a complex biological adaptation with 10 necessary parts. If each of the 10 genes is independently at 50% frequency in the gene pool – each gene possessed by only half the organisms in that species – then, on average, only 1 in 1024 organisms will possess the full, functioning adaptation. A fur coat is not a significant evolutionary advantage unless the environment reliably challenges organisms with cold. Similarly, if gene B depends on gene A, then gene B has no significant advantage unless gene A forms a reliable part of the genetic environment. Complex, interdependent machinery is necessarily universal within a sexually reproducing species; it cannot evolve otherwise (Tooby and Cosmides, 1992). One robin may have smoother feathers than another, but both will have wings. Natural selection, while feeding on variation, uses it up (Sober, 1984).\n\nIn every known culture, humans experience joy, sadness, disgust, anger, fear, and surprise (Brown, 1991), and exhibit these emotions through the same means, namely facial expressions (Ekman and Keltner, 1997). We all run the same engine under our hoods, although we may be painted in different colours – a principle that evolutionary psychologists call the psychic unity of humankind (Tooby and Cosmides, 1992). This observation is both explained and required by the mechanics of evolutionary biology.\n\nAn anthropologist will not excitedly report of a newly discovered tribe: ‘They eat food! They breathe air! They use tools! They tell each other stories!’ We humans forget how alike we are, living in a world that only reminds us of our differences.\n\nHumans evolved to model other humans – to compete against and cooperate with our own conspecifics. It was a reliable property of the ancestral environment that every powerful intelligence you met would be a fellow human. We evolved to understand our fellow humans empathically, by placing ourselves in their shoes; for that which needed to be modelled was similar to the modeller. Not surprisingly, human beings often ‘anthropomorphize’ – expect humanlike properties of that which is not human. In The Matrix (Wachowski and Wachowski, 1999), the supposed ‘artificial intelligence’Agent Smith initially appears utterly cool and collected, his face passive and unemotional. But later, while interrogating the human Morpheus, Agent Smith gives vent to his disgust with humanity – and his face shows the human-universal facial expression for disgust.\n\nQuerying your own human brain works fine, as an adaptive instinct, if you need to predict other humans. If you deal with any other kind of optimization process – if, for example, you are the eighteenth-century theologian William Paley, looking at the complex order of life and wondering how it came to be – then anthropomorphism is flypaper for unwary scientists, a trap so sticky that it takes a Darwin to escape.\n\nExperiments on anthropomorphism show that subjects anthropomorphize unconsciously, often flying in the face of their deliberate beliefs. In a study by Barrett and Keil (1996), subjects strongly professed belief in non-anthropomorphic properties of God: that God could be in more than one place at a time, or pay attention to multiple events simultaneously. Barrett and Keil presented the same subjects with stories in which, for example, God saves people from drowning. The subjects answered questions about the stories, or retold the stories in their own words, in such ways as to suggest that God was in only one place at a time and performed tasks sequentially rather than in parallel. Serendipitously for our purposes, Barrett and Keil also tested an additional group using otherwise identical stories about a superintelligent computer named ‘Uncomp’. For example, to simulate the property of omnipresence, subjects were told that Uncomp’s sensors and effectors ‘cover every square centimetre of the earth and so no information escapes processing’. Subjects in this condition also exhibited strong anthropomorphism, though significantly less than the God group. From our perspective, the key result is that even when people consciously believe an AI is unlike a human, they still visualize scenarios as if the AI were anthropomorphic (but not quite as anthropomorphic as God).\n\nBack in the era of pulp science fiction, magazine covers occasionally depicted a sentient monstrous alien-colloquially known as a bug-eyed monster (BEM) – carrying off an attractive human female in a torn dress. It would seem the artist believed that a non-humanoid alien, with a wholly different evolutionary history, would sexually desire human females. People do not usually make mistakes like that by explicitly reasoning: ‘All minds are likely to be wired pretty much the same way, so presumably a BEM will find human females sexually attractive’. Probably the artist did not ask whether a giant bug perceives human females as attractive. Rather, a human female in a torn dress is sexy -inherently so, as an intrinsic property. They who made this mistake did not think about the insectoid’s mind; they focused on the woman’s torn dress. If the dress were not torn, the woman would be less sexy; the BEM does not enter into it.¹\n\nIt is also a serious error to begin from the conclusion and search for a neutral-seeming line of reasoning leading there; this is rationalization. If it is self-brain query that produced that first fleeting mental image of an insectoid chasing a human female, then anthropomorphism is the underlying cause of that belief, and no amount of rationalization will change that.\n\nAnyone seeking to reduce anthropomorphic bias in himself or herself would be well advised to study evolutionary biology for practice, preferably evolutionary biology with maths. Early biologists often anthropomorphized natural selection – they believed that evolution would do the same thing they would do; they tried to predict the effects of evolution by putting themselves ‘in evolution’s shoes’. The result was a great deal of nonsense, which first began to be systematically exterminated from biology in the late 1960s, for example, by Williams (1966). Evolutionary biology offers both mathematics and case studies to help hammer out anthropomorphic bias.\n\nEvolution strongly conserves some structures. Once other genes that depend on a previously existing gene evolve, the early gene is set in concrete; it cannot mutate without breaking multiple adaptations. Homeotic genes – genes controlling the development of the body plan in embryos – tell many other genes when to activate. Mutating a homeotic gene can result in a fruit fly embryo that develops normally except for not having a head. As a result, homeotic genes are so strongly conserved that many of them are the same in humans and fruit flies – they have not changed since the last common ancestor of humans and bugs. The molecular machinery of ATP synthase is essentially the same in animal mitochondria, plant chloroplasts, and bacteria; ATP synthase has not changed significantly since the rise of eukaryotic life 2 billion years ago.\n\nAny two AI designs might be less similar to one another than you are to a petunia.\n\nThe term ‘Artificial Intelligence’ refers to a vastly greater space of possibilities than does the term Homo sapiens. When we talk about ‘AIs’ we are really talking about minds-in-general, or optimization processes in general. Imagine a map of mind design space. In one corner, a tiny little circle contains all humans, within a larger tiny circle containing all biological life; and all the rest of the huge map is the space of minds-in-general. The entire map floats in a still vaster space, the space of optimization processes. Natural selection creates complex functional machinery without mindfulness; evolution lies inside the space of optimization processes but outside the circle of minds.\n\nIt is this enormous space of possibilities that outlaws anthropomorphism as legitimate reasoning.\n\n15.3 Prediction and design\n\nWe cannot query our own brains for answers about non-human optimization processes – whether bug-eyed monsters, natural selection, or Artificial Intelligences. How then may we proceed? How can we predict what Artificial Intelligences will do? I have deliberately asked this question in a form that makes it intractable. By the halting problem, it is impossible to predict whether an arbitrary computational system implements any input-output function, including, say, simple multiplication (Rice, 1953). So how is it possible that human engineers can build computer chips that reliably implement multiplication? Because human engineers deliberately use designs that they can understand.\n\nAnthropomorphism leads people to believe that they can make predictions, given no more information than that something is an ‘intelligence’ -anthropomorphism will go on generating predictions regardless, your brain automatically putting itself in the shoes of the ‘intelligence’. This may have been one contributing factor to the embarrassing history of AI, which stems not from the difficulty of AI as such, but from the mysterious ease of acquiring erroneous beliefs about what a given AI design accomplishes.\n\nTo make the statement that a bridge will support vehicles up to 30 tons, civil engineers have two weapons: choice of initial conditions, and safety margin. They need not predict whether an arbitrary structure will support 30-ton vehicles, only design a single bridge of which they can make this statement. And though it reflects well on an engineer who can correctly calculate the exact weight a bridge will support, it is also acceptable to calculate that a bridge supports vehicles of at least 30 tons – albeit to assert this vague statement rigorously may require much of the same theoretical understanding that would go into an exact calculation.\n\nCivil engineers hold themselves to high standards in predicting that bridges will support vehicles. Ancient alchemists held themselves to much lower standards in predicting that a sequence of chemical reagents would transform lead into gold. How much lead into how much gold? What is the exact causal mechanism? It is clear enough why the alchemical researcher wants gold rather than lead, but why should this sequence of reagents transform lead to gold, instead of gold to lead or lead to water?\n\nSome early AI researchers believed that an artificial neural network of layered thresholding units, trained via back propagation, would be ‘intelligent’. The wishful thinking involved was probably more analogous to alchemy than civil engineering. Magic is on Donald Brown’s list of human universals (Brown, 1991); science is not. We do not instinctively see that alchemy will not work. We do not instinctively distinguish between rigorous understanding and good storytelling. We do not instinctively notice an expectation of positive results that rests on air.\n\nThe human species came into existence through natural selection, which operates through the non-chance retention of chance mutations. One path leading to global catastrophe – to someone pressing the button with a mistaken idea of what the button does – is that AI comes about through a similar accretion of working algorithms, with the researchers having no deep understanding of how the combined system works. Nonetheless, they believe the AI will be friendly, with no strong visualization of the exact processes involved in producing friendly behaviour, or any detailed understanding of what they mean by friendliness. Much as early AI researchers had strong mistaken vague expectations for their programmes’ intelligence, we imagine that these AI researchers succeed in constructing an intelligent programme, but have strong mistaken vague expectations for their programme’s friendliness.\n\nNot knowing how to build a friendly AI is not deadly by itself, in any specific instance, if you know you do not know. It is a mistaken belief that an AI will be friendly, which implies an obvious path to global catastrophe.\n\n15.4 Underestimating the power of intelligence\n\nWe tend to see individual differences instead of human universals. Thus, when someone says the word ‘intelligence’, we think of Einstein, instead of humans.\n\nIndividual differences of human intelligence have a standard label, Spearman’ s g aka g-factor, a controversial interpretation of the solid experimental result that different intelligence tests are highly correlated with each other and with real-world outcomes such as lifetime income (Jensen, 1999). Spearman’s g is a statistical abstraction from individual differences of intelligence between humans, who as a species are far more intelligent than lizards. Spearman’s g is abstracted from millimetre height differences among a species of giants.\n\nWe should not confuse Spearman’s g with human general intelligence, our capacity to handle a wide range of cognitive tasks incomprehensible to other species. General intelligence is a between-species difference, a complex adaptation, and a human universal found in all known cultures. There may as yet be no academic consensus on intelligence, but there is no doubt about the existence, or the power, of the thing-to-be-explained. There is something about humans that let us set our footprints on the Moon. And jokes aside, you will not find many CEOs, or yet professors of academia, who are chimpanzees. You will not find many acclaimed rationalists, artists, poets, leaders, engineers, skilled networkers, martial artists, or musical composers who are mice. Intelligence is the foundation of human power, the strength that fuels our other arts.\n\nThe danger of confusing general intelligence with g -factor is that it leads to tremendously underestimating the potential impact of Artificial Intelligence. (This applies to underestimating potential good impacts, as well as potential bad impacts.) Even the phrase ‘trans human AI’ or ‘artificial superintelligence’ may still evoke images of book-smarts-in-a-box: an AI that is really good at cognitive tasks stereotypically associated with ‘intelligence’, like chess or abstract mathematics. But not superhumanly persuasive, or far better than humans at predicting and manipulating human social situations, or inhumanly clever in formulating long-term strategies. So instead of Einstein, should we think of, say, the nineteenth-century political and diplomatic genius Otto von Bismarck? But that is only the mirror version of the error. The entire range from the village idiot to Einstein, or from the village idiot to Bismarck, fits into a small dot on the range from amoeba to human.\n\nIf the word ‘intelligence’ evokes Einstein instead of humans, then it may sound sensible to say that intelligence is no match for a gun, as if guns had grown on trees. It may sound sensible to say that intelligence is no match for money, as if mice used money. Human beings did not start out with major assets in claws, teeth, armour, or any of the other advantages that were the daily currency of other species. If you had looked at humans from the perspective of the rest of the ecosphere, there was no hint that the soft pink things would eventually clothe themselves in armoured tanks. We invented the battleground on which we defeated lions and wolves. We did not match them claw for claw, tooth for tooth; we had our own ideas about what mattered.\n\nVinge (1993) aptly observed that a future containing smarter-than-human minds is different in kind. AI does not belong to the same graph that shows progress in medicine, manufacturing, and energy. AI is not something you can casually mix into a lumpenfuturistic scenario of skyscrapers and flying cars and nanotechnological red blood cells that let you hold your breath for eight hours. Sufficiently tall skyscrapers do not potentially start doing their own engineering. Humanity did not rise to prominence on Earth by holding its breath longer than other species.\n\nThe catastrophic scenario that stems from underestimating the power of intelligence is that someone builds a button, and does not care enough what the button does, because they do not think the button is powerful enough to hurt them. Or the wider field of AI researchers will not pay enough attention to risks of strong AI, and therefore good tools and firm foundations for friendliness will not be available when it becomes possible to build strong intelligences.\n\nAnd one should not fail to mention – for it also impacts upon existential risk -that AI could be the powerful solution to other existential risks, and by mistake we will ignore our best hope of survival. The point about underestimating the potential impact of AI is symmetrical around potential good impacts and potential bad impacts. That is why the title of this chapter is ‘Artificial Intelligence as a positive and negative factor in global risk’, not ‘Global risks of Artificial Intelligence’. The prospect of AI interacts with global risk in more complex ways than that.\n\n15.5 Capability and motive\n\nThere is a fallacy often committed in discussion of Artificial Intelligence, especially AI of superhuman capability. Someone says: ‘When technology advances far enough, we’ll be able to build minds far surpassing human intelligence. Now, it’s obvious that how large a cheesecake you can make depends on your intelligence. A superintelligence could build enormous cheesecakes – cheesecakes the size of cities – by golly, the future will be full of giant cheesecakes!’ The question is whether the superintelligence wants to build giant cheesecakes. The vision leaps directly from capability to actuality, without considering the necessary intermediate of motive.\n\nThe following chains of reasoning, considered in isolation without supporting argument, all exhibit the Fallacy of the Giant Cheesecake:\n\n• A sufficiently powerful AI could overwhelm any human resistance and wipe out humanity. (And the AI would decide to do so.) Therefore we should not build AI.\n\n• A sufficiently powerful AI could develop new medical technologies capable of saving millions of human lives. (And the AI would decide to do so.) Therefore we should build AI.\n\n• Once computers become cheap enough, the vast majority of jobs will be performable by AI more easily than by humans. A sufficiently powerful AI would even be better than us at maths, engineering, music, art, and all the other jobs we consider meaningful. (And the AI will decide to perform those jobs.) Thus after the invention of AI, humans will have nothing to do, and we will starve or watch television.\n\n15.5.1 Optimization processes\n\nThe above deconstruction of the Fallacy of the Giant Cheesecake invokes an intrinsic anthropomorphism – the idea that motives are separable; the implicit assumption that by talking about ‘capability’ and ‘motive’ as separate entities, we are carving reality at its joints. This is a useful slice but an anthropomorphic one.\n\nTo view the problem in more general terms, I introduce the concept of an optimization process: a system that hits small targets in large search spaces to produce coherent real-world effects.\n\nAn optimization process steers the future into particular regions of the possible. I am visiting a distant city, and a local friend volunteers to drive me to the airport. I do not know the neighbourhood. When my friend comes to a street intersection, I am at a loss to predict my friend’s turns, either individually or in sequence. Yet I can predict the result of my friend’s unpredictable actions: we will arrive at the airport. Even if my friend’s house were located elsewhere in the city, so that my friend made a wholly different sequence of turns, I would just as confidently predict our destination. Is this not a strange situation to be in, scientifically speaking? I can predict the outcome of a process, without being able to predict any of the intermediate steps in the process. I will speak of the region into which an optimization process steers the future as that optimizer’s target.\n\nConsider a car, say a Toyota Corolla. Of all possible configurations for the atoms making up the Corolla, only an infinitesimal fraction qualifies as a useful working car. If you assembled molecules at random, many many ages of the universe would pass before you hit on a car. A tiny fraction of the design space does describe vehicles that we would recognize as faster, more efficient, and safer than the Corolla. Thus the Corolla is not optimal under the designer’s goals. The Corolla is, however, optimized, because the designer had to hit a comparatively infinitesimal target in design space just to create a working car, let alone a car of the Corolla’s quality. You cannot build so much as an effective wagon by sawing boards randomly and nailing according to coinflips. To hit such a tiny target in configuration space requires a powerful optimization process.\n\nThe notion of an ‘optimization process’ is predictively useful because it can be easier to understand the target of an optimization process than to understand its step-by-step dynamics. The above discussion of the Corolla assumes implicitly that the designer of the Corolla was trying to produce a ‘vehicle’, a means of travel. This assumption deserves to be made explicit, but it is not wrong, and it is highly useful in understanding the Corolla.\n\n15.5.2 Aiming at the target\n\nThe temptation is to ask what ‘AIs’ will ‘want’, forgetting that the space of minds-in-general is much wider than the tiny human dot. One should resist the temptation to spread quantifiers over all possible minds. Storytellers spinning tales of the distant and exotic land called Future, say how the future will be. They make predictions. They say, ‘AIs will attack humans with marching robot armies’ or ‘AIs will invent a cure for cancer’. They do not propose complex relations between initial conditions and outcomes – that would lose the audience. But we need relational understanding to manipulate the future, steer it into a region palatable to humankind. If we do not steer, we run the danger of ending up where we are going.\n\nThe critical challenge is not to predict that ‘AIs’ will attack humanity with marching robot armies, or alternatively invent a cure for cancer. The task is not even to make the prediction for an arbitrary individual AI design. Rather the task is choosing into existence some particular powerful optimization process whose beneficial effects can legitimately be asserted.\n\nI strongly urge my readers not to start thinking up reasons why a fully generic optimization process would be friendly. Natural selection is not friendly, nor does it hate you, nor will it leave you alone. Evolution cannot be so anthropomorphized, it does not work like you do. Many pre-1960s biologists expected natural selection to do all sorts of nice things, and rationalized all sorts of elaborate reasons why natural selection would do it. They were disappointed, because natural selection itself did not start out knowing that it wanted a humanly nice result, and then rationalize elaborate ways to produce nice results using selection pressures. Thus the events in Nature were outputs of causally different process from what went on in the pre-1960s biologists’ minds, so that prediction and reality diverged.\n\nWishful thinking adds detail, constrains prediction, and thereby creates a burden of improbability. What of the civil engineer who hopes a bridge will not fall? Should the engineer argue that bridges in general are not likely to fall? But Nature itself does not rationalize reasons why bridges should not fall. Rather, the civil engineer overcomes the burden of improbability through specific choice guided by specific understanding. A civil engineer starts by desiring a bridge; then uses a rigorous theory to select a bridge design that supports cars; then builds a real-world bridge whose structure reflects the calculated design; and thus the real-world structure supports cars, thus achieving harmony of predicted positive results and actual positive results.\n\n15.6 Friendly Artificial Intelligence\n\nIt would be a very good thing if humanity knew how to choose into existence a powerful optimization process with a particular target. In more colloquial terms, it would be nice if we knew how to build a nice AI.\n\nTo describe the field of knowledge needed to address that challenge, I have proposed the term ‘Friendly AI’. In addition to referring to a body of technique, ‘Friendly AI’ might also refer to the product of technique – an AI created with specified motivations. When I use the term Friendly in either sense, I capitalize it to avoid confusion with the intuitive sense of ‘friendly’.\n\nOne common reaction I encounter is for people to immediately declare that Friendly AI is an impossibility because any sufficiently powerful AI will be able to modify its own source code to break any constraints placed upon it.\n\nThe first flaw you should notice is a Giant Cheesecake Fallacy. Any AI with free access to its own source would, in principle, possess the ability to modify its own source code in a way that changed the AI’s optimization target. This does not imply the AI has the motive to change its own motives. I would not knowingly swallow a pill that made me enjoy committing murder, because currently I prefer that my fellow humans do not die.\n\nBut what if I try to modify myself and make a mistake? When computer engineers prove a chip valid – a good idea if the chip has 155 million transistors and you cannot issue a patch afterwards – the engineers use human-guided, machine-verified formal proof. The glorious thing about formal mathematical proof is that a proof of 10 billion steps is just as reliable as a proof of 10 steps. But human beings are not trustworthy to peer over a purported proof of 10 billion steps; we have too high a chance of missing an error. And present-day theorem-proving techniques are not smart enough to design and prove an entire computer chip on their own – current algorithms undergo an exponential explosion in the search space. Human mathematicians can prove theorems far more complex than what modern theorem-provers can handle, without being defeated by exponential explosion. But human mathematics is informal and unreliable; occasionally, someone discovers a flaw in a previously accepted informal proof. The up shot is that human engineers guide a theorem-prover through the intermediate steps of a proof. The human chooses the next lemma, and a complex theorem-prover generates a formal proof, and a simple verifier checks the steps. That is how modern engineers build reliable machinery with 155 million interdependent parts.\n\nProving a computer chip correct requires a synergy of human intelligence and computer algorithms, as currently neither suffices on its own. Perhaps a true AI could use a similar combination of abilities when modifying its own code – that would have both the capability to invent large designs without being defeated by exponential explosion, and also the ability to verify its steps with extreme reliability. That is one way a true AI might remain knowably stable in its goals, even after carrying out a large number of self-modifications.\n\nThis chapter will not explore the above idea in detail (see Schmidhuber [2003] for a related notion). But one ought to think about a challenge and study it in the best available technical detail, before declaring itimpossible – especially if great stakes are attached to the answer. It is disrespectful to human ingenuity to declare a challenge unsolvable without taking a close look and exercising creativity. It is an enormously strong statement to say that you cannot do a thing – that you cannot build a heavier-than-air flying machine, that you cannot get useful energy from nuclear reactions, or that you cannot fly to the Moon. Such statements are universal generalizations, quantified over every single approach that anyone ever has or ever will think up for solving the problem. It only takes a single counterexample to falsify a universal quantifier. The statement that Friendly (or friendly) AI is theoretically impossible, dares to quantify over every possible mind design and every possible optimization process – including human beings, who are also minds, some of whom are nice and wish they were nicer. At this point there are any number of vaguely plausible reasons why Friendly AI might be humanly impossible, and it is still more likely that the problem is solvable but no one will get around to solving it in time. But one should not write off the challenge so quickly, especially considering the stakes involved.\n\n15.7 Technical failure and philosophical failure\n\nBostrom (2001) defines an existential catastrophe as one that extinguishes Earth-originating intelligent life or permanently destroys a substantial part of its potential. We can divide potential failures of attempted Friendly AI into two informal fuzzy categories, technical failure and philosophical failure. Technical failure is when you try to build an AI and it does not work the way you think it should – you have failed to understand the true workings of your own code. Philosophical failure is trying to build the wrong thing, so that even if you succeeded you would still fail to help anyone or benefit humanity. Needless to say, the two failures are not mutually exclusive.\n\nThe border between these two cases is thin, since most philosophical failures are much easier to explain in the presence of technical knowledge. In theory you ought to first say what you want, then figure out how to get it. In practice it often takes a deep technical understanding to figure out what you want.\n\n15.7.1 An example of philosophical failure\n\nIn the late nineteenth century, many honest and intelligent people advocated communism, all in the best of good intentions. The people who first invented and spread and swallowed the communist meme were usually, in sober historical fact, idealists. The first communists did not have the example of Soviet Russia to warn them. At that time, without benefit of hindsight, it must have sounded like a pretty good idea. After the revolution, when communists came into power and were corrupted by it, other motives came into play; but this itself was not something the first idealists predicted, however predictable it may have been. It is important to understand that the authors of huge catastrophes need not be evil, or even unusually stupid. If we attribute every tragedy to evil or unusual stupidity, we will look at ourselves, correctly perceive that we are notevil or unusually stupid, and say: ‘But that would never happen to us’.\n\nWhat the first communist revolutionaries thought would happen, as the empirical consequence of their revolution, was that people’s lives would improve: labourers would no longer work long hours at backbreaking labour and make little money from it. This turned out to be not the case, to put itmildly. But what the first communists thought would happen, was not so very different from what advocates of other political systems thought would be the empirical consequence of their favourite political systems. They thought people would be happy. They were wrong.\n\nNow imagine that someone should attempt to programme a ‘Friendly’ AI to implement communism, or libertarianism, or anarcho-feudalism, or favourite political system, believing that this shall bring about utopia. People’s favourite political systems inspire blazing suns of positive affect, so the proposal will sound like a really good idea to the proposer.\n\nWe could view the programmer’s failure on a moral or ethical level – say that it is the result of someone trusting themselves too highly, failing to take into account their own fallibility, refusing to consider the possibility that communism might be mistaken after all. But in the language of Bayesian decision theory, there is a complementary technical view of the problem. From the perspective of decision theory, the choice for communism stems from combining an empirical belief with a value judgement. The empirical belief is that communism, when implemented, results in a specific outcome or class of outcomes: people will be happier, work fewer hours, and possess greater material wealth. This is ultimately an empirical prediction; even the part about happiness is a real property of brain states, though hard to measure. If you implement communism, either this outcome eventuates or it does not. The value judgement is that this outcome satisfies or is preferable to current conditions. Given a different empirical belief about the actual real-world consequences of a communist system, the decision may undergo a corresponding change.\n\nWe would expect a true AI, an Artificial General Intelligence, to be capable of changing its empirical beliefs (or its probabilistic world model, etc.). If somehow Charles Babbage had lived before Nicolaus Copernicus, somehow computers had been invented before telescopes, and somehow the programmers of that day and age successfully created an Artificial General Intelligence, it would not follow that the AI would believe forever after that the Sun orbited the Earth. The AI might transcend the factual error of its programmers, provided that the programmers understood inference rather better than they understood astronomy. To build an AI that discovers the orbits of the planets, the programmers need not know the maths of Newtonian mechanics, only the maths of Bayesian probability theory.\n\nThe folly of programming an AI to implement communism, or any other political system, is that you are programming means instead of ends. You are programming in a fixed decision, without that decision being re-evaluable after acquiring improved empirical knowledge about the results of communism. You are giving the AI a fixed decision without telling the AI how to re-evaluate, at a higher level of intelligence, the fallible process that produced that decision.\n\nIf I play chess against a stronger player, I cannot predict exactly where my opponent will move against me – if I could do that, I would necessarily be at least that strong at chess myself. But I can predict the end result, which is a win for the other player. I know the region of possible futures my opponent is aiming for, which is what lets me predict the destination, even if I cannot see the path. When I am at my most creative, that is when it is hardest to predict my actions, and easiest to predict the consequences of my actions (providing that you know and understand my goals!). If I want a better-than-human chess player, I have to programme a search for winning moves. I cannot programme in specific moves because then the chess player will not be any better than I am. When I launch a search, I necessarily sacrifice my ability to predict the exact answer in advance. To get a really good answer you must sacrifice your ability to predict the answer, albeit not your ability to say what the question is.\n\n15.7.2 An example of technical failure\n\nIn place of laws constraining the behavior of intelligent machines, we need to give them emotions that can guide their learning of behaviors. They should want us to be happy and prosper, which is the emotion we call love. We can design intelligent machines so their primary, innate emotion is unconditional love for all humans. First we can build relatively simple machines that learn to recognize happiness and unhappiness in human facial expressions, human voices and human body language. Then we can hard-wire the result of this learning as the innate emotional values of more complex intelligent machines, positively reinforced when we are happy and negatively reinforced when we are unhappy. Machines can learn algorithms for approximately predicting the future, as for example investors currently use learning machines to predict future security prices. So we can program intelligent machines to learn algorithms for predicting future human happiness, and use those predictions as emotional values.      Bill Hibbard (2001), Super-intelligent Machines\n\nOnce upon a time, the US Army wanted to use neural networks to automatically detect camouflaged enemy tanks. The researchers trained a neural net on 50 photos of camouflaged tanks in trees, and 50 photos of trees without tanks. Using standard techniques for supervised learning, the researchers trained the neural network to a weighting that correctly loaded the training set – output ‘yes’ for the 50 photos of camouflaged tanks, and output ‘no’ for the 50 photos of empty forest. This did not ensure, or even imply, that new examples would be classified correctly. The neural network might have ‘learned’ 100 special cases that would not generalize to any new problem. Wisely, the researchers had originally taken 200 photos, 100 photos of tanks and 100 photos of trees. They had used only 50 of each for the training set. The researchers ran the neural network on the remaining 100 photos, and without further training, the neural network classified all remaining photos correctly. Success confirmed! The researchers handed the finished work to the Pentagon, which soon handed it back, complaining that in their own tests the neural network did no better than chance at discriminating photos.\n\nIt turned out that in the researchers’ data set, photos of camouflaged tanks had been taken on cloudy days, while photos of plain forest had been taken on sunny days. The neural network had learned to distinguish cloudy days from sunny days, instead of distinguishing camouflaged tanks from empty forest.²\n\nA technical failure occurs when the code does not do what you think it would do, though it faithfully executes as you programmed it. More than one model can load the same data. Suppose we trained a neural network to recognize smiling human faces and distinguish them from frowning human faces. Would the network classify a tiny picture of a smiley-face into the same attractor as a smiling human face? If an AI ‘hard-wired’ to such code possessed the power – and Hibbard (2001) spoke of superintelligence – would the galaxy end up tiled with tiny molecular pictures of smiley-faces?³\n\nThis form of failure is especially dangerous because it will appear to work within a fixed context, then fail when the context changes. The researchers of the ‘tank classifier’ story tweaked their neural network until it correctly loaded the training data, and then verified the network on additional data (without further tweaking). Unfortunately, both the training data and verification data turned out to share an assumption that held over all the data used in development but not in all the real-world contexts, where the neural network was called upon to function. In the story of the tank classifier, the assumption is that tanks are photographed on cloudy days.\n\nLet us suppose we wish to develop an AI of increasing power. The AI possesses a developmental stage where the human programmers are more powerful than the AI – not in the sense of mere physical control over the AI’s electrical supply, but in the sense that the human programmers are smarter, more creative, and more cunning than the AI. During the developmental period, we suppose that the programmers possess the ability to make changes to the AI’s source code without needing the consent of the AI to do so. However, the AI is also intended to possess post-developmental stages, including, in the case of Hibbard’s scenario, superhuman intelligence. An AI of superhuman intelligence is very unlikely to be modified without its consent by humans. At this point, we must rely on the previously laid-down goal system to function correctly, because if it operates in a sufficiently unforeseen fashion, the AI may actively resist our attempts to correct it – and, if the AI is smarter than a human, probably win.\n\nTrying to control a growing AI by training a neural network to provide its goal system faces the problem of a huge context change between the AI’s developmental stage and post-developmental stage. During the developmental stage, the AI may be able to produce only stimuli that fall into the ‘smiling human faces’ category, by solving humanly provided tasks, as its makers intended. Flash forward to a time when the AI is superhumanly intelligent and has built its own nanotech infrastructure, and the AI may be able to produce stimuli classified into the same attractor by tiling the galaxy with tiny smiling faces.\n\nThus the AI appears to work fine during development, but produces catastrophic results after it becomes smarter than the programmers(!).\n\nThere is a temptation to think, ‘But surely the AI will know that is not what we meant?’ But the code is not given to the AI, for the AI to look over and hand back if it does the wrong thing. The code is the AI. Perhaps with enough effort and understanding, we can write code that cares if we have written the wrong code – the legendary DWIM instruction, which among programmers stands for Do-What-I-Mean (Raymond, 2003). But effort is required to write a DWIM dynamic, and nowhere in Hibbard’s proposal is there mention of designing an AI that does what we mean, not what we say. Modern chips do not DWIM their code; it is not an automatic property. And if you messed up the DWIM itself, you would suffer the consequences. For example, suppose DWIM was defined as maximizing the satisfaction of the programmer with the code; when the code executed as a superintelligence, it might rewrite the programmers’ brains to be maximally satisfied with the code. I do not say this is inevitable; I only point out that Do-What-I-Mean is a major, non-trivial technical challenge of Friendly AI.\n\n15.8 Rates of intelligence increase\n\nFrom the standpoint of existential risk, one of the most critical points about Artificial Intelligence is that an AI might increase in intelligence extremely fast. The obvious reason to suspect this possibility is recursive self-improvement (Good, 1965). The AI becomes smarter, including becoming smarter at the task of writing the internal cognitive functions of an AI, so the AI can rewrite its existing cognitive functions to work even better, which makes the AI still smarter, including being smarter at the task of rewriting itself, so that it makes yet more improvements.\n\nAlthough human beings improve themselves to a limited extent (by learning, practicing, honing of skills and knowledge), our brains today are much the same as they were 10,000 years ago. In a similar sense, natural selection improves organisms, but the process of natural selection does not itself improve – not in a strong sense. Adaptation can open up the way for additional adaptations. In this sense, adaptation feeds on itself. But even as the gene pool boils, there is still an underlying heater, the process of mutation and recombination and selection, which is not itself re-architected. A few rare innovations increased the rate of evolution itself, such as the invention of sexual recombination. But even sex did not change the essential nature of evolution: its lack of abstract intelligence, its reliance on random mutations, its blindness and incrementalism, its focus on allele frequencies. Similarly, the inventions of language or science did not change the essential character of the human brain: its limbic core, its cerebral cortex, its prefrontal self-models, its characteristic speed of 200Hz.\n\nAn Artificial Intelligence could rewrite its code from scratch – it could change the underlying dynamics of optimization. Such an optimization process would wrap around much more strongly than either evolution accumulating adaptations or humans accumulating knowledge. The key implication for our purposes is that an AI might make a huge jump in intelligence after reaching some threshold of criticality.\n\nOne often encounters scepticism about this scenario – what Good (1965) called an ‘intelligence explosion’ – because progress in AI has the reputation of being very slow. At this point, it may prove helpful to review a loosely analogous historical surprise. (What follows is taken primarily from Rhodes, 1986.)\n\nIn 1933, Lord Ernest Rutherford said that no one could ever expect to derive power from splitting the atom: ‘Anyone who looked for a source of power in the transformation of atoms was talking moonshine.’ At that time laborious hours and weeks were required to fission a handful of nuclei.\n\nFlash forward to 1942, in a squash court beneath Stagg Field at the University of Chicago. Physicists are building a shape like a giant doorknob out of alternate layers of graphite and uranium, intended to start the first self-sustaining nuclear reaction. In charge of the project is Enrico Fermi. The key number for the pile is k, the effective neutron multiplication factor: the average number of neutrons from a fission reaction that cause another fission reaction. At k less than one, the pile is sub-critical. At k > 1, the pile should sustain a critical reaction. Fermi calculates that the pile will reach k = 1 between layers 56 and 57.\n\nA work crew led by Herbert Anderson finishes Layer 57 on the night of 1 December 1942. Control rods, strips of wood covered with neutron-absorbing cadmium foil, prevent the pile from reaching criticality. Anderson removes all but one control rod and measures the pile’s radiation, confirming that the pile is ready to chain-react the next day. Anderson inserts all cadmium rods and locks them into place with padlocks, then closes up the squash court and goes home.\n\nThe nextday, 2 December 1942, on a windy Chicago morning of sub-zero temperatures, Fermi begins the final experiment. All but one of the control rods are withdrawn. At 10.37 a.m., Fermi orders the final control rod withdrawn about half-way out. The Geiger counters click faster, and a graph pen moves upwards. ‘This is not it’, says Fermi, ‘the trace will go to this point and level off, indicating a spot on the graph. In a few minutes the graph pen comes to the indicated point, and does not go above it. Seven minutes later, Fermi orders the rod pulled out another foot. Again the radiation rises, then levels off. The rod is pulled out another six inches, then another, then another. At 11.30 a.m., the slow rise of the graph pen is punctuated by an enormous CRASH – an emergency control rod, triggered by an ionization chamber, activates and shuts down the pile, which is still short of criticality. Fermi calmly orders the team to break for lunch.\n\nAt 2 p.m., the team reconvenes, withdraws and locks the emergency control rod, and moves the control rod to its last setting. Fermi makes some measurements and calculations, then again begins the process of withdrawing the rod in slow increments. At 3.25 p.m., Fermi orders the rod withdrawn by another 12 inches. ‘This is going to do it’, Fermi says. ‘Now it will become self-sustaining. The trace will climb and continue to climb. It will not level off.\n\nHerbert Anderson recounts (from Rhodes, 1986, p. 27):\n\nAt first you could hear the sound of the neutron counter, clickety-clack, clickety-clack. Then the clicks came more and more rapidly, and after a while they began to merge into a roar; the counter couldn’t follow anymore. That was the moment to switch to the chart recorder. But when the switch was made, everyone watched in the sudden silence the mounting deflection of the recorder’s pen. It was an awesome silence. Everyone realized the significance of that switch; we were in the high intensity regime and the counters were unable to cope with the situation anymore. Again and again, the scale of the recorder had to be changed to accommodate the neutron intensity which was increasing more and more rapidly. Suddenly Fermi raised his hand. ‘The pile has gone critical’, he announced. No one present had any doubt about it.\n\nFermi kept the pile running for 28 minutes, with the neutron intensity doubling every two minutes. The first critical reaction had k of 1.0006. Even at k = 1.0006, the pile was only controllable because some of the neutrons from a uranium fission reaction are delayed – they come from the decay of short-lived fission by-products. For every 100 fissions in U₂₃₅, 242 neutrons are emitted almost immediately (0.0001s), and 1.58 neutrons are emitted an average of 10 seconds later. Thus the average lifetime of a neutron is approximately 0.1 second, implying 1200 generations in 2 minutes, and a doubling time of 2 minutes because 1.0006 to the power of 1200 is approximately 2. A nuclear reaction that is prompt critical is critical without the contribution of delayed neutrons. If Fermi’s pile had been prompt critical with k = 1.0006, neutron intensity would have doubled every tenth of a second.\n\nThe firstmoral is that confusing the speed of AI research with the speed of a real AI once built is like confusing the speed of physics research with the speed of nuclear reactions. It mixes up the map with the territory. It took years to get that first pile built, by a small group of physicists who did not generate much in the way of press releases. But, once the pile was built, interesting things happened on the timescale of nuclear interactions, not the timescale of human discourse. In the nuclear domain, elementary interactions happen much faster than human neurons fire. Much the same may be said of transistors.\n\nAnother moral is that there is a huge difference between one self-improvement triggering 0.9994 further improvements on average and another self-improvement triggering 1.0006 further improvements on average. The nuclear pile did not cross the critical threshold as the result of the physicists suddenly piling on a lot more material. The physicists piled on material slowly and steadily. Even if there is a smooth underlying curve of brain intelligence as a function of optimization pressure previously exerted on that brain, the curve of recursive self-improvement may show a huge leap.\n\nThere are also other reasons why an AI might show a sudden huge leap in intelligence. The species Homo sapiens showed a sharp jump in the effectiveness of intelligence, as the result of natural selection exerting a more-or-less steady optimization pressure on hominids for millions of years, gradually expanding the brain and prefrontal cortex, tweaking the software architecture. A few tens of thousands of years ago, hominid intelligence crossed some key threshold and made a huge leap in real-world effectiveness; we went from caves to skyscrapers in the blink of an evolutionary eye. This happened with a continuous underlying selection pressure – there was no huge jump in the optimization power of evolution when humans came along. The underlying brain architecture was also continuous – our cranial capacity did not suddenly increase by two orders of magnitude. So it might be that, even if the AI is being elaborated from outside by human programmers, the curve for effective intelligence will jump sharply.\n\nOr perhaps someone builds an AI prototype that shows some promising results, and the demo attracts another $100 million in venture capital, and this money purchases a thousand times as much supercomputing power. I doubt a 1000-fold increase in hardware would purchase anything like a 1000-fold increase in effective intelligence – butmere doubt is not reliable in the absence of any ability to perform an analytical calculation. Compared to chimps, humans have a threefold advantage in brain and a sixfold advantage in prefrontal cortex, which suggests (1) software is more important than hardware and (2) small increases in hardware can support large improvements in software. It is one more point to consider.\n\nFinally, AI may make an apparently sharp jump in intelligence purely as the result of anthropomorphism, the human tendency to think of ‘village idiot’ and ‘Einstein’ as the extreme ends of the intelligence scale, instead of nearly indistinguishable points on the scale of minds-in-general. Everything dumber than a dumb human may appear to us as simply ‘dumb’. One imagines the ‘AI arrow’ creeping steadily up the scale of intelligence, moving past mice and chimpanzees, with AIs still remaining ‘dumb’ because AIs cannot speak fluent language or write science papers, and then the AI arrow crosses the tiny gap from infra-idiot to ultra-Einstein in the course of one month or some similarly short period. I do not think this exact scenario is plausible, mostly because I do not expect the curve of recursive self-improvement to move at a linear creep. But I am not the first to point out that ‘AI’ is a moving target. As soon as a milestone is actually achieved, it ceases to be ‘AI’. This can only encourage procrastination.\n\nLet us concede for the sake of argument, for all we know (and it seems to me also probable in the real world), that an AI has the capability to make a sudden, sharp, large leap in intelligence. What follows from this?\n\nFirst and foremost: it follows that a reaction I often hear, ‘We don’t need to worry about Friendly AI because we don’t yet have AI’, is misguided or downright suicidal. We cannot rely on having distant advance warning before AI is created; past technological revolutions usually did not telegraph themselves to people alive at the time, whatever was said afterwards in hindsight. The mathematics and techniques of Friendly AI will not materialize from nowhere when needed; it takes years to lay firm foundations. Furthermore, we need to solve the Friendly AI challenge before Artificial General Intelligence is created, not afterwards; I should not even have to point this out. There will be difficulties for Friendly AI because the field of AI itself is in a state of low consensus and high entropy. But that does not mean we do not need to worry about Friendly AI. It means there will be difficulties. The two statements, sadly, are not remotely equivalent.\n\nThe possibility of sharp jumps in intelligence also implies a higher standard for Friendly AI techniques. The technique cannot assume the programmers’ ability to monitor the AI against its will, rewrite the AI against its will, bring to bear the threat of superior military force; nor may the algorithm assume that the programmers control a ‘reward button’, which a smarter AI could wrest from the programmers; etc. Indeed no one should be making these assumptions to begin with. The indispensable protection is an AI that does not want to hurt you. Without the indispensable, no auxiliary defence can be regarded as safe. No system is secure that searches for ways to defeat its own security. If the AI would harm humanity in any context, you must be doing something wrong on a very deep level, laying your foundations awry. You are building a shotgun, pointing the shotgun at your foot, and pulling the trigger. You are deliberately setting into motion a created cognitive dynamic that will seek in some context to hurt you. That is the wrong behaviour for the dynamic, but a right code that does something else instead.\n\nFor much the same reason, Friendly AI programmers should assume that the AI has total access to its own source code. If the AI wants to modify itself to be no longer Friendly, then Friendliness has already failed, at the point when the AI forms that intention. Any solution that relies on the AI not being able to modify itself must be broken in some way or other, and will still be broken even if the AI never does modify itself. I do not say it should be the only precaution, but the primary and indispensable precaution is that you choose into existence an AI that does not choose to hurt humanity.\n\nTo avoid the Giant Cheesecake Fallacy, we should note that the ability to self-improve does not imply the choice to do so. The successful exercise of Friendly AI technique might create an AI that had the potential to grow more quickly, but chose instead to grow along a slower and more manageable curve. Even so, after the AI passes the criticality threshold of potential recursive self-improvement, you are then operating in a much more dangerous regime. If Friendliness fails, the AI might decide to rush full speed ahead on self-improvement – metaphorically speaking, it would go prompt critical.\n\nI tend to assume arbitrarily large potential jumps for intelligence because (1) this is the conservative assumption; (2) it discourages proposals based on building AI without really understanding it; and (3) large potential jumps strike me as probable-in-the-real-world. If I encountered a domain where it was conservative from a risk-management perspective to assume slow improvement of the AI, then I would demand that a plan not break down catastrophically if an AI lingers at a near-human stage for years or longer. This is not a domain over which I am willing to offer narrow confidence intervals.\n\n15.9 Hardware\n\nPeople tend to think of large computers as the enabling factor for AI. This is, to put it mildly, an extremely questionable assumption. Outside futurists discussing AI talk about hardware progress because hardware progress is easy to measure – in contrast to understanding of intelligence. It is not that there has been no progress, but that the progress cannot be charted on neat Power Point graphs. Improvements in understanding are harder to report on and therefore less reported.\n\nRather than thinking in terms of the ‘minimum’ hardware ‘required’ for AI, think of a minimum level of researcher understanding that decreases as a function of hardware improvements. The better the computing hardware, the less understanding you need to build an AI. The extreme case is natural selection, which used a ridiculous amount of brute computational force to create intelligence using no understanding, and only non-chance retention of chance mutations.\n\nIncreased computing power makes it easier to build AI, but there is no obvious reason why increased computing power would help make the AI Friendly. Increased computing power makes it easier to use brute force; easier to combine poorly understood techniques that work. Moore’s Law steadily lowers the barrier that keeps us from building AI without a deep understanding of cognition.\n\nIt is acceptable to fail at AI and at Friendly AI. Similarly, it is acceptable to succeed at AI and at Friendly AI. What is not acceptable is succeeding at AI and failing at Friendly AI. Moore’s Law makes it easier to do exactly that – ‘easier’ but thankfully not easy. I doubt that AI will be ‘easy’ at the time it is finally built – simply because there are parties who will exert tremendous effort to build AI, and one of them will succeed after AI first becomes possible to build with tremendous effort.\n\nMoore’s Law is an interaction between Friendly AI and other technologies, which adds of t-overlooked existential risk to other technologies. We can imagine that molecular nanotechnology is developed by a benign multinational governmental consortium and that they successfully avert the physical-layer dangers of nanotechnology. They straightforwardly prevent accidental replicator releases, and with much greater difficulty they put global defences in place against malicious replicators; they restrict access to ‘root level’ nanotechnology while distributing configurable nanoblocks, etc. (see Chapter 21, this volume). But nonetheless, nanocomputers become widely available, either because attempted restrictions are bypassed, or because no restrictions are attempted. And then someone brute-forces an AI that is non-Friendly; and so the curtain is rung down. This scenario is especially worrying because incredibly powerful nanocomputers would be among the first, the easiest, and the safest-seeming applications of molecular nanotechnology.\n\nWhat of regulatory controls on supercomputers? I certainly would not rely on it to prevent AI from ever being developed; yesterday’s supercomputer is tomorrow’s laptop. The standard reply to a regulatory proposal is that when nanocomputers are outlawed, only outlaws will have nanocomputers. The burden is to argue that the supposed benefits of reduced distribution outweigh the inevitable risks of uneven distribution. For myself, I would certainly not argue in favour of regulatory restrictions on the use of supercomputers for AI research; it is a proposal of dubious benefit that would be fought tooth and nail by the entire AI community. But in the unlikely event that a proposal made it that far through the political process, I would not expend any significant effort on fighting it, because I do not expect the good guys to need access to the ‘supercomputers’ of their day. Friendly AI is not about brute-forcing the problem.\n\nI can imagine regulations effectively controlling a small set of ultra-expensive computing resources that are presently considered ‘supercomputers’. But computers are everywhere. It is not like the problem of nuclear proliferation, where the main emphasis is on controlling plutonium and enriched uranium. The raw materials for AI are already everywhere. That cat is so far out of the bag that it is in your wristwatch, cellphone, and dishwasher. This too is a special and unusual factor in AI as an existential risk. We are separated from the risky regime, not by large visible installations like isotope centrifuges or particle accelerators, but only by missing knowledge. To use a perhaps over-dramatic metaphor, imagine if sub-critical masses of enriched uranium had powered cars and ships throughout the world, before Leo Szilard first thought of the chain reaction.\n\n15.10 Threats and promises\n\nIt is a risky intellectual endeavour to predict specifically how a benevolent AI would help humanity, or an unfriendly AI harm it. There is the risk of conjunction fallacy: added detail necessarily reduces the joint probability of the entire story, but subjects often assign higher probabilities to stories that include strictly added details (See Chapter 5, this volume, on cognitive biases). There is the risk – virtually the certainty – of failure of imagination; and the risk of Giant Cheesecake Fallacy that leaps from capability to motive. Nonetheless, I will try to solidify threats and promises.\n\nThe future has a reputation for accomplishing feats that the past thought was impossible. Future civilizations have even broken what past civilizations thought (incorrectly, of course) to be the laws of physics. If prophets of 1900 AD – never mind 1000 AD – had tried to bound the powers of human civilization a billion years later, some of those impossibilities would have been accomplished before the century was out – for example, transmuting lead into gold. Because we remember future civilizations surprising past civilizations, it has become cliché that we cannot put limits on our great-grandchildren. And yet everyone in the twentieth century, in the nineteenth century, and in the eleventh century, was human.\n\nWe can distinguish three families of unreliable metaphors for imagining the capability of a smarter-than-human Artificial Intelligence:\n\n• G-factor metaphors: Inspired by differences of individual intelligence between humans. AIs will patent new technologies, publish groundbreaking research papers, make money on the stock market, or lead political power blocs.\n\n• History metaphors: Inspired by knowledge differences between past and future human civilizations. AIs will swiftly invent the kind of capabilities that cliché would attribute to human civilization a century or millennium from now: molecular nanotechnology, interstellar travel, computers performing 10²⁵ operations per second and so on.\n\n• Species metaphors: Inspired by differences of brain architecture between species. AIs have magic.\n\nThe g -factor metaphors seem most common in popular futurism: when people think of ‘intelligence’ they think of human geniuses instead of humans. In stories about hostile AI, g metaphors make for a Bostromian ‘good story’: an opponent that is powerful enough to create dramatic tension, but not powerful enough to instantly squash the heroes like bugs, and ultimately weak enough to lose in the final chapters of the book. Goliath against David is a ‘good story’, but Goliath against a fruitfly is not.\n\nIf we suppose the g -factor metaphor, then global catastrophic risks of this scenario are relatively mild; a hostile AI is not much more of a threat than a hostile human genius. If we suppose a multiplicity of AIs, then we have a metaphor of conflict between nations, between the AI tribe and the human tribe. If the AI tribe wins in military conflict and wipes out the humans, then that is an existential catastrophe of the Bang variety (Bostrom, 2001). If the AI tribe dominates the world economically and attains effective control of the destiny of Earth-originating intelligent life, but the AI tribe’s goals do not seem to us interesting or worthwhile, then that is a Shriek, Whimper, or Crunch.\n\nBut how likely is it that AI will cross the entire vast gap from amoeba to village idiot, and then stop at the level of human genius?\n\nThe fastest observed neurons fire 1000 times per second; the fastest axon fibres conduct signals at 150 m/second, a half-millionth the speed of light; each synaptic operation dissipates around 15,000 attojoules, which is more than a million times the thermodynamic minimum for irreversible computations at room temperature, 0.003 attojoules per bit.⁴ It would be physically possible to build a brain that computed a million times as fast as a human brain, without shrinking the size, or running at lower temperatures, or invoking reversible computing or quantum computing. If a human mind were thus accelerated, a subjective year of thinking would be accomplished for every 31 physical seconds in the outside world, and a millennium would fly by in eight-and-a-half hours. Vinge (1993) referred to such sped-up minds as ‘weak superintelligence’: a mind that thinks like a human but much faster.\n\nWe suppose there comes into existence an extremely fast mind, embedded in the midst of human technological civilization as it exists at that time. The failure of imagination is to say, ‘No matter how fast it thinks, it can only affect the world at the speed of its manipulators; it cannot operate machinery faster than it can order human hands to work; therefore a fast mind is no great threat’. It is no law of Nature that physical operations must crawl at the pace of long seconds. Critical times for elementary molecular interactions are measured in femtoseconds, sometimes picoseconds. Drexler (1992) has analysed controllable molecular manipulators that would complete > 10⁶ mechanical operations per second – note that this is in keeping with the general theme of ‘millionfold speedup’. (The smallest physically sensible increment of time is generally thought to be the Planck interval, 5 • 10⁻⁴⁴ seconds, on which scale even the dancing quarks are statues.)\n\nSuppose that a human civilization were locked in a box and allowed to affect the outside world only through the glacially slow movement of alien tentacles, or mechanical arms that moved at microns per second. We would focus all our creativity on finding the shortest possible path to building fast manipulators in the outside world. Pondering over fast manipulators, one immediately thinks of molecular nanotechnology – though there may be other ways. What is the shortest path you could take to molecular nanotechnology in the slow outside world, if you had eons to ponder over each move? The answer is that I do not know because I do not have eons to ponder. Here is one imaginable fast pathway:\n\n• Crack the protein folding problem, to the extent of being able to generate DNA strings whose folded peptide sequences fill specific functional roles in a complex chemical interaction.\n\n• Email sets of DNA strings to one or more online laboratories that offer DNA synthesis, peptide sequencing, and FedEx delivery. (Many labs currently offer this service, and some boast of 72-hour turnaround times.)\n\n• Find at least one human connected to the Internet who can be paid, blackmailed, or fooled by the right background story, into receiving FedExed vials and mixing them in a specified environment.\n\n• The synthesized proteins form a very primitive ‘wet’ nanosystem, which, ribosome-like, is capable of accepting external instructions; perhaps patterned acoustic vibrations delivered by a speaker attached to the beaker.\n\n• Use the extremely primitive nanosystem to build more sophisticated systems, which construct still more sophisticated systems, bootstrapping to molecular nanotechnology – or beyond.\n\nThe elapsed turnaround time would be, imaginably, on the order of a week from when the fast intelligence first became able to solve the protein folding problem. Of course this whole scenario is strictly something I am thinking of. Perhaps in 19,500 years of subjective time (one week of physical time at a millionfold speedup) I would think of a better way. Perhaps you can pay for rush courier delivery instead of FedEx. Perhaps there are existing technologies, or slight modifications of existing technologies, that combine synergetically with simple protein machinery. Perhaps if you are sufficiently smart, you can use waveformed electrical fields to alter reaction pathways in existing biochemical processes. I do not know. I am not that smart.\n\nThe challenge is to chain your capabilities – the physical-world analogue of combining weak vulnerabilities in a computer system to obtain root access. If one path is blocked, you choose another, seeking always to increase your capabilities and use them in synergy. The presumptive goal is to obtain rapid infrastructure, means of manipulating the external world on a large scale in fast time. Molecular nanotechnology fits this criterion, first because its elementary operations are fast, and second because there exists a ready supply of precise parts – atoms – which can be used to self-replicate and exponentially grow the nanotechnological infrastructure. The pathway alleged above has the AI obtaining rapid infrastructure within a week – this sounds fast to a human with 200Hz neurons, but is a vastly longer time for the AI.\n\nOnce the AI possesses rapid infrastructure, further events happen on the AI’s timescale, not a human timescale (unless the AI prefers to act on a human timescale). With molecular nanotechnology, the AI could (potentially) rewrite the solar system unopposed.\n\nAn unFriendly AI with molecular nanotechnology (or other rapid infrastructure) need not bother with marching robot armies or blackmail or subtle economic coercion. The unFriendly AI has the ability to repattern all matter in the solar system according to its optimization target. This is fatal for us if the AI does not choose specifically according to the criterion of how this transformation affects existing patterns such as biology and people. The AI neither hates you, nor loves you, but you are made out of atoms that it can use for something else. The AI runs on a different timescale than you do; by the time your neurons finish thinking the words ‘I should do something’ you have already lost.\n\nA Friendly AI in addition to molecular nanotechnology is presumptively powerful enough to solve any problem, which can be solved either by moving atoms or by creative thinking. One should beware of failures of imagination: curing cancer is a popular contemporary target of philanthropy, but it does not follow that a Friendly AI with molecular nanotechnology would say to itself, ‘Now I shall cure cancer’. Perhaps a better way to view the problem is that biological cells are not programmable. To solve the latter problem cures cancer as a special case, along with diabetes and obesity. A fast, nice intelligence wielding molecular nanotechnology is power on the order of getting rid of disease, not getting rid of cancer.\n\nThere is finally the family of species metaphors, based on between-species differences of intelligence. The AI has magic – not in the sense of incantations and potions, but in the sense that a wolf cannot understand how a gun works, or what sort of effort goes into making a gun, or the nature of that human power that lets us invent guns. Vinge (1993) wrote:\n\nStrong superhumanity would be more than cranking up the clock speed on a human-equivalent mind. It’s hard to say precisely what strong superhumanity would be like, but the difference appears to be profound. Imagine running a dog mind at very high speed. Would a thousand years of doggy living add up to any human insight?\n\nThe species metaphor would seem the nearest analogy a priori, but it does not lend itself to making up detailed stories. The main advice the metaphor gives us is that we had better get Friendly AI right, which is good advice in any case. The only defence it suggests against hostile AI is not to build it in the first place, which is also excellent advice. Absolute power is a conservative engineering assumption in Friendly AI, exposing broken designs. If an AI will hurt you given magic, the Friendliness architecture is wrong.\n\n15.11 Local and Majoritarian Strategies\n\nOne may classify proposed risk-mitigation strategies into the following:\n\n• Strategies that require unanimous cooperation: strategies that can be catastrophically defeated by individual defectors or small groups.\n\n• Strategies that require majority action: a majority of a legislature in a single country, or a majority of voters in a country, or a majority of countries in the United Nations; the strategy requires most, but not all, people in a large pre-existing group to behave in a particular way.\n\n• Strategies that require local action: a concentration of will, talent, and funding which overcomes the threshold of some specific task.\n\nUnanimous strategies are unworkable, but it does not stop people from proposing them.\n\nA majoritarian strategy is sometimes workable, if you have decades in which to do your work. One must build a movement, from its first beginnings over the years, to its debut as a recognized force in public policy, to its victory over opposing factions. Majoritarian strategies take substantial time and enormous effort. People have set out to do such, and history records some successes. But beware: history books tend to focus selectively on movements that have an impact, as opposed to the vast majority that never amount to anything. There is an element involved of luck, and of the public’s prior willingness to hear. Critical points in the strategy will involve events beyond your personal control. If you are not willing to devote your entire life to pushing through a majoritarian strategy, do not bother; and just one life devoted will not be enough, either.\n\nOrdinarily, local strategies are most plausible. A 100 million dollars of funding is not easy to obtain, and a global political change is not impossible to push through, but it is still vastly easier to obtain a 100 million dollars of funding than to push through a global political change.\n\nTwo assumptions that give rise to a majoritarian strategy for AI are as follows:\n\n• A majority of Friendly AIs can effectively protect the human species from a few unFriendly AIs.\n\n• The firstAI built cannot by itself do catastrophic damage.\n\nThis reprises essentially the situation of a human civilization before the development of nuclear and biological weapons: most people are cooperators in the overall social structure, and defectors can do damage but not global catastrophic damage. Most AI researchers will not want to make unFriendly AIs. So long as someone knows how to build a stable Friendly AI – so long as the problem is not completely beyond contemporary knowledge and technique – researchers will learn from each other’s successes and repeat them. Legislation could (for example) require researchers to publicly report their Friendliness strategies, or penalize researchers whose AIs cause damage; and while this legislation will not prevent all mistakes, it may suffice that a majority of AIs are built Friendly.\n\nWe can also imagine a scenario that implies an easy local strategy:\n\n• The first AI cannot by itself do catastrophic damage.\n\n• If even a single Friendly AI exists, that AI plus human institutions can fend off any number of unFriendly AIs.\n\nThe easy scenario would hold if, for example, human institutions can reliably distinguish Friendly AIs from unFriendly ones, and give revocable power into the hands of Friendly AIs. Thus we could pick and choose our allies. The only requirement is that the Friendly AI problem must be solvable (as opposed to being completely beyond human ability).\n\nBoth of the above scenarios assume that the first AI (the first powerful, general AI) cannot by itself do global catastrophic damage. Most concrete visualizations that imply this use a g metaphor: AIs as analogous to unusually able humans. In Section 11.7 on rates of intelligence increase, I listed some reasons to be wary of a huge, fast jump in intelligence:\n\n• The distance from idiot to Einstein, which looms large to us, is a small dot on the scale of minds-in-general.\n\n• Hominids made a sharp jump in real-world effectiveness of intelligence, despite natural selection exerting roughly steady optimization pressure on the underlying genome.\n\n• An AI may absorb a huge amount of additional hardware after reaching some brink of competence (i.e., eat the Internet).\n\n• Criticality threshold of recursive self-improvement. One self-improve-ment triggering 1.0006 self-improvements is qualitatively different from one self-improvement triggering 0.9994 self-improvements.\n\nAs described in Section 15.9, a sufficiently powerful intelligence may need only a short time (from a human perspective) to achieve molecular nanotechnology, or some other form of rapid infrastructure.\n\nWe can therefore visualize a possible first-mover effect in superintelligence. The first-mover effect is when the outcome for Earth-originating intelligent life depends primarily on the makeup of whichever mind first achieves some key threshold of intelligence – such as criticality of self-improvement. The two necessary assumptions are as follows:\n\n• The first AI to surpass some key threshold (e.g., criticality of self-improvement), if unFriendly, can wipe out the human species.\n\n• The first AI to surpass the same threshold, if Friendly, can prevent a hostile AI from coming into existence or from harming the human species; or find some other creative way to ensure the survival and prosperity of Earth-originating intelligent life.\n\nMore than one scenario qualifies as a first-mover effect. Each of these examples reflects a different key threshold:\n\n• Post-criticality, self-improvement reaches superintelligence on a timescale of weeks or less. AI projects are sufficiently sparse that no other AI achieves criticality before the first mover is powerful enough to overcome all opposition. The key threshold is criticality of recursive self-improvement.\n\n• AI-1 cracks protein folding three days before AI-2. AI-1 achieves nanotechnology six hours before AI-2. With rapid manipulators, AI-1 can (potentially) disable AI-2’s R&D before fruition. The runners are close, but whoever crosses the finish line first, wins. The key threshold is rapid infrastructure.\n\n• The first AI to absorb the Internet can (potentially) keep it out of the hands of other AIs. Afterwards, by economic domination or covert action or blackmail or supreme ability at social manipulation, the first AI halts or slows other AI projects so that no other AI catches up. The key threshold is absorption of a unique resource.\n\nThe human species, Homo sapiens, is a first mover. From an evolutionary perspective, our cousins, the chimpanzees, are only a hairbreadth away from us. Homo sapiens still wound up with all the technological marbles because we got there a little earlier. Evolutionary biologists are still trying to unravel which order the key thresholds came in, because the first-mover species was first to cross so many: Speech, technology, abstract thought (see, however, also the findings Chapter 3, this volume).\n\nA first-mover effect implies a theoretically localizable strategy (a task that can, in principle, be carried out by a strictly local effort), but it invokes a technical challenge of extreme difficulty. We only need to get Friendly AI right in one place and one time, not every time everywhere. But someone must get Friendly AI right on the first try, before anyone else builds AI to a lower standard.\n\nI cannot perform a precise calculation using a precisely confirmed theory, but my current opinion is that sharp jumps in intelligence are possible, likely, and constitute the dominant probability. But a much more serious problem is strategies visualized for slow-growing AIs, which fail catastrophically if there is a first-mover effect. This is considered a more serious problem for the following reasons:\n\n• Faster-growing AIs represent a greater technical challenge.\n\n• Like a car driving over a bridge built for trucks, an AI designed to remain Friendly in extreme conditions should (presumptively) remain Friendly in less extreme conditions. The reverse is not true.\n\n• Rapid jumps in intelligence are counterintuitive in everyday social reality. The g-factor metaphor for AI is intuitive, appealing, reassuring, and conveniently implies fewer design constraints.\n\nMy current strategic outlook tends to focus on the difficult local scenario: the first AI must be Friendly. With the caveat that, if no sharp jumps in intelligence materialize, it should be possible to switch to a strategy for making a majority of AIs Friendly. In either case, the technical effort that went into preparing for the extreme case of a first mover should leave us better off, not worse.\n\nThe scenario that implies an impossible, unanimous strategy is as follows:\n\n• A single AI can be powerful enough to destroy humanity, even despite the protective efforts of Friendly AIs.\n\n• No AI is powerful enough to prevent human researchers from building one AI after another (or find some other creative way of solving the problem).\n\nIt is good that this balance of abilities seems unlikely a priori, because in this scenario we are doomed. If you deal out cards from a deck, one after another, you will eventually deal out the ace of clubs.\n\nThe same problem applies to the strategy of deliberately building AIs that choose not to increase their capabilities past a fixed point. If capped AIs are not powerful enough to defeat uncapped AIs, or preventuncapped AIs from coming into existence, then capped AIs cancel out of the equation. We keep dealing through the deck until we deal out a superintelligence, whether it is the ace of hearts or the ace of clubs.\n\nA majoritarian strategy only works if it is not possible for a single defector to cause global catastrophic damage. For AI, this possibility or impossibility is a natural feature of the design space – the possibility is not subject to human decision any more than the speed of light or the gravitational constant.\n\n15.12 Interactions of Artificial Intelligence with other technologies\n\nSpeeding up a desirable technology is a local strategy, while slowing down a dangerous technology is a difficult majoritarian strategy. Halting or relinquishing an undesirable technology tends to require an impossible unanimous strategy. I would suggest that we think, not in terms of developing or not-developing technologies, but in terms of our pragmatically available latitude to accelerate or slow down technologies; and ask, within the realistic bounds of this latitude, which technologies we might prefer to see developed before or after one another.\n\nIn nanotechnology, the goal usually presented is to develop defensive shields before offensive technologies. I worry a great deal about this, because a given level of offensive technology tends to require much less sophistication than a technology that can defend against it. Guns were developed centuries before bullet-proof vests were made. Smallpox was used as a tool of war before the development of smallpox vaccines. Today there is still no shield that can deflect a nuclear explosion; nations are protected not by defences that cancel offences, but by a balance of offensive terror.\n\nSo should we prefer that nanotechnology precede the development of AI, or that AI precede the development of nanotechnology? So far as ordering is concerned, the question we should ask is, ‘Does AI help us deal with nanotechnology? Does nanotechnology help us deal with AI?’\n\nIt looks to me like a successful resolution of Artificial Intelligence should help us considerably in dealing with nanotechnology. I cannot see how nanotechnology would make it easier to develop Friendly AI. If huge nanocomputers make it easier to develop AI without making it easier to solve the particular challenge of Friendliness, that is a negative interaction. Thus, all else being equal, I would greatly prefer that Friendly AI precede nanotechnology in the ordering of technological developments. If we confront the challenge of AI and succeed, we can call on Friendly AI to help us with nanotechnology. If we develop nanotechnology and survive, we still have the challenge of AI to deal with after that.\n\nGenerally speaking, a success on Friendly AI should help solve nearly any other problem. Thus, if a technology makes AI neither easier nor harder, but carries with it a catastrophic risk, we should prefer all else being equal to first confront the challenge of AI.\n\nAny technology that increases available computing power decreases the minimum theoretical sophistication necessary to develop AI, but does not help at all on the Friendly side of things, and I count it as a net negative. Moore’s Law of Mad Science: Every 18 months, the minimum IQ necessary to destroy the world drops by one point.\n\nA success on human intelligence enhancement would make Friendly AI easier, and also help on other technologies. But human augmentation is not necessarily safer, or easier, than Friendly AI; nor does it necessarily lie within our realistically available latitude to reverse the natural ordering of human augmentation and Friendly AI, if one technology is naturally much easier than the other.\n\n15.13 Making progress on Friendly Artificial Intelligence\n\nWe propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.\n\nMcCarthy, Minsky, Rochester, and Shannon (1955)\n\nThe Proposal for the Dartmouth Summer Research Project on Artificial Intelligence is the first recorded use of the phrase ‘artificial intelligence’. They had no prior experience to warn them that the problem was hard. I would still label it a genuine mistake, that they said ‘a significant advance can be made’, not might be made, with a summer’s work. That is a specific guess about the problem difficulty and solution time, which carries a specific burden of improbability. But if they had said might, I would have no objection. How were they to know?\n\nThe Dartmouth Proposal included, among others, the following topics: linguistic communication, linguistic reasoning, neural nets, abstraction, randomness and creativity, interacting with the environment, modelling the brain, originality, prediction, invention, discovery, and self-improvement.\n\nNow it seems to me that an AI capable of language, abstract thought, creativity, environmental interaction, originality, prediction, invention, discovery, and above all self-improvement, is well beyond the point where it needs also to be Friendly.\n\nThe Dartmouth Proposal makes no mention of building nice/good/benevolent AI. Questions of safety are not mentioned even for the purpose of dismissing them. This, even in that bright summer, when human-level AI seemed just around the corner. The Dartmouth Proposal was written in 1955, before the Asilomar conference on biotechnology, thalidomide babies, Chernobyl, or 11 September. If today the idea of artificial intelligence were proposed for the first time, then someone would demand to know what specifically was being done to manage the risks. I am not saying whether this is a good change or a bad change in our culture. I am not saying whether this produces good or bad science. But the point remains that if the Dartmouth Proposal had been written 50 years later, one of the topics would have been safety.\n\nAt the time of this writing in 2007, the AI RESEARCH community still does not see Friendly AI as part of the problem. I wish I could cite a reference to this effect, but I cannot cite an absence of literature. Friendly AI is absent from the conceptual landscape, not just unpopular or unfunded. You cannot even call Friendly AI a blank spot on the map, because there is no notion that something is missing.⁵, ⁶ If you have read popular/semi-technical books proposing how to build AI, suchas Gödel, Escher, Bach (Hof stadter, 1979)or The Society of Mind (Minsky, 1986), you may think back and recall that you did not see Friendly AI discussed as part of the challenge. Neither have I seen Friendly AI discussed in the technical literature as a technical problem. My attempted literature search turned up primarily brief non-technical papers, unconnected to each other, with no major reference in common except Isaac Asimov’s ‘Three Laws of Robotics’ (Asimov, 1942).\n\nThe field of AI has techniques, such as neural networks and evolutionary programming, which have grown in power with the slow tweaking over decades. But neural networks are opaque – the user has no idea how the neural net is making its decisions – and cannot easily be rendered non-opaque; the people who invented and polished neural networks were not thinking about the long-term problems of Friendly AI. Evolutionary programming (EP) is stochastic, and does not precisely preserve the optimization target in the generated code; EP gives you code that does what you ask, most of the time, under the tested circumstances, but the code may also do something else on the side. EP is a powerful, still maturing technique that is intrinsically unsuited to the demands of Friendly AI. Friendly AI, as I have proposed it, requires repeated cycles of recursive self-improvement that precisely preserve a stable optimization target.\n\nThe mostpowerful current AI techniques, as they were developed and then polished and improved over time, have basic incompatibilities with the requirements of Friendly AI as I currently see them. The Y2K problem -although not a global-catastrophe, but which proved very expensive to fix -analogously arose from failing to foresee tomorrow’s design requirements. The nightmare scenario is that we find ourselves stuck with a catalogue of mature, powerful, publicly available AI techniques, which combine to yield non-Friendly AI, but which cannot be used to build Friendly AI without redoing the last three decades of AI work from scratch.\n\n15.14 Conclusion\n\nIt once occurred to me that modern civilization occupies an unstable state. I.J. Good’s hypothesized intelligence explosion describes a dynamically unstable system, like a pen precariously balanced on its tip. If the pen is exactly vertical, it may remain upright; but if the pen tilts even a little from the vertical, gravity pulls it farther in that direction, and the process accelerates. So too would smarter systems have an easier time making themselves smarter.\n\nA dead planet, lifelessly orbiting its star, is also stable. Unlike an intelligence explosion, extinction is not a dynamic attractor – there is a large gap between almost extinct, and extinct. Even so, total extinction is stable.\n\nMust not our civilization eventually wander into one mode or the other?\n\nAs a logic, the above argument contains holes. Giant Cheesecake Fallacy, for example: minds do not blindly wander into attractors, they have motives. Even so, I suspect that, pragmatically speaking, our alternatives boil down to becoming smarter or becoming extinct.\n\nNature is, not cruel, but indifferent; a neutrality that often seems indistinguishable from outright hostility. Reality throws at you one challenge after another, and when you run into a challenge you cannot handle, you suffer the consequences. Often, Nature poses requirements that are grossly unfair, even on tests where the penalty for failure is death. How is a tenth-century medieval peasant supposed to invent a cure for tuberculosis? Nature does not match her challenges to your skill, or your resources, or how much free time you have to think about the problem. And when you run into a lethal challenge too difficult for you, you die. It may be unpleasant to think about, but that has been the reality for humans, for thousands upon thousands of years. The same thing could as easily happen to the whole human species, if the human species runs into an unfair challenge.\n\nIf human beings did not age, so that 100-year-olds had the same death rate as 15-year-olds, we would not be immortal. We would last only until the probabilities caught up with us. To live even a million years, as an unaging human in a world as risky as our own, you must somehow drive your annual probability of accident down to nearly zero. You may not drive; you may not fly; you may not walk across the street even after looking both ways, for it is still too great a risk. Even if you abandoned all thoughts of fun, gave up living to preserve your life, you could not navigate a million-year obstacle course. It would be, not physically impossible, but cognitively impossible.\n\nThe human species, Homo sapiens, is unaging but not immortal. Hominids have survived this long only because, for the last million years, there were no arsenals of hydrogen bombs, no spaceships to steer asteroids towards Earth, no biological weapons labs to produce superviruses, no recurring annual prospect of nuclear war or nanotechnological war or rogue AI. To survive any appreciable time, we need to drive down each risk to nearly zero. ‘Fairly good’ is not good enough to last another million years.\n\nIt seems like an unfair challenge. Such competence is not historically typical of human institutions, no matter how hard they try. For decades, the United States and the USSR avoided nuclear war, but not perfectly; there were close calls, such as the Cuban Missile Crisis in 1962. If we postulate that future minds exhibit the same mixture of foolishness and wisdom, the same mixture of heroism and selfishness, as the minds we read about in history books – then the game of existential risk is already over; it was lost from the beginning. We might survive for another decade, even another century, but not another million years.\n\nBut the human mind is not the limit of the possible. Homo sapiens represent the first general intelligence. We were born into the uttermost beginning of things, the dawn of mind. With luck, future historians will look back and describe the present world as an awkward in-between stage of adolescence, when humankind was smart enough to create tremendous problems for itself, but not quite smart enough to solve them.\n\nYet before we can pass out of that stage of adolescence, we must, as adolescents, confront an adult problem: the challenge of smarter-than-human intelligence. This is the way out of the high-mortality phase of the life cycle, the way to close the window of vulnerability; it is also probably the single most dangerous risk we face. Artificial Intelligence is one road into that challenge; and I think it is the road we will end up taking.\n\nI do not want to play down the colossal audacity of trying to build, to a precise purpose and design, something smarter than ourselves. But let us pause and recall that intelligence is not the first thing human science has ever encountered that proved difficult to understand. Stars were once mysteries, and chemistry, and biology. Generations of investigators tried and failed to understand those mysteries, and they acquired the reputation of being impossible to mere science. Once upon a time, no one understood why some matter was inert and lifeless, while other matter pulsed with blood and vitality. No one knew how living matter reproduced itself, or why our hands obeyed our mental orders. Lord Kelvin wrote:\n\nThe influence of animal or vegetable life on matter is infinitely beyond the range of any scientific inquiry hitherto entered on. Its power of directing the motions of moving particles, in the demonstrated daily miracle of our human free-will, and in the growth of generation after generation of plants from a single seed, are infinitely different from any possible result of the fortuitous concurrence of atoms. (Quoted in MacFie, 1912)\n\nAll scientific ignorance is hallowed by ancientness. Each and every absence of knowledge dates back to the dawn of human curiosity; and the hole lasts through the ages, seemingly eternal, right up until someone fills it. I think it is possible for mere fallible humans to succeed on the challenge of building Friendly AI. But only if intelligence ceases to be a sacred mystery to us, as life was a sacred mystery to Lord Kelvin. Intelligence must cease to be any kind of mystery whatever, sacred or not. We must execute the creation of Artificial Intelligence as the exact application of an exact art. And maybe then we can win.\n\nAcknowledgement\n\nI thank Michael Roy Ames, Nick Bostrom, Milan M. (Ćirković, John K Clark, Emil Gilliam, Ben Goertzel, Robin Hanson, Keith Henson, Bill Hibbard, Olie Lamb, Peter McCluskey, and Michael Wilson for their comments, suggestions and criticisms. Needless to say, any remaining errors in this paper are my own.\n\nReferences\n\nAllen, C., Wallach, W., and Smit, I. (2006). Why machine ethics? IEEE Intell. Syst., 21(4), 12–17.\n\nAnderson, M. and Anderson, S. (2006). Guest editors’ introduction: machine ethics. IEEE Intell. Syst., 21(4), 1550–1604.\n\nAsimov, I. (March 1942). Runaround. Astounding Science Fiction.\n\nAnderson, M. and Anderson, S. (2006). Guest Editors’ Introduction: Machine Ethics. IEEE Intelligent Systems, 21(4), pp. 1550–1604.\n\nAllen, C., Wallach, W. and Smit, I. (2006). Why Machine Ethics? IEEE Intelligent Systems, 21(4), pp. 12–17.\n\nBarrett, J.L. and Keil, F. (1996). Conceptualizing a non-natural entity: anthropomorphism in God concepts. Cogn. Psychol., 31, 219–247.\n\nBostrom, N. (1998). How long before superintelligence? Int. J. Future Studies, 2.\n\nBostrom, N. (2001). Existential risks: analyzing human extinction scenarios. J.Evol. Technol., 9.\n\nBrown, D.E. (1991). Human Universals (New York: McGraw-Hill).\n\nCrochat, P. and Franklin, D. (2000). Back-propagation neural network tutorial. http://ieee.uow.edu.au/~daniel/sof tware/libneural/\n\nDeacon, T. (1997). The Symbolic Species: The Co-evolution of Language and the Brain (New York: Norton).\n\nDrexler, K.E. (1992). Nano systems: Molecular Machinery, Manufacturing, and Computation (New York: Wiley-Interscience).\n\nEkman, P. and Keltner, D. (1997). Universal facial expressions of emotion: an old controversy and new findings. In Segerstrale, U. and Molnar, P. (eds.), Nonverbal Communication: Where Nature Meets Culture, pp. 27–46 (Mahwah, NJ: Lawrence Erlbaum Associates).\n\nGood, I.J. (1965). Speculations concerning the first ultraintelligent machine. In Alt, F.L. and Rubin off, M. (eds.), Advances in Computers, Vol 6, pp. 31–88 (New York: Academic Press).\n\nHayes, J.R. (1981). The Complete Problem Solver (Philadelphia, PA: Franklin Institute Press).\n\nHibbard, B. (2001). Super-intelligent machines. ACM SIGGRAPH Computer Graphics, 35(1), 11–13.\n\nHibbard, B. (2004). Reinforcement learning as a Context for Integrating AI Research. Presented at the 2004 AAAI Fall Symposium on Achieving Human-level Intelligence through Integrated Systems and Research. edited by N. Cassimatis &D. Winston, The AAAI Press, Mento Park, California.\n\nHibbard, B. (2006). Replyto AI Risk. http://www.ssec.wisc.edu/~billh/g/AIRisk\\_Reply.html\n\nHofstadter, D. (1979). Gödel, Escher, Bach: An Eternal Golden Braid (New York: Random House).\n\nJaynes, E.T. and Bretthorst, G.L. (2003). Probability Theory: The Logic of Science (Cambridge: Cambridge University Press). Jensen, A.R. (1999). The G factor: the science of mental ability. Psycoloquy, 10(23).\n\nMacFie, R.C. (1912). Heredity, Evolution, and Vitalism: Some of the Discoveries of Modern Research into These Matters – Their Trend and Significance (New York: William Wood and Company).\n\nMcCarthy, J., Minsky, M.L., Rochester, N., and Shannon, C.E. (1955). A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence. http://www.formal.stanford.edu/jmc/history/dartmouth/dartmouth.html.\n\nMerkle, R.C. (November 1989). Large scale analysis of neural structure. Xerox PARC Technical Report CSL-89-10.\n\nMerkle, R.C. and Drexler, K.E. (1996). Helical logic. Nanotechnology, 7, 325–339.\n\nMinsky, M.L. (1986). The Society of Mind (New York: Simon and Schuster).\n\nMonod, J.L. (1974). On the Molecular Theory of Evolution (New York: Oxford).\n\nMoravec, H. (1988). Mind Children: The Future of Robot and Human Intelligence (Cambridge: Harvard University Press).\n\nMoravec, H. (1999). Robot: Mere Machine to Transcendent Mind (New York: Oxford University Press).\n\nRaymond, E.S. (ed.) (December 2003). DWIM. The on-line hacker Jargon File, version 4.4.7, 29\n\nRhodes, R. (1986). The Making of the Atomic Bomb (New York: Simon &Schuster). Rice, H.G. (1953). Classes of recursively enumerable sets and their decision problems. Trans. Am. Math. Soc., 74, 358–366.\n\nRussell, S.J. and Norvig, P. (2003). Artificial Intelligence: A Modern Approach, pp. 962–964 (NJ: Prentice Hall).\n\nSandberg, A. (1999). The physics of information processing superobjects: daily life mong the Jupiter brains. J. Evol. Technol., 5. http://ftp.nada.kth.se/pub/home/asa/work/Brains/Brains2\n\nSchmidhuber, J. (2003). Goedel machines: self-referential universal problem solvers making provably optimal self-improvements. In Goertzel, B. and Pennachin, C. (eds.), Artificial General Intelligence, (New York: Springer-Verlag).\n\nSober, E. (1984). The Nature of Selection (Cambridge, MA: MIT Press).\n\nTooby, J. and Cosmides, L. (1992). The psychological foundations of culture. In Barkow, J.H., Cosmides, L. and Tooby, J. (eds.), The Adapted Mind: Evolutionary Psychology and the Generation of Culture, (New York: Oxford University Press).\n\nVinge, V. (March 1993). The Coming Technological Singularity. Presented at the VISION-21 Symposium, sponsored by NASA Lewis Research Center and the Ohio Aerospace Institute.\n\nWachowski, A. and Wachowski, L. (1999). The Matrix (Warner Bros, 135 min, USA).\n\nWeisburg, R. (1986). Creativity, Genius and Other Myths (New York: W.H. Freeman).\n\nWilliams, G.C. (1966). Adaptation and Natural Selection: A Critique of Some Current Evolutionary Thought (Princeton, NJ: Princeton University Press).\n\nYudkowsky, E. (2006). Reply to AI Risk. http://www.ssec.wisc.edu/~billh/g/AIRisk\\_Reply.html\n\n• 16 • Big troubles, imagined and real\n\nFrank Wilczek\n\nModern physics suggests several exotic ways in which things could go terribly wrong on a very large scale. Most, but not all, are highly speculative, unlikely, or remote. Rare catastrophes might well have decisive influences on the evolution of life in the universe. So also might slow but inexorable changes in the cosmic environment in the future.\n\n16.1 Why look for trouble?\n\nOnly a twisted mind will find joy in contemplating exotic ways to shower doom on the world as we know it. Putting aside that hedonistic motivation, there are several good reasons for physicists to investigate doomsday scenarios that include the following:\n\nLooking before leaping : Experimental physics often aims to produce extreme conditions that do not occur naturally on Earth (or perhaps elsewhere in the universe). Modern high-energy accelerators are one example; nuclear weapons labs are another. With new conditions come new possibilities, including – perhaps – the possibility of large-scale catstrophe. Also, new technologies enabled by advances in physics and kindred engineering disciplines might trigger social or ecological instabilities. The wisdom of ‘Look before you leap’ is one important motivation for considering worst-case scenarios.\n\nPreparing to prepare: Other drastic changes and challenges must be anticipated, even if we forego daring leaps. Such changes and challenges include exhaustion of energy supplies, possible asteroid or cometary impacts, orbital evolution and precessional instability of Earth, evolution of the Sun, and – in the verylong run – some form of ‘heat death of the universe’. Many of these are long-term problems, but tough ones that, if neglected, will only loom larger. So we should prepare, or at least prepare to prepare, well in advance of crises.\n\nWondering : Catastrophes might leave a mark on cosmic evolution, in both the physical and (exo)biological senses. Certainly, recent work has established a major role for catastrophes in sculpting terrestrial evolution (see http://www.answers.com/topic/timeline-of-evolution). So to understand the universe, we must take into account their possible occurrence. In particular, serious consideration of Fermi’s question ‘Where are they?’, or logical pursuit of anthropic reasoning, cannot be separated from thinking about how things could go drastically wrong.\n\nThis will be a very unbalanced essay. The most urgent and realistic catastrophe scenarios, I think, arise from well-known and much-discussed dangers: the possible use of nuclear weapons and the alteration of global climate. Here those dangers will be mentioned only in passing. The focus instead will be scenarios for catastrophe that are not-so-urgent and/or highly speculative, but involve interesting issues in fundamental physics and cosmology. Thinking about these exotic scenarios needs no apology; but I do want to make it clear that I, in no way, want to exaggerate their relative importance, or to minimize the importance of plainer, more imminent dangers.\n\n16.2 Looking before leaping\n\n16.2.1 Accelerator disasters\n\nSome accelerators are designed to be dangerous. Those accelerators are the colliders that bring together solid uranium or plutonium ‘beams’ to produce a fission reaction – in other words, nuclear weapons. Famously, the physicists of the Manhattan project made remarkably accurate estimates of the unprecedented amount of energy their ‘accelerator’ would release (Rhodes, 1986). Before the Alamogordo test, Enrico Fermi seriously considered the possibility that they might be producing a doomsday weapon, that would ignite the atmosphere. He concluded, correctly, that it would not. (Later calculations that an all-out nuclear exchange between the United States and Soviet Union might produce a world-wide firestorm and/or inject enough dust into the atmosphere to produce nuclear winter were not universally accepted; fortunately, they have not been put to the test. Lesser, but still staggering catastrophe is certain [see http://www.sciencedaily.com/releases/2006/12/061211090729.htm].)\n\nSo physicists, for better or worse, got that one right. What about accelerators that are designed not as weapons, but as tools for research? Might they be dangerous?\n\nWhen we are dealing with well-understood physics, we can do conventional safety engineering. Such engineering is not foolproof – bridges do collapse, astronauts do perish – but at least we foresee the scope of potential problems. In contrast, the whole point of great accelerator projects like the Brook haven Relativistic Heavy lon Collider (RHIC) or the Counseil Europeen pour la Recherche Nucleaire (CERN) Large Hadron Collider (LHC) is to produce extreme conditions that take us beyond what is well understood. In that context, safety engineering enters the domain of theoretical physics.\n\nIn discussing possible dangers associated with frontier research accelerators, the first thing to say is that while these machines are designed to produce unprecedented density of energy, that density is packed within such a miniscule volume of space that the total energy is, by most standards, tiny. Thus a proton-proton collision at the LHC involves about 40 erg of energy – less energy than a dried pea acquires in falling through 1 centimetre. Were that energy to be converted into mass, it would amount to about one-ten thousandth of a gram. Furthermore, the high energy density is maintained only very briefly, roughly for 10⁻²⁴ seconds.\n\nTo envision significant dangers that might be triggered with such limited input, we have to exercise considerable imagination. We have to imagine that a tiny seed disturbance will grow vast, by tapping into hidden instabilities. Yet the example of nuclear weapons should give pause. Nuclear weapons tap into instabilities that were totally unsuspected just five decades before their design. Both ultraheavy(for fission) and ultralight (for fusion) nuclei can release energy by cooking toward the more stable nuclei of intermediate size.\n\nThree possibilities have dominated the discussion of disaster scenarios at research accelerators. I will now discuss each one briefly. Much more extensive, authoritative technical discussions are available (Jaffe et al., 2000).\n\nBlack holes: The effect of gravityis extra ordinarily feeble in accelerator environments, according to both conventional theoryand experiment. (That is to say, the results of precision experiments to investigate delicate properties of the other fundamental interactions agree with theoretical calculations that predict gravityis negligible, and therefore ignore it.) Conventional theorysuggests that the relative strength of the gravitational compared to the electromagnetic interactions is, bydimensional analysis, approximately\n\n[Image]\n\nwhere G is the Newton constant, a is the fine-structure constant, and we adopt units with h = c = 1. Even for LHC energies E ~ 10 TeV, this is such a tiny ratio that more refined estimates are gratuitous.\n\nBut what if, within a future accelerator, the behaviour of gravityis drastically modified? Is there anyreason to think it might? At present, there is no empirical evidence for deviations from general relativity, but speculation that drastic changes in gravitymight set in starting at E ~ 1 TeV have been popular recentlyin parts of the theoretical physics community(Antoniadis et al., 1998; Arkani-Hamed et al., 1998; Randall and Sundrm, 1999). There are two broad motivations for such speculation:\n\nPrecocious unification? Physicists seek to unify their description of the different interactions. We have compelling ideas about how to unify our description of the strong, electromagnetic, and weak interactions. But the tiny ratio Equation (1) makes it challenging to put gravity on the same footing. One line of thought is that unification takes place only at extraordinarily high energies, namely, E ~ 10¹⁵ TeV the Planck energy. At this energy, which also corresponds to an extraordinarily small distance of approximately 10⁻³³cm, the coupling ratio is near unity. Nature has supplied a tantalizing hint, from the other interactions, that this is indeed the scale at which unification becomes manifest (Dimopoulos et al., 1981, 1991). A competing line of thought has it that unification could take place at lower energies. That could happen if Equation (1) fails drastically, in such a way that the ratio increases much more rapidly. Then the deepest unity of physics would be revealed directly at energies that we might hope to access – an exciting prospect.\n\nExtra dimensions: One way that this could happen, is if there are extra, curled-up spatial dimensions, as suggested by superstring theory. The short-distance behaviour of gravity will then be drastically modified at lengths below the size of the extra dimensions. Schematic world-models implementing these ideas have been proposed. While existing models appear highly contrived, at least to my eye, they can be fashioned so as not to avoid blatant contradiction with established facts. They provide a concrete framework in which the idea that gravity becomes strong at accessible energies can be realized.\n\nIf gravity becomes strong at E ~ 1 − 10² TeV, then particle collisions at those energies could produce tiny black holes. As the black holes encounter and swallow up ordinary matter, they become bigger black holes … and we have ourselves a disaster scenario! Fortunately, a more careful look is reassuring. While the words ‘black hole’ conjure up the image of a great gaping maw, the (highly) conjectural black holes that might be produced at an accelerator are not like that. They would weigh about one ten-thousandth of a gram, with a Compton radius of 10⁻¹⁸cm and, formally, a Schwarzschild radius of 10⁻⁴⁷cm. (The fact that the Compton radius, associated with the irreducible quanum-mechanical uncertainty in position, is larger than the Schwarzschild radius, that is, the nominal radius inside which light is trapped, emphases the quantum-mechanical character of these ‘black holes’.) Accordingly, their capture zone is extremely small, and they would be very slow eaters. If, that is, these mini-black holes did not spontaneously decay. Small black holes are subject to the Hawking (1974) radiation process, and very small ones are predicted to decay very rapidly, on timescales of order 10⁻¹⁸ seconds or less. This is not enough time for a particle moving at the speed of light to encounter more than a few atoms. (And the probability of a hit is in any case miniscule, as mentioned above.) Recent theoretical work even suggests that there is an alternative, dual description of the higher-dimension gravity theory in terms of a four-dimensional strongly interacting quantum field theory, analogous to quantum chromodynamics (QCD) (Maldacena, 1998, 2005). In that description, the short-lived mini-black holes appear only as subtle features in the distribution of particles emerging from collisions; they are similar to the highly unstable resonances of QCD. One might choose to question both the Hawking process and the dual description of strong gravity, both of which are theoretical conceptions with no direct empirical support. But these ideas are inseparable from, and less speculative than, the theories that motivate the mini-black hole hypothesis; so denying the former erodes the foundation of the latter.\n\nStrangelets: From gravity, the feeblest force in the world of elementary particles, we turn to QCD, the strongest, to confront our next speculative disaster scenario. For non-experts, a few words ofreview are in order. QCD is our theory of the so-called strong interaction (Close, 2006). The ingredients of QCD are elementary particles called quarks and gluons. We have precise, well-tested equations that describe the behaviour of quarks and gluons. There are six different flavours of quarks. The flavours are denoted u, d, s, c, b, t for up, down strange, charm, bottom, top. The heavyquarks c, b, and t are highly unstable. Though they are of great interest to physicists, they play no significant role in the present-day natural world, and they have not been implicated in any, even remotely plausible disaster scenario. The lightest quarks, u and d, together with gluons, are the primary building blocks of protons and neutrons, and thus of ordinary atomic nuclei. Crudely speaking, protons are composites uud of two up quarks and a down quark, and neutrons are composites udd of one up quark and two down quarks. (More accurately, protons and neutrons are complex objects that contain quark-antiquark pairs and gluons in addition to those three ‘valence’ quarks.) The mass-energy of the (u, d ) quarks is ~(5, 10) MeV, respectively, which is very small compared to the mass-energy of a proton or neutron of approximately 940 MeV. Almost all the mass of the nucleons-that is, protons and neutrons – arises from the energy of quarks and gluons inside, according to m = E/c².\n\nStrange quarks occupy an intermediate position. This is because their intrinsic mass-energy, approximately 100 MeV, is comparable to the energies associated with interquark interactions. Strange quarks are known to be constituents of so-called hyperons. The lightest hyperon is the Λ, with a mass of approximately 1116 MeV. The internal structure of the Λ resembles that of nucleons, but it is built from uds rather than uud or udd.\n\nUnder ordinary conditions, hyperons are unstable, with lifetimes of the order 10 – 10 seconds or less. The Λ hyperon decays into a nucleon and a π meson, for example. This process involves conversion of an s quark into a u or d quark, and so it cannot proceed through the strong QCD interactions, which do not change quark flavours. (For comparison, a typical lifetime for particles – ‘resonances’ – that decay by strong interactions is ~10⁻²⁴ seconds.) Hyperons are not so extremely heavy or unstable that they play no role whatsoever in the natural world. They are calculated to be present with small but not insignificant density during supernova explosions and within neutron stars.\n\nThe reason for the presence of hyperons in neutron stars is closely related to the concept of ‘strangelets’, so let us briefly review it. It is connected to the Pauli exclusion principle. According to that principle, no two fermions can occupy the same quantum state. Neutrons (and protons) are fermions, so the exclusion principle applies to them. In a neutron star’s interior, very high pressures – and therefore very high densities – are achieved, due to the weight of the overlying layers. In order to obey the Pauli exclusion principle, then, nucleons must squeeze into additional quantum states, with higher energy. Eventually, the extra energy gets so high that it becomes economical to trade a high-energy nucleon for a hyperon. Although the hyperon has larger mass, the marginal cost of that additional mass-energy is less than the cost of the nucleon’s exclusion principle-energy.\n\nAt even more extreme densities, the boundaries between individual nucleons and hyperons break down, and it becomes more appropriate to describe matter directly in terms of quarks. Then we speak of quark matter. In quark matter, a story very similar to what we just discussed again applies, now with the lighter u and d quarks in place of nucleons and the s quarks in place of hyperons. There is a quantitative difference, however, because the s quark mass is less significant than the hyperon-nucleon mass difference. Quark matter is therefore expected to be rich in strange quarks, and is sometimes referred to as strange matter. Thus there are excellent reasons to think that under high pressure, hadronic – that is, quark-based – matter undergoes a qualitative change, in that it comes to contain a significant fraction of strange quarks. Bodmer and Witten (1984) posed an interesting question: Might this new kind of matter, with higher density and significant strangeness, which theory tells us is surely produced at high pressure, remain stable at zero pressure? If so, then the lowest energy state of a collection of quarks would not be the familiar nuclear matter, based on protons and neutrons, but a bit of strange matter – a strangelet. In a (hypothetical) strangelet, extra strange quarks permit higher density to be achieved, without severe penalty from the exclusion principle. If there are attractive interquark forces, gains in interaction energy might compensate for the costs of additional strange quark mass.\n\nAt first hearing, the answer to the question posed in the preceding paragraph seems obvious: No, on empirical grounds. For, if ordinary nuclear matter is not the most energetically favourable form, why is it the form we find around us (and, of course, in us)? Or, to put it another way, if ordinary matter could decay into matter based on strangelets, why has it not done so already? On reflection, however, the issue is not so clear. If only sufficiently large strangelets are favourable – that is, if only large strangelets have lower energy than ordinary matter containing the same net number of quarks – ordinary matter would have a very difficult time converting into them. Specifically, the conversion would require many simultaneous conversions of u or d quarks into strange quarks. Since each such quark conversion is a weak interaction process, the rate for multiple simultaneous conversions is incrediblysmall.\n\nWe know that for small numbers of quarks, ordinary nuclear matter is the most favourable form, that is, that small strangelets do not exist. If a denser, differently organized version of the Λ existed, for example, nucleons would decay into it rapidly, for that decay requires only one weak conversion. Experiments searching for an alternative Λ – Λ – the so-called ‘H particle’ – have come up empty handed, indicating that such a particle could not be much lighter than two separate Λ particles, let alone light enough to be stable (Borer et al., 1994).\n\nAfter all this preparation, we are ready to describe the strangelet disaster scenario. A strangelet large enough to be stable is produced at an accelerator. It then grows by swallowing up ordinary nuclei, liberating energy. And there is nothing to stop it from continuing to grow until it produces a catastrophic explosion (and then, having burped, resumes its meal), or eats up a big chunk of Earth, or both.\n\nFor this scenario to occur, four conditions must be met:\n\n1. Strange matter must be absolutely stable in bulk.\n\n2. Strangelets would have to be at least metastable for modest numbers of quarks, be cause only objects containing small numbers of strange quarks might conceivably be produced in an accelerator collision.\n\n3. Assuming that small metastable strangelets exist, it must be possible to produce them at an accelerator.\n\n4. The stable configuration of a strangelet must be negatively charged (see below).\n\nOnly the last condition is not self-explanatory. A positively charged strangelet would resemble an ordinary atomic nucleus (though, to be sure, with an unusually small ratio of charge to mass). Like an ordinary atomic nucleus, it would surround itself with electrons, forming an exotic sort of atom. It would not eat other ordinary atoms, for the same reasons that ordinary atoms do not spontaneously eat one another – no cold fusion! – namely, the Coulomb barrier. As discussed in detail in Jaffe et al. (2000), there is no evidence that any of these conditions is met. Indeed, there is substantial theoretical evidence that none is met, and direct experimental evidence that neither condition (2) nor (3) can be met. Here are the summary conclusions of that report:\n\n1. At present, despite vigorous searches, there is no evidence whatsoever on the existence of stable strange matter anywhere in the Universe.\n\n2. On rather general grounds, theory suggests that strange matter becomes unstable in small lumps due to surface effects. Strangelets small enough to be produced in heavy ion collisions are not expected to be stable enough to be dangerous.\n\n3. Theory suggests that heavy ion collisions (and hadron-hadron collisions in general) are not a good place to produce strangelets. Furthermore, it suggests that the production probability is lower at RHIC than at lower energy heavy ion facilities like the Alternating Gradient Synchrotron (AGS) and CERN. Models and data from lower energy heavy ion colliders indicate that the probability of producing a strangelet decreases very rapidly with the strangelet’s atomic mass.\n\n4. It is over whelmingly likely that the most stable configuration of strange matter has positive electric charge.\n\nIt is not appropriate to review all the detailed and rather technical arguments supporting these conclusions here, but two simple qualitative points, that suggest conclusions (3) and (4) above, are easy to appreciate.\n\nConclusion 3: To produce a strangelet at an accelerator, the crucial condition is that one produces a region where there are many strange quarks (and few strange antiquarks) and not too much excess energy. Too much energy density is disadvantageous, because it will cause the quarks to fly apart: when things are hot you get steam, not ice cubes. Although higher energy at an accelerator will make it easier to produce strange-antistrange quark pairs, higher energy also makes it harder to segregate quarks from antiquarks, and to suppress extraneous background (i.e., extra light quarks and antiquarks, and gluons). Thus conditions for production of strangelets are less favourable at frontier, ultra-high accelerators than at older, lower-energy accelerators – for which, of course, the (null) results are already in. For similar reasons, one does not expect that strangelets will be produced as cosmological relics of the big bang, even if they are stable in isolation.\n\nConclusion 4: The maximum leeway for avoiding Pauli exclusion, and the best case for minimizing other known interaction energies, occurs with equal numbers of u, d, and s quarks. This leads to electrical neutrality, since the charges of those quarks are ⅔, – ⅓, – ⅓ times the charge of the proton, respectively. Since the s quark, being significantly heavier than the others, is more expensive, one expects that there will be fewer s quarks than in this otherwise ideal balance (and nearly equal numbers of u and d quarks, since both their masses are tiny). This leads to an overall positive charge.\n\nThe strangelet disaster scenario, though ultimately unrealistic, is not silly. It brings in subtle and interesting physical questions, that require serious thought, calculation, and experiment to address in a satisfactory way. Indeed, if the strange quark were significantly lighter than it is in our world, then big strangelets would be stable, and small ones at least metastable. In such an alternative universe, life in anything like the form we know it, based on ordinary nuclear matter, might be precarious or impossible.\n\nVacuum instability: In the equations of modern physics, the entity we perceive as empty space, and call vacuum, is a highly structured medium full of spontaneous activity and a variety of fields. The spontaneous activity is variously called quantum fluctuations, zero-point motion, or virtual particles. It is directly responsible for several famous phenomena in quantum physics, including Casimir forces, the Lamb shift, and asymptotic freedom. In a more abstract sense, within the framework of quantum field theory, all forces can be traced to the interaction of real with virtual particles (Feynman, 1988; Wilczek, 1999; Zee, 2003).\n\nThe space-filling fields can also be viewed as material condensates, just as an electromagnetic field can be considered as a condensate of photons. One such condensation is understood deeply. It is the quark-anti quark condensate that plays an important role in strong interaction theory. A field of quark-antiquark pairs of opposite helicity fills space-time.¹ That quark-antiquark field affects the behaviour of particles that move through it. That is one way we know it is there! Another is by direct solution of the well-established equations of QCD. Low-energy n mesons can be modeled as disturbances in the quark-antiquark field; many properties of n mesons are successfully predicted using that model.\n\nAnother condensate plays a central role in our well-established theory of electroweak interactions, though its composition is presently unknown. This is the so-called Higgs condensate. The equations of the established electroweak theory indicate that the entity we perceive as empty space is in reality an exotic sort of superconductor. Conventional superconductors are super(b) conductors of electric currents, the currents that photons care about. Empty space, we learn in electroweak physics, is a super(b) conductor of other currents: specifically, the currents that W and Z bosons care about. Ordinary superconductivity is mediated by the flow of paired electrons – Cooper pairs – in a metal. Cosmic superconductivity is mediated by the flow of something else. No presently known form of matter has the right properties to do the job; for that purpose, we must postulate the existence of new form(s) of matter. The simplest hypothesis, at least in the sense that it introduces the fewest new particles, is the so-called minimal standard model. In the minimal standard model, we introduce just one new particle, the so-called Higgs particle. According to this model, cosmic superconductivity is due to a condensation of Higgs particles. More complex hypotheses, notably including low-energy supersym metry, introduce several contributions to the electroweak condensate. These models predict that there are several contributors to the electroweak condensate, and that there is a complex of several ‘Higgs particles’, not just one. A major goal of on going research at the Fermilab Tevatron and the CERN LHC is to find the Higgs particle, or particles.\n\nSince ‘empty’ space is richly structured, it is natural to consider whether that structure might change. Other materials exist in different forms – might emptyspace? To put it another way, could empty space exist in different phases, supporting in effect different laws of physics?\n\nThere is every reason to think the answer is ‘Yes’. We can calculate, for example, that at sufficiently high temperature the quark-antiquark condensate of QCD will boil away. And although the details are much less clear, essentially all models of electroweak symmetry breaking likewise predict that at sufficiently high temperatures the Higgs condensate will boil away. Thus in the early moments of the big bang, empty space went through several different phases, with qualitatively different laws of physics. (For example, when the Higgs condensate melts, the W and Z bosons become massless particles, on the same footing as photons. So then the weak interactions are no longer so weak!) Somewhat more speculatively, the central idea of inflationary cosmology is that in the very early universe, empty space was in a different phase, in which it had non-zero energy density and negative pressure.\n\nThe empirical success of inflationary cosmology therefore provides circumstantial evidence that empty space once existed in a different phase.\n\nMore generally, the structure of our basic framework for understanding fundamental physics, relativistic quantum field theory, comfortably supports theories in which there are alternative phases of empty space. The different phases correspond to different configurations of fields (condensates) filling space. For example, attractive ideas about unification of the apparently different forces of Nature postulate that these forces appear on the same footing in the primary equations of physics, but that in their solution, the symmetry is spoiled by space-filling fields. Superstring theory, in particular, supports vast numbers of such solutions, and postulates that our world is described by one of them: for certainly, our world exhibits much less symmetry than the primary equations of superstring theory.\n\nGiven, then, that empty space can exist in different phases, it is natural to ask: Might our phase, that is, the form of physical laws that we presently observe, be suboptimal? Might, in other words, our vacuum be only metastable? If so, we can envisage a terminal ecological catastrophe, when the field configuration of empty space changes, and with it the effective laws of physics, instantly and utterly destabilizing matter and life in the form we know it.\n\nHow could such a transition occur? The theory of empty space transitions is entirely analogous to the established theory of other, more conventional first-order phase transitions. Since our present-day field configuration is (at least) metastable, any more favourable configuration would have to be significantly different, and to be separated from ours by intermediate configurations that are less favourable than ours (i.e., that have higher energy density). It is most likely that a transition to the more favourable phase would begin with the emergence of a rather small bubble of the new phase, so that the required rearrangement of fields is not too drastic and the energetic cost of intermediate configurations is not prohibitive. On the other hand the bubble cannot be too small, for the volume energy gained in the interior must compensate unfavourable surface energy (since between the new phase and the old metastable phase one has unfavourable intermediate configurations).\n\nOnce a sufficiently large bubble is formed, it could expand. Energy liberated in the bulk transition between old and new vacuum goes into accelerating the wall separating them, which quickly attains near-light speed. Thus the victims of the catastrophe receive little warning: by the time they can see the approaching bubble, it is upon them.\n\nHow might the initial bubble form? It might form spontaneously, as a quantum fluctuation. Or it might be nucleated by some physical event, such as – perhaps? – the deposition of lots of energy into a small volume at an accelerator.\n\nThere is not much we can do about quantum fluctuations, it seems, but it would be prudent to refrain from activity that might trigger a terminal ecological catastrophe. While the general ideas of modern physics support speculation about alternative vacuum phases, at present there is no concrete candidate for a dangerous field whose instability we might trigger. We are surely in the most stable state of QCD. The Higgs field or fields involved in electroweak symmetry breaking might have instabilities – we do not yet know enough about them to be sure. But the difficulty of producing even individual Higgs particles is already a crude indication that triggering instabilities which require coordinated condensation of many such particles at an accelerator would be prohibitively difficult. In fact there seems to be no reliable calculation of rates of this sort – that is, rates for nucleating phase transitions from particle collisions – even in model field theories. It is an interesting problem of theoretical physics. Fortunately, the considerations of the following paragraph assure us that it is not a practical problem for safety engineering.\n\nAs the matching bookend to our initial considerations on size, energy, and mass, let us conclude our discussion of speculative accelerator disaster scenarios with another simple and general consideration, almost independent of detailed theoretical considerations, and which makes it implausible that any of these scenarios apply to reality. It is that Nature has, in effect, been doing accelerator experiments on a grand scale for a very long time (Hut, 1984; Hut and Rees, 1984). For, cosmic rays achieve energies that even the most advanced terrestrial accelerators will not match at any time soon. (For experts: Even by the criterion of center-of-mass energy, collisions of the highest energy cosmic rays with stationary targets beat top-of-the-line accelerators.) In the history of the universe, many collisions have occurred over a very wide spectrum of energies and ambient conditions (Jaffe et al., 2000). Yet in the history of astronomy, no candidate unexplained catastrophe has ever been observed. And many such cosmic rays have impacted Earth, yet Earth abides and we are here. This is reassuring (Bostrom and Tegmark, 2005).\n\n16.2.2 Runaway technologies\n\nNeither general source of reassurance – neither miniscule scale nor natural precedent – necessarily applies to other emergent technologies.\n\nTechnologies that are desirable in themselves can get out of control, leading to catastrophic exhaustion of resources or accumulation of externalities. Jared Diamond has argued that history presents several examples of this phenomenon (Diamond, 2005), on scales ranging from small island cultures to major civilizations. The power and agricultural technologies of modern industrial civilization appear to have brought us to the cusp of severe challenges of both these sorts, as water resources, not to speak of oil supplies, come under increasing strain, and carbon dioxide, together with other pollutants, accumulates in the biosphere. Here it is not a question of whether dangerous technologies will be employed – they already are – but on what scale, how rapidly, and how we can manage the consequences.\n\nAs we have already discussed in the context of fundamental physics at accelerators, runaway instabilities could also be triggered by inadequately considered research projects. In that particular case, the dangers seem farfetched. But it need not always be so. Vonnegut’s ‘Ice 9’ was a fictional example (Vonnegut, 1963), very much along the lines of the runaway strangelet scenario – a new form of water, that converts the old. An artificial protein that turned out to catalyse crystallization of natural proteins – an artifical ‘prion’ – would be another example of the same concept, from yet a different realm of science.\n\nPerhaps more plausibly, runaway technological instabilities could be triggered as an unintended byproduct of applications (as in the introduction of cane toads to Australia) or sloppy practices (as in the Chernobyl disaster); or by deliberate pranksterism (as in computer virus hacking), warfare, or terrorism.\n\nTwo technologies presently entering the horizon of possibility have, by their nature, especially marked potential to lead to runaways:\n\nAutonomous, capable robots: As robots become more capable and autonomous, and as their goals are specified more broadly and abstractly, they could become formidable antagonists. The danger potential of robots developed for military applications is especially evident. This theme has been much explored in science fiction, notably in the writings of Isaac Asimov (1950) and in the Star Wars movies.\n\nSelf-reproducing machines, including artificial organisms: The danger posed by sudden introduction of new organisms into unprepared populations is exemplified by the devastation of New World populations by smallpox from the Old World, among several other catastrophes that have had a major influence on human history. This is documented in William McNeill’s (1976) marvelous Plagues and Peoples. Natural organisms that have been re-engineered, or ‘machines’ of any sort capable of self-reproduction, are by their nature poised on the brink of exponential spread. Again, this theme has been much explored in science fiction, notably in Greg Bear’s Blood Music [19]. The chain reactions of nuclear technology also belong, in a broad conceptual sense, to this class – though they involve exceedingly primitive ‘machines’, that is, self-reproducing nuclear reactions.\n\n16.3 Preparing to Prepare\n\nRunaway technologies: The problem of runaway technologies is multi-faceted. We have already mentioned several quite distinct potential instabilities, involving different technologies, that have little in common. Each deserves separate, careful attention, and perhaps there is not much useful that can be said in general. I will make just one general comment. The majority of people, and of scientists and engineers, by far, are well-intentioned; they would much prefer not to be involved in any catastrophe, technological or otherwise. Broad-based democratic institutions and open exchange of information can coalesce this distributed good intention into an effective instrument of action.\n\nImpacts: We have discussed some exotic – and, it turns out, unrealistic – physical processes that could cause global catastrophes. The possibility that asteroids or other cosmic debris might impact Earth, and cause massive devastation, is not academic – it has happened repeatedly in the past. We now have the means to address this danger, and certainly should do so (http://impact.arc.nasa.gov/intro.cfm).\n\nAstronomical instabilities: Besides impacts, there are other astronomical effects that will cause Earth to become much less hospitable on long time scales. Ice ages can result from small changes in Earth’s obliquity, the eccentricity of its orbit, and the alignment of its axis with the eccentricity (which varies as the axis precesses) (see http://www.aip.org/history/climate/cycles.htm). These changes occur on time scales of tens of thousands of years. At present the obliquity oscillates within the range 22.1-24.5°. However as the day lengthens and the moon recedes, over time scales of a billion years or so, the obliquity enters a chaotic zone, and much larger changes occur (Laskar et al., 1993). Presumably, this leads to climate changes that are both extreme and highly variable. Finally, over yet longer time scales, our Sun evolves, gradually becoming hotter and eventually entering a red giant phase.\n\nThese adverse and at least broadly predictable changes in the global environment obviously pose great challenges for the continuation of human civilization. Possible responses include moving (underground, underwater, or into space), re-engineering our physiology to be more tolerant (either through bio-engineering, or through man-machine hybridization), or some combination thereof.\n\nHeat death: Over still longer time scales, some version of the ‘heat death of the universe’ seems inevitable. This exotic catastrophe is the ultimate challenge facing the mind in the universe.\n\nStars will burn out, the material for making new ones will be exhausted, the universe will continue to expand – it now appears, at an accelerating rate − and, in general, useful energy will become a scarce commodity. The ultimate renewable technology is likely to be pure thought, as I will now describe.\n\nIt is reasonable to suppose that the goal of a future-mind will be to optimize a mathematical measure of its wellbeing or achievement, based on its internal state. (Economists speak of ‘maximizing utility’, normal people of ‘finding happiness’.) The future-mind could discover, by its powerful introspective abilities or through experience, its best possible state the Magic Moment – or several excellent ones. It could build up a library of favourite states. That would be like a library of favourite movies, but more vivid, since to recreate magic moments accurately would be equivalent to living through them. Since the joys of discovery, triumph, and fulfillment require novelty, to re-live a magic moment properly, the future-mind would have to suppress memory of that moment’s previous realizations.\n\nA future-mind focused upon magic moments is well matched to the limitations of reversible computers, which expend no energy. Reversible computers cannot store new memories, and they are as likely to run backwards as forwards. Those limitations bar adaptation and evolution, but invite eternal cycling through magic moments. Since energy becomes a scarce quantity in an expanding universe, that scenario might well describe the long-term future of mind in the cosmos.\n\n16.4 Wondering\n\nA famous paradox led Enrico Fermi to ask, with genuine puzzlement, ‘Where are they?’ He was referring to advanced technological civilizations in our Galaxy, which he reckoned ought to be visible to us.\n\nSimple considerations strongly suggest that technological civilizations whose works are readily visible throughout our Galaxy (that is, given current or imminent observation technology techniques we currently have available, or soon will) ought to be common. But they are not. Like the famous dog that did not bark in the night time, the absence of such advanced technological civilizations speaks through silence.\n\nMain-sequence stars like our Sun provide energy at a stable rate for several billions of years. There are billions of such stars in our Galaxy. Although our census of planets around other stars is still in its infancy, it seems likely that many millions of these stars host, within their so-called habitable zones, Earth-like planets. Such bodies meet the minimal requirements for life in something close to the form we know it, notably including the possibility of liquid water.\n\nOn Earth, a species capable of technological civilization first appeared about one hundred thousand years ago. We can argue about defining the precise time when technological civilization itself emerged. Was it with the beginning of agriculture, of written language, or of modern science? But whatever definition we choose, its age will be significantly less than one hundred thousand years.\n\nIn any case, for Fermi’s question, the most relevant time is not one hundred thousand years, but more nearly one hundred years. This marks the period of technological ‘breakout’, when our civilization began to release energies and radiations on a scale that may be visible throughout our Galaxy. Exactly what that visibility requires is an interesting and complicated question, whose answer depends on the hypothetical observers. We might already be visible to a sophisticated extraterrestrial intelligence, through our radio broadcasts or our effects on the atmosphere, to a sophisticated extraterrestrial version of SETI. The precise answer hardly matters, however, if anything like the current trend of technological growth continues. Whether we are barely visible to sophisticated though distant observers today, or not quite, after another thousand years of technological expansion at anything like the prevailing pace, we should be easily visible. For, to maintain even modest growth in energy consumption, we will need to operate on astrophysical scales.\n\nA 1000 years is just one millionth of the billion-year span over which complex life has been evolving on Earth. The exact placement of breakout within the multi-billion year timescale of evolution depends on historical accidents. With a different sequence of the impact events that lead to mass extinctions, or earlier occurrence of lucky symbioses and chromosome doublings, Earth’s breakout might have occurred one billion years ago, instead of 100 years.\n\nThe same considerations apply to those other Earth-like planets. Indeed, many such planets, orbiting older stars, came out of the starting gate billions of years before we did. Among the millions of experiments in evolution in our Galaxy, we should expect that many achieved breakout much earlier, and thus became visible long ago. So: Where are they?\n\nSeveral answers to that paradoxical question have been proposed. Perhaps this simple estimate of the number of life-friendly planets is for some subtle reason wildly over-optimistic. For example, our Moon plays a crucial role in stabilizing the Earth’s obliquity, and thus its climate; probably, such large moons are rare (ours is believed to have been formed as a consequence of an unusual, giant impact), and plausibly extreme, rapidly variable climate is enough to inhibit the evolution of intelligent life. Perhaps on Earth the critical symbioses and chromosome doublings were unusually lucky, and the impacts extraordinarily well-timed. Perhaps, for these reasons or others, even iflife of some kind is widespread, technologically capable species are extremely rare, and we happen to be the first in our neighbourhood.\n\nOr, in the spirit of this essay, perhaps breakout technology inevitably leads to catastrophic runaway technology, so that the period when it is visible is sharply limited. Or – an optimistic variant of this – perhaps a sophisticated, mature society avoids that danger by turning inward, foregoing power engineering in favour of information engineering. In effect, it thus chooses to become invisible from afar. Personally, I find these answers to Fermi’s question to be the most plausible. In any case, they are plausible enough to put us on notice.\n\nSuggestions for further reading\n\nJaffe, R., Busza, W., Sandweiss, J., and Wilczek, F. (2000). Review of speculative ‘disaster scenarios’ at RHIC. Rev. Mod. Phys., 72, 1125–1140, available on the web at arxiv.org:hepph/9910333. A major report on accelerator disaster scenarios, written at the request of the director of Brookhaven National Laboratory, J. Marburger, before the commissioning of the RHIC. It includes a non-technical summary together with technical appendices containing quantitative discussions of relevant physics issues, including cosmic ray rates. The discussion of strangelets is especially complete.\n\nRhodes, R. (1986). The Making of the Atomic Bomb (Simon &Schuster). A rich history of the one realistic ‘accelerator catastrophe’. It is simply one of the greatest books ever written. It includes a great deal of physics, as well as history and high politics. Many of the issues that first arose with the making of the atom bomb remain, of course, very much alive today.\n\nKurzweil, R. (2005). The Singularity Is Near (Viking Penguin). Makes a case that runaway technologies are endemic – and that is a good thing! It is thought-provoking, if not entirely convincing.\n\nReferences\n\nAntoniadis, I., Arkani-Hamed, N., Dimopoulos, S., and Dvali, G. (1998). Phys. Lett. B, 436, 257.\n\nArkani-Hamed, N., Dimopoulos, S., and Dvali, G. (1998). Phys. Lett., 429, 263.\n\nAsimov, I. (1950). I, Robot (New York: Gnome Press).\n\nBear, G. (1985). Blood Music (New York: Arbor House).\n\nBorer, K., Dittus, F., Frei, D., Hugentobler, E., Klingenberg, R., Moser, U., Pretzl, K., Schacher, J., Stoffel, F., Volken, W., Elsener, K., Lohmann, K.D., Baglin, C., Bussière, A., Guillaud, J.P., Appelquist, G., Bohm, C., Hovander, B., Selldèn, B., and Zhang, Q.P. (1994). Strangelet search in S-W collisions at 200A Ge V/c. Phys. Rev. Lett., 72, 1415–1418.\n\nBostrom, N. and Tegmark, M. (2005). How unlikely is a doomsday catastrophe. Nature, 438, 754–756. http://www.arxiv.org/astro-ph/0512204v2\n\nClose, F. (2006). The New Cosmic Onion (New York and London: Taylor &Francis).\n\nDiamond, J. (2005). Collapse: How Societies Choose to Fail or Succeed (New York: Viking).\n\nDimopoulos, S., Raby, S., and Wilczek, F. (1981). Supersymmetry &the scale of unification. Phys. Rev., D24, 1681–1683.\n\nDimopoulos, S., Raby, S., and Wilczek, F. (1991). Unification of Couplings. Physics Today, 44, October 25, pp. 25–33.\n\nFeynman, R. (1988). QED: The Strange Theory of Light and Matter (Princeton, NJ: Princeton University Press).\n\nHawking, S.W. (1974). Black hole explosion? Nature, 248, 30–31.\n\nHut, P. (1984). Is it safe to distribute the vacuum? Nucl. Phys., A418, 301C.\n\nHut, P. and Rees, M.J. How stable is our vacuum? Report-83-0042 (Princeton: IAS).\n\nJaffe, R., Busza, W., Sandweiss, J., and Wilczek, F. (2000). Review of speculative “disaster scenarios” at RHK. Rev. Mod. Phys., 72, 1125–1140. Laskar, J., Joutel, F., and Robutel, P. (1993). Stabilization of the earth’s obliguity by the Moon. Nature, 361, 615–617.\n\nMaldacena, J. (1998). The cage-N limit of superconformal field theories &supergravity. Adv. Theor. Math. Phys., 2, 231 -252.\n\nMaldacena, J. (2005). The illusion of gravity. Scientific American, November, 56–63.\n\nMcNeill, W. (1976). Plagues and Peoples (New York: Bantam).\n\nRandall, L. and Sundrm, R. (1999). Large mass hierarchy from a small extra demensia. Phys. Rev. Lett., 83, 3370–3373.\n\nRhodes, R. (1986). The Making of the Atomic Bomb (New York: Simon &Schuster).\n\nSchroder, P., Smith, R., and Apps, K. (2001). Solar evolution &the distant future of earth. Astron. Geophys., 42(6), 26–32.\n\nVonnegut, K. (1963). Cat’s Cradle (New York: Holt, Rinehart, &Wilson).\n\nWilczek, F. (1999). Quantum field theory. Rev. Mod. Phys., 71, S85-S05.\n\nWitten, E. (1984). Cosmic separation of phases. Phys. Rev., D30, 272–285.\n\nZee, A. (2003). Quantum Field Theory in a Nutshell (Princeton, NJ: Princeton University Press).\n\n• 17 • Catastrophe, social collapse, and human extinction\n\nRobin Hanson\n\n17.1 Introduction\n\nModern society is a bicycle, with economic growth being the forward momentum that keeps the wheels spinning. As long as the wheels of a bicycle are spinning rapidly, it is a very stable vehicle indeed. But, [Friedman] argues, when the wheels stop – even as the result of economic stagnation, rather than a downturn or a depression – political democracy, individual liberty, and social tolerance are then greatly at risk even in countries where the absolute level of material prosperity remains high …\n\nDeLong, 2006\n\nThe main reason to be careful when you walk up a flight of stairs is not that you might slip and have to retrace one step, but rather that the first slip might cause a second slip, and so on until you fall dozens of steps and break your neck. Similarly, we are concerned about the sorts of catastrophes explored in this book not only because of their terrible direct effects, but also because they may induce an even more damaging collapse of our economic and social systems. In this chapter, I consider the nature of societies, the nature of social collapse, and the distribution of disasters that might induce social collapse, and possible strategies for limiting the extent and harm of such collapse.\n\n17.2 What is society?\n\nBefore we can understand how societies collapse, we must first understand how societies exist and grow. Humans are far more numerous, capable, and rich than were our distant ancestors. How is this possible? One answer is that today we have more of most kinds of ‘capital’, but by itself this answer tells us little; after all, ‘capital’ is just anything that helps us to produce or achieve more. We can understand better by considering the various types of capital we have.\n\nFirst, we have natural capital, such as soil to farm, ores to mine, trees to cut, water to drink, animals to domesticate, and so on. Second, we have physical capital, such as cleared land to farm, irrigation ditches to move water, buildings to live in, tools to use, machines to run, and so on. Third, we have human capital, such as healthy hands to work with, skills we have honed with practice, useful techniques we have discovered, and abstract principles that help us think. Fourth, we have social capital, that is, ways in which groups of people have found to coordinate their activities. For example, households organize who does what chores, firms organize which employees do which tasks, networks of firms organize to supply inputs to each other, cities and nations organize to put different activities in different locations, culture organizes our expectations about the ways we treat each other, law organizes our coalitions to settle small disputes, and governments coordinate our largest disputes.\n\nThere are several important things to understand about all this capital. First, the value of almost any piece of capital depends greatly on what other kinds of capital are available nearby. A fence may be very useful in a prairie but useless in a jungle, while a nuclear engineer’s skills may be worth millions in a rich nation, but nothing in a poor nation. The productivity of an unskilled labourer depends greatly on how many other such labourers are available.\n\nSecond, scale makes a huge difference. The more people in a city or nation, the more each person or group can narrow their specialty, and get better at it. Special products or services that would just not be possible in a small society can thrive in a large society. So anything that lets people live more densely, or lets them talk or travel more easily, can create large gains by increasing the effective social scale.\n\nThird, coordination and balance of capital are very important. For example, places with low social capital can stay poor even after outsiders contribute huge resources and training, while places with high social capital can quickly recover from wars that devastate their natural, physical, and human capital.\n\n17.3 Social growth\n\nThe opposite of collapse is growth. Over history, we have dramatically increased our quantities of most, though not all, kinds of capital. How has this been possible?\n\nOver the last few decades, economists have learned a lot about how societies grow (Aghion and Howitt, 1998; Barro and Sala-I-Martin, 2003; Jones, 2002). While much ignorance remains, a few things seem clear. Social capital is crucial; rich places can grow fast while poor places decline. Also crucial is scale and neighbouring social activity; we each benefit greatly on average from other productive activity nearby.\n\nAnother key point is that better’technology’, that is, better techniques and coordination, drive growth more than increased natural or physical capital. Better technology helps us produce and maintain more natural and physical capital, a stronger effect than the ability of more natural and physical capital to enable better technology (Grubler, 1998).\n\nLet us quickly review the history of growth (Hanson, 2000), starting with animals, to complete our mental picture,. All animal species have capital in the form of a set of healthy individuals and a carefully honed genetic design. An individual animal may also have capital in the form of a lair, a defended territory, and experience with that area. Social animals, such as ants, also have capital in the form of stable organized groups.\n\nOver many millions of years the genetic designs of animals slowly acquired more possibilities. For example, over the last half billion years, the size of the largest brains doubled roughly every 35 million years. About 2 million years ago some primates acquired the combination of a large social brain, hands that could handle tools, and mouths that could voice words; a combination that allowed tools, techniques, and culture to become powerful forms of capital.\n\nThe initial human species had perhaps ten thousand members, which some estimate to be the minimum for a functioning sexual species. As human hunter-gatherers slowly accumulated more kinds of tools, clothes, and skills, they were able to live in more kinds of places, and their number doubled every quarter million years. Eventually, about 10,000 years ago, humans in some places knew enough about how to encourage local plants and animals that these humans could stop wandering and stay in one place.\n\nNon-wandering farmers could invest more profitably in physical capital such as cleared land, irrigation ditches, buildings, and so on. The increase in density that farming allowed also enabled our ancestors to interact and coordinate with more people. While a hunter-gatherer might not meet more than a few hundred people in his or her life, a farmer could meet and trade with many thousands.\n\nSoon, however, these farming advantages of scale and physical capital reached diminishing returns, as the total productivity of a region was limited by its land area and the kinds of plants and animals available to grow. Growth was then limited importantly by the rate at which humans could domesticate new kinds of plants and animals, allowing the colonization of new land. Since farmers talked more, they could spread such innovations much faster than hunter-gatherers; the farming population doubled every 1000 years.\n\nA few centuries ago, the steady increase in farming efficiency and density, as well as travel ease, finally allowed humans to specialize enough to support an industrial society. Specialized machines, factories, and new forms of social coordinate, allowed a huge increase in productivity. Diminishing returns quickly set in regarding the mass of machines we produced, however. We still make about the same mass of items per person as we did two centuries ago.\n\nToday’s machines are far more capable though, because of improving technologies. And networks of communication between specialists in particular techniques have allowed the rapid exchange of innovations; during the industrial era, world product (the value of items and services we produce) has doubled roughly every 15 years.\n\nOur history has thus seen four key growth modes: animals with larger brains; human hunter-gatherers with more tools and culture enabling them to fill more niches; human farmers domesticating more plants, animals, and land types; and human industry improving its techniques and social capital. During each mode, growth was over a hundred times faster than before, and production grew by a factor of over two hundred. While it is interesting to consider whether even faster growth modes might appear in the future, in this chapter we turn our attention to the opposite of growth: collapse.\n\n17.4 Social collapse\n\nSocial productivity fluctuates constantly in response to various disturbances, such as changes in weather, technology, or politics. Most such disturbances are small, and so induce only minor social changes, but the few largest disturbances can induce great social change. The historical record shows at least a few occasions where social productivity fell rapidly by a large enough degree to be worthy of the phrase ‘social collapse’. For example, there have been famous and dramatic declines, with varying speeds, among ancient Sumeria, the Roman empire, and the Pueblo peoples. A century of reduced rain, including three droughts, apparently drove the Mayans from their cities and dramatically reduced their population, even though the Mayans had great expertise and experience with irrigation and droughts (Haug et al., 2003).\n\nSome have explained these historical episodes of collapse as due to a predictable internal tendency of societies to overshoot ecological capacity (Diamond, 2005), or to create top-heavy social structures (Tainter, 1988). Other analysis, however, suggests that most known ancient collapses were initiated by external climate change (deMenocal, 2001; Weiss and Bradley, 2001). The magnitude of the social impact, however, often seems out of proportion to the external disturbance. Similarly, in recent years, relatively minor external problems often translate into much larger reductions in economic growth (Rodrik, 1999). This disproportionate response is of great concern; what causes it?\n\nOne obvious explanation is that the intricate coordination that makes a society more productive also makes it more vulnerable to disruptions. For example, productivity in our society requires continued inputs from a large number of specialized systems, such as for electricity, water, food, heat, transportation, communication, medicine, defense, training, and sewage. Failure of any one of these systems for an extended period can destroy the entire system. And since geographic regions often specialize in supplying particular inputs, disruption of one geographic region can have a disproportionate effect on a larger society. Transportation disruptions can also reduce the benefits of scale societies enjoy.\n\nCapital that is normally carefully balanced can become unbalanced during a crisis. For example, a hurricane may suddenly increase the value of gas, wood, and fresh water relative to other goods. The sudden change in the relative value of different kinds of capital produces inequality, that is, big winners and losers, and envy – a feeling that winner gains are undeserved. Such envy can encourage theft and prevent ordinary social institutions from functioning; consider the widespread resistance to letting market prices rise to allocate gas or water during a crisis.\n\n‘End game’ issues can also dilute reputational incentives in severe situations. A great deal of social coordination and cooperation is possible today because the future looms large. We forgo direct personal benefits now for fear that others might learn later of such actions and avoid us as associates. For most of us, the short-term benefits of ‘defection’ seem small compared to the long-term benefits of continued social ‘cooperation’.\n\nBut in the context of a severe crisis, the current benefits of defection can loom larger. So not only should there be more personal grabs, but the expectation of such grabs should reduce social coordination. For example, a judge who would not normally consider taking a bribe may do so when his life is at stake, allowing others to expect to get away with theft more easily, which leads still others to avoid making investments that might be stolen, and so on. Also, people may be reluctant to trust bank accounts or even paper money, preventing those institutions from functioning.\n\nSuch multiplier effects of social collapse can induce social elites to try to deceive the rest about the magnitude of any given disruption. But the rest of society will anticipate such deception, making it hard for social elites to accurately communicate the magnitude of any given disruption. This will force individuals to attend more to their private clues, and lead to less social coordination in dealing with disruptions.\n\nThe detailed paths of social collapse depend a great deal on the type of initial disruption and the kind of society disrupted. Rather than explore these many details, let us see how far we can get thinking in general about social collapse due to large social disruptions.\n\n17.5 The distribution of disaster\n\nFirst, let us consider some general features of the kinds of events that can trigger large social disruptions. We have in mind events such as earthquakes, hurricanes, plagues, wars, and revolutions. Each such catastrophic event can be described by its severity, which might be defined in terms of energy released, deaths induced, and so on.\n\nFor many kinds of catastrophes, the distribution of event severity appears to follow a power law over a wide severity range. That is, sometimes the chance that within a small time interval one will see an event with severity S that is greater than a threshold s is given by\n\n[Image]\n\nwhere k is a constant and α is the power of this type of disaster.\n\nNow we should keep in mind that these powers α can only be known to apply within the scales sampled by available data, and that many have disputed how widely such power laws apply (Bilham, 2004), and whether power laws are the best model form, compared, for example, to the lognormal distribution (Clauset et al., 2007a).\n\nAddressing such disputes is beyond the scope of this chapter. We will instead consider power law distributed disasters as an analysis reference case. Our conclusions would apply directly to types of disasters that continue to be distributed as a power law even up to very large severity. Compared to this reference case, we should worry less about types of disasters whose frequency of very large events is below a power law, and more about types of disasters whose frequency is greater.\n\nThe higherthe power α, the fewer larger disasters there are, relative to small disasters. For example, if they followed a power law, then car accidents would have a high power, as most accidents involve only one or two cars, and very few accidents involve one hundred or more cars. Supernovae deaths, on the other hand, would probably have a small power; if anyone on Earth is killed by a supernova, most likely many will be killed.\n\nDisasters with a power of one are right in the middle, with both small and large disasters being important. For example, the energy of earthquakes, asteroid impacts, and Pacific hurricanes all seem to be distributed with a power of about one (Christensen et al., 2002; Lay and Wallace, 1995; Morrison et al., 2003; Sanders, 2005). (The land area disrupted by an earthquake also seems to have a power of one [Turcotte, 1999].) This implies that for any given earthquake energy E and for any time interval, as much energy will on average be released in earthquakes with energies in the range from E to 2E as in earthquakes with energies in the range from E /2 to E . While there should be twice as many events in the second range, each event should only release half as much energy.\n\nDisasters with a high power are not very relevant for social collapse, as they have little chance of being large. So, assuming published power estimates are reliable and that the future repeats the past, we can set aside windstorms (energy power of 12), and worry only somewhat about floods, tornadoes, and terrorist attacks (with death powers of 1.35, 1.4, and 1.4). But we should worry more about disasters with lower powers, such as forest fires (area power of 0.66), hurricanes (dollar-loss power of 0.98, death power of 0.58), earthquakes (energy power of 1, dollar-loss and death powers of 0.41), wars (death power of 0.41), and plagues (death power of 0.26 for Whooping Cough and Measles) (Barton and Nishenko, 1997; Ceder man, 2003; Clauset et al., 2007b; Nishenko &Barton, 1995; Rhodes et al., 1997; Sanders, 2005; Turcotte, 1999; Watts et al., 2005).\n\nNote that energy power tends to be higher than economic loss power, which tends to be higher than death power. This says that compared to the social loss produced by a small disturbance, the loss produced by a large disturbance seems out of proportion to the disturbance, an effect that is especially strong for disasters that threaten lives and not just property. This may (but not necessarily) reflect the disproportionate social collapse that large disasters induce.\n\nFor a type of disaster where damage is distributed with a power below one, if we are willing to spend time and effort to prevent and respond to small events, which hurt only a few people, we should be willing to spend far more to prevent and respond to very large events, which would hurt a large fraction of the Earth’s population. This is because while large events are less likely, their enormous damage more than makes up for their low frequency. If our power law description is not misleading for very large events, then in terms of expected deaths, most of the deaths from war, earthquakes, hurricanes, and plagues occur in the very largest of such events, which kill a large fraction of the world’s population. And those deaths seem to be disproportionately due to social collapse, rather than the direct effect of the disturbance.\n\n17.6 Existential disasters\n\nHow much should we worry about even larger disasters, triggered by disruptions several times stronger than the ones that can kill a large fraction of humanity? Well, if we only cared about the expected number of people killed due to an event, then we would not care that much whether 99% or 99.9% of the population was killed. In this case, for low power disasters, we would care the most about events large enough to kill roughly half of the population; our concern would fall away slowly as we considered smaller events, and fall away quickly as we considered larger events.\n\nA disaster large enough to kill off humanity, however, should be of special concern. Such a disaster would prevent the existence of all future generations of humanity. Of course, it is possible that humanity was about to end in any case, and it is also possible that without humans, within a few million years, some other mammal species on Earth would evolve to produce a society we would respect. Nevertheless, since it is also possible that neither of these things would happen, the complete destruction of humanity must be considered a great harm, above and beyond the number of humans killed in such an event.\n\nIt seems that groups of about seventy people colonized both Polynesia and the New World (Hey, 2005; Murray-McIntosh et al., 1998). So let us assume, as a reference point for analysis, that the survival of humanity requires that 100 humans remain, relatively close to one another, after a disruption and its resulting social collapse. With a healthy enough environment, 100 connected humans might successfully adopt a hunter-gatherer lifestyle. If they were in close enough contact, and had enough resources to help them through a transition period, they might maintain a sufficiently diverse gene pool, and slowly increase their capabilities until they could support farming.\n\nOnce they could communicate to share innovations and grow at the rate that our farming ancestors grew, humanity should return to our population and productivity level within 20,000 years. (The fact that we have used up some natural resources this time around would probably matter little, as growth rates do not seem to depend much on natural resource availability.) With less than 100 survivors near each other, on the other hand, we assume humanity would become extinct within a few generations.\n\n\"Figure 17.1 illustrates a concrete example to help us explore some issues regarding existential disruptions and social collapse. It shows a log-log graph of event severity versus event frequency. For the line marked ‘Post-collapse deaths’, the part of the line on the right side of the figure is set to be roughly the power law observed for war deaths today (earthquake deaths have the same slope, but are one-third as frequent). The line marked ‘Direct deaths’ is speculative and represents the idea that a disruption only directly causes some deaths; the rest are due to social collapse following a disruption. The additional deaths due to social collapse are a small correction for small events, and become a larger correction for larger events.\n\n[Image]\n\nFig. 17.1 A soft cut-off power law scenario.\n\nOf course the data to which these power laws have been fitted do not include events where most of humanity was destroyed. So in the absence of direct data, we must make guesses about how to project the power law into the regime where most people are killed. If S is the severity of a disaster, to which a power law applies, T is the total population just before the disaster, and D is the number killed by the disaster, then one simple approach would be to set\n\n[Image]\n\nThis would produce a very hard cut-off.\n\nIn this case, much of the population would be left alive or everyone would be dead; there would be little chance of anything close to the borderline. This model expresses the idea that whether a person dies from a disaster depends primarily on the strength of that disaster, and depends little on a varying individual ability to resist disaster. Given the parameters of Fig. 17.1, there would be a roughly a 1 in a 1000 chance each year of seeing an event that destroyed all of humanity.\n\nFigure 17.1 instead shows a smoother projection, with a softer cut-off,\n\n[Image]\n\nIn the regime where most people are left alive, D « T, this gives D ≈ S, and so gives the familiar power law,\n\n[Image]\n\nBut in the regime where the number of people left alive, L = T – D, is small, with L « T, we have a new but similar power law,\n\n[Image]\n\nFor this projection, it takes a much stronger event to destroy all of humanity.\n\nThis model expresses the idea that in addition to the strength of the disaster, variations in individual ability to resist disaster are also very important. Such power law survival fractions have been seen in some biological cases (Burchell et al., 2004). Variable resistance might be due to variations in geographic distance, stockpiled wealth, intelligence, health, and military strength.\n\nFigure 17.1 shows a less than 1 in 3 million chance per year of an event that would kill everyone in the ensuing social collapse. But there is a 1 in 500,000 chance of an event that leaves less than 100 people alive; by assumption, this would not be enough to save humanity. And if the remaining survivors were not all in one place, but distributed widely across the Earth and unable to move to come together, it might take many thousands of survivors to save humanity.\n\nThe figure illustrates some of the kinds of trade-offs involved in preventing the extinction of humanity. We assumed somewhat arbitrarily above that 100 humans were required to preserve humanity. Whatever this number is, if it could be reduced somehow by a factor of 2, for the survival of humanity, that would be equivalent to making this type of disaster a factor of 2 less damaging, or increasing our current human population by a factor of 2. In the figure, that is equivalent to about a 25% reduction in rate at which this type of event occurs. This figure also predicts that of every 50 people left alive directly after the disruption, only one remains alive after the ensuing social collapse. A factor of 2 improvement in the number who survive social collapse would also bring the same benefits.\n\n17.7 Disaster policy\n\nFor some types of disasters, like car accidents and windstorms, frequency falls so quickly with event severity that large events can be ignored; they just do not happen. For other types of disasters, such as floods, tornadoes, and terrorist attacks, the frequency falls quickly enough that disasters large enough to cause serious social collapse can be mostly ignored; they are very rare.\n\nBut for still other types of disasters, such as fires, hurricanes, earthquakes, wars, and plagues, most of the expected harm may be in the infrequent but largest events, which would hurt a large fraction of the world. So if we are willing to invest at all in preventing or preparing for these type of events, it seems we should invest the most in preventing and prepare for these largest events. (Of course this conclusion is muted if there are other benefits of preparing for smaller events, benefits which do not similarly apply to preparing for large events.)\n\nFor some types of events, such as wars or plagues, large events often arise from small events that go wrong, and so preparing for and preventing small events may in fact be the best way to prevent large events. But often there are conflicts between preparing for small versus large events. For example, the best response to a small fire in a large building is to stay put until told to move, but at the World Trade Center many learned the hard way that this is bad advice for a large fire. Also, allowing nations to have nuclear weapons can discourage small wars, but encourage large ones.\n\nSimilarly, the usual advice for an earthquake is to ‘duck and cover’ under a desk or doorway. This is good advice for small earthquakes, where the main risk is being hit by items falling from the walls or ceiling. But some claim that in a large earthquake where the building collapses, hiding under a desk will most likely get you flattened under that desk; in this case the best place is said to be pressed against the bottom of something incompressible like file cabinets full of paper (Copp, 2000). Unfortunately, our political systems may reward preparing for the most common situations, rather than the greatest expected damage situations.\n\nFor some kinds of disruptions, like asteroid strikes, we can work to reduce the rate and severity of events. For other kinds of disruptions, like earthquakes, floods, or hurricanes, we can design our physical systems to better resist damage, such as making buildings that sway rather than crack, and keeping buildings out of flood plains. We can also prevent nuclear proliferation and reduce existing nuclear arsenals.\n\nWe can similarly design our social systems to better resist damage. We can consider various crisis situations ahead of time, and make decisions about how to deal with them. We can define who would be in charge of what, and who would have what property rights.\n\nWe can even create special insurance or crisis management organizations which specialize in dealing with such situations.\n\nIf they could count on retaining property rights in a crisis, private organizations would have incentives to set aside private property that they expect to be valuable in such situations. For public goods, or goods with large positive externalities, governments might subsidize organizations that set aside such goods in preparation for a disaster.\n\nUnfortunately, the fact that large disasters are rare makes it hard to evaluate claims about which mechanisms will actually help in such situations. An engineering organization may claim that a dike would only fail once in a century, and police may claim they will keep the peace even with serious social collapse, but track records are not of much use in evaluating such claims.\n\nIf we value future generations of humanity, we may be willing to take extra efforts to prevent the extinction of humanity. For types of disasters where variations in individual ability to resist disruptions are minor, however, there is little point in explicitly preparing for human extinction possibilities. This is because there is almost no chance that an event of this type would put us very near an extinction borderline. The best we could do here would be to try to prevent all large disruptions. Of course there can be non extinction-related reasons to prepare for such disruptions.\n\nOn the other hand, there may be types of disasters where variations in resistance abilities can be important. If so, there might be a substantial chance of finding a post-disaster population that is just above, or just below, a threshold for preserving humanity. In this case it is reasonable to wonder what we might do now to change the odds. The most obvious possibility would be to create refuges with sufficient resources to help preserve a small group of people through a very large disruption, the resulting social collapse, and a transition period to a post-disaster society. Refuges would have to be strong enough to survive the initial disruption. If desperate people trying to survive a social collapse could threaten a refuge’s long-term viability, such as by looting the refuge’s resources, then refuges might need to be isolated, well-defended, or secret enough to survive such threats.\n\nWe have actually already developed similar refuges to protect social elites during a nuclear war (McCamley, 2007). Though nuclear sanctuaries may not be designed with other human extinction scenarios in mind, it is probably worth considering how they might be adapted to deal with non-nuclear-war disasters. It is also worth considering whether to create a distinct set of refuges, intended for other kinds of disasters. I imagine secret rooms deep in a mine, well stocked with supplies, with some way to monitor the surface and block entry.\n\nAn important issue here is whether refuges could by themselves preserve enough humans to supply enough genetic diversity for a post-disaster society. If not, then refuges would either have to count on opening up at the right moment to help preserve enough people outside the sanctuary, or they would need some sort of robust technology for storing genes and implanting them. Perhaps a sperm bank would suffice.\n\nDeveloping a robust genetic technology might be a challenging task; devices would have to last until the human population reached sufficient size to hold enough genetic diversity on its own. But the payoff could be to drastically reduce the required post-collapse population, perhaps down to a single fertile female. For the purpose of saving humanity reducing the required population from 1000 down to 10 is equivalent to a factor of one hundred in current world population, or a factor of one hundred in the severity of each event. In the example of Fig. 17.1, it is the same as reducing the disaster event rate by a factorof 50.\n\nRefuges could in principle hold many kinds of resources which might ease and speed the restoration of a productive human society. They could preserve libraries, machines, seeds, and much more. But the most important resources would clearly be those that ensure that humanity survives. By comparison, on a cosmic scale, it is a small matter whether humanity takes 1000 or 100,000 years to return to our current level of development. Thus the priority should be resources to support a return to at least a hunter-gatherer society.\n\nIt is important to realize that a society rebuilding after a near-extinction crisis would have a vastly smaller scale than our current society; very different types and mixes of capital would be appropriate. Stocking a sanctuary full of the sorts of capital that we find valuable today could be even less useful than the inappropriate medicine, books, or computers often given by first world charities to the third world poor today. Machines would quickly fall into disrepair, and books would impart knowledge that had little practical application.\n\nInstead, one must accept that a very small human population would mostly have to retrace the growth path of our human ancestors; one hundred people cannot support an industrial society today, and perhaps not even a farming society. They might have to start with hunting and gathering, until they could reach a scale where simple farming was feasible. And only when their farming population was large and dense enough could they consider returning to industry.\n\nSo it might make sense to stock a refuge with real hunter-gatherers and subsistence farmers, together with the tools they find useful. Of course such people would need to be disciplined enough to wait peacefully in the refuge until the time to emerge was right. Perhaps such people could be rotated periodically from a well-protected region where they practiced simple lifestyles, so they could keep their skills fresh. And perhaps we should test our refuge concepts, isolating real people near them for long periods to see how well particular sorts of refuges actually perform at returning their inhabitants to a simple sustainable lifestyle.\n\n17.8 Conclusion\n\nWhile there are many kinds of catastrophes that might befall humanity, most of the damage that follows large disruptions may come from the ensuing social collapse, rather than from the direct effects of the disruption. In thinking about how to prevent and respond to catastrophe, it is therefore crucial to consider the nature of social collapse and how we might minimize it.\n\nAfter reviewing the nature of society and of social collapse, we have considered how to fit social collapse into a framework where disaster severity follows a reference power law distribution. We made two key distinctions. The first distinction is between types of disasters where small events are the most important, and types of disasters where large events are the most important. The second key distinction is whether individual variation in resistance to a disaster is minor or important.\n\nFor types of disaster where both large events and individual resistance variation are important, we have considered some of the trade-offs involved in trying to preserve humanity. And we have briefly explored the possibility of building special refuges to increase the chances of saving humanity in such situations.\n\nIt should go without saying that this has been a very crude and initial analysis; a similar but more careful and numerically precise analysis might be well worth the effort.\n\nAcknowledgement\n\nI thank Jason Matheny, the editors, and an anonymous referee. For their financial support, I thank the Center for Study of Public Choice and the Mercatus Center.\n\nReferences\n\nAghion, P., and Howitt, P. (1998). Endogenous Growth Theory. London: MIT Press. Barro, R.J. and Sala-I-Martin, X. (2003). Economic Growth, 2nd edition. London: MIT Press.\n\nBarton, C. and Nishenko, S. (1997). Natural Disasters: Forecasting Economic and Life Losses. http://pubs.usgs.gov/fs/natural-disasters/\n\nBilham, R. (2004). Urban earthquake fatalities-a saferworldorworse to come? Seismol. Rev. Lett. 75, 706–712.\n\nBurchell, M.J., Mann, J.R., and Bunch, A.W. (2004). Survival of bacteria and spores under extreme shock pressures. MNRAS, 352(4), 1273–1278.\n\nCaplan, B. (2003). The idea trap: the political economy of growth divergence. Eur. J. Polit. Econ., 19(2), 183–203.\n\nCederman, L.-E. (2003). Modeling the size of wars: from Billiard Balls to Sandpiles. Am. Polit. Sci. Rev., 97(1), 135–150.\n\nChristensen, K., Danon, L., Scanlon, T., and Bak, P. (2002). Unified scaling law for earth-quakes. Proc. Natl. Acad. Sci., 99(1), 2509–2513.\n\nClauset, A., Shalizi, C.R., and Newman, M.E.J. (2007a). Power-law distributions in empirical data. arXiv:0706.1062v1.\n\nClauset, A., Young, M., and Gleditsch, K.S. (2007b). Scale invariance in the severity of terrorism. J. Confl. Resol., 5. http://xxx.lanl.gov/abs/physics/0606007\n\nCopp, D. (2000). Triangle of Life. American Survival Guide. http://www.amerrescue.org.triangle of life.html\n\nDeLong, J.B. (2006). Growth is good. Harvard Magazine, 19–20.\n\ndeMenocal, P.B. (2001). Cultural responses to climate change during the late Holocene. Science, 292(5517), 667–673.\n\nDiamond, J. (2005). Collapse: How Societies Choose to Fail or Succeed (New York: Viking Adult).\n\nGrubler, A. (1998). Technology and Global Change. (New York: Cambridge Universtity Press).\n\nHanson, R. (2000). Long-term growth as a sequence of exponential modes. http://hanson.gmu.edu/longgrow.html\n\nHaug, G.H., Gnther, D., Peterson, L.C., Sigman, D.M., Hughen, K.A., and Aeschlimann, B. (2003). Climate and the collapse of Maya civilization. Science, 299(5613), 1731 -1735.\n\nHey, J. (2005). On the number of new world founders: a population genetic portrait of the peopling of the Americas. PLoS Biol., 3(6), 965–975.\n\nJones, C.I. (2002). Introduction to Economic Growth, 2nd edition. W. W. Norton &Company.\n\nLay, T. and Wallace, T. (1995). Modern Global Seismology. (San Deigo, CA: Academic Press).\n\nMcCamley, N. (2007). Cold War Secret Nuclear Bunkers (Pen and Sword).\n\nMorrison, D., Harris, A.W., Sommer, G., Chapman, C.R., and Carusi, A. (2003). Dealing with the impact hazard. In Bottke, W., Cellino, A., Paolicchi, P., and Binzel, R.P. (eds.), Asteroids III (Tucson, AZ: University of Arizona Press).\n\nMurray-McIntosh, R.P., Scrimshaw, B.J., Hatfield, P.J., and Penny, D. (1998). Testing migration patterns and estimating founding population size in Polynesia by using human mtDNA sequences. Proc. Natl. Acad. Sci. USA, 95, 90479052.\n\nNishenko, S. and Barton, C. (1995). Scaling laws for natural disaster fatalities. In Rundle, J., Klein, F., and Turcotte, D. (eds.), Reduction and Predictability of Natural Disasters, Volume 25, p.32 (Addison Wesley). Princeton.\n\nPosner, R.A. (2004). Catastrophe: Risk and Response (NewYork: Oxford University Press).\n\nRhodes, C.J., Jensen, H.J., and Anderson, R.M. (1997). On the critical behaviour of simple epidemics. Proc. Royal Soc. B: Biol. Sci., 264(1388), 1639–1646.\n\nRodrik, D. (1999). Where did all the growth go? External shocks, social conflict, and growth collapses. J. Econ. Growth, 44), 385–412.\n\nSanders, D.E.A. (2005). The Modeling of Extreme Events., British Actuarial Journal, 11(3), 519–557.\n\nTainter, J. (1988). The Collapse of Complex Societies. (New York: Cambridge University Press).\n\nTurcotte, D.L. (1999). Self-organized criticality. Reports Prog. Phys., 62, 13771429. Watts, D., Muhamad, R., Medina, D., and Dodds, P. (2005). Multiscale, resurgent epidemics in a hierarchical metapopulation model. Proc. Natl. Acad. Sci., 102(32), 11157–11162.\n\nWeiss, H. and Bradley, R.S. (2001). What drives societal collapse? Science, 291(5504), 609–610.\n\nPART IV Risks from hostile acts\n\n• 18 • The continuing threat of nuclear war\n\nJoseph Cirincione\n\n18.1 Introduction\n\nThe American poet Robert Frost famously mused on whether the world will end in fire or in ice. Nuclear weapons can deliver both. The fire is obvious: modern hydrogen bombs duplicate on the surface of the earth the enormous thermonuclear energies of the Sun, with catastrophic consequences. But it might be a nuclear cold that kills the planet. A nuclear war with as few as a100 hundred weapons exploded in urban cores could blanket the Earth in smoke, ushering in a years-long nuclear winter, with global droughts and massive crop failures.\n\nThe nuclear age is now entering its seventh decade. For most of these years, citizens and officials lived with the constant fear that long-range bombers and ballistic missiles would bring instant, total destruction to the United States, the Soviet Union, many other nations, and, perhaps, the entire planet. Fifty years ago, Nevil Shute’s best-selling novel, On the Beach, portrayed the terror of survivors as they awaited the radioactive clouds drifting to Australia from a northern hemisphere nuclear war. There were then some 7000 nuclear weapons in the world, with the United States outnumbering the Soviet Union 10 to 1. By the 1980s, the nuclear danger had grown to grotesque proportions. When Jonathan Schell’s chilling book, The Fate of the Earth, was published in 1982, there were then almost 60,000 nuclear weapons stockpiled with a destructive force equal to roughly 20,000 megatons (20 billion tons) of TNT, or over 1 million times the power of the Hiroshima bomb. President Ronald Reagan’s ‘Star Wars’ anti-missile system was supposed to defeat a first-wave attack of some 5000 Soviet SS-18 and SS-19 missile warheads streaking over the North Pole. ‘These bombs’, Schell wrote, ‘were built as “weapons” for “war”, but their significance greatly transcends war and all its causes and outcomes. They grew out of history, yet they threaten to end history. They were made by men, yet they threaten to annihilate man’.¹\n\nThe threat of a global thermonuclear war is now near-zero. The treaties negotiated in the 1980s, particularly the START agreements that began the reductions in US and Soviet strategic arsenals and the Intermediate Nuclear Forces agreement of 1987 that eliminated an entire class of nuclear-tipped missiles, began a process that accelerated with the end of the Cold War. Between 1986 and 2006 the nuclear weapons carried by long-range US and Russian missiles and bombers decreased by 61%.² Overall, the number of total nuclear weapons in the world has been cut in half, from a Cold War high of 65,000 in 1986 to about 26,000 in 2007, with approximately 96% held by the United States and Russia. These stockpiles will continue to decline for at least the rest of this decade.\n\nBut the threat of global war is not zero. Even a small chance of war each year, for whatever reason, multiplied over a number of years sums to an unacceptable chance of catastrophe. This is not mere statistical musings. We came much closer to Armageddon after the Cold War ended than many realize. In January 1995, a global nuclear war almost started by mistake. Russian military officials mistook a Norwegian weather rocket for a US submarine-launched ballistic missile. Boris Yelstin became the first Russian president to ever have the ‘nuclear suitcase’ open in front of him. He had just a few minutes to decide if he should push the button that would launch a barrage of nuclear missiles. Thankfully, he concluded that his radars were in error. The suitcase was closed.\n\nSuch a scenario could repeat today. The Cold War is over, but the Cold War weapons remain, and so does the Cold War posture that keep thousands of them on hair-trigger alert, ready to launch in under 15 minutes. As of January 2007, the US stockpile contains nearly 10,000 nuclear weapons; about 5000 of them deployed atop Minuteman intercontinental ballistic missiles based in Montana, Wyoming, and North Dakota, a fleet of 12 nuclear-powered Trident submarine that patrol the Pacific, Atlantic, and Artic oceans and in the weapon bays of long-range B-2 bombers housed in Missouri and B-52 based in Louisiana and North Dakota. Russia has as many as 15,000 weapons, with 3300 atop its SS-18, SS-19, SS-24, and SS-27 missiles deployed in silos in six missile fields arrayed between Moscow and Siberia (Kozelsk, Tatishchevo, Uzhur, Dombarovskiy, Kartalay, and Aleysk), 11 nuclear-powered Delta submarines that conduct limited patrols with the Northern and Pacific fleets from three naval bases (Nerpich’ya, Yagel’Naya, and Rybachiy), and Bear and Blackjack bombers stationed at Ukrainka and Engels air bases (see Table 18.2).³\n\nAlthough the Soviet Union collapsed in 1991 and Russian and American presidents now call each other friends, Washington and Moscow continue to maintain and modernize these huge nuclear arsenals. In July 2007, just before Russian President Vladimir Putin vacationed with American President George W. Bush at the Bush home in Kennebunkport, Maine, Russia successfully tested a new submarine-based missile. The missile carries six nuclear warheads and can travel over 6000 miles, that is, it is designed to strike targets in the United States, including, almost certainly, targets in the very state of Maine Putin visited. For his part, President Bush’s administration adopted a nuclear posture that included plans to produce new types of weapons, begin development of a new generation of nuclear missiles, submarines and bombers, and to expand the US nuclear weapons complex so that it could produce thousands of new warheads on demand.\n\nAlthough much was made of the 1994 joint decision by Presidents Bill Clinton and Boris Yeltsin to no longer target each other with their weapons, this announcement had little practical consequences. Target coordinates can be uploaded into a warhead’s guidance systems within minutes. The warheads remain on missiles on a high alert status similar to that they maintained during the tensest moments of the Cold War. This greatly increases the risk of an unauthorized or accidental launch. Because there is no time buffer built into each state’s decision-making process, this extreme level of readiness enhances the possibility that either side’s president could prematurely order a nuclear strike based on flawed intelligence.\n\nBruce Blair, a former Minuteman launch officer now president of the World Security Institute says, ‘If both sides sent the launch order right now, without any warning or preparation, thousands of nuclear weapons – the equivalent in explosive firepower of about 70,000 Hiroshima bombs – could be unleashed within a few minutes’.⁴\n\nBlair describes the scenario in dry but chilling detail:\n\nIf early warning satellites or ground radar detected missiles in flight, both sides would attempt to assess whether a real nuclear attack was under way within a strict and short deadline. Under Cold War procedures that are still in practice today, early warning crews manning their consoles 24/7 have only three minutes to reach a preliminary conclusion. Such occurrences happen on a daily basis, sometimes more than once per day…. if an apparent nuclear missile threat is perceived, then an emergency teleconference would be convened between the president and his top nuclear advisers. On the US side, the top officer on duty at Strategic Command in Omaha, Neb., would brief the president on his nuclear options and their consequences. That officer is allowed all of 30 seconds to deliver the briefing.\n\nThen the US or Russian president would have to decide whether to retaliate, and since the command systems on both sides have long been geared for launch-on-warning, the presidents would have little spare time if they desired to get retaliatory nuclear missiles off the ground before they – and possibly the presidents themselves – were vaporized. On the US side, the time allowed to decide would range between zero and 12 minutes, depending on the scenario. Russia operates under even tighter deadlines because of the short flight time of US Trident submarine missiles on forward patrol in the North Atlantic.⁵\n\nRussia’s early warning systems remain in a serious state of erosion and disrepair, making it all the more likely that a Russian president could panic and reach a different conclusion than Yeltsin did in 1995.⁶ As Russian capabilities continue to deteriorate, the chances of accidents only increase. Limited spending on the conventional Russian military has led to greater reliance on an ageing nuclear arsenal, whose survivability would make any deterrence theorist nervous. Yet, the missiles remain on a launch status begun during the worst days of the Cold War and never turned off.\n\nAs Blair, concludes: ‘Such rapid implementation of war plans leaves no room for real deliberation, rational thought, or national leadership’.⁷ Former chairman of the Senate Armed Services Committee Sam Nunn agrees ‘We are running the irrational risk of an Armageddon of our own making … The more time the United States and Russia build into our process for ordering a nuclear strike, the more time is available to gather data, to exchange information, to gain perspective, to discover an error, to avoid an accidental or unauthorized launch’.⁸\n\n18.1.1 US nuclear forces\n\nAs of January 2007, the US stockpile contains nearly 10,000 nuclear warheads. This includes about 5521 deployed warheads: 5021 strategic warheads and 500 non-strategic warheads, including cruise missiles and bombs (Table 18.1). Approximately 4441 additional warheads are held in the reserve or inactive/responsive stockpiles or awaiting dismantlement. Under current plans, the stockpile is to be cut ‘almost in half’ by 2012, leaving approximately 6000 warheads in the total stockpile.\n\nTable 18.1 US NuclearForces\n\n[Image]\n\nTable 18.2 Russian NuclearForces\n\n[Image]\n\n18.1.2 Russian nuclear forces\n\nAs of March 2007, Russia has approximately 5670 operational nuclear warheads in its active arsenal. This includes about 3340 strategic warheads and approximately 2330 non-strategic warheads, including artillery, short-range rockets and landmines. An additional 9300 warheads are believed to be in reserve or awaiting dismantlement, for a total Russian stockpile of approximately 15,000 nuclear warheads (Table 18.2).\n\n18.2 Calculating Armageddon\n\n18.2.1 Limited war\n\nThere are major uncertainties in estimating the consequences of nuclear war. Much depends on the time of year of the attacks, the weather, the size of the weapons, the altitude of the detonations, the behaviour of the populations attacked, etc. But one thing is clear: the numbers of casualties, even in a small, accidental nuclear attack, are overwhelming. If the commander of just one Russian Delta-IV ballistic-missile submarine were to launch 12 of its 16 missiles at the United States, 7 million Americans could die.⁹\n\nExperts use various models to calculate nuclear war casualties. The most accurate estimate the damage done from a nuclear bomb’s three sources of destruction: blast, fire and radiation. Fifty percent of the energy of the weapon is released through the blast, 35% as thermal radiation, and 15% through radiation.\n\nLike a conventional weapon, a nuclear weapon produces a destructive blast, or shock wave. A nuclear explosion, however, can be thousands and even millions of times more powerful than a conventional one. The blast creates a sudden change in air pressure that can crush buildings and other objects within seconds of the detonation. All but the strongest buildings within 3 km (1.9 miles) of a 1 megaton hydrogen bomb would be levelled. The blast also produces super-hurricane winds that can destroy people and objects like trees and utility poles. Houses up to 7.5 km (4.7 miles) away that have not been completely destroyed would still be heavily damaged.\n\nA nuclear explosion also releases thermal energy (heat) at very high temperatures, which can ignite fires at considerable distances from the detonation point, leading to further destruction, and can cause severe skin burns even a few miles from the explosion. Stanford University historian Lynn Eddy calculates that if a 300 kiloton nuclear weapon were dropped on the U.S. Department of Defense, ‘within tens of minutes, the entire area, approximately 40 to 65 square miles – everything within 3.5 or 6.4 miles of the Pentagon – would be engulfed in a mass fire’ that would ‘extinguish all life and destroy almost everything else’. The creation of a ‘hurricane of fire’, Eden argues, is a predictable effect of a high-yield nuclear weapon, but is not taken into account by war planners in their targeting calculations.¹⁰\n\nUnlike conventional weapons, a nuclear explosion also produces lethal radiation. Direct ionizing radiation can cause immediate death, but the more significant effects are long term. Radioactive fallout can inflict damage over periods ranging from hours to years. If no significant decontamination takes place (such as rain washing away radioactive particles, or their leaching into the soil), it will take 8 to 10 years for the inner circle near the explosion to return to safe levels of radiation. In the next circles such decay will require 3 to 6 years. Longer term, some of the radioactive particles will enter the food chain.¹¹ For example, a 1-megaton hydrogen bomb exploded at ground level would have a lethal radiation inner circle radius of about 50 km (30 miles) where death would be instant. At 145 km radius (90 miles), death would occurwithin two weeks of exposure. The outermost circle would be at 400 km (250 miles) radius where radiation would still be harmful, but the effects not immediate.\n\nTable 18.3 Immediate Deaths from Blast and Firestorms in eight US Cities, Submarine Attack\n\n[Image]\n\nIn the accidental Delta IV submarine strike noted above, most of the immediate deaths would come from the blast and ‘superfires’ ignited by the bomb. Each of the 100 kiloton warheads carried by the submarine’s missiles would create a circle of death 8.6 km (5.6 miles) in diameter. Nearly 100%of the people within this circle would die instantly. Firestorms would kill millions more (see Table 18.3). The explosion would produce a cloud of radioactive dust that would drift downwind from the bomb’s detonation point. If the bomb exploded at a low altitude, there would be a 10–60 km (20-40 miles) long and 3–5 km (2-3 miles) wide swath of deadly radiation that would kill all exposed and unprotected people within six hours of exposure.¹² As the radiation continued and spread over thousands of square kilometres, it is very possible that secondary deaths in dense urban populations would match or even exceed the immediate deaths caused by fire and blast, doubling the total fatalities listed in Table 18.4. The cancerdeaths and genetic damage from radiation could extend for several generations.\n\nTable 18.4 Casualties and Fatalities in a South Asian Nuclear War\n\n[Image]\n\n18.2.2 Global war\n\nNaturally, the number of casualties in a global nuclear war would be much higher. US military commanders, for example, might not know that the commander of the Delta submarine had launched by accident or without authorization. They could very well order a US response. One such response could be a precise ‘counter-force’ attack on all Russian nuclear forces. An American attack directed solely against Russian nuclear missiles, submarines and bomber bases would require some 1300 warheads in a coordinated barrage lasting approximately 30 minutes, according to a sophisticated analysis of US war plans by experts at the Natural Resources Defense Council in Washington, DC.¹³ It would destroy most of Russia’s nuclear weapons and development facilities. Communications across the country would be severely degraded. Within hours after the attack, the radioactive fallout would descend and accumulate, creating lethal conditions over an area exceeding 775,000 km2 -larger than France and the United Kingdom combined. The attack would result in approximately 11–17 million civilian casualties, 8–12 million of which would be fatalities, primarily due to the fallout generated by numerous ground bursts.\n\nAmerican war planners could also launch another of the plans designed and modelled over the nuclear age: a ‘limited’ attack against Russian cities, using only 150–200 warheads. This is often called a ‘counter-value’ attack, and would kill or wound approximately one-third of Russia’s citizenry. An attack using the warheads aboard just a single US Trident submarine to attack Russian cities will result in 30–45 million casualties. An attack using 150 US Minuteman III ICBMs on Russian cities would produce 40–60 million casualties.¹⁴\n\nIf, in either of these limited attacks, the Russian military command followed their planned operational procedures and launched their weapons before they could be destroyed, the results would be an all-out nuclear war involving most of the weapons in both the US and Russian arsenals. The effects would be devastating. Government studies estimate that between 35 and 77% of the US population would be killed (105-230 million people, based on current population figures) and 20–40% of the Russian population (28-56 million people).¹⁵\n\nA 1979 report to Congress by the U.S. Office of Technology Assessment, The Effects of Nuclear War, noted the disastrous results of a nuclear war would go far beyond the immediate casualties:\n\nIn addition to the tens of millions of deaths during the days and weeks after the attack, there would probably be further millions (perhaps further tens of millions) of deaths in the ensuing months or years. In addition to the enormous economic destruction caused by the actual nuclear explosions, there would be some years during which the residual economy would decline further, as stocks were consumed and machines wore out faster then recovered production could replace them…. For a period of time, people could live of supplies (and, in a sense, off habits) left over from before the war. But shortages and uncertainties would get worse. The survivors would find themselves in a race to achieve viability … before stocks ran out completely. A failure to achieve viability, or even a slow recovery, would result in many additional deaths, and much additional economic, political, and social deterioration. This postwar damage could be as devastating as the damage from the actual nuclear explosions.¹⁶\n\nAccording to the report’s comprehensive analysis, if production rose to the rate of the consumption before stocks were exhausted, then viability would be achieved and economic recovery would begin. If not, then ‘each post-war year would see a lower level of economic activity than the year before, and the future of civilization itself in the nations attacked would be in doubt’.¹⁷ It is doubtful that either the United States or Russia would ever recover as viable nation states.\n\n18.2.3 Regional war\n\nThere are grave dangers inherent not only in countries such as the United States and Russia maintaining thousands of nuclear weapons but also in China, France, the United Kingdom, Israel, India, and Pakistan holding hundreds of weapons. While these states regard their own nuclear weapons as safe, secure, and essential to security, each views others’ arsenals with suspicion.\n\nExisting regional nuclear tensions already pose serious risks. The decades-long conflict between India and Pakistan has made South Asia the region most likely to witness the first use of nuclear weapons since World War II. An active missile race is under way between the two nations, even as India and China continue their rivalry. Although some progress towards détente has been made, with each side agreeing to notify the other before ballistic missile tests, for example, quick escalation in a crisis could put the entire subcontinent right back on the edge of destruction. Each country has an estimated 50 to 100 nuclear weapons, deliverable via fighter-bomber aircraft or possibly by the growing arsenal of short- and medium-range missiles each nation is building. Their use could be devastating.\n\nSouth Asian urban populations are so dense that a 50 kiloton weapon would produce the same casualties that would require megaton-range weapons on North American or European cities. Robert Batcher with the U.S. State Department Office of Technology Assessment notes:\n\nCompared to North America, India and Pakistan have higher population densities with a higher proportion of their populations living in rural locations. The housing provides less protection against fallout, especially compared to housing in the U.S. Northeast, because it is light, often single-story, and without basements. In the United States, basements can provide significant protection against fallout. During the Cold War, the United States anticipated 20 minutes or more of warning time for missiles flown from the Soviet Union. For India and Pakistan, little or no warning can be anticipated, especially for civilians. Fire fighting is limited in the region, which can lead to greater damage as a result of thermal effects. Moreover, medical facilities are also limited, and thus, there will be greater burn fatalities. These two countries have limited economic assets, which will hinder economic recovery.¹⁸\n\n18.2.4 Nuclear winter\n\nIn the early 1980s, scientists used models to estimate the climatic effect of a nuclear war. They calculated the effects of the dust raised in high-yield nuclear surface bursts and of the smoke from city and forest fires ignited by airbursts of all yields. They found that ‘a global nuclear war could have a major impact on climate – manifested by significant surface darkening over many weeks, subfreezing land temperatures persisting for up to several months, large perturbations in global circulation patterns, and dramatic changes in local weather and precipitation rates’.¹⁹ Those phenomena are known as ‘Nuclear Winter’.\n\nSince this theory was introduced it has been repeatedly examined and reaffirmed. By the early 1990s, as tools to asses and quantify the production and injection of soot by large-scale fires, and its effects, scientists were able to refine their conclusions. The prediction for the average land cooling beneath the smoke clouds was adjusted down a little bit, from the 10–25°C estimated in the 1980s to 10–20°C. However, it was also found that the maximum continental interior land cooling can reach 40°C, more than the 30–35 degrees estimate in the 1980s, ‘with subzero temperatures possible even in the summer’.²⁰\n\nIn a Science article in 1990, the authors summarized:\n\nShould substantial urban areas or fuel stocks be exposed to nuclear ignition, severe environmental anomalies – possibly leading to more human casualties globally than the direct effects of nuclear war – would be not just a remote possibility, but a likely outcome.²¹\n\nCarl Sagan and Richard Turco, two of the original scientists developing the nuclear winter analysis, concluded in 1993:\n\nEspecially through the destruction of global agriculture, nuclear winter might be considerably worse than the short-term blast, radiation, fire, and fallout of nuclear war. It would carry nuclear war to many nations that no one intended to attack, including the poorest and most vulnerable.²²\n\nIn 2007, members of the original group of nuclear winter scientists collectively performed a new comprehensive quantitative assessment utilizing the latest computer and climate models. They concluded that even a small-scale, regional nuclear war could kill as many people as died in all of World War II and seriously disrupt the global climate for a decade or more, harming nearly everyone on Earth.\n\nThe scientists considered a nuclear exchange involving 100 Hiroshima-size bombs (15 kilotons) on cities in the subtropics, and found that:\n\nSmoke emissions of 100 low-yield urban explosions in a regional nuclear conflict would generate substantial global-scale climate anomalies, although not as large as the previous ‘nuclear winter’ scenarios for a full-scale war. However, indirect effect on surface land temperatures, precipitation rates, and growing season lengths would be likely to degrade agricultural productivity to an extent that historically has led to famines in Africa, India and Japan after the 1784 Laki eruption or in the northeastern United States and Europe after the Tambora eruption of 1815. Climatic anomalies could persist fora decade ormore because of smoke stabilization, farlongerthan in previous nuclear winter calculations oraftervolcanic eruptions.²³\n\nThe scientists concluded that the nuclear explosions and firestorms in modern cities would inject black carbon particles higher into the atmosphere than previously thought and higher than normal volcanic activity (see Chapter 10, this volume). Blocking the Sun’s thermal energy, the smoke clouds would lower temperatures regionally and globally for several years, open up new holes in the ozone layer protecting the Earth from harmful radiation, reduce global precipitation by about 10% and trigger massive crop failures. Overall, the global cooling from a regional nuclear war would be about twice as large as the global warming of the past century ‘and would lead to temperatures cooler than the pre-industrial Little Ice Age’.²⁴\n\n18.3 The current nuclear balance\n\nDespite the horrific consequences of their use, many national leaders continue to covet nuclear weapons. Some see them as a stabilizing force, even in regional conflicts. There is some evidence to support this view. Relations between India and Pakistan, for example, have improved overall since their 1998 nuclear tests. Even the conflict in the Kargil region between the two nations that came to a boil in 1999 and again in 2002 (with over one million troops mobilized on both sides of the border) ended in negotiations, not war. Columbia University scholar Kenneth Waltz argues, ‘Kargil showed once again that deterrence does not firmly protect disputed areas but does limit the extent of the violence. Indian rear admiral Raja Menon put the larger point simply: “The Kargil crisis demonstrated that the subcontinental nuclear threshold probably lies territorially in the heartland of both countries, and not on the Kashmir cease-fire line”’.²⁵\n\nIt would be reaching too far to say the Kargil was South Asia’s Cuban missile crisis, but since the near-war, both nations have established hotlines and other confidence-building measures (such as notification of military exercises), exchanged cordial visits of state leaders, and opened transportation and communications links. War seems less likely now than at any point in the past.\n\nThis calm is deceiving. Just as in the Cold War stand off between the Soviet Union and the United States, the South Asian détente is fragile. A sudden crisis, such as a terrorist attack on the Indian parliament, or the assassination of Pakistan President Pervez Musaraf, could plunge the two countries into confrontation. As noted above, it would not be thousands that would die, but millions. Michael Krepon, one of the leading American experts on the region and its nuclear dynamics, notes:\n\nDespite or perhaps because of the inconclusive resolution of crises, some in Pakistan and India continue to believe that gains can be secured below the nuclear threshold. How might advantage be gained when the presence of nuclear weapons militates against decisive end games? … If the means chosen to pursue advantage in the next IndoPakistan crisis show signs of success, they are likely to prompt escalation, and escalation might not be easily controlled. If the primary alternative to an ambiguous outcome in the next crisis is a loss of face or a loss of territory, the prospective loser will seek to change the outcome.²⁶\n\nMany share Krepon’s views both in and out of South Asia. Indian scholar P.R. Chari, for example, further observes:\n\n[S]ince the effectiveness of these weapons depends ultimately on the willingness to use them in some situations, there is an issue of coherence of thought that has to be addressed here. Implicitly or explicitly an eventuality of actual use has to be a part of the possible alternative scenarios that must be contemplated, if some benefit is to be obtained from the possession and deployment of nuclear weapons. To hold the belief that nuclear weapons are useful but must never be used lacks cogency. …²⁷\n\nA quickly escalating crisis over Taiwan is another possible scenario in which nuclear weapons could be used, not accidentally as with any potential US-Russian exchange, but as a result of miscalculation. Neither the United States nor China is eager to engage in a military confrontation over Taiwan’s status, and both sides believe they could effectively manage such a crisis. But crises work in mysterious ways – political leaders are not always able to manipulate events as they think they can, and events can escalate very quickly. A Sino-US nuclear exchange may not happen even in the case of a confrontation over Taiwan’s status, but it is possible and should not be ignored.\n\nThe likelihood of use depends greatly on the perceived utility of nuclear weapons. This, in turn, is greatly influenced by the policies and attitudes of the existing nuclear weapon states. Recent advocacy by some in the United States of new battlefield uses for nuclear weapons (e.g., in pre-emptive strikes on Iran’s nuclear facilities) and for programmes for new nuclear weapon designs could lead to fresh nuclear tests, and possibly lower the nuclear threshold, that is, the willingness of leaders to use nuclear weapons. The five nuclear weapon states recognized by the Non-proliferation Treaty have not tested since the signing of the Comprehensive Test Ban Treaty in 1996, and until North Korea’s October 2007 test, no state had tested since India and Pakistan did so in May 1998. If the United States again tested nuclear weapons, political, military and bureaucratic forces in several other countries would undoubtedly pressure their governments to follow suit. Indian scientists, for example, are known to be unhappy with the inconclusive results of their 1998 tests. Indian governments now resist their insistent demands for new tests for fear of the damage they would do to India’s international image. It is a compelling example of the power of international norms. New US tests, however, would collapse that norm, trigger tests by India, then perhaps China, Russia, and other nations. The nuclear test ban treaty, an agreement widely regarded as a pillar of the nonproliferation regime, would crumble, possibly bringing down the entire regime.\n\nAll of these scenarios highlight the need for nuclear weapons countries to decrease their reliance on nuclear arms. While the United States has reduced its arsenal to the lowest levels since the 1950s it has received little credit for these cuts because they were conducted in isolation from a real commitment to disarmament. The United States, Russia, and all nuclear weapons states need to return to the original bargain of the NPT – the elimination of nuclear weapons. The failure of nuclear weapons states to accept their end of the bargain under Article VI of the NPT has undermined every other aspect of the non-proliferation agenda.\n\nUniversal Compliance, a 2005 study concluded by the Carnegie Endowment for International Peace, reaffirmed this premise:\n\nThe nuclear-weapon states must show that tougher nonproliferation rules not only benefit the powerful but constrain them as well. Nonproliferation is a set of bargains whose fairness must be self-evident if the majority of countries is to support their enforcement … The only way to achieve this is to enforce compliance universally, not selectively, including the obligations the nuclear states have taken on themselves … The core bargain of the NPT, and of global nonproliferation politics, can neither be ignored nor wished away. It underpins the international security system and shapes the expectations of citizens and leaders around the world.²⁸\n\nWhile nuclear weapons are more highly valued by national officials than chemical or biological weapons ever were, that does not mean they are a permanent part of national identity. A January 2007 op-ed in the Wall Street Journal co-authored by George Shultz, Henry Kissinger, William Perry and Sam Nunn, marked a significant change in the thinking of influential policy and decision makers in the United States and other nuclear weapons states. They contend that the leaders of the countries in possession of nuclear weapons should turn to the goal of a world without nuclear weapons into a ‘joint enterprise’. They detail that a nine-point programme that includes substantial reductions in the size of nuclear forces in all states, the elimination of short-range nuclear weapons, and the ratification of the Comprehensive Test Ban Treaty. The op-ed concludes that, ‘Reassertion of the vision of a world free of nuclear weapons and practical measures towards achieving that goal would be, and would be perceived as, a bold initiative consistent with America’s moral heritage. The effort could have a profoundly positive impact on the security of future generations’.²⁹\n\nBreaking the nuclear habit will not be easy, but there are ways to minimize the unease some may feel as they are weaned away from dependence on these weapons. The United States and Russia account for over 96% of the world’s nuclear weapons. The two nations have such redundant nuclear capability that it would not compromise any vital security interests to quickly reduce down to several hundred warheads each. Further reductions and the possibility of complete elimination could then be examined in detailed papers prepared by and for the nuclear-weapon states. If accompanied by reaffirmation of the ban on nuclear testing, removal of all weapons from rapid-launch alert status, establishment of a firm norm against the first use of these weapons, and commitments to make the reductions in weapons irreversible and verifiable, the momentum and example generated could fundamentally alter the global dynamic.\n\nSuch an effort would hearken back to US President Harry Truman’s proposals in 1946 which coupled weapons elimination with strict, verified enforcement of nonproliferation. Dramatic reductions in nuclear forces could be joined, for example, with reforms making it more difficult for countries to withdraw from the NPT (by clarifying that no state may withdraw from the treaty and escape responsibility for prior violations of the treaty or retain access to controlled materials and equipment acquired for ‘peaceful’ purposes).³⁰ It would make it easier to obtain national commitments to stop the illegal transfer of nuclear technologies and reform the fuel cycle. The reduction in the number of weapons and the production of nuclear materials would also greatly decrease the risk of terrorists acquiring such materials.\n\n18.4 The good news about proliferation\n\nIs it reasonable to expect such dramatic changes in international security strategies? Absolutely; there is nothing inevitable about either proliferation or nuclear arsenals. Though obscured by the media and political emphasize on terror and turmoil, the nations of the world have made remarkable progress in the past two decades in reducing many nuclear dangers.\n\nThere are farfewer countries that have nuclear weapons or weapon programmes today than there were in the 1960s, 1970s, or 1980s. In the 1960s, 23 countries had weapons or were pursuing programmes, including Australia, Canada, China, Egypt, India, Japan, Norway, Sweden, Switzerland, and West Germany. Today, nine countries have weapons (China, France, India, Israel, North Korea, Pakistan, Russia, United Kingdom, and the United States). Iran may be pursuing a weapons programme under the guise of peaceful nuclear power, but no other nation is believed to be doing so.\n\nIn fact, more countries have given up nuclear weapons or weapons programmes in the past 20 years than have started them. South Africa, Belarus, Kazakhstan, and Ukraine all gave up weapons in the 1990s. Similarly, civilian governments in Brazil and Argentina in the 1980s stopped the nuclear weapon research military juntas had started. We now know that United Nations inspection and dismantlement programmes ended Iraq’s nuclear weapon programme in 1991. In December 2003, Libya became the most recent nation to abandon a secret weapons programme.\n\nThe Non-proliferation Treaty itself is widely considered one of the most successful security pacts in history, with every nation of the world a member except for Israel, India, Pakistan, and North Korea. And until North Korea tested in October 2006, no nation had exploded a nuclear weapon in a test for 8 years – the longest period in the atomic age. The outrage that greeted the test shows how strong this anti-nuclear sentiment had become.\n\nThere is more good news. The ballistic missile threat that dominated American and NATO national security debates in the late 1990s is declining by most measures: There are far fewer nuclear-tipped missiles capable of hitting the United States or any European country today than there were 10 years ago. Agreements negotiated by American Presidents Ronald Reagan, George H.W. Bush and George W. Bush have slashed the former Soviet arsenal by 71% from 1987, while China has retained about 20 missiles that could reach US shores. No other country can strike the United States from its own soil. Most of the news about missile tests in Iran, North Korea, or South Asia are of short- or medium-range missiles that threaten those nations’ neighbours but not America.³¹\n\nNonetheless, four critical challenges – nuclear terrorism, the threats from existing arsenals, the emergence of new weapon states, and the spread of dual-use nuclear fuel facilities – threaten to unravel today’s nuclear non-proliferation regime. Overcoming these challenges will require forging a consensus of expert opinion, focusing the attention of senior officials, securing the necessary funding, and, above all, executive leadership. The decisions taken over the next few years with regard to these challenges by the president of the United States and the leaders of other nuclear-weapon states as well as other key countries will determine whether the progress of the last two decades in reducing and eliminating nuclear weapons and materials continues, or whether the world launches into the second wave of proliferation since World War II.\n\n18.5 A comprehensive approach\n\nTo understand the need for and the practicality of a comprehensive approach to reducing the danger of nuclear war, consider what is required to prevent new nations from acquiring nuclear weapons. For some scholars and officials, the addition of new nations to the nuclear club is as natural and inevitable as population growth. Kenneth Waltz argues, ‘Nuclear weapons will nevertheless spread, with a new member occasionally joining the club … Someday the world will be populated by fifteen or eighteen nuclear-weapon states’.³²\n\nAmerican expert William Potter says this view is shared by many in the Bush administration. ‘Principle one’, for many officials he says, is ‘that nuclear proliferation is inevitable, at best it can be managed, not prevented’.³³ Currently, however, there are only two countries of serious concern. If the nuclear programmes in North Korea and Iran can be checked, then prospects for halting and reversing proliferation globally improve dramatically. If they are not, then they may start a momentum that tips neighbouring countries over the nuclear edge.\n\nThe specifics and politics vary from country to country, but all of the threats from new nations acquiring weapons share the same need for a comprehensive, multi-dimensional approach. Iran, by far the more difficult of the cases, can serve as a model of how such an approach could work.\n\nThe current Iranian government plans to build between 6 and 20 nuclear power reactors and all the facilities needed to make and reprocess the fuel for these reactors. The same facilities that can make fuel for nuclear reactors can also make the fuel for nuclear bombs. Whatever its true intentions, convincing Iran that while it could proceed with construction of power reactors, the country must abandon construction of fuel manufacturing facilities will not be easy. It will require both threats of meaningful sanctions, and promises of the economic benefits of cooperation.\n\nThis is the package of carrots and sticks that comprised the negotiations between the European Union and Iran. Calibrating the right balance in this mix is difficult enough, but the package itself is not sufficient to seal a deal. The hard-line government of President Mahmoud Ahmadinejad further complicates the issue with its harsh rhetorical insistence on proceeding with the nuclear plans and pointed threats to Israel. While the rhetoric may eventually fade, at the core, Iran or any country’s reasons for wanting its own fuel cycle capabilities are similar to the reasons some countries want nuclear weapons: security, prestige and domestic political pressures. All of these will have to be addressed in order to craft a permanent solution.\n\nPart of the security equation can be addressed by the prospect of a new relationship with the United States that ends regime change efforts. Iran would need some assurances that agreements on nuclear programmes could end efforts by the United States and Israel to remove the current regime. The United States has told North Korea that it has no hostile intentions towards the state and that an end to that country’s programme would lead to the restoration of diplomatic relations. Similar assurances will be needed forIran.\n\nBut there is also a regional dimension. Ending the threat from an Iranian nuclear programme will require placing the Iranian decision in the context of the long-standing goal of a Middle East free of nuclear weapons. It will be impossible for a country as important as Iran to abstain permanently from acquiring the technologies for producing nuclear weapons – at least as a hedge – if other countries in the region have them. This dynamic was noted in the very first US National Intelligence Estimates of the proliferation threats done in 1958 and 1961 and is still true today.\n\nIran’s leaders will want some assurances that there is a process underway that can remove what they see as potential threats from their neighbours, including Israel. For domestic political reasons, they will want to present their nuclear abstinence as part of a movement towards a shared and balanced regional commitment.\n\nSome may argue that Israel would never give up its nuclear weapons. But such nuclear free zones have been created in other regions which, though not as intensely contested as the Middle East, still had to overcome substantial rivalries and involved the abandonment of existing programmes (in South America) and the dismantlement of actual weapons (in Africa and Central Asia). All the states in the region rhetorically support this goal, as do many of the major powers, but in recent years, little diplomatic effort has been put behind this policy – certainly nothing on the scale of the effort needed to create the nuclear Non-proliferation Treaty and its support mechanisms in the 1960s and 1970s.\n\nRidding the region of nuclear weapons will, of course, be difficult, but it is far better than the alternative of a Middle East with not one nuclear power (Israel) but two, three or four nuclear weapon states – and with unresolved territorial, religious and political disputes. This is a recipe for nuclear war.\n\nThis is not a distant fear. In late 2006 and early 2007, a dozen Muslim nations expressed their interest in starting their own nuclear power programmes. In the entire 62-year history of the nuclear age there has been exactly one nuclear power reactor built in the Middle East (the one under construction in Iran) and two in Africa (in South Africa). Suddenly, Saudi Arabia, Egypt, Turkey, Jordan, Morocco, and several Gulf states have begun exploring nuclear power programmes. This is not about energy; it is about hedging against a nuclear Iran.\n\nThe key to stopping this process is to get a counter-process going. States in the region must have some viable alternative to the pessimistic view that the Middle East will eventually be a nuclear free-for-all. In order for this effort to succeed, there will have to be a mechanism to change fundamentally the way nuclear fuel is produced and reprocessed. Doing so would satisfy a nation’s security considerations that it does not have to build its own facilities in order to have a secure supply of fuel for its reactors. Some Iranians see the current negotiations as a new effort by the West to place them, once again, in a dependent relationship. This time the West would not control their oil, they say, but the energy of the future, nuclear fuel. Iran, indeed any nation, will not permanently acquiesce to a discriminatory regime that adds to the existing inequality allowing some countries to have nuclear weapons while others cannot, by now allowing some countries to make nuclear fuel while others cannot.\n\nA comprehensive approach operating at several levels is the only sure way to prevent more and more nations from wanting and acquiring the technology that can bring them – legally – right up to the threshold of nuclear weapons capability.\n\n18.6 Conclusion\n\nUltimately, reducing the risks from nuclear weapon in the twenty-first century cannot be just a military or nuclear energy strategy. At the beginning of the nuclear age, it was already clear that unless we solved the underlying political conflicts that encourage some states to seek security in nuclear arms, we would never prevent nuclear competition. Robert Oppenheimer, the scientific head of the Manhattan Project, said, ‘We must ask, of any proposals for the control of atomic energy, what part they can play in reducing the probability of war. Proposals that in no way advance the general problem of the avoidance of war are not satisfactory proposals’.³⁴ Thus, nuclear-weapon-specific efforts should be joined by focused initiatives to resolve conflicts in key regions. A quick look at the map should make clear that nuclear weapons have not spread around the world uniformly, like a drop of ink diffusing in a glass of water. Vast areas of the world – entire continents – are nuclear-weapon free. There are no nuclear weapons in South America, Africa, Australia, or Southeast Asia. Rather, the states of proliferation concern are in an arc of crisis that flows from the Middle East through South Asia up to Northeast Asia. In other words, in regions within which unresolved territorial, political and religious disputes give rise to the desire to gain some strategic advantage by acquiring nuclear weapons.\n\nCountries have given up nuclear weapons and programmes in the past only when these disputes have been resolved. The pattern of the past should be the template for the future. Avoiding nuclear war in South Asia requires continuing the progress in normalizing relations between India and Pakistan and achieving a permanent resolution of the Kashmir issue. Ridding the Middle East of nuclear weapons and new nuclear programmes requires normalization of relations between Israel and other regional states and groups based on a just resolution to the Israeli-Palestinian conflict.\n\nResolution of some of these may come more quickly than most imagine. Even 10 years ago it was inconceivable to many that Ian Paisley, the leader of the militant Protestant Democratic Union Party would ever share power with Martin McGuinness, a leader of the militant Catholic IRA. Both called the other terrorist. Both swore to wipe each other’s groups from the face of the earth. Yet, in 2007, they shook hands and were sworn into office as the joint leaders of a united Northern Ireland.\n\nOthers conflicts may take more time to resolve, but as history teaches, it is the direction in which we are moving that informs national attitudes and shapes each state’s security decisions. The more arrows we can get pointed in the right direction, the easier it becomes to make progress on all fronts.\n\nFormer U.S. State Department official Robert Einhorn and former U.S. Defense Department official Kurt Campbell note that the wisdom of societies and states that have gone without nuclear weapons is reinforced by ‘a world in which the goals of the NPT are being fulfilled – where existing nuclear arsenals are being reduced, parties are not pursuing clandestine nuclear programmes, nuclear testing has been stopped, the taboo against the use of nuclear weapons is being strengthened, and in general, the salience of nuclear weapons in international affairs is diminishing’.\n\nThere is every reason to believe that in the first half of this century the peoples and nations of the world will come to see nuclear weapons as the ‘historic accident’ Mohamed El Baradei says they are. It may become clearer that nations have no need for the vast destructive force contained in a few kilograms of enriched uranium or plutonium. These weapons still appeal to national pride but they are increasingly unappealing to national budgets and military needs. It took just 60 years to get to this point in the nuclear road. If enough national leaders decide to walk the path together; is should not take another 60 to get to a safer, better world.\n\nAcknowledgement\n\nCampbell, K.M., Einhorn, R., and Reiss, M. (eds.). (2004). The Nuclear Tipping Point: Global Prospects for Revisiting Nuclear Renunciation (Washington, D.C.: Brookings Institution Press), cited in Universal Compliance, p. 130.\n\nSuggestions for further reading\n\nCampbell, K., Einhorn, R., Reiss, M. (eds.). (2004). The Nuclear Tipping Point: Why States Reconsider Their Nuclear Choices (Washington, D.C.: Brookings Institution). A comprehensive set of case studies of why and how eight key nations decided not to acquire nuclear weapons … fornow.\n\nCirincione, J. (2007). Bomb Scare: The History and Future of Nuclear Weapons (New York: Columbia University Press). A concise guide to the science, strategy, and politics of the nuclear age.\n\nOffice of Technology Assessment. (1979). The Effects of Nuclear War (Washington, D.C.). The seminal source for the blast, fire, radiation, and strategic effects of the use of nuclear weapons.\n\nRhodes, R. (1986). The Making of the Atomic Bomb (New York: Simon and Schuster). Rhodes won the Pulitzer Prize for this gripping, comprehensive history of how the first atomic bombs were built, used, and regretted.\n\nShute, N. (1957). On the Beach (New York: Ballantine Books). The best-selling, classic novel about the world after Nuclear War.\n\nThe Weapons of Mass Destruction Commission (2006). Weapons of Terror: Feeing the World of Nuclear, Biological and Chemical Arms (Stockholm). An international prescription for a nuclear-free world.\n\n• 19 • Catastrophic nuclear terrorism: a preventable peril\n\nGary Ackerman and William C. Potter\n\n19.1 Introduction\n\nOne can conceive of at least three potentially catastrophic events involving the energy of the atom: a nuclear accident in which massive quantities of radiation inadvertently are released into the environment including inadvertent nuclear missile launches; nuclear war among nation-states; and nuclear violence inflicted by non-state actors. This chapter focuses on the latter threat – the dangers posed by nuclear terrorism, a phenomenon that lies at the nexus between what are widely considered to be two of the primary security threats of the modern era.\n\nNon-state actors have essentially four mechanisms by which they can exploit civilian and military nuclear assets intentionally to serve their terrorist¹ goals:\n\n• the dispersal of radioactive material by conventional explosives or other means;\n\n• attacks against or sabotage of nuclear facilities, in particular nuclear power plants and fuel storage sites, causing the release of radioactivity;\n\n• the theft, purchase, or receipt of fissile material leading to the fabrication and detonation of a crude nuclear explosive, usually referred to as an improvised nuclear device (IND); and\n\n• the theft, purchase, or receipt and detonation of an intact nuclear weapon.\n\nAll of these nuclear threats are real; all merit the attention of the international community; and all require the expenditure of significant resources to reduce their likelihood and potential impact. The threats, however, are different andvary widely in their probability of occurrence, in consequences for human and financial loss, and in the ease with which intervention might reduce destructive outcomes (for a detailed analysis, see Ferguson and Potter, 2005).\n\nNuclear terrorism experts generally agree that the nuclear terror scenarios with the highest consequences – those involving nuclear explosives – are the least likely to occur because they are the most difficult to accomplish. Conversely, the scenarios with the least damaging consequences – those involving the release of radioactivity but no nuclear explosion – are the most likely to occur because they are the easiest to carry out. Constructing and detonating an IND, for example, is far more challenging than building and setting off a radiological dispersal device (RDD), because the former weapon is far more complex technologically and because the necessary materials are far more difficult to obtain. Thus, an IND presents a less likely threat than does an RDD, but the potential consequences of an IND explosion are orders of magnitude more devastating than the potential damage from the use of an RDD.\n\nIt is difficult to conceive of any scenario in which either the terrorist use of RDDs or the sabotage of or attack on one or more nuclear facilities would approach the category of global, terminal risks referred to in this volume as existential risks. The remainder of this chapter will focus on the two forms of ‘high consequence’ nuclear terrorism, those involving INDs and intact nuclear weapons. Although it also is hard to devise plausible scenarios in which nonstate actors could employ these forms of nuclear terrorism to achieve a situation in which humankind as a whole is imperiled, under certain circumstances terrorist action might create global catastrophic risks of the endurable kind.\n\nThis chapter examines the theoretical requirements for engaging in nuclear terrorism, outlines the current evidence for and possible future shape of the threat, and then discusses the potential short- and long-term global consequences of nuclear terrorism. It concludes with policy recommendations for mitigating this particular species of global catastrophic risk.\n\n19.2 Historical recognition of the risk of nuclear terrorism\n\nAsked in a closed Senate hearing room ‘whether three or four men couldn’t smuggle units of an [atomic] bomb into New York and blow up the whole city’. Oppenheimer responded, ‘Of course it could be done, and people could destroy New York’. When a startled senator then followed by asking, ‘What instrument would you use to detect an atomic bomb hidden somewhere in the city?’ Oppenheimer quipped, ‘A screwdriver [to open each and every crate or suitcase]’. There was no defense against nuclear terrorism – and he felt there never would be.\n\n(Bird and Sherwin, 2005, p. 349)\n\nThe subject of non-state actors and nuclear explosives has been the focus of sporadic scholarship, journalistic commentary, and government attention for over three decades. While it was anticipated from the beginning of the nuclear age, as the quote above notes, few worried about non-state actors as a nuclear threat before the 1970s. One of the earliest efforts to examine the issue was by a panel of experts convened by the US Atomic Energy Commission (AEC) under the chairmanship of Ralph F. Lumb. In its 1967 report, the Lumb panel addressed the need to strengthen safeguards to discourage diversion of nuclear materials from the expanding civilian nuclear fuel cycle in the United States.² The report followed the discovery by the AEC in 1965 that it could not account for a large quantity of weapons grade uranium at the US naval nuclear fuel plant in Apollo, Pennsylvania (Walker, 2001, p. 109).³\n\nThe potential for home grown nuclear terrorism gained greater recognition in the United States at the end of the 1960s and in the early 1970s when incidents of politically motivated violence, including a number of incidents at nuclear research and power reactor sites, increased. For example, explosives were discovered at a research reactor at the Illinois Institute of Technology and the Point Beach nuclear power plant (Walker, 2001, p. 111). Another dimension of the terrorism problem – nuclear extortion – was also highlighted following a threat in October 1970 to detonate a hydrogen bomb in Orlando, Florida unless $1 million was provided. The extortion threat, accompanied by a drawing of the alleged device, was judged sufficiently credible by authorities that city officials considered paying the ransom until a slip-up revealed that it was a hoax perpetrated by a local 14-year-old honours student (Willrich and Taylor, 1974, p. 80).\n\nIn addition to a spate of press reports about the dangers of nuclear terrorism (including Ingram, 1972; Lapp, 1973; Zorza, 1972, 1974), the early to mid-1970s generated a second AEC report on the subject, released to considerable public fanfare by Senator Abraham Ribicoff and the first book length nongovernmental studies. The 1974 volume Nuclear Theft: Risks and Safeguards by Mason Willrich and Theodore Taylor was particularly noteworthy because of its thorough treatment of the risks of nuclear theft and the fact that Taylor was a professional nuclear weapons designer who spoke with great authority about the technical obstacles – or lack thereof – a non-state actor would have to overcome to build a nuclear explosive (Zorza, 1972).⁴ According to the account by Willrich and Taylor,\n\nUnder conceivable circumstances, a few persons, possibly even one person working alone, who possessed about ten kilograms of plutonium and substantial amount of chemical high explosive could, within several weeks, design and build a crude fission bomb. … This could be done using materials and equipment that could be purchased at a hardware store and from commercial suppliers of scientific equipment for student laboratories. … Statements similar to those made above about a plutonium oxide bomb, could also be made about a fission bomb made with high-enriched uranium.\n\n(McPhee, 1974, pp. 20–21)\n\nThis view was not shared uniformly by other experts, and, as is noted below, may understate the difficulty of producing a crude nuclear explosive with plutonium (as opposed to uranium). Taylor, himself, subsequently appeared to raise the degree of difficulty facing non-state actors. Nevertheless, the warning about the ability of terrorists to inflict high consequence nuclear violence could not be easily dismissed. This perspective was reinforced in a 1977 assessment by the U.S. Office of Technology Assessment, which drew upon relevant classified information. It reported that ‘a small group of people, none of whom have ever had access to the classified literature, could possibly design and build a crude nuclear explosive device’. According to the report, modest machine shop facilities could suffice and necessary equipment could be obtained for ‘a fraction of a million dollars’. The group, however, ‘would have to include, at a minimum, a person capable of researching and understanding the literature in several fields, and a jack-of-all trades technician’ (U.S. Office of Technology Assessment, 1977, p. 30).\n\nTwo other influential authors during the 1970s were David Rosenbaum, a consultant who challenged the adequacy of AEC regulations to prevent nuclear theft (Rosenbaum, 1977) and Brian Jenkins, an analyst at the Rand Corporation. Best known for his assertion that ‘Terrorists want a lot of people watching, not a lot of people dead …’, Jenkins, probably more than anyone else, encouraged the notion that terrorists were unlikely to employ weapons of mass destruction (WMD) in pursuit of their objectives (Jenkins, 1977).\n\nAlthough it is hard to gauge how much impact Jenkins’ thesis had on government policy, the dangers of high consequence nuclear terrorism received relatively little attention in the scholarly literature during the period between the mid-1970s and early 1990s. An important exception was an indepth report of an international task force whose findings were published under the editorship of Paul Leventhal and Yonah Alexander (1987). A frequently cited chapter in that book by J. Carson Mark and a team of other former US bombmakers concluded that terrorists could indeed build a nuclear explosive device. It was more circumspect than Willrich and Taylor, however, in attributing that capability to any single individual. ‘The number of specialists required’, Mark et al. maintained, ‘would depend on the background and experience of those enlisted, but their number could scarcely be fewer than three or four and might well have to be more’ (Mark et al., 1987, p. 58).\n\nThe collapse of the Soviet Union in 1991 and the risk of ‘loose nukes’ prompted a new wave of government, academic, and popular concern about nuclear terrorism. A series of studies organized by Harvard University’s Center for Science and International Studies, in particular, helped to frame the nature of the nuclear threat posed by the disintegration of the Soviet Union, and also contributed to major new legislation in the form of the Nunn-Lugar Cooperative Threat Reduction Program (Allison, 1996; Campbell, 1991).⁵\n\nIf the demise of the Soviet Union refocused scholarly and governmental attention on the risk that insecure nuclear weapons and material might find their way into the hands of terrorists, the use of sarin nerve agent by the Japanese cult Aum Shinrikyo in 1995 and the events of 11 September 2001, resulted in a surge in research and government spending on many dimensions of WMD terrorism, including a reassessment of the readiness and ability of non-state actors to resort to nuclear violence.\n\n19.3 Motivations and capabilities for nuclear terrorism\n\n19.3.1 Motivations: the demand side of nuclear terrorism\n\nAn intentional risk such as nuclear terrorism can only result from the conscious decision of a human actor, yet the integral motivational component of such threats has often been overshadowed by assessments of terrorist capabilities and weapon consequences.⁶ What is needed is a systematic dissection of the variety of factors that might induce terrorists to pursue the use of nuclear weapons.⁷ The following discussion examines possible terrorist motivations that reflect strategic, operational, and tactical incentives for using nuclear weapons (i.e., where nuclear weapons are used as a means to an end) as well as more esoteric motives where the use of nuclear weapons is an end in itself.⁸\n\n1. Mass Casualties: The most obvious reason for terrorists to seek nuclear weapons is for the purpose of inflicting massive casualties upon their perceived enemies.⁹ Indeed, while conventional (and even most unconventional) weapons will suffice to kill thousands or perhaps even tens of thousands of people if used by terrorist groups, for perpetrators who seek to cause the maximum possible immediate carnage (on the order of hundreds of thousands or millions of fatalities) the most viable means is to utilize the kinetic and thermal effects of a nuclear blast.¹⁰ Much of the concern surrounding terrorism involving WMD stems from the belief that there is a growing number of non-state actors prepared to inflict catastrophic violence.¹¹ The majority of terrorist attacks, however, are carried out for a multiplicity of motives, so one should not assume that the desire to inflict mass casualties is necessarily the sole, or even predominant, motive for resorting to a nuclear option.¹²\n\n2. Inordinate Psychological Impact: It is a truism that one of the core elements of terrorism is the terror it evokes. For a terrorist group seeking to traumatize a targeted society and generate public and official disorientation, nuclear weapons must hold a particular allure, for there can be few images that are guaranteed to leave as indelible a mark on the collective psyche of the targeted country as that of a mushroom cloud over one of its major cities.¹³ Anthony Cordesman asserts that it is not even necessary for a nuclear weapon to have catastrophic physical effects for it to have far-ranging psychological and political impact (Cordesman, 2001, pp. 9, 10, 38–39).\n\n3. Prestige: Historically, nuclear weapons have remained under the exclusive purview of nation-states, with one of the key motivations for state acquisition being the status which nuclear weapons are believed to bestow upon their possessors. How much more appealing then might the possession of nuclear weapons seem for non-state groups, many of whomseek international legitimization? To the extent that terrorists believe that nuclear weapons could enable them to attain quasi-state standing or redress military imbalances vis-à-vis their purported enemies, the possession of such weapons, but not necessarily their use, becomes an attractive proposition. It is even conceivable that a terrorist group might pursue nuclear weapons in the hope of deterring, blackmailing or coercing a particular state or group of states. Thomas Schelling explores the prestige and deterrence aspects for non-state terrorists (Schelling, 1982). Also see Cameron (1999, p. 134).\n\n4. Incentives for Innovation and Escalation: In a milieu in which terrorists groups may have to compete with rival groups for ‘market share’ of media attention and constituency support, terrorist decision makers may feel compelled to outdo the destruction wrought by previous attacks. For a discussion of why terrorists seek mass-casualty events that ‘out-do’ previous attacks, see Post (2000, pp. 280–282). The asymptote of such escalatory pressures, especially in the wake of such attacks as those of September 11, may be the detonation of a nuclear weapon on enemy territory, which would guarantee unrivalled attention upon the terrorists and their cause. While most terrorist supporters and sympathizers would be appalled by such horrific actions, there are certain subsets of disaffected populations that could condone the use of nuclear weapons against a hated enemy. For example, brutalized communities motivated by revenge may be more likely to condone such use.\n\n5. Mass Destruction and Area Denial: In certain cases, terrorists may desire not only mass casualties, but also to physically destroy the infrastructure of their enemies and deny them the use or functioning of vital areas. These are tasks for which nuclear weapons are well-suited because they have both immediately destructive blast effects and persistent radiological contamination effects.\n\n6. Ideology: The worldview of a terrorist group or individual demarcates allies and enemies and forms the basis for deciding between legitimate and illegitimate targets and tactics.¹⁴ As such it is likely to be one of the most important factors in any decision to resort to the use of nuclear weapons. It is often asserted that the use of a weapon as destructive and reviled as nuclear weapons would alienate the supporters and perceived constituency of any terrorist group motivated primarily by a nationalist or secular political ideology (Cameron, 1999, pp. 156–157), and therefore that such groups would mostly refrain from using nuclear weapons. Whatever the accuracy of this assertion, a corollary is widely accepted by terrorism experts, that is, that groups motivated by religion, which are focused on cosmic as opposed to mortal concerns, are far more willing to engage in attacks involving mass casualties and hence would be more prone to use nuclear weapons or other means of mass destruction (Cameron, 2000; Campbell, 2000; Gurr and Cole, 2002; Hoffman, 1993a; Hoffman, 1997, pp. 45–50; Hoffman, 1998, p. 94). As one analyst observed, ‘to the extent that violent extremist groups are absolutely convinced that they are doing God’s bidding, virtually any action that they decide to undertake can be justified, no matter how heinous, since the “divine” ends are thought to justify the means’ (Bale, 2005, p. 298; cf. Hoffman, 1993a, p. 12). The resurgence in religiously inspired terrorism in recent decades could imply that there is now a greater possibility of terrorists seeking to use WMD.¹⁵ The situation, however, is more complex. First, not all religious terrorists are equally likely to pursue mass destruction – many religiously motivated terrorist organizations have political components, represent constituencies that are well-defined geographically (and thus are subject to retribution), or depend for financial or logistical support on parties whose views may not be quite as radical as their own. Moreover, it is the theological and cultural content of the particular strand of religious belief which is of greatest significance (Gressang, 2001), rather than the mere fact that a group has a religious bent. It has been asserted that the ideologies most conducive to the pursuit of catastrophic violence are those which simultaneously reflect an apocalyptic millenarian character, in which an irremediably corrupt world must be purged to make way for a utopian future, and emphasize the capacity for purification from sins through sacrificial acts of violence (Ackerman and Bale, 2004, pp. 2930; Cameron, 1999, pp. 80–83; see also Chapter 4 in this volume). Such ideologies are of ten, though not exclusively, found amongst unorthodox religious cults, such as Aum Shinrikyo, the Covenant, the Sword, and the Arm of the Lord, and R.I.S.E.,¹⁶ and one can conceive of an affinity between the ‘the relentless impulse toward world-rejecting purification’ (Lifton, 1999, p. 204) displayed by such groups and the levels of ‘cathartic’ destruction only achievable using nuclear weapons. Moreover, Jessica Stern has suggested that religious terrorists might embrace WMD, including nuclear weapons, as a means of ‘emulat[ing] God’ (Stern, 1999, p. 70). One must bear in mind, however, that possessing an ideology with a religious character may at most be a contributing factor to any desire to engage in nuclear terrorism, and is certainly not determinative, an assertion which has been validated empirically for CBRN weapons in toto (Asal and Ackerman, 2006).\n\n7. Atomic Fetishism: A terrorist group whose ideology or key decision makers display a peculiar fascination for things nuclear or radiological might be more likely to consider pursuing a nuclear weapons capability. It is not hard to imagine that a group whose ideology is based, for instance, upon a nuclear holocaust motif or whose leader is obsessed with the science-fiction genre, could be drawn towards nuclear weapons as their preferred instruments of destruction. The archetype amongst known terrorist groups is Aum Shinrikyo, whose leader, Shoko Asahara, whose view of several types of unconventional weapons, including the nuclear variety verged on fetishism.\n\n8. Revenge and Other ‘Expressive’ Motives: It is believed that individuals from heavily brutalized and traumatized communities (such as those who fall victim to genocide) might be capable of unrestrained levels of violence in the pursuit of revenge against their perceived persecutors, ¹⁷ and thus might consider a retributive act as devastating as a nuclear detonation. Other expressive motives might also come into play, for example, an extreme form of defensive aggression wherein a group perceives its own imminent destruction (or that of those it purports to represent) and thus resorts to the most violent measures imaginable as a ‘swan song’ (Cameron, 1999, p. 135).\n\nIn addition to the possible set of instrumental, ideological or psychological motives already described, there are two other considerations that, while not necessarily sufficient on their own to motivate terrorists to pursue nuclear weapons, might influence this choice indirectly. The first of these is opportunity: a terrorist group manifesting one or more of the above motives may be propelled to consider the nuclear option more seriously by happenstance. For example, governmental collapse in a nuclear weapons state could provide increased scope for the terrorists’ procurement of intact nuclear weapons and thus might precipitate for the first time the consideration of using a nuclear device. The second salient consideration is the impact of organizational structure and dynamics. It has been suggested that groups exhibiting certain structural characteristics might be more likely to engage in acts of violence as extreme as nuclear terrorism. Some of these allegedly pernicious traits include: control by megalomaniacal or sadistic, but nonetheless charismatic and authoritarian leaders; isolation from their broader society, with little display of concern for outgroups; an intentional focus on recruiting technical or scientifically skilled members; a record of innovation and excessive risk-taking; and the possession of sufficient resources, whether financial, human or logistical, to enable long-term research and development into multiple advanced weapons systems.¹⁸\n\nWhile none of the above motives will necessarily lead to a decision to use nuclear weapons, the existence of such a broad array of potential motives provides a prima facie theoretical case that the most extreme and violent of terrorists might find either strategic, tactical, or emotional advantage in utilizing the destructive power of nuclear weapons. Any group possessing several of the abovementioned attributes deserves close scrutiny in this regard. Moreover, many (though not all) of the motives listed could also be served by lower-scale attacks, including using RDDs or attacking nuclear facilities. For instance, RDDs would likely result in a disproportionate psychological impact and area denial, but would not satisfy terrorists seeking mass fatalities.\n\n19.3.2 The supply side of nuclear terrorism\n\nFortunately, even for those terrorist organizations that are not dissuaded from high consequence nuclear terrorism by moral considerations or fears of reprisal, there are major implementation challenges. These include access to nuclear assets and a variety of technical hurdles.\n\n19. 3.2.1 Improvised nuclear devices\n\nA terrorist group motivated to manufacture and detonate an IND would need to:\n\n1. acquire sufficient fissile material to fabricate an IND\n\n2. fabricate the weapon\n\n3. transport the intact IND (or its components) to a high-value target and\n\n4. detonate the IND.¹⁹\n\nIn this ‘chain of causation’, the most difficult challenge for a terrorist organization would most likely be obtaining the fissile material necessary to construct an IND.²⁰\n\nThe problem of protecting fissile material globally has many dimensions, the most significant of which is the vast quantity of highly enriched uranium (HEU) and plutonium situated at approximately 350 different sites in nearly five dozen countries. It is estimated that there are more than 2000 metric tons of fissile material – enough for over 200,000 nuclear weapons. Many of the sites holding this material lack adequate material protection, control, and accounting measures; some are outside of the International Atomic Energy Agency’s (IAEA) safeguard system; and many exist in countries without independent nuclear regulatory bodies or rules, regulations, and practices consistent with a meaningful safeguards culture.\n\n19.3.2.2 The special dangers of HEU\n\nTwo types of fissile material can be used for the purpose of fabricating a nuclear explosive – plutonium and HEU. The most basic type of nuclear weapon and the simplest to design and manufacture is a HEU-based gun-type device. As its name implies, it fires a projectile – in this case a piece of HEU – down a gun barrel into another piece of HEU. Each piece of HEU is sub-critical and by itself cannot sustain an explosive chain reaction. Once combined, however, they form a supercritical mass and can create a nuclear explosion.\n\nWeapons-grade HEU – uranium enriched to over 90% of the isotope U-235 – is the most effective material for a HEU-based device. However, even HEU enriched to less than weapons-grade can lead to an explosive chain reaction. The Hiroshima bomb, for example, used about 60 kg of 80% enriched uranium. Terrorists would probably need at least 40 kg of weapons-grade or near weapons-grade HEU to have reasonable confidence that the IND would work (McPhee, 1974, pp. 189–194).²¹\n\nAs indicated above, the potential for non-state actors to build a nuclear explosive already had been expressed publicly by knowledgeable experts as early as the 1970s. Their view is shared today by many physicists and nuclear weapons scientists, who concur that construction of a gun-type device would pose few technological barriers to technically competent terrorists (Alvarez, 1988, p. 125; Arbman et al., 2004; Barnaby, 1996; Boutwell et al., 2002; Civiak, 2002; Garwin and Charpak, 2001; Maerli, 2000; National Research Council, 2002; Union of Concerned Scientists, 2003; von Hippel, 2001, p. 1; Wald, 2000). In 2002, for example, the U.S. National Research Council warned, ‘Crude HEU weapons could be fabricated without state assistance’ (National Research Council, 2002, p. 45). The Council further specified, ‘The primary impediment that prevents countries or technically competent terrorist groups from developing nuclear weapons is the availability of [nuclear material], especially HEU’ (National Research Council, 2002, p. 40). This perspective was echoed in testimony before the Senate Foreign Relations Committee during the Clinton administration when representatives from the three US nuclear weapons laboratories all acknowledged that terrorists with access to fissile material could produce a crude nuclear explosion using components that were commonly available (Bunn and Weir, 2005, p. 156).\n\nWhile there appears to be little doubt among experts that technically competent terrorists could make a gun-type device given sufficient quantities of HEU, no consensus exists as to how technically competent they have to be and how large a team they would need. At one end of the spectrum, there is the view that a suicidal terrorist could literally drop one piece of HEU metal on top of another piece to form a supercritical mass and initiate an explosive chain reaction. Nobel laureate Luis Alvarez’s oft-cited quote exemplifies this view:\n\nWith modern weapons-grade uranium, the background neutron rate is so low that terrorists, if they have such material, would have a good chance of setting off a high-yield explosion simply by dropping one half of the material onto the other half. Most people seem unaware that if separated HEU is at hand it’s a trivial job to set off a nuclear explosion … even a high school kid could make a bomb in short order.\n\n(Alvarez, 1988, p. 125)\n\nHowever, to make sure that the group could surmount any technical barriers, it would likely want to recruit team members who have knowledge of conventional explosives (needed to fire one piece of HEU into another), metalworking, and draftsmanship. A well-financed terrorist organization such as al Qaeda would probably have little difficulty recruiting personnel with these skills.\n\nThere are many potential sources of HEU for would-be nuclear terrorists. It is estimated that there are nearly 130 research reactors and associated fuel facilities in the civilian nuclear sector around the world with 20 kg or more of HEU, many of which lack adequate security (Bunn and Weir, 2005, p. 39; GAO, 2004). Also vulnerable is HEU in the form of fuel for naval reactors and targets for the production of medical isotopes. Indeed, a number of the confirmed cases involving illicit nuclear trafficking involve naval fuel.\n\nAlthough an HEU-fuelled gun-type design would be most attractive to a would-be nuclear terrorist, one cannot altogether rule out an IND using plutonium. Such an explosive would require an implosion design in which the sphere of fissile material was rapidly compressed. In order to accomplish this feat, a terrorist group would require access to and knowledge of high speed electronics and high explosive lenses. A US government sponsored experiment in the 1960s suggests that several physics graduates without prior experience with nuclear weapons and with access to only unclassified information could design a workable implosion type bomb.²² The participants in the experiment pursued an implosion design because they decided a gun-type device was too simple and not enough of a challenge (Stober, 2003).\n\nAssuming that terrorists were able to acquire the necessary fissile material and manufacture an IND, they would need to transport the device (or its components) to the target site. Although an assembled IND would likely be heavy – perhaps weighing up to 1 ton – trucks and commercial vans could easily haul a device that size. In addition, container ships and commercial airplanes could provide delivery means. Inasmuch as, by definition, terrorists constructing an IND would be familiar with its design, the act of detonating the device would be relatively straightforward and present few technical difficulties.\n\n19.3.2.3 Intact nuclear weapons\n\nIn order for terrorists to detonate an intact nuclear weapon at a designated target they would have to:\n\n1. acquire an intact nuclear charge\n\n2. bypass or defeat any safeguards against unauthorized use incorporated into the intact weapons and.\n\n3. detonate the weapon.\n\nBy far the most difficult challenge in the aforementioned pathway would be acquisition of the intact weapon itself.²³\n\nAccording to conventional wisdom, intact nuclear weapons are more secure than are their fissile material components. Although this perspective is probably correct, as is the view that the theft of a nuclear weapon is less likely than most nuclear terrorist scenarios, one should not be complacent about nuclear weapons security. Of particular concern are tactical nuclear weapons (TNW), of which thousands exist, none covered by formal arms control accords. Because of their relatively small size, large number, and, in some instances, lack of electronic locks and deployment outside of central storage sites, TNW would appear to be the nuclear weapon of choice for terrorists.\n\nThe overwhelming majority of TNW reside in Russia, although estimates about the size of the arsenal vary widely (see, for example, the estimate provided by Sokov, Arkin, and Arbatov, in Potter et al., 2000, pp. 58–60). The United States also deploys a small arsenal of under 500 TNW in Europe in the form of gravity bombs. A major positive step enhancing the security of TNW was taken following the parallel, unilateral Presidential Nuclear Initiatives of 1991–1992. In their respective declarations, the American and Soviet/Russian presidents declared that they would eliminate many types of TNW, including artillery-fired atomic projectiles, tactical nuclear warheads, and atomic demolition munitions, and would place most other classes of TNW in ‘central storage’. Although Russia proceeded to dismantle several thousand TNW, it has been unwilling to withdraw unilaterally all of its remaining TNW from forward bases or even to relocate to central storage in a timely fashion those categories of TNW covered by the 1991–1992 declarations. Moreover, in recent years, neither the United States nor Russia has displayed any inclination in pursuing negotiations to reduce further TNW or to reinforce the informal and fragile TNW regime based on parallel, unilateral declarations.\n\nAlthough Russia has been the focus of most international efforts to enhance the security of nuclear weapons, many experts also are concerned about nuclear weapons security in South Asia, particularly in Pakistan. Extremist Islamic groups within Pakistan and the surrounding region, a history of political instability, uncertain loyalties of senior officials in the civilian and military nuclear chain of command, and a nascent nuclear command and control system increase the risk that Pakistan’s nuclear arms could fall into the hands of terrorists. Little definite information is available, however, on the security of Pakistan’s nuclear weapons or those in its nuclear neighbour, India.\n\nShould a terrorist organization obtain an intact nuclear weapon, in most instances it would still need to overcome mechanisms in the weapon designed to prevent its use by unauthorized persons. In addition to electronic locks known as Permissive Action Links (PALs), nuclear weapons also may be safeguarded through so-called safing, arming, fusing, and firing procedures. For example, the arming sequence for a warhead may require changes in altitude, acceleration, or other parameters verified by sensors built into the weapon to ensure that the warhead can only be used according to a specific mission profile. Finally, weapons are likely to be protected from unauthorized use by a combination of complex procedural arrangements (requiring the participation of many individuals) and authenticating codes authorizing each individual to activate the weapon.\n\nAll operational US nuclear weapons have PALs. Most authorities believe that Russian strategic nuclear weapons and modern shorter range systems also incorporate these safeguards, but are less confident that older Russian TNW are equipped with PALs (Sokov, 2004). Operational British and French nuclear weapons (with the possible exception of French SLBM warheads) also probably are protected by PALs. The safeguards on warheads of the other nuclear-armed states cannot be determined reliably from open sources, but are more likely torely on procedures (e.g., a three-man rule) than PALs to prevent unauthorized use (Ferguson and Potter, 2005, p. 62).\n\nUnless assisted by sympathetic experts, terrorists would find it difficult, though not necessarily impossible, to disable or bypass PALs or other safeguard measures. If stymied, terrorists might attempt to open the weapon casing to obtain fissile material in order to fabricate an IND. However, the act of prying open the bomb casing might result in terrorists blowing themselves up with the conventional high explosives associated with nuclear warheads. Thus, terrorists would likely require the services of insiders to perform this operation safely.\n\nAssuming a terrorist organization could obtain a nuclear weapon and had the ability to overcome any mechanisms built into the device to prevent its unauthorized detonation, it would still need to deliver the weapon to the group’s intended target. This task could be significantly complicated if the loss of the weapon were detected and a massive recovery effect were mounted.\n\nIt is also possible terrorists might adopt strategies that minimized transportation. These include detonating the weapon at a nearby, less-than-optimal target, or even at the place of acquisition.\n\nIf a nuclear weapon were successfully transported to its target site, and any PALs disabled, a degree of technical competence would nonetheless be required to determine how to trigger the device and provide the necessary electrical or mechanical input for detonation. Here, again, insider assistance would be of considerable help.\n\n19.4 Probabilities of occurrence\n\n19.4.1 The demand side: who wants nuclear weapons?\n\nThankfully, there have been no instances of non-state actor use of nuclear weapons. The historical record of pursuit by terrorists of nuclear weapons is also very sparse, with only two cases in which there is credible evidence that terrorists actually attempted to acquire nuclear devices.²⁴ The most commonly cited reasons for this absence of interest include the technical and material difficulties associated with developing and executing a nuclear detonation attack, together with the alleged technological and operational ‘conservatism’²⁵ of most terrorists, fears of reprisal, and the moral and political constraints on employing such frightful forms of violence. These propositions are thencombined and cast in a relative manner to conclude that the overwhelming majority of terrorists have thus far steered clear of nuclear weapons because they have found other weapon types to be (1) easier to develop and use, (2) more reliable and politically acceptable, and (3) nonetheless eminently suitable of accomplishing their various political and strategic goals. In short, the basic argument is that interest has been lacking because large-scale unconventional weapons, especially of the nuclear variety, were seen to be neither necessary nor sufficient²⁶ for success from the terrorists’ point of view.\n\nAlthough the past non-use of nuclear weapons may ab initio be a poor indicator of future developments, it appears that some prior restraints on terrorist pursuit and use of nuclear weapons (and other large scale unconventional weapons systems) may be breaking down. For example, one can point to an increase in the number of terrorist-inclined individuals and groups who subscribe to beliefs and goals that are concordant with several of the motivational factors described earlier. In terms of mass casualties, for instance, there are now groups who have expressed the desire to inflict violence on the order of magnitude that would result from the use of a nuclear weapon. Illustrative of this perspective was the claim in 2002 by Sulaiman Abu Ghaith, Usama bin Laden’s former official press spokesman, of the right for jihadists ‘to kill four million Americans’ (The Middle East Media Research Institute, 2002). One can also point to groups displaying an incipient techno-fetishism, who are simultaneously less interested in global, or sometimes any external, opinion, such as Aum Shinrikyo. It might be of little surprise, then, that it is these very two groups which have manifested more than a passing interest in nuclear weapons.\n\nPrior to Aum Shinrikyo, most would-be nuclear terrorists were more kooks than capable, but Aum made genuine efforts to acquire a nuclear capability. As is described in more detail in the next section, during the early 1990s, Aum repeatedly attempted to purchase, produce, or otherwise acquire a nuclear weapon. Aum’s combination of apocalyptic ideology, vast financial and technical resources, and the non-interference by authorities in its activities enabled it to undertake a generous, if unsuccessful, effort to acquire nuclear weapons. Although some analysts at the time sought to portray Aum as a one-off nexus of factors that were unlikely to ever be repeated, in fact, a far larger transnational movement emerged shortly thereafter with a similar eye on the nuclear prize.\n\nAl Qaeda, the diffuse jihadist network responsible for many of the deadliest terrorist attacks in the past decade, has not been shy about its nuclear ambitions. As early as 1998, its self-styled emir, Usama bin Ladin, declared that ‘To seek to possess the weapons [of mass destruction] that could counter those of the infidels is a religious duty … It would be a sin for Muslims not to seek possession of the weapons that would prevent the infidels from inflicting harm on Muslims’ (Scheuer, 2002, p. 72). As noted previously, the group has asserted ‘the right to kill 4 million Americans – 2 million of them children’, in retaliation for the casualties it believes the United States and Israel have inflicted on Muslims. Bin Laden also sought and was granted a religious edict or fatwa from a Saudi cleric in 2003, authorizing such action (Bunn, 2006). In addition to their potential use as a mass-casualty weapon for punitive purposes, ²⁷ al Qaeda ostensibly also sees strategic political advantage in the possession of nuclear weapons, perhaps to accomplish such tasks as coercing the ‘Crusaders’ to leave Muslim territory. When combined with millenarian impulses among certain quarters of global jihadis and a demonstrated orientation towards martyrdom, it is apparent that many (if not most) of the motivational factors associated with nuclear terrorism apply to the current violent jihadist movement. The manner in which these demands on motivations have been translated into concrete efforts to obtain nuclear explosives is described in the section below on ‘The Supply Side’.\n\nAt present, the universe of non-state actors seeking to acquire and use nuclear weapons appears to be confined to violent jihadhists, a movement that is growing in size and scope and spawning a host of radical offshoots and followers. Although in the short term at least, the most likely perpetrators of nuclear violence will stem from operationally sophisticated members of this milieu, in the longer term, they may be joined by radical rightwing groups (especially those components espousing extremist Christian beliefs),²⁸ an as-yet-unidentified religious cult, or some other group of extremists who limn the ideological and structural arcs associated with nuclear terrorism.\n\nWithin any society, there will always be some people dissatisfied with the status quo. A very small subset of these angry and alienated individuals may embark on violent, terrorist campaigns for change, in some cases aiming globally. An even tinier subset of these non-state actors with specific ideological, structural, and operational attributes may seek nuclear weapons. Perhaps the most frightening possibility would be the development of technology or the dissolution of state power in a region to the point where a single disgruntled individual would be able to produce or acquire a working nuclear weapon. Since there are far more hateful, delusional and solipsistic individuals than organized groups in this world, ²⁹ this situation would indeed be deserving of the label of a nuclear nightmare. This and other simila capability related issues are discussed in the following section.\n\n19.4.2 The supply side: how far have terrorists progressed?\n\nAs a recent briefing by the Rand Corporation points out, ‘On the supply side of the nuclear market, the opportunities for [non-state] groups to acquire nuclear material and expertise are potentially numerous’ (Daly et al., 2005, p. 3). These opportunities include huge global stocks of fissile material, not all of which are adequately secured; tens of thousands of weapons in various sizes and states of deployment and reserve in the nuclear arsenals of at least eight states; and a large cadre of past and present nuclear weaponeers with knowledge of the science and art of weapons design and manufacture. In addition, the extraordinarily brazen and often successful nuclear supply activities of A.Q. Khan and his wide-flung network demonstrate vividly the loopholes in current national and international export control arrangements. Although not in great evidence to date, one also cannot discount the emergence of organized criminal organizations that play a Khan-like middleman role in finding a potential nuclear supplier, negotiating the purchase/sale of the contraband, and transporting the commodity to the terrorist end-user. Taken together, these factors point to the need to assess carefully the past procurement record of terrorist groups and the potential for them in the future to obtain not only highly sensitive nuclear technology and know-how, but also nuclear weapon designs and the weapons themselves. The most instructive cases in examining the level of success attained by terrorists thus far are once again to be found in Aum Shinrikyo and al Qaeda.\n\nThe religious cult Aum Shinrikyo initiated an ambitious programme to acquire chemical, biological, and nuclear weapons in the early 1990s. In the nuclear sphere, this effort was first directed at purchasing a nuclear weapon. To this end, Aum appears to have sought to exploit its large following in Russia (at its peak estimated to number in the tens of thousands) and its access to senior Russian officials, to obtain a nuclear warhead. Aum members are reported to have included a number of scientists at the Kurchatov Institute, an important nuclear research facility in Moscow possessing large quantities of HEU, which at the time were poorly secured (Bunn, 2006; Daly et al., 2005, p. 16). Aum leader Shoko Asahara, himself, led a delegation to Russia in 1992, which met with former Vice President Aleksandr Rutskoi, Russian Parliament speaker Rusian Khasbulatov, and Head of Russia’s Security Council, Oleg Lobov (Daly et al., 2005, p. 14).³⁰ Inscribed in a notebook confiscated from senior Aum leader Hayakawa Kiyohi, who reportedly made over 20 trips to Russia, was the question, ‘Nuclear warhead. How much?’ Following the questions were several prices in the millions of dollars (Bunn, 2006; Daly et al., 2005, p. 13).\n\nDespite its deep pockets and unusually high-level contacts in Russia, possibly extending into the impoverished nuclear scientific complex, Aum’s persistent efforts were unsuccessful in obtaining either intact nuclear weapons or the fissile material for their production. A similarly ambitious if more far-fetched attempt to mine uranium ore on a sheep farm in Australia also resulted in failure.³¹ Aum, therefore, at least temporarily turned its attention away from nuclear weapons in order to pursue the relatively easier task of developing chemical weapons.³²\n\nThere is substantial evidence that al Qaeda, like Aum Shinrikyo, has attempted to obtain nuclear weapons and their components. According to the federal indictment of Osama bin Laden for his role in the attacks on US embassies in Kenya and Tanzania, these procurement activities date back to at least 1992 (Bunn, 2006). According to Jamal Ahmed al-Fadl, a Sudanese national who testified against bin Laden in 2001, an attempt was made by al Qaeda operatives in 1993 to buy HEU for a bomb (McLoud and Osborne, 2001). This effort, as well as several other attempts to purchase fissile material appear to have been unproductive, and were undermined by the lack of technical knowledge on the part of al Qaeda aides. In fact, al Qaeda may well have fallen victim to various criminal scams involving the sale of low-grade reactor fuel and a bogus nuclear material called ‘Red Mercury’.^(.33)\n\nIf al Qaeda’s early efforts to acquire nuclear weapons were unimpressive, the leadership did not abandon this objective and pursued it with renewed energy once the organization found a new sanctuary in Afghanistan after 1996. Either with the acquiescence or possible assistance of the Taliban, al Qaeda appears to have sought not only fissile material but also nuclear weapons expertise, most notably from the Pakistani nuclear establishment. Bin Laden and his deputy al-Zawahiri, for example, are reported to have met at length with two Pakistani nuclear weapons experts, who also were Taliban sympathizers, and to have sought to elicit information about the manufacture of nuclear weapons (Glasser and Khan, 2001).\n\nAccording to one analyst who has closely followed al Qaeda’s nuclear activities, the Pakistanis may have provided bin Laden with advice about potential suppliers for key nuclear components (Albright and Higgens, 2003, pp. 9–55). In addition, although there is no solid evidence, one cannot rule out he possibility that al Qaeda received the same kind of weapons design blueprints from the A.Q. Khan network that were provided to Libya and also, conceivably, to Iran.\n\nThe Presidential Commission on the Intelligence Capabilities of the United States Regarding WMD reported in March 2005 that in October 2001 the US intelligence community assessed al Qaeda as capable of making at least a ‘crude’ nuclear device if it had access to HEU or plutonium (Commission, 2005, pp. 267, 271, 292). The commission also revealed that the CIA’s non-proliferation intelligence and counterterrorism centres had concluded in November 2001 that al Qaeda ‘probably had access to nuclear expertise and facilities and that there was a real possibility of the group developing a crude nuclear device’ (Commission, 2005, pp. 267, 271, 292). These assessments appear to be based, at least in part, on documents uncovered at al Qaeda safe houses after the US toppled the Taliban regime, which revealed that al Qaeda operatives were studying nuclear explosives and nuclear fuel cycle technology.³⁴\n\nIn addition to compelling evidence about al Qaeda’s efforts to obtain nuclear material and expertise, there have been numerous unsubstantiated reports about the possible acquisition by al Qaeda of intact nuclear weapons from the former Soviet Union. Most of these accounts are variants of the storyline that one or more Soviet-origin ‘suitcase nukes’ were obtained by al Qaeda on the black market in Central Asia.³⁵ Although one cannot dismiss these media reports out of hand due to uncertainties about Soviet weapons accounting practices, the reports provide little basis for corroboration, and most US government analysts remain very skeptical that al Qaeda or any other non-state actor has yet acquired a stolen nuclear weapon on the black market.\n\nDespite the loss of its safe haven in Afghanistan, US government officials continue to express the belief that al Qaeda is in a position to build an improvised nuclear device (Jane’s Intelligence Digest, 2003; Negroponte statement, 2005). And yet even at their heyday, there is no credible evidence that either al Qaeda or Aum Shinrikyo were able to exploit their high motivations, substantial financial resources, demonstrated organizational skills, far-flung network of followers, and relative security in a friendly or tolerant host country to move very far down the path towards acquiring a nuclear weapons capability. As best one can tell from the limited information available in public sources, among the obstacles that proved most difficult for them to overcome was access to the fissile material needed to fabricate an IND. This failure probably was due in part to a combination of factors, including the lack of relevant ‘in house’ technical expertise, unfamiliarity with the nuclear black market and lack of access to potential nuclear suppliers, and greater security and control than was anticipated at sites possessing desired nuclear material and expertise. In the case of the former Soviet Union, the group’s procurement efforts also probably underestimated the loyalty of underpaid nuclear scientists when it came to sharing nuclear secrets. In addition, both Aum Shinrikyo and al Qaeda’s pursuit of nuclear weapons may have suffered from too diffuse an effort in which large but limited organizational resources were spread too thinly over a variety of ambitious tasks.³⁶\n\n19.4.3 What is the probability that terrorists will acquire nuclear explosive capabilities in the future?\n\nPast failures by non-state actors to acquire a nuclear explosive capability may prove unreliable indicators of future outcomes. What then are the key variables that are likely to determine if and when terrorists will succeed in acquiring INDs and/or nuclear weapons, and if they are to acquire them, how many and of what sort might they obtain? The numbers, in particular, are relevant in determining whether nuclear terrorism could occur on a scale and scope that would constitute a global catastrophic threat.\n\nAs discussed earlier, the inability of two very resourceful non-state actors-Aum Shinrikyo and al Qaeda – to acquire a credible nuclear weapons capability cautions against the assumption that terrorists will any time soon be more successful. Although weapons usable nuclear material and nuclear weapons themselves are in great abundance and, in some instances, lack adequate physical protection, material control, and accounting, not withstanding the activities of the A.Q. Khan network, there are few confirmed reports of illicit trafficking of weapons usable material, and no reliable accounts of the diversion or sale of intact nuclear weapons (Potter and Sokova, 2002).\n\nThis apparent positive state of affairs may be an artifact of poor intelligence collection and analysis as well as the operation of sophisticated smugglers able to avoid detection (Potter and Sokova, 2002). It also may reflect the relatively small number of would-be nuclear terrorists, the domination of the nuclear marketplace by amateur thieves and scam artists, and the inclination of most organized criminal organizations to shy away from nuclear commerce when they can make their fortune in realms less likely to provoke harsh governmental intervention. Finally, one cannot discount the role of good luck.\n\nThat being said, a number of respected analysts have proffered rather dire predictions about the looming threat of terrorist initiated nuclear violence. Graham Allison, author of one of the most widely cited works on the subject, offers a standing bet of 51 to 49 odds that, barring radical new antiproliferation steps, there will be a terrorist nuclear strike within the next ten years. Allison gave such odds in August 2004, and former US Secretary of Defense William Perry agreed, assessing the chance of a terror strike as even (Allison, 2004; Kristof, 2004). More recently, in June 2007, Perry and two other former US government officials concluded that the probability of a nuclear weapon going off in an American city had increased in the past 5 years although they did not assign a specific probability or time frame (Perry et al., 2007, p. 8). Two other well-known analysts, Matthew Bunn and David Albright, also are concerned about the danger, but place the likelihood of a terrorist attack involving an IND at 5% and 1%, respectively (Sterngold, 2004).³⁷\n\nIt is difficult to assess these predictions as few provide much information about their underlying assumptions. Nevertheless, one can identify a number of conditions, which if met, would enable non-state actors to have a much better chance of building an IND or seizing or purchasing one or more intact nuclear weapons.\n\nPerhaps the single most important factor that could alter the probability of a successful terrorist nuclear weapons acquisition effort is state-sponsorship or state-complicity. Variants of this pathway to nuclear weapons range from the deliberate transfer of nuclear assets by a national government to unauthorized assistance by national government officials, weapons scientists, or custodians. At one end of the continuum, for example, a terrorist group could conceivably acquire a number of intact operational nuclear weapons directly from a sympathetic government. Such action would make it unnecessary to bypass security systems protecting the weapons. This ‘worst case’ scenario has shaped US foreign policy towards so-called rogue states, and continues to be a concern with respect to countries such as North Korea and Iran.³⁸\n\nEven ifa state’s most senior political leaders were not prepared to transfer a nuclear weapon to a terrorist organization, it is possible that other officials with access to their country’s nuclear assets might take this step for financial or ideological reasons. If President Musharraf and A.Q. Khan are to be believed, the latter’s unauthorized sale of nuclear know-how to a number of states demonstrated the feasibility of such transfer, including the provision of nuclear weapon designs, and, in principle, could as easily have been directed to non-state actors. Aid from one or more insiders lower down the chain of command, such as guards at a nuclear weapons storage or deployment site, also could facilitate the transfer of a nuclear weapon into terrorist hands.\n\nMoreover, one can conceive of a plausible scenario in which terrorist groups might take advantage of a coup, political unrest, a revolution, or a period of anarchy to gain control over one or more nuclear weapons. Nuclear assets could change hands, for example, because of a coup instigated by insurgents allied to or cooperating with terrorists. Although the failed coup attempt against Soviet President Mikhail Gorbachev during August 1991 did not involve terrorists, during the crisis Gorbachev reportedly lost control of the Soviet Union’s nuclear arsenal to his would-be successors when they cut off his communications links (Ferguson and Potter, 2005, p. 59; Pry, 1999, p. 60). It is also possible that during a period of intense political turmoil, nuclear custodians might desert their posts or otherwise be swept aside by the tide of events. During China’s Cultural Revolution of the mid-1960s, for example, leaders of China’s nuclear establishment feared that their institutes and the Lop Nor nuclear test site might be overrun by cadres of Red Guards (Spector, 1987, p. 32). Similarly in 1961, French nuclear authorities appeared to have rushed to test a nuclear bomb at a site in Algeria out of fear that a delay could enable rebel forces to seize the weapon (Spector, 1987, pp. 27–30). Although it is unlikely that political unrest would threaten nuclear controls in most weapons states today, the situation is not clear cut everywhere, and many analysts express particular concerns with respect to Pakistan and North Korea.\n\nAlthough the preceding examples pertain to operational nuclear weapons, similar state-sponsored or state-complicit scenarios apply with equal force to the transfer of weapons usable material and technical know-how. Indeed, the transfers of highly sensitive nuclear technology to Iran, Libya, and North Korea by A.Q. Khan and his associates over an extended period of time (1989-2003), demonstrate the feasibility of similar transfers to non-state actors. As in an attempted seizure of a nuclear weapon, political instability during a coup or revolution could provide ample opportunities for terrorists to gain control of fissile material, stocks of which are much more widely dispersed and less well guarded than nuclear weapons. There is one confirmed case, for example, in which several kilograms of HEU disappeared from the Sukhumi Nuclear Research Center in the breakaway Georgian province of Abkhazia. Although the details of the case remain murky, and there is no evidence that a terrorist organization was involved, the HEU was diverted during a period of civil turmoil in the early 1990s (Potter and Sokova, 2002, p. 113).\n\nAside from the assistance provided by a state sponsor, the most likely means by which a non-state actor is apt to experience a surge in its ability to acquire nuclear explosives is through technological breakthroughs. Today, the two bottlenecks that most constrain non-state entities from fabricating a nuclear weapon are the difficulty of enriching uranium and the technical challenge of correctly designing and building an implosion device.\n\nAlthough almost all experts believe uranium enrichment is beyond the capability of any non-state entity acting on its own today, it is possible that new enrichment technologies, especially involving lasers, may reduce this barrier. Unlike prevailing centrifuge and diffusion enrichment technology, which require massive investments in space, infrastructure, and energy, laser enrichment could theoretically involve smaller facilities, less energy consumption, and a much more rapid enrichment process. These characteristics would make it much easier to conceal enrichment activities and could enable a terrorist organization with appropriate financial resources and technical expertise to enrich sufficient quantities of HEU covertly for the fabrication of multiple INDs. However, despite the promise of laser enrichment, it has proved much more complex and costly to develop than was anticipated (Boureston and Ferguson, 2005).\n\nUnlike a gun-type device, an implosion-type nuclear explosive can employ either HEU or plutonium. However, it also requires more technical sophistication and competence than an HEU-based IND. Although these demands are likely to be a major impediment to would-be nuclear terrorists at present, the barriers may erode over time as relevant technology such as high-speed trigger circuitry and high explosive lenses become more accessible.\n\nWere terrorists to acquire the ability to enrich uranium or manufacture implosion devices using plutonium, it is much more likely that they would be able to produce multiple INDs, which could significantly increase the level of nuclear violence. Neither of these developments, however, are apt to alter significantly the yield of the IND. Indeed, nothing short of a hard to imagine technological breakthrough that would enable a non-state actor to produce an advanced fission weapon or a hydrogen bomb would produce an order of magnitude increase in the explosive yield of a single nuclear device.³⁹\n\n19.4.4 Could terrorists precipitate a nuclear holocaust by non-nuclear means?\n\nThe discussion to this point has focused on the potential for terrorists to inflict nuclear violence. A separate but related issue is the potential for terrorists to instigate the use of nuclear explosives, possibly including a full-fledged nuclear war. Ironically, this scenario involves less demanding technical skills and is probably a more plausible scenario in terms of terrorists approaching the level of a global catastrophic nuclear threat.\n\nOne can conceive of a number of possible means by which terrorists might seek to provoke a nuclear exchange involving current nuclear weapon states. This outcome might be easiest to accomplish in South Asia, given the history of armed conflict between India and Pakistan, uncertainties regarding the command and control arrangements governing nuclear weapons release, and the inclination on the part of the two governments to blame each other whenever any doubt exists about responsibility for terrorist actions. The geographical proximity of the two states works to encourage a ‘use them or lose them’ crisis mentality with nuclear assets, especially in Pakistan which is at a severe military disadvantage vis a vis India in all indices but nuclear weapons. It is conceivable, therefore, that a terrorist organization might inflict large scale but conventional violence in either country in such manner as to suggest the possibility of state complicity in an effort to provoke a nuclear response by the other side.\n\nA number of films and novels rely on cyber terror attacks as the source of potential inter-state nuclear violence.⁴⁰ Although the plot lines vary, they frequently involve an individual or group’s ability to hack into a classified military network, thereby setting in motion false alarms about the launch of foreign nuclear-armed missiles, or perhaps even launching the missiles themselves. It is difficult to assess the extent to which these fictional accounts mirror real-life vulnerabilities in the computer systems providing the eyes and ears of early warning systems for nuclear weapon states, but few experts attach much credence to the ability of non-state actors to penetrate and spoof these extraordinarily vital military systems.\n\nA more credible, if still unlikely means, to mislead an early warning system and trigger a nuclear exchange involves the use by terrorists of scientific rockets. The ‘real world’ model for such a scenario is the 1995 incident in which a legitimate scientific sounding rocket (used to take atmospheric measurements) launched from Norway led the Russian early warning system to conclude initially that Russia was under nuclear attack (Forden, 2001).⁴¹\n\nAlthough terrorists might find it very difficult to anticipate the reaction of the Russian early warning system (or those of other nuclear weapon states) to various kinds of rocket launches, access to and use of scientific rockets is well within the reach of many non-state actors, and the potential for future false alerts to occur is considerable.\n\n19.5 Consequences of nuclear terrorism\n\n19.5.1 Physical and economic consequences\n\nThe physical and health consequences of a nuclear terrorist attack in the foreseeable future, while apt to be devastating, are unlikely to be globally catastrophic. This conclusion is derived, in part, from a review of various government and scholarly calculations involving different nuclear detonation/exchange scenarios which range from a single 20 kiloton IND to multiple megaton weapons.\n\nThe most likely scenario is one in which an IND of less than 20 kilotons is detonated at ground level in a major metropolitan area such as New York. The size of the IND is governed by the amount of HEU available to would-be nuclear terrorists and their technical skills.⁴²\n\nA blast from such an IND would immediately wipe out the area within about a one and a half mile radius of the weapon. Almost all non-reinforced structures within that radius would be destroyed and between 50,000 and 500,000 people would probably die, with a similar number seriously injured.⁴³ The amount of radioactive fallout after such an attack is difficult to estimate because of uncertain atmospheric factors, but one simulation predicts that 1.5 million people would be exposed to fallout in the immediate aftermath of the blast, with 10,000 immediate deaths from radiation poisoning and an eventual 200,000 cancer deaths (Helfandetal., 2002, p. 357). Very soon after the attack, hospital facilities would be overwhelmed, especially with burn victims, and many victims would die because of a lack of treatment. These estimates are derived from computer models, nuclear testing results, and the experiences of Hiroshima and Nagasaki.⁴⁴ A firestorm might engulf the area within one to two miles of the blast, killing those who survived the initial thermal radiation and shockwave.⁴⁵\n\nOf course, no one really knows what would happen if a nuclear weapon went off today in a major metropolis. Most Cold War studies focus on weapons exploding at least 2000 feet above the ground, which result in more devastation and a wider damage radius than ground level detonations. But such blasts, like those at Hiroshima and Nagasaki, also produce less fallout. Government studies of the consequences of nuclear weapons use also typically focus on larger weapons and multiple detonations. A terrorist, however, is unlikely to expend more than a single weapon in one city or have the ability to set off an airburst. Still, the damage inflicted by a single IND would be extreme compared to any past terrorist bombing. By way of contrast, the Oklahoma City truck bomb was approximately 0.001 kiloton, a fraction of the force of an improvised nuclear device.\n\nTNW typically are small in size and yield and would not produce many more casualties than an IND. Although they are most likely to be the intact weapon of choice for a terrorist because of their relative portability, if a larger nuclear weapon were stolen it could increase the number of deaths by a factor of 10. If, for example, a 1-megaton fusion weapon (the highest yield of current US and Russian arsenals) were to hit the tip of Manhattan, total destruction might extend as far as the middle of Central Park, five miles away. One study by the Office of Technology Assessment estimated that a 1-megaton weapon would kill 250,000 people in less-dense Detroit (U.S. Office of Technology Assessment, 1979), and in Manhattan well over a million might die instantly. Again, millions would be exposed to radioactive fallout, and many thousands would die depending on where the wind happened to blow and how quickly people took shelter.\n\nRegardless of the precise scenarios involving an IND or intact nuclear weapon, the physical and health consequences would be devastating. Such an attack would tax health care systems, transportation, and general commerce, perhaps to a breaking point. Economic studies of a single nuclear attack estimate property damage in the many hundreds of billions of dollars (ABT Associates, 2003, p.7), ⁴⁶ and the direct cost could easily exceed one trillion dollars if lost lives are counted in economic terms. For comparison, the economic impact of the September 11 attacks has been estimated at $83 billion in both direct and indirect costs (GAO, 2002).\n\nA nuclear attack, especially a larger one, might totally destroy a major city. The radioactivity of the surrounding area would decrease with time, and outside the totally destroyed area (a mile or two), the city could be repopulated within weeks, but most residents would probably be hesitant to return due to fear of radioactivity.\n\nA more globally catastrophic scenario involves multiple nuclear warheads going off at the same time or in rapid succession. If one attack is possible, multiple attacks also are conceivable as weapons and weapons material are often stored and transported together. Should terrorists have access to multiple INDs or weapons they might target a number of financial centres or trade hubs. Alternatively, they could follow a Cold War attack plan and aim at oil refineries. In either scenario, an attack could lead to severe global economic crisis with unforeseen consequences. Whereas the September 11 attacks only halted financial markets and disrupted transportation for a week, even a limited nuclear attack on lower Manhattan might demolish the New York Stock Exchange and numerous financial headquarters. Combined with attacks on Washington, DC, Paris, London, and Moscow, major centres of government and finance could nearly disappear. The physical effects of the attacks would be similar in each city, though many cities are less densely packed than New York and would experience fewer immediate deaths.\n\n19.5.2 Psychological, social, and political consequences\n\nUnlike the more tangible physical and economic effects of nuclear terrorism, it is almost impossible to model the possible psychological, social, and political consequences of nuclear terrorism, especially in the long term and following multiple incidents. One is therefore forced to rely on proxy data from the effects of previous cases of large-scale terrorism, a variety of natural disasters, and past nuclear accidents such as the Chernobyl meltdown. The psychological, social, and political effects of nuclear terrorism are likely to extend far beyond the areas affected by blast or radiation, although many of these effects are likely to be more severe closer to ground zero.\n\nIt can be expected that the initial event will induce a number of psychological symptoms in victims, responders, and onlookers. In an age of instantaneous global communication, the last category might rapidly encompass most of the planet. The constellation of possible symptoms might include anxiety, grief, helplessness, initial denial, anger, confusion, impaired memory, sleep disturbance and withdrawal (Alexander and Klein, 2006). Based on past experience with terrorism and natural disasters, these symptoms will resolve naturally in the majority of people, with only a fraction⁴⁷ going on to develop persistent psychiatric illness such as post-traumatic stress disorder. However, the intangible, potentially irreversible, contaminating, invasive and doubt-provoking nature of radiation brings with it a singular aura of dread and high levels of stress and anxiety. Indeed, this fear factor is one of the key reasons why some terrorists might select weapons emitting radiation.\n\nIn addition to significant physical casualties, a nuclear terrorism event would most likely result in substantially greater numbers of unexposed individuals seeking treatment, thereby complicating medical responses.⁴⁸ In the 1987 radiological incident in Goiania, Brazil, up to 140,000 unexposed people flooded the health care system seeking treatment (Department of Homeland Security, 2003, p. 26). Although genuine panic – in the sense of maladaptive responses such as ‘freezing’ – is extremely rare (Jones, 1995), a nuclear terrorism incident might provoke a mass exodus from cities as individuals make subjective decisions to minimize their anxiety. Following the Three Mile Island nuclear accident in the United States in 1979, 150,000 people took to the highways – 45 people evacuated for every person advised to leave (Becker, 2003).\n\nWere nuclear terrorism to become a repeating occurrence, the question would arise regarding whether people would eventually be able to habituate to such events, much as the Israeli public currently manages to maintain a functional society despite continual terrorist attacks. While desensitization to extremely high levels of violence is possible, multiple cases of nuclear terrorism over an extended period of time might prove to be beyond the threshold of human tolerance.\n\nEven a single incidence of nuclear terrorism could augur negative social changes. While greater social cohesion is likely in the immediate aftermath of an attack (Department of Homeland Security, 2003, p. 38), over time feelings of fear, anger and frustration could lead to widespread anti-social behaviour, including the stigmatization of those exposed to radiation and the scapegoating of population groups associated with the perceived perpetrators of the attack. This reaction could reach the level of large-scale xenophobia and vigilantism. Repeated attacks on major cities might even lead to behaviours encouraging social reversion and the general deterioriation of civil society, for example, ifmany people adopt a survivalist attitude and abandon populated areas. There is, of course, also the possibility that higher mortality salience might lead to positive social effects, including more constructive approaches to problem-solving (Calhoun and Tedeschi, 1998). Yet higher morality could just as easily lead to more pathological behaviours. For instance, during outbreaks of the Black Death plague in the Middle Ages, some groups lost all hope and descended into a self-destructive epicureanism.\n\nA nuclear terrorist attack, or series of such attacks, would almost certainly alter the fabric of politics (Becker, 2003). The use of a nuclear weapon might trigger a backlash against current political or scientific establishments for creating and failing to prevent the threat. Such attacks might paralyse an open or free society by causing the government to adopt draconian methods (Stern, 1999, pp. 2–3), or massively restrict movement and trade until all nuclear material can be accounted for, an effort that would take years and which could never be totally complete. The concomitant loss of faith in governing authorities might eventually culminate in the fulfillment of John Herz’s initial vision of the atomic age, resulting in the demise of the nation-state as we know it (1957).\n\nWhile the aforementioned effects could occur in a number of countries, especially if multiple states were targeted by nuclear-armed terrorists, nuclear terrorism could also impact the overall conduct of international relations. One issue that may arise is whether the terrorists responsible, as non-state actors, would have the power to coerce or deter nation-states.⁴⁹ Nuclear detonation by a non-state group virtually anywhere would terrorize citizens in potential target countries around the globe, who would fear that the perpetrators had additional weapons at their disposal. The organization could exploit such fears in order to blackmail governments into political concessions – for example, it could demand the withdrawal of military forces or political support from states the terrorists opposed. The group might even achieve these results without a nuclear detonation, by providing proof that it had a nuclear weapon in its possession at a location unknown to its adversaries. The addition on the world stage of powerful non-state actors as the ostensible military equals of (at least some) states would herald the most significant change in international affairs since the advent of the Westphalian system. While one can only speculate about the nature of the resultant international system, one possibility is that ‘superproliferation’ would occur. In this case every state (and non-state) actor with the wherewithal pursues nuclear weapons, resulting in an extremely multipolar and unstable world order and greater possibility for violent conflict. On the other hand, a nuclear terrorist attack might finally create the international will to control or eliminate nuclear weaponry.\n\nIt is almost impossible to predict the direction, duration, or extent of the above-mentioned changes since they depend on a complex set of variables. However, it is certainly plausible that a global campaign of nuclear terrorism would have serious and harmful consequences, not only for those directly affected by the attack, but for humanity as a whole.\n\n19.6 Risk assessment and risk reduction\n\n19.6.1 The risk of global catastrophe\n\nWhile any predictions under conditions of dynamic change are inherently complicated by the forces described earlier, the exploration of the motivations, capabilities and consequences associated with nuclear terrorism permit a preliminary estimate of the current and future overall risk. Annotated estimates of the risk posed by nuclear terrorism are given in Tables 19.1 to 19.3 under three different scenarios.\n\nThe tables represent a static analysis of the risk posed by nuclear terrorism. When one considers dynamics, it is more difficult to determine which effects will prevail. On the one hand, a successful scenario of attack as in Table 19.2 or Table 19.3 might have precedent-setting and learning effects, establishing proof-of-concept and spurring more terrorists to follow suit, which would increase the overall risk. Similarly, the use of nuclear weapons by some states (such as the United States or Israel against a Muslim country) might redouble the efforts of some terrorists to acquire and use these weapons or significantly increase the readiness of some state actors to provide assistance to terrorists. Moreover, one must consider the possibility of discontinuous adoption practices. This is rooted in the idea that certain technologies (perhaps including nuclear weapons) possess inherently ‘disruptive’ traits and that the transition to the use of such technologies need not be incremental, but could be rapid, wholesale and permanent once a tipping point is reached in the technology’s maturization process.⁵⁰\n\nOn the other hand, the acquisition of a nuclear weapons capability by states with which a terrorist group feels an ideological affinity might partially satiate their perceived need for nuclear weapons. At the same time, the advent of improved detection and/or remediation technologies might make terrorists less inclined to expend the effort of acquiring nuclear weapons. As a counterargument to the above assertions regarding the adoption of new weapons technologies, it is possible that following the initial use by terrorists of a nuclear weapon, the international community may act swiftly to stem the availability of nuclear materials and initiate a severe crackdown on any terrorist group suspected of interest in causing mass casualties.\n\nTable 19.1 Most Extreme Overall Scenario: Terrorists Precipitate a Full-scale Interstate Nuclear War (Either by Spoof ing, Hacking, or Conducting a False Flag Operation)\n\n[Image]\n\nTable 19.2 Most Extreme ‘Terrorist Weapon Only’ Scenario: Multiple Attacks With Megaton-Scale Weapons\n\n[Image]\n\nTable 19.3 Most Likely Scenario: A Single to a Few <50 kilotons Detonations\n\n[Image]\n\nScholars have asserted that acquisition of nuclear weapons is the most difficult form of violence for a terrorist to achieve, even when compared with other unconventional weapons (Gurr and Cole, 2002, p. 56). Indeed, this is one of the main reasons for the relatively low overall risk of global devastation from nuclear terrorism in the near term. However, the risk is not completely negligible and in the absence of intervention, could grow significantly in the future, eventually acquiring the potential to cause a global catastrophe.\n\n19.6.2 Risk reduction\n\nAmong the most thoughtful analyses of how to accomplish this objective are a series of five volumes co-authored by a team of researchers at Harvard University and sponsored by the Nuclear Threat Initiative, a US foundation (Bunn, 2006; Bunn and Wier, 2005a, 2005b; Bunn et al., 2002, 2004). Since 2002, these annual monographs have tracked progress (and on occasion regression) in controlling nuclear warheads, material, and expertise, and have provided many creative ideas for reducing the supply side opportunities for would-be nuclear terrorists. A number of their insights, including proposals for accelerating the ‘global cleanout’ of HEU and building a nuclear security culture, inform the recommendations that follow in the next section below.\n\nSeveral recent studies have called attention to the manner in which gaps in the international non-proliferation regime have impeded efforts to curtail nuclear terrorism (Ferguson and Potter, 2005; Perkovich et al., 2004). They note, for example, the state-centric orientation of the Treaty on the Non-proliferation of Nuclear Weapons (NPT) – the most widely subscribed to treaty in the world – and its failure to address the relatively new and very different dangers posed by non-state actors. They also call attention to the importance of developing new international mechanisms that provide a legal basis for implementing effective controls on the safeguarding and export of sensitive nuclear materials, technology, and technical know-how.⁵¹ In addition, they point to the urgency of developing far more comprehensive and coordinated global responses to nuclear terrorism threats. Both the potential and limitations of current initiatives in this regard are illustrated by the G-8 Global Partnership, which in 2002 set the ambitious target of 20 billion dollars to be committed over a 10-year period for the purpose of preventing terrorists from acquiring weapons and materials of mass destruction (Einhorn and Flourney, 2003). Although this initiative, like most others in the realm of combating nuclear terrorism, can point to some successes, it has been stronger on rhetoric than on sustained action. This phenomenon has led some observers to note that the ‘most fundamental missing ingredient of the U.S. and global response to the nuclear terrorism threat to date is sustained high-level leadership’ (Bunn and Wier, 2006, p. viii).\n\nAlthough there is no consensus among analysts about how best to intervene to reduce the risks of nuclear terrorism, most experts share the view that a great deal can be done to reduce at least the supply side opportunities for would-be nuclear terrorists. At the core of this optimism is the recognition that the problem ultimately is an issue of physics. As one leading exponent of this philosophy puts its, the logic is simple, ‘No fissile material, no nuclear explosion, no nuclear terrorism’ (Allison, 2004, p.140).\n\n19.7 Recommendations\n\nMajor new initiatives to combat the nuclear proliferation and terrorism threats posed by non-state actors have been launched by national governments and international organizations, and considerable sums of financial and political capital have been committed to new and continuing programmes to enhance nuclear security. These initiatives include the adoption in April 2004 of UN Security Council Resolution 1540, the U.S. Department of Energy’s May 2004 Global Threat Reduction Initiative, the expanded G-8 Global Partnership, the Proliferation Security Initiative, and the 2006 Global Initiative to Combat Nuclear Terrorism. Although these and other efforts are worthy of support, it is not obvious that they reflect a clear ordering of priorities or are being implemented with a sense of urgency. In order to correct this situation it is imperative to pursue a multi-track approach, the core elements of which should be to enhance the security of nuclear weapons and fissile material globally, consolidate nuclear weapons and fissile material stockpiles, reduce their size, and move towards their elimination, while at the same time working to reduce the number of terrorists seeking these weapons.\n\n19.7.1 Immediate priorities\n\nThe following initiatives, most of which focus on the supply side of the nuclear terrorism problem, should be given priority: (1) minimize HEU use globally, (2) implement UN Security Council Resolution 1540, (3) promote adoption of stringent, global nuclear security standards, (4) secure vulnerable Russian TNW, and (5) accelerate international counterterrorism efforts to identify and interdict would-be nuclear terrorists.\n\n1. Minimize HEU Use Globally. Significant quantities of fissile materials exist globally which are not needed, are not in use, and, in many instances, are not subject to adequate safeguards. From the standpoint of nuclear terrorism, the risk is most pronounced with respect to stockpiles of HEU in dozens of countries. It is vital to secure, consolidate, reduce, and, when possible, eliminate these HEU stocks. The principle should be one in which fewer countries retain HEU, fewer facilities within countries posses HEU, and fewer locations within those facilities have HEU present. Important components of a policy guided by this principle include, rapid repatriation of all US- and Soviet/Russian-origin HEU (both fresh and irradiated), international legal prohibitions of exports of HEU-fuelled research and power reactors, and down-blending of existing stocks of HEU to low-enriched uranium (LEU). Use of spallation sources also can contribute to this HEU minimization process. Particular attention should be given to de-legitimizing the use of HEU in the civilian nuclear sector – a realistic objective given the relatively few commercial applications of HEU and the feasibility of substituting LEU for HEU in most, if not all, of these uses (Potter, 2006).\n\n2. Implement UN Security Council Resolution 1540. One of the most important new tools to combat nuclear terrorism is UN Security Council Resolution 1540 (http://www.un.org/News/Press/docs/2004/sc8076.doc.htm). Adopted in April 2004, this legally binding measure on all UN members prohibits states from providing any form of support to non-state actors attempting to acquire or use nuclear, chemical, or biological weapons and their means of delivery. It also requires states to adopt and enforce ‘appropriate effective measures’ to account for and secure such items, including fissile material, and to establish and maintain effective national export and trans-shipment controls over these commodities. This United Nations mandate provides an unusual opportunity for those states most concerned about nuclear security to develop the elements of a global nuclear security standard, to assess the specific needs of individual states in meeting this standard, and to render necessary assistance (Bunn and Wier, 2005, p. 109). This urgent task is complicated by the fact that many states are not persuaded about the dangers of nuclear terrorism or doubt if the risk applies to them. Therefore, in order for the potential of 1540 to be realized, it is necessary for all states to recognize that a nuclear terror act anywhere has major ramifications everywhere. As discussed below, a major educational effort will be required to counter complacency and the perception that nuclear terrorism is someone else’s problem. In the meantime, it is imperative that those states already convinced of the danger and with stringent nuclear security measures in place assist other countries to meet their 1540 obligations.\n\n3. Promote Adoption of Stringent, Global Security Standards. Renewed efforts are required to establish binding international standards for the physical protection of fissile material. An important means to accomplish that objective is to ratify the recent amendment to the Convention on the Physical Protection of Nuclear Material to make it applicable to civilian nuclear material in domestic storage, use, and transport. Ideally, the amendment would oblige parties to provide protection comparable to that recommended in INFCIRC 225/Rev 4 and to report to the IAEA on the adoption of measures to bring national obligations into conformity with the amendment. However, because ratifying the amended Convention is likely to require a long time, as many like-minded states as possible should agree immediately to meet a stringent material protection standard, which should apply to all civilian and military HEU.\n\n4. Secure and Reduce TNW. Preventing non-state actors from gaining access to intact nuclear weapons is essential in combating nuclear terrorism. Priority should be given to safeguarding and reducing TNW, the category of nuclear arms most vulnerable to theft. Although it would be desirable to initiate negotiations on a legally binding and verifiable treaty to secure and reduce such arms, this approach does not appear to have much prospect of success, at least in the foreseeable future. As a consequence, one should concentrate on encouraging Russia to implement its pledges under the 1991–1992 Presidential Nuclear Initiatives, including the removal to central storage of all but one category of TNW. Ideally, all TNW should be stored at exceptionally secure facilities far from populated regions. In parallel, the United States should declare its intention to return to US territory the small number of air-launched TNW currently deployed in Europe. Although probably less vulnerable to terrorist seizure than TNW forward deployed in Russia, there no longer is a military justification for their presence in Europe. The US action, while valuable in its own right, might be linked to Russian agreement to move its tactical nuclear arms to more secure locations.\n\n5. Accelerate International Counterterrorism Efforts to Pre-emptively Identify and Interdict Nuclear Terrorists. On the demand side, one can identify several recommendations for reducing the threat of nuclear terrorism that are generally less specific than those on the supply side. The most effective measures in the near term involve improved law enforcement and intelligence. As only a small proportion of non-state actors is likely to possess both the motivation and capability necessary for high consequence nuclear terrorism, it should be at least possible to identify potential nuclear perpetrators in advance and concentrate counterterrorism efforts – including surveillance and prosecution – against these groups and individuals.⁵² However, counterterrorism agencies have traditionally proven to be less proficient at terrorist threat preemption than response after an attack. Given the likely horrific consequences of nuclear terrorism, it is crucial to invest more resources wisely in apprehending terrorists known to harbour nuclear ambitions and to be more vigilant and savvy in anticipating the emergence of new and evolving non-state actors who may be predisposed to seek a nuclear terrorist option. Successful efforts in this regard will require much greater international collaboration in intelligence sharing, law enforcement, and prosecution – developments more likely to occur if global perceptions of nuclear terrorism threats converge.\n\n19.7.2 Long-term priorities\n\nImplementation of the aforementioned short-term measures should reduce significantly the risks of nuclear terrorism. However, the threat will remain unless certain underlying factors are addressed. On the demand side, the most basic long-term strategy is to decrease the absolute number of terrorists (and hence the number of would-be nuclear terrorists). While the root causes of terrorism are beyond the scope of this essay, it should be noted that terrorist grievances stem from a complex and poorly understood interplay of social, political and psychological factors, some of which can be assuaged by policy. The ideological make-up of potential nuclear terrorists, however, reduces their susceptibility to such measures as political concessions or improved socioeconomic conditions, which may take decades to implement in any case.\n\nAnother way to reduce motivations for nuclear terrorism is to remove at least part of the subjective benefit that terrorists might derive from conducting acts of nuclear violence. Useful steps include strengthening normative taboos against the use of nuclear weapons, vilifying terrorists who have attempted to obtain nuclear weapons, and increasing public education programmes in order to psychologically immunize the public against some irrational fears related to radiation. Implementation of these measures might help to dissuade some terrorists that the strategic benefits of nuclear violence outweigh the costs.\n\nMaintaining high standards of nuclear forensics and attribution, coupled with strict warnings to states that they will be held responsible for any terrorism involving fissile material of their national origin also may be useful. In addition to providing states with greater incentive to increase the protection of fissile material under their control, these steps could provide some measure of deterrence against state complicity in terrorist acts.\n\nOn the supply side, vigorous implementation of the previously noted priority measures should significantly reduce the risk of nuclear terrorism. However, these dangers will not be completely eliminated as long as countries attach value to and retain nuclear weapons and weapons-usable material. Although it is unrealistic to assume that prevailing national views regarding nuclear weapons will change anytime soon, it nevertheless is important to initiate steps today to change mindsets and forge norms consistent with formal national obligations to nuclear disarmament and non-proliferation. An important but underutilized tool for this purpose is education.\n\nAlthough few national governments or international organizations have invested significantly in this sphere, there is growing recognition among states of the need to rectify this situation. This positive development is reflected in the broad support for recommendations of a UN study on disarmament and non-proliferation education and in related initiatives within the NPT review process (Potter, 2001; Toki and Potter, 2005). Among specific steps states should take to utilize education and training to combat complacency and reduce the proliferation risks posed by non-state actors are:\n\n• Develop educational materials that illustrate the urgency of the proliferation threats posed by non-state actors and their potential impact on all states.\n\n• Cooperate with regional and international organizations to provide training courses on best practices related to nuclear materials and security and non-proliferation exports controls for government officials and law enforcement officers.\n\n• Adopt national non-proliferation education legislation to support graduate training in the field – the best guarantee that states and international organizations will have an adequate pool of knowledgeable nuclear proliferation and counter-terrorism intelligence analysts.\n\nPerhaps the greatest promise of non-proliferation education in the long term is the potential growth of a global network of non-proliferation experts and practitioners who increasingly share common norms and promote their countries’ adherence to non-proliferation and anti-terrorism treaties and agreements. This desirable and necessary state of affairs is most likely to be realized if many more bright young individuals enter the nuclear non-proliferation field and, by the strength of their idealism and energy, prod national governments to abandon old ways of doing things and adjust their non-proliferation strategies and tactics to take account of new realities involving non-state actors and nuclear weapons.\n\n19.8 Conclusion\n\nIt is difficult to find many reasons for optimism regarding the threat of high consequence nuclear terrorism. It is a growing danger and one that could result in enormously devastating and enduring local, regional, national, and even global consequences. One, therefore, should not take great solace in the conclusion that nuclear terrorism is unlikely to pose an existential, end-of-the world threat. It can still cause sufficient perturbation to severely disrupt economic and cultural life and adversely affect the nature of human civilization.\n\nGiven the rising potential for terrorists to inflict nuclear violence, what then accounts for the failure on the part of the most powerful nations on earth to take corrective action commensurate with the threat? Is it a lack of political leadership, a failure of imagination, faulty conceptualization, domestic politics, bureaucratic inertia, competing national security objectives, wishful thinking, the intractable nature of the problem, or simply incompetence?\n\nAll of these factors contribute to the current predicament, but some are more amenable to correction than others. Perhaps the most fundamental shortcoming, and one that can be remedied, is the failure by government and academic analysts alike to distinguish clearly between the proliferation risks posed by state and non-state actors, and to devise and employ tools that are appropriate for combating these very different threats. The challenge is an urgent but manageable one, affording the world a reasonable second chance.\n\nAcknowledgements\n\nThe authors are grateful to Michael Miller for his excellent research assistance. His contribution to the section on ‘The Consequences of Nuclear Terrorism’ was especially valuable. The authors also wish to thank Erika Hunsicker for her editorial assistance.\n\nSuggestions for further reading\n\nFerguson, C.D. and Potter, W.C. (2005). The Four Faces of Nuclear Terrorism (New York: Routledge).\n\nLevi, M. (2007). On Nuclear Terrorism. (Cambridge, MA. Harvard University Press).\n\nZimmerman, P.D. and Lewis, J.G. (2006). The bomb in the backyard. Foreign Policy (November/December 2006), 33–40.\n\nReferences\n\nABT Associates. (2003). The Economic Impact of Nuclear Terrorist Attacks on Freight Transport Systems in an Age of Seaport Vulnerability. Executive summary. Accessed 15 September 2006 http://www.abtassociates.com/reports/ES-Economic\\_Impact\\_of \\_Nuclear\\_Terrorist\\_Attacks.pdf\n\nAckerman, G. and Bale, J.M. (2004). How Serious is the ‘WMD Terrorism’ Threat?: Terrorist Motivations and Capabilities for Using Chemical, Biological, Radiological, and Nuclear (CBRN) Weapons. Report prepared for Los Alamos National Laboratory.\n\nAlbright, D. and Higgens, H. (March/April 2003). A bomb for the Ummah. Bulletin of the Atomic Scientists, 49–55.\n\nAlexander, D.A. and Klein, S. (2006). The challenge of preparation for a chemical, biological, radiological or nuclear terrorist attack. Journal of Postgraduate Medicine, 52, 126–131.\n\nAllison, G. (1996). Avoiding Nuclear Anarchy: Containing the Threat of Loose Russian Nuclear Weapons and Fissile Material (Cambridge, MA: MIT Press).\n\nAllison, G. (2004). Nuclear Terrorism: The Ultimate Preventable Catastrophe (New York: Henry Holt).\n\nAlvarez, L.W. (1988). Adventures of a Physicist (New York: Basic Books).\n\nArbman, G., Calogero, F., Cotta-Ramusino, P., van Dessen, L., Martellini, M., Maerli, M.B., Nikitin, A., Prawitz, J., and Wredberg, L. (2004). Eliminating Stockpiles of Highly-Enriched Uranium. Report submitted to the Swedish Ministry for Foreign Affairs, SKI Report 2004.\n\nAsal, V. and Ackerman, G. (2006). Size Matters: Terrorist Organizational Factors and the Pursuit and Use of CBRN Terrorism. Submitted for publication.\n\nBale, J. (2005). Islamism. In Pilch, R.F. and Zilinskas, R. (eds.), Encyclopedia of Bioterrorism Defense (New York: Wiley).\n\nBandura, A. (1998). Mechanisms of moral disengagement. In Reich, W. (ed.), Origins of Terrorism: Psychologies, Ideologies, Theologies, States of Mind, pp. 161–191 (Washington, DC: Woodrow Wilson Center).\n\nBarnaby, F. (1996). Issues Surrounding Crude Nuclear Explosives in Crude Nuclear Weapons: Proliferation and the Terrorist Threat, IPPNW Global Health Watch Report Number 1.\n\nBecker, S.M. (2003). Psychosocial Issues in Radiological Terrorism and Response: NCRP 138 and After. Presented at the International Workshop on Radiological Sciences and Applications: Issues and Challenges of Weapons of Mass Destruction Proliferation. Albuquerque, New Mexico. 21 April 2003.\n\nBird, K. and Sherwin, M.J. (2005). American Prometheus: The Triumph and Tragedy of J. Robert Oppenheimer (New York: Alfred A Knopf).\n\nBostrom, N. (2002). Existential risks: analyzing human extinction scenarios and related hazards. Journal of Evolution and Technology, 9. http://www.nickbostrom.com/existential/risks.html\n\nBoureston, J. and Ferguson, C.D. (March/April 2005). Laser enrichment: separation anxiety. Bulletin of the Atomic Scientists, 14–18.\n\nBoutwell, J., Calegero, F., and Harris, J. (2002). Nuclear Terrorism: The Danger of Highly Enriched Uranium (HEU). Pugwash Issue Brief.\n\nBower, J.L. and Christensen, C.M. (1995). Disruptive technologies: catching the wave. Harvard Business Review, 73, 43–53.\n\nBunn, M. and Wier, A. (2005a). Securing the Bomb 2005: The New Global Imperative. (Cambridge, MA: Project on Managing the Atom, Harvard University).\n\nBunn, M. and Wier, A. (April 2005b). The seven myths of nuclear terrorism. Current History.\n\nBunn, M. (2006). The Demand for Black Market Fissile Material. NTI Web site, accessed at [http://www.nti.org/e\\_research/cnwm/threat/demand.asp?print=true] on August 20, 2006.\n\nBunn, M., Holdren, J.P., and Wier, A. (2002). Securing Nuclear Weapons and Materials: Seven Steps for Immediate Action (Cambridge, MA: Project on Managing the Atom, Harvard University).\n\nBunn, M., Holdren, J.P., and Wier, A. (2003). Controlling Nuclear Warheads and Materials: A Report Card and Action Plan (Cambridge, MA: Project on Managing the Atom, Harvard University).\n\nBunn, M., Holdren, J.P., and Wier, A. (2004). Securingthe Bomb: An Agenda for Action (Cambridge, MA: Project on Managing the Atom, Harvard University).\n\nCalhoun, L.G. and Tedeschi, R.G. (1998). Posttraumatic growth: future directions. In Tedeschi, R.G., Park, C.L., and Calhoun, L.G. (eds.), Posttraumatic Growth: Positive Changes in the Aftermath of Crisis, pp. 215–238 (Mahwah, NJ: Lawrence Earlbaum Associates).\n\nCameron, G. (1999). Nuclear Terrorism: A Threat Assessment for the 21 st Century (New York: St. Martin’s Press, Inc.).\n\nCameron, G. (2000). WMD terrorism in the United States: the threat and possible The Nonproliferation Review, 7(1), 169–170.\n\nCampbell K.M. et al. (1991). Soviet Nuclear Fission: Control of the Nuclear Arsenal in a Disintegrating Soviet Union (Cambridge, MA: MIT Press).\n\nCampbell, J.K. (2000). On not understanding the problem. In Roberts, B. (ed.), Hype or Reality?: The ‘New Terrorism’ and Mass Casualty Attacks (Alexandria, VA: Chemical and Biological Arms Control Institute).\n\nCarus, W.S. (2000). R.I.S.E. (1972). In Tucker 2000: 55–70.\n\nCiviak, R.L. (May 2002). Closing the Gaps: Securing High Enriched Uranium in the Former Soviet Union and Eastern Europe. Report for the Federation of American Scientists.\n\nClutterbuck, R. (1993). Trends in terrorist weaponry. Terrorism Political Violence, 5, 130.\n\nCommission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction. (2005). Report to the President (Washington, DC: WMD Commission).\n\nConnor, T. (22 March 2004). Al Qaeda: we bought nuke cases. New York Daily News.\n\nCordesman, A.H. (2001). Defending America: Asymmetric and Terrorist Attacks with Radiological and Nuclear Weapons, Center for Strategic and International Studies.\n\nDaly, S., Parachini, J., and Rosenau, W. (2005). Aum Shinrikyo, Al Qaeda, and the Kinshasa Reactor: Implications of Three Studies for Combatting Nuclear Terrorism. Document Briefing (Santa Monica, CA: RAND).\n\nDepartment of Homeland Security (2003). ‘Radiological Countermeasures.’ Prepared by the Department of Homeland Security, Working Group on Radiological Dispersion Device Preparedness, May 1, 2003, www.va.gov/emshq/docs/Radiologic Medical Countermeasures 051403.pdf.\n\nDolnik, A. (2004). All God’s poisons: re-evaluating the threat of religious terrorism with respect to non-conventional weapons. In Howard, R.D. and Sawyer, R.L. (eds.), Terrorism and Counterterrorism: Understanding the New Security Environment (Guilford, CT: McGraw-Hill).\n\nEden, L. (2004). Whole World on Fire (Ithaca, NY: Cornell University Press).\n\nEinhorn, R. and Flourney, M. (2003). Protecting Against the Spread of Nuclear, Biological, and Chemical Weapons: An Action Agenda for the Global Partnership. Center for Strategic and International Studies.\n\nFalkenrath, R.A. (1998). Confronting nuclear, biological and chemical terrorism. Survival, 40, 3.\n\nFalkenrath, R.A., Newman, R.D., and Thayer, B.A. (1998). America’s Achilles’ Heel: Nuclear, Biological, and Chemical Terrorism and Covert Attack (Cambridge, MA: MIT Press).\n\nFerguson, C.D. and Potter, W.C. (2005). The Four Faces of Nuclear Terrorism (New York: Routledge).\n\nForden, G. (3 May 2001). Reducing a common danger: improving Russia’s early-warning system. Policy Analysis, 1–20.\n\nFoxell, J.W. (1999). The debate on the potential for mass-casualty terrorism: the challenge to US security. Terrorism and Political Violence, 11, 1.\n\nGarwin, R.L. and Charpak G. (2001). Megawatts and Megatons: A Turning Point in the Nuclear Age? (New York: Alfred A. Knopf).\n\nGeneral Accounting Office. (2004). DOE Needs to take Action to Further Reduce the Use of Weapons-Usable Uranium in Civilian Research Reactors. GAO-04-807.\n\nGilinsky, V. (2004). Israel’s bomb. Letter to the Editor, The New York Review of Books, 51 /8. http://www.nybooks.com/articles/17104\n\nGlasser, S. and Khan, K. (24 November 2001). Pakistan continues probe of nuclear scientists. Washington Post, p.A13.\n\nGlasstone, S. and Dolan, P.J. (eds.) (1977). The Effects of Nuclear Weapons. U.S. Department Defense and Department of Energy and their own simulations.\n\nGovernment Accountability Office. (2002). Impact of Terrorist Attacks on the World Trade Center. Report GAO-02-700R.\n\nGressang, D.S., IV (2001). Audience and message: assessing terrorist WMD potential. Terrorism and Political Violence, 13(3), 83–106.\n\nGurr, N. and Cole, B. (2002). The New Face of Terrorism: Threats from Weapons of Mass Destruction (London: I. B. Tauris).\n\nHelfand, I., Forrow, L., and Tiwari, J. (2002). Nuclear terrorism. British Medicine Journal, 324, 357.\n\nHerz, J. (1957). The rise and demise of the territorial state. World Politics, 9, 473–493.\n\nHoffman, B. (1993a). ’Holy Terror’: The Implications of Terrorism Motivated by a Religious Imperative (Santa Monica: RAND).\n\nHoffman, B. (1993b). Terrorist targeting: tactics, trends, and potentialities. Terrorism and Political Violence, 5, 12–29.\n\nHof fman, B. (1997). Terrorism and WMD: some preliminary hypotheses. The Nonproliferation Review, 4(3), 45–50.\n\nHoffman, B. (1998). Inside Terrorism (New York: Columbia University).\n\nIngram, T.H. (December 1972). Nuclear hijacking: now within the grasp of any bright lunatic. Washington Monthly, pp. 20–28.\n\nJackson, B.A. (2001). Technology acquisition by terrorist groups: threat assessment informed by lessons from private sector technology adoption. Studies in Conflict and Terrorism, 24, 3.\n\nJane’s Intelligence Digest. (3 July 2003). Al-Qaeda and the Bomb.\n\nJenkins B. (1977). International terrorism: a new mode of conflict. In Carlton, D. and Schaerf, C. (eds.), International Terrorism and World Security (London: Croom Helm.\n\nJenkins, B. (1986). Defense against terrorism. Political Science Quarterly, 101, 777.\n\nJones, F.D. (1995). Neuropsychiatric casualties of nuclear, biological, and chemical warfare. In Textbook of Military Medicine: War Psychiatry. Department of the Army, Office of The Surgeon General, Borden Institute.\n\nJones, S. (2006). Resolution 1540: universalizing export control standards? Arms Control Today. Available at http://www.armscontrol.org/act/2006\\_05/1540.asp\n\nKaplan, D.E. (2000). Aum Shinrikyo (1995). In Tucker, J. (ed.), Toxic Terror: Assessing Terrorist Use of Chemical and Biological Weapons, pp. 207–226 (Cambridge, MA: MIT Press).\n\nKristof, N.D. (11 August 2004). An American Hiroshima. New York Times.\n\nLapp, R.E. (4 February 1973). The ultimate blackmail. The New York Times Magazine.\n\nLeader, S. (June 1999). Osama Bin Laden and the terrorist search for WMD. Jane’s Intelligence Review.\n\nLevanthal, P. and Alexander, Y. (1987). Preventing Nuclear Terrorism (Lexington, MA: Lexington Books).\n\nLifton, R.J. (1999). Destroying the World to Save It: Aum Shinrikyo, Apocalyptic Violence, and the New Global Terrorism (New York: Metropolitan Books).\n\nLugar, R.G. (2005). The Lugar Survey on Proliferation Threats and Responses (Washington, DC: U.S. Senate).\n\nMacdonald, A. [pseudonym for Pierce] (1999). The Turner Diaries: A Novel. Hillsboro, W.V.: National Vanguard; originally published 1980.\n\nMaerli, M.B. (Summer, 2000). Relearning the ABCs: terrorists and ‘weapons of mass destruction’. The Nonproliferation Review.\n\nMaerli, M.B. (2004). Crude Nukes on the Loose? Preventing Nuclear Terrorism by Means of Optimum Nuclear Husbandry, Transparency, and Non-Intrusive Fissile Material Verification. Dissertation, University of Oslo.\n\nMark, J.C., Taylor, T., Eyster, E., Maraman, W., and Wechsler, J. (1987). Can Terrorists Build Nuclear Weapons? in Leventhal and Alexander.\n\nMarlo, F.H. (Autumn 1999). WMD terrorism and US intelligence collection. Terrorism and Political Violence, 11, 3.\n\nMcCormick, G.H. (2003). Terrorist decision making. Annual Reviews in Political Science, 6, 479–480.\n\nMcLoud, K. and Osborne, M. (2001). WMD Terrorism and Usama bin Laden. (Monterey, CA: Center for Nonproliferation Studies) Available at [http://cns.miis.edu/pubs/reports/binladen.htm.]\n\nMcPhee, J. (1974). The Curve of Binding Energy (New York: Farrar, Straus, and Giroux).\n\nMir, H. (10 November 2001). Osama Claims He Has Nukes: If US Uses N-Arms It Will Get Same Response. Dawn Internet Edition. Karachi, Pakistan.\n\nNational Research Council. (2002). Committee on Science and Technology for Countering Terrorism. Making the Nation Safer: The Role of Science and Technology in Countering Terrorism (Washington, DC: National Academy Press).\n\nNegroponte, J. (2005). Annual Threat Assessment of the Director of National Intelligence. Statement to the Senate Armed Services Committee. Accessed at [http://armed-services.senate.gov/statemnt/2006/February/Negroponte%2002-28-06.pdf]\n\nNorth, C. and Pfefferbaum, B. (2002). Research on the mental health effects of terrorism. Journal of the American Medical Association, 288 633–636.\n\nPangi, R. (2002). After the attack: the psychological consequences of terrorism. Perspectives on Preparedness, 7, 1–20.\n\nPerkovich, G., Cirincione, J., Gottemoelle, R., Wolfsthal, J., and Mathews, J. (2004). Universal Compliance (Washington, DC: Carnegie Endowment).\n\nPerry, W.J., Carter, A., and May, M. (12 June 2007). After the bomb. New York Times.\n\nPetersen, J.L. (2000). Out of the Blue (Lanham, MD: Madison Books).\n\nPost, J. (1987). Prospects for nuclear terrorism: psychological motivations and constraints. In Levanthal and Alexander.\n\nPost, J. (2000). Psychological and motivational factors in terrorist decision-making: implications for CBW terrorism. In Tucker, J. (ed.), Toxic Terror: Assessing Terrorist Use of Chemical and Biological Weapons (Cambridge, MA: MIT Press).\n\nPotter, W.C. (2001). A New Agenda for Disarmament and Non-Proliferation Education. Disarmament Forum, No. 3. pp. 5–12.\n\nPotter, W.C. (2006). A Practical Approach to Combat Nuclear Terrorism: Phase Out HEU in the Civilian Nuclear Sector. Paper presented at the International Conference on the G8 Global Security Agenda: Challenges and Interests. Toward the St. Petersburg Summit. Moscow, April 20–22, 2006.\n\nPotter, W.C. and Sokova, E. (Summer, 2002). Illicit nuclear trafficking in the NIS: what’s new? what’s true? Nonproliferation Review, 112–120.\n\nPotter, W.C., Sokov, N., Mueller, H., and Schaper, A. (2000). Tactical Nuclear Weapons: Options for Control (Geneva: United Nations Institute for Disarmament Research).\n\nPry, P.V. (1999). War Scare: Russia and America on the Nuclear Brink (Westport, CT: Praeger).\n\nRapoport, D.C. (1999). Terrorism and weapons of the apocalypse. Nonproliferation Review, 6(3), 49–67.\n\nRhodes, R. (1986). The Making of the Atomic Bomb (New York: Simon &Schuster).\n\nRosenbaum, D.M. (Winter 1977). Nuclear terror. International Security, pp. 140–161.\n\nSagan, S.D. (1993). The Limits of Safety: Organizations, Accidents, and Nuclear Weapons (Princeton, NJ: Princeton University Press).\n\nSchelling, T. (1982). Thinking about nuclear terrorism. International Security, 6(4), 61–77.\n\nScheuer, M. (2002). Through Our Enemies’ Eyes: Osama bin Laden, Radical Islam, and the Future of America (Washington, DC: Potomac Books, Inc.).\n\nSchlenger, W.E. (2002). Psychological reactions to terrorist attacks: findings from the national study of Americans’ reactions to September 11. Journal of the American Medical Association, 288 581–588.\n\nSchollmeyer, J. (May/June 2005). Lights, camera, Armageddon. Bulletin of the Atomic Scientists, pp. 42–50.\n\nSokov, N. (2002). Suitcase Nukes: A Reassessment. Research Story of the Week, Center for Nonproliferation Studies, Monterey Institute of International Studies. Available at http://www.cns.miis.edu/pubs/week/020923.htm.\n\nSokov, N. (2004). ‘Tactical Nuclear Weapons’. Available at the Nuclear Threat Initiative website http://www.nti.org/e\\_research/e3\\_10b.html\n\nSpector, L.S. (1987). Going Nuclear (Cambridge, MA: Ballinger Publishing Co).\n\nSprinzak, E. (2000). On not overstating the problem In Roberts, B. (ed.), Hype or Reality?: The ‘New Terrorism’ and Mass Casualty Attacks (Alexandria, VA: Chemical and Biological Arms Control Institute).\n\nSprinzak, E. and Zertal, I. (2000). Avenging Israel’s Blood (1946). In Tucker, J. (ed.), Toxic Terror: Assessing Terrorist Use of Chemical and Biological Weapons, pp. 17–42 (Cambridge, MA: MIT Press).\n\nStern, J.E. (1999). The Ultimate Terrorists (Cambridge, MA: Harvard University Press).\n\nStern, J.E. (2000). The Covenant, the Sword, and the Arm of the Lord (1985). In Tucker, J. (ed.), Toxic Terror: Assessing Terrorist Use of Chemical and Biological Weapons, pp. 139–157 (Cambridge, MA: MIT Press).\n\nSterngold, J. (18 April 2004). Assessing the risk on nuclear terrorism; experts differ on likelihood of ‘dirty bomb’ attack. San Francisco Chronicle.\n\nStober, D. (March/April 2003). Noexperience necessary. Bulletin of the Atomic Scientists, pp. 57–63.\n\nTaleb, N.N. (2004). The Black Swan: Why Don’t We Learn that We Don’t Learn? in United States Department of Defense Highlands Forum papers.\n\nThe Frontier Post. (20 November 2001). Al Qaeda Network May Have Transported Nuclear, Biological, and Chemical Weapons to the United States, The Frontier Post, Peshawar.\n\nThe Middle East Media Research Institute. (12 June 2002). ‘Why we fight America’: Al-Qa’ida Spokesman Explains September 11 and Declares Intentions to Kill 4 Million Americans with Weapons of Mass Destruction. 2002. The Middle East Media Research Institute. Special Dispatch Series No. 388. Accessed at http://memri.org/bin/articles.cgi?Page=archives&Area=sd&ID=SP38802 on 20 August 2006.\n\nToki, M. and Potter, W. C. (March 2005). How we think about peace and security: The ABCs of initiatives for disarmament &non-proliferation education. IAEA Bulletin, 56–58.\n\nU.S. Office of Technology Assessment. (1977). Nuclear Proliferation and Safeguards. Vol. 1 (New York: Praeger).\n\nU.S. Office of Technology Assessment. (1979). The Effects of Nuclear War.\n\nUnion of Concerned Scientists. (2003). Scientists’ Letter on Exporting Nuclear Material to W. J. ‘Billy’ Tauzin, 25 September 2003. Available at\n\nvonHippel, F. (2001). Recommendations for Preventing Nuclear Terrorism. Federation of American Scientists Public Interest Report.\n\nWald, M.L. (23 January 2000). Suicidal nuclear threat is seen at weapons plants. New York Times, A9.\n\nWalker, S. (2001). Regulating against nuclear terrorism: the domestic safeguards issue, 1970–1979. Technology and Culture, 42, 107–132.\n\nWar Games. (1983). Film, Metro-Goldwyn-Mayer. Directed by John Badham.\n\nWillrich, M. and Taylor, T.B. (1974). Nuclear Theft: Risks and Safeguards (Cambridge, MA: Ballinger Publishing Co.).\n\nZorza, V. (9 September 1972). World could be held to ransom. The Washington Post, p. A19.\n\nZorza, V. (2 May 1974). The basement nuclear bomb. The Washington Post.\n\n• 20 • Biotechnology and biosecurity\n\nAli Nouri and Christopher F. Chyba\n\n20.1 Introduction\n\nBiotechnological power is increasing exponentially, reminiscent of the increase in computing power since the invention of electronic computers. The co-founder of Intel Corporation, Gordon Moore, pointed out in 1965 that the number of transistors per computer chip – a measure of how much computation can be done in a given volume – has doubled roughly every 18 months (Moore, 1965). This exponential increase in computing power, now called ‘Moore’s Law’, has continued to hold in the decades since then (Lundstrom, 2003) and is the reason that individuals now have more computing power available in their personal computers than that was available only to the most advanced nations only decades ago. Although biotechnology’s exponential lift off began decades after that of computing, its rate of increase, as measured, for example, by the time needed to synthesize a given DNA sequence, is as fast or faster than that of Moore’s Law (Carlson, 2003). Just as Moore’s Law led to a world of personal computing and home appliance microprocessors, so biotechnological innovation is moving us into a world where the synthesis of DNA, as well as other biological manipulations, will be increasingly available to small groups of technically competent and even individual users.\n\nThere is already a list of well-known experiments – and many others that have received less public attention – that illustrates the potential dangers intrinsic to modern biological research and development. We review several examples of these in some detail below, including: genetic manipulations that have rendered certain viruses far more deadly to their animal hosts (Jackson et al., 2001); the synthesis of polio virus from readily purchased chemical supplies (Cello et al., 2002) – so that even if the World Health Organization (WHO) succeeds in its important task for eradicating polio worldwide, the virus can be reconstituted in laboratories around the world; the reduction in the time needed to synthesize a virus genome comparable in size to the polio virus from years to weeks; the laboratory re-synthesis of the 1918 human influenza virus that killed tens of millions of people worldwide (Tumpey et al., 2005); the discovery of ‘RNA interference’, which allows researchers to turn off certain genes in humans or other organisms (Sen et al., 2006); and the new field of ‘synthetic biology’, whose goal is to allow practitioners to fabricate small ‘biological devices’ and ultimately new types of microbes (Fu, 2006).\n\nThe increase in biological power illustrated by these experiments, and the global spread of their underlying technologies, is predicted to lead to breathtaking advances in medicine, food security, and other areas crucial to human health and economic development. For example, the manipulation of biological systems is a powerful tool that allows controlled analysis of the function – and therefore vulnerabilities of and potential defences against – disease organisms. However, this power also brings with it the potential for misuse (NRC, 2006). It remains unclear how civilization can ensure that it reaps the benefits of biotechnology while protecting itself from the worst misuse. Because of the rapid spread of technology this problem is an intrinsically international one. However, there are currently no good models from Cold War arms control or non-proliferation diplomacy that are suited to regulating this uniquely powerful and accessible technology (Chyba and Greninger, 2004). There are at least two severe challenges to any regulatory scheme (Chyba, 2006). The first is the mismatch between the rapid pace of biotechnological advances and the comparative sluggishness of multilateral negotiation and regime building. The second is the questionable utility of large-scale monitoring and inspections strategies to an increasingly widespread, small-scale technology.\n\nHowever, this is not a counsel for despair. What is needed is a comprehensive strategy for the pursuit of biological security – which we assume here to be the protection of people, animals, agriculture and the environment against natural or intentional outbreaks of disease. Such a strategy is not yet in place, either nationally or globally, but its contours are clear. Importantly, this strategy must be attentive to different categories of risk, and pay attention to how responses within one category strengthen or weaken the response to another. These categories of risk include: naturally occurring diseases; illicit state biological weapons programmes; non-state actors and bio-hackers; and laboratory accidents or other inadvertent release of disease agents.\n\nJust this listing alone emphasizes several important facts. The first is that while about 14 million people die annually from infectious diseases (WHO, 2004) (mostly in the developing world), only five people died in the 2001 anthrax attacks in the United States (Jernigan et al., 2002), and there have been very few other modern acts of biological terrorism. Any approach to the dual-use challenge of biotechnology, which substantially curtails the utility of biotechnology to treat and counter disease, runs the risk of sacrificing large numbers of lives to head off hypothetical risks. Yet it is already clear that humans can manipulate pathogens in ways that go beyond what evolution has so far wrought, so the hypothetical must nevertheless be taken seriously. A proper balance is needed, and an African meeting on these issues in October 2005 suggested one way to strike it. The Kampala Compact declared that while ‘the potential devastation caused by biological weapons would be catastrophic for Africa’, it is ‘illegitimate’ to address biological weapons threats without also addressing key public health issues such as infectious disease. The developed and developing world must find common ground.\n\nA second important observation regarding biological terrorism is that there have, so far, been very few actual attacks by non-state groups. It is clearly important to understand why this has been the case, and to probe the extent to which it has been due to capabilities or motivations – and how whatever inhibitions may have been acting can be strengthened. Sceptical treatments of the biological terrorism threat, and of the dangers of apocalyptic dramatization more generally, can place important focus on these issues – though the examples of dual-use research already mentioned, plus recent US National Academy of Sciences studies and statements by the UN Secretary-General make it clear that the problem is real, not hype.¹\n\nWhile the focus of this chapter will be on biotechnological capabilities, and how those capabilities may be responsibly controlled, it should be remembered that a capabilities-based threat assessment only provides part of the comprehensive picture that is required. Indeed, it is striking to compare the focus on capabilities in many technology oriented threat assessments with the tenor of one of the most influential threat assessments in US history, George Kennan’s ‘X’ article in Foreign Affairs in 1947. In this piece, Kennan crystallized the US policy of containment of the Soviet Union that prevailed for decades of the Cold War. On reading today, one is struck by how little of the × article addressed Soviet capabilities. Rather, nearly all of it concerned Soviet intentions and motives, with information based on Kennan’s experience in Soviet society, his fluency in Russian, and his knowledge of Russian history and culture. To the extent that the biological security threat emanates from terrorist groups or irresponsible nations, a similar sophistication with respect to motives and behaviour must be brought to bear (see also Chapters 4 and 19 inthis volume).\n\nIn this chapter, we first provide a survey of biological weapons in history and efforts to control their use by states via multilateral treaties. We then describe the biotechnological challenge in more detail. Finally, we survey a variety of approaches that have been considered to address these risks. As we will see, there are no easy answers.\n\n20.2 Biological weapons and risks\n\nThe evolutionary history of life on Earth has, in some instances, led to biological weapons in the form of harmful toxins (and their underlying genes) that are carried by simple organisms like bacteria and fungi, as well more complex ones like spiders and snakes. Humans learned that such natural phenomena could be used to their advantage; long before the identification of microbes and the chemical characterization of toxins, humans were engaged in rudimentary acts of biological warfare that included unleashing venomous snakes on adversaries, poisoning water wells with diseased animal flesh, and even catapulting plague-infested human bodies into enemy fortifications (Wheelis, 2002).\n\nAs scientists learned to optimize growth conditions for microbes, stockpiling and storing large quantities of infectious living organisms became feasible and dramatically increased the destructive potential of germ warfare. These advances and a better understanding of disease-causing microbes, together with the horror and carnage that was caused by non-conventional weapons during WWI, elevated fears of germ warfare and provided the impetus for the 1925 Geneva protocol, an international treaty that outlawed the use of chemical and bacteriological (biological) weapons in war. With the notable exception of Japan (Unit 731 Criminal Evidence Museum, 2005), warring states refrained from using biological weapons throughout WWII – but some continued to engage in offensive weapons programmes, which were not prohibited until the Bacteriological (Biological) and Toxins Weapons Convention (BWC) was opened for signature in 1972.\n\nThe BWC is the world’s first international disarmament treaty outlawing one entire class of weapons – namely, the development, production, and stockpiling of biological agents and toxins for anything other than peaceful (i.e., prophylactic) purposes. Despite 155 ratifications out of the 171 states that are signatory to the convention, the BWC suffers from the lack of a monitoring and inspection mechanism to assess whether a country is engaged in illegal activities. This institutional weakness permitted sophisticated offensive programmes to continue long after the convention was signed, such as one in the former Soviet Union. Efforts to develop monitoring and verification protocols within the framework of the BWC began in 1991, but were suddenly terminated 10 years later when the United States withdrew its support in July 2001, arguing that the additional measures would not help to verify compliance, would harm export control regimes, and place US national security and confidential business information at risk.²\n\nSimilarly to what is found in the nuclear and chemical weapons realm, the BWC could also be strengthened by a rigorous verification process. However, the affordability and accessibility of biotechnologies, and the absence of any severe weapon-production bottlenecks analogous to that of the production of plutonium or high-enriched uranium in the nuclear case, render verification inherently more difficult in the biological realm. This is a distinguishing feature of biological weapons that is obscured by the tendency to include them with nuclear, chemical, and radiological weapons as a ‘weapon of mass destruction’ (Chyba, 2002).\n\n20.3 Biological weapons are distinct from other so-called weapons of mass destruction\n\nProducing a nuclear bomb is difficult; it requires expensive and technologically advanced infrastructure and involves uranium enrichment or plutonium production and reprocessing capacity that are difficult to hide. These features render traditional non-proliferation approaches feasible; despite being faced with many obstacles to non-proliferation, the International Atomic Energy Agency (IAEA) is able to conduct monitoring and verification inspections on a large number (over a thousand) of nuclear facilities throughout the world.\n\nThese traditional approaches are also reasonably effective in the chemical realm where the Organization for the Prohibition of Chemical Weapons (OPCW) can, inter alia, monitor and verify the destruction of declared chemical stockpiles.\n\nBut biological weapons proliferation is far more challenging for any future inspection regime – and it will only become more so as the underlying technologies continue to advance. In some respects, biological weapons proliferation poses challenges more similar to those presented by cyber attacks or cyber terrorism than to those due to nuclear or chemical weapons. An IAEA- or OPCW-like monitoring body against the proliferation of cyber attack capabilities would present a reductio ad absurdum for a verification and monitoring regime. Internet technology is so widely available that only a remarkably invasive inspection regime could possibly monitor it. Instead, society has decided to respond in other ways, including creating rapidly evolving defences like downloadable virus software and invoking law enforcement to pursue egregious violators.\n\nSomewhat similar challenges are presented in the biological realm where the spread of life science research in areas like virology, microbiology, and molecular biology are contributing to a growing number of laboratories worldwide that engage in genetically based pathogen research. In addition, an expanding biotech industry and pharmaceutical sector is contributing to the spread of advanced and increasingly ‘black box’³ technologies that enable high consequence research to be carried out by a growing number of individuals; biotechnology is already commonplace in undergraduate institutions, it is beginning to enter high school classes, and is increasingly popular among amateur biotech enthusiasts. Moreover, an increasing number of countries are investing in biotechnology applications for health, agriculture, and more environment-friendly fuels. These trends are contributing to the increasing affordability of these technologies; the initial draft of the human genome cost an estimated $300 million (the final draft and all technologies that made it possible cost approximately $3 billion). Just 6 years later, one company hopes to finish an entire human genome for only $100,000 – a 3000-fold cost reduction. Researchers, spurred by government funding and award incentives from private foundations are now working towards a $1000 genome (Service, 2006).\n\nThese are exciting times for biologists; whereas the twentieth century saw great progress in physics, the early decades of the twenty-first century may well ‘belong’ to biology. These advances, however, provide unprecedented challenges for managing biotechnology’s risks from misuse – challenges that are compounded by the ease with which biological materials can be hidden and the speed by which organisms can proliferate. Some bacteria can replicate in just 20 minutes, allowing microscopic amounts of organisms to be mass-produced in a brief period of time.\n\n20.4 Benefits come with risks\n\nStudies that uncovered DNA as life’s genetic material and the discovery that genes encode for proteins that govern cellular characteristics and processes ushered in an era of modern molecular biology that saw rapid advances in our knowledge of living systems and our ability to manipulate them. At first, studying gene function involved introducing random mutations into the genomes of organisms and assessing physical and behavioural changes. Soon after, scientists learned to control gene function directly by introducing exogenous pieces of DNA into the organism of interest. Experiments like these began to shed light on the molecular mechanisms that underlie cellular processes and resulted in a better understanding of, and better tools to fight, human disease.\n\nModern molecular biology continues to develop medical solutions to global health issues such as newly occurring, re-emerging, and endemic infectious diseases. To address these threats, researchers are working to develop rational-design vaccines and antivirals. Microbiologists are exploring new avenues to counter antibiotic resistance in bacteria, while synthetic biologists are programming microorganisms to mass produce potent and otherwise rare anti-malarial drugs. Biotechnology’s contribution to health is visible in medical genomics, where rapid improvements in DNA sequencing technology combined with better characterization of genes, are beginning to unravel the genetic basis of disease. Other advances are apparent in the genetic modification of crops that render them resistant to disease and increase their yield. Biotechnology is even beneficial in industrial applications such as the development of new biological materials, potentially environment-friendly biological fuels, and bioremediation – the breakdown of pollutants by microorganisms.\n\nBut the same technologies and know-how that are driving the revolution in modern medicine are also capable of being misused to harm human health and agriculture (NRC, 2003a, 2003b). Traditional threats created by the misuse of biotechnology involve the acquisition, amplification, and release of harmful pathogens or toxins into the environment. One area of concern, for example, is a potential bioterrorist attack using pathogens or toxins on centralized food resources. Some toxins, like those produced by the bacterium Clostridium botulinum, are extremely damaging; small amounts are sufficient to inhibit communication between the nervous system and muscles, causing respiratory paralysis and death. In 2001, the United States found itself unprepared to cope with the intentional spread of anthrax, a bacterium⁴ that can be obtained from the wild and amplified in laboratories. Anthrax can enter the body orally, or through cuts and skin lesions, after which it proliferates and releases illnesscausing toxins. A more dangerous and deadly infection, however, can result if stable, dormant spores of the bacterium are ‘weaponized’, or chemically coated and milled into a fine powder consisting of small particles that can be suspended in air. These bacterial particles can travel long distances, be taken into the victim’s respiratory airways and drawn into the lungs, where the spores germinate into active bacterium that divide and release toxic substances to surrounding cells. If left untreated, inhalation anthrax infects the lymph nodes, causing septic shock and death in the vast majority of its victims.\n\nWhereas many bacterial pathogens, like anthrax, are free-living organisms that require proper conditions and nutrients for growth, other bioterrorism agents, like viruses, are parasitic and rely on their hosts’ cellular machinery for replication and propagation. Viral propagation in laboratories involves propagating viruses in cells that are often maintained in incubators that precisely mimic the host’s physiological environment. A trained individual, with the proper know-how and the wrong intentions, could co-opt these life science tools to amplify, harvest, and release deadly pathogens into the environment. This threat is compounded by advances in microbiology, virology, and molecular biology, which enable directed changes in the genomes of organisms that can make them more stable, contagious and resistant to vaccines, antibiotics (in the case of bacteria), or antivirals.\n\nUnless it is properly used, biotechnology’s dual-use nature – the fact that beneficial advances can also be used to cause harm – poses a potential threat to human health and food security. But risk management measures aimed at minimizing these threats should not excessively impede biotechnology’s benefits to health and food security and should also take care not to unnecessarily hinder scientific progress; inhibiting a developing country’s access to health tools that are used in vaccines and pharmaceutical drug production would pose a serious ethical dilemma. Even if humanitarian arguments were set aside, solely from the perspective of self-interest in the developed world, restricting access to biotechnology could undermine desired security objectives by encouraging secrecy and impeding collaborative exchanges among different laboratories.\n\nBiology’s dual-use challenges extend beyond technology and include knowledge and know-how: better understanding of the molecular mechanisms that underlie cellular processes expose the human body’s weaknesses and sensitivities, which can be exploited by those who intend to harm. Consider immunology research, which has characterized the interleukins, proteins that participate in the body’s immune response to an infection. Foreign pathogens can disrupt the normal activity of these proteins and result in a defective immune response, serious illness, and death. A set of experiments that inadvertently illustrated some dual-use applications of this knowledge involved a group of Australian researchers who, in an attempt to sterilize rodents, added one of the interleukins, interleukin-4, to a mousepox virus (among other genetic modifications), hoping to elicit an autoimmune reaction that would destroy female eggs without eliminating the virus (Jackson et al., 2001). However, the virus unexpectedly exhibited more generalized effects; it inhibited the host’s immune system and caused death – even in rodents that were naturally resistant to the virus or had previously been vaccinated. In a separate study, researchers showed that mice resistant to mousepox, if injected with neutralizing antibodies to the immune system regulator, IFN-γ become susceptible to the virus and exhibit 100% lethalilty (Chaudhri et al., 2004).\n\nAlthough there are genetic variations and different modes of infection between mousepox and its human counterpart, smallpox, these published experiments provide a possible road map for creating a more virulent and vaccine-resistant smallpox virus – a chilling notion given that the virus has been a major killer throughout human history (Tucker, 2001). Although the United States has enough supplies to vaccinate its entire population (assuming the vaccine would be effective against a genetically modified virus), current world supplies can only cover 10% of the global population(Arita, 2005). (However, a ‘ring vaccination’ strategy, rather than a ‘herd immunity’ strategy, might be able to stop an outbreak well before complete vaccination was required.) Fortunately, smallpox has been eradicated from the natural world. The only known remaining stocks are located in US and Russian facilities. Although the WHO’s decision-making body, the World Health Agency, had initially called for the destruction of these stocks by the end of 2002, it later suspended its decision, allowing smallpox research with the live virus to continue (Stone, 2002). Currently, researchers are using a variety of approaches to study the biology of smallpox, including research projects that involve infecting animals with the live virus (Rubins et al., 2004).\n\nIn addition to fears that smallpox could be accidentally or intentionally released or stolen from research facilities, there are also concerns that the smallpox virus could be regenerated from scratch. The latter requires piecing together the different fragments of the genome, which would be a difficult task, requiring knowledge of molecular biology, a standard molecular biology laboratory with appropriate reagents and equipment, and the skills and substantial time for trial and error. While synthesizing the smallpox virus from scratch in the laboratory is theoretically possible, it is fortunate that this challenge is both quantitatively and qualitatively much harder than for some other viruses, such as polio. Advances in the life sciences, however, are beginning to remove these hurdles.\n\n20.5 Biotechnology risks go beyond traditional virology, micro- and molecular biology\n\nTo date, complete genomes from hundreds of bacteria, fungi, viruses, and a number of higher organisms have been sequenced and deposited in a public online database. While many of these genomes belong to inert microbes and laboratory research strains that cannot infect people or animals, others include those from some of the most pathogenic viruses known to humans, such as Ebola and Marburg, and even extinct ones like smallpox and the 1918 Spanish influenza virus. Alongside better DNA sequencing, biotechnology research has also seen the evolution of de novo DNA synthesis technologies; it is now possible to commercially order pieces of DNA as long as 40,000 bases⁵ – longer than the genomes of many viruses; SARS for instance is roughly 30,000 bases, while the Ebola genome is less than 20,000 bases long. Moreover, the emerging rapid improvement of the technology over the next several years should enable even the synthesis of bacterial genomes, many of which are around one million bases long. For example, just recently, scientists at the Venter institute transplanted an entire genome from one bacteria species into another, causing the host cell to effectively become the donor cell (Lartigue et al., 2007). The study demonstrates how a bacterial cell can be used as a platform to create new species for specialized functions (provided their genomes are available). The donor and host bacterial species that were chosen for the study are highly related to each other and contain relatively small genomes, features which facilitated the success of the transplantation experiment. Nevertheless, the study does point the way to more general applications, including transplantation of synthesized pathogen genomes, for the creation of otherwise difficult-to-obtain bacterial pathogens.\n\nAutomated DNA synthesis removes much of the time-consuming and technically difficult aspects of manipulating DNA; using commercial DNA synthesis, a researcher can copy a sequence of interest from an online public database and ‘paste’ it into the commercial DNA provider’s website. Within days or weeks (depending on length of the sequence) the fragment, or even the entire genome of interest, is artificially synthesized and mail-delivered. For many viruses, a synthesized viral genome could then be introduced into a population of cells, which would treat the foreign DNA as if it were their own: ‘reading’ it, transcribing the genes into RNA molecules that are processed by the cell’s internal machinery and translated into proteins. These proteins can then assemble themselves into infectious viral particles that are ejected from the cell, harvested and used in infection studies.⁶\n\nIn the wrong hands or in laboratories that lack proper safety precautions, this technology poses a serious security risk as it renders some of the traditional regulatory frameworks for the control of biological agents obsolete. A number of countries control the possession and movement of these substances. In the United States, these are referred to as ‘select agents’ and include a number of bacteria, viruses, fungi, and toxins that are harmful to humans, animals, or plants. Conducting research on these high-risk organisms and toxins require special licenses or security clearances. The ability to order genomes and create organisms de novo, however, necessitates revisiting these regulations. As we have seen, experiments have already been published in the highest profile international scientific journals that describe the re-creation of the poliovirus as well as the Spanish influenza virus, the agent that killed 50 million people in 1918. This virus, which was previously extinct, now exists and is used in research facilities in both the United States and Canada. The ability to synthesize genomes and create organisms from them has spurred a US-based biosecurity advisory board to call for regulating the possession and movement of pathogen genomes, rather than pathogens themselves (Normile, 2006).\n\nIn addition to risks arising from the intentional misuse of these pathogens, there are serious laboratory safety considerations; many facilities worldwide lack the expensive safeguards needed for handling highly pathogenic organisms – even though they may have the technology to create these organisms. Moreover, the ease with which genomes can be synthesized raises the concern that highly pathogenic viruses and bacteria will become increasingly distributed in laboratories and among researchers interested in high consequence pathogen research. The accidental contamination of workers and the subsequent escape of viruses from highly contained laboratories, have occurred a number of times. In one such case, a researcher at the National Defense University in Taipei was, unknowingly, infected with the SARS virus, after which he left Taiwan for a conference in Singapore. The event prompted quarantine of 90 individuals with whom the infected researcher had come into contact (Bhattacharjee, 2004). Although there were no known secondary infections in this particular case, the escape of pathogenic viruses or bacteria from contained laboratories could have serious consequences.\n\n20.6 Addressing biotechnology risks\n\nDual-use risks posed by biotechnology may be addressed at a number of points. Efforts may be made to oversee, regulate, or prevent the most dangerous research altogether, or the publication of that research, or to restrict certain lines of research to particular individuals. One may also focus on recognizing disease outbreaks quickly whether natural or intentional when they occur, and responding to them effectively. This requires both improvements in surveillance and response capacity and infrastructure. Finally, one may encourage research, development, and production of appropriate vaccines, antibiotics, antivirals, and other approaches to mitigating an outbreak – along with the required infrastructure for meeting surges in both drug requirements and numbers of patients. Of course, none of these approaches is exclusive. Perhaps their one commonality is that each faces important drawbacks. We consider a variety of suggested approaches to each, and their challenges.\n\n20.6.1 Oversight of research\n\nThe US National Research Council (NRC) has recommended a variety of monitoring mechanisms and guidelines for federally funded, high-risk research (NRC, 2003a). These ‘experiments of concern’ would be subjected to greater scrutiny at the funding stage, during the research phase, and at the publication stage.⁷ They would include experiments that could make pathogens impervious to vaccines and antibiotics, allow pathogens to escape detection and diagnosis, increase the transmissibility or host range of a pathogen, and experiments that aim to ‘weaponize’ biological agents and toxins. But the NRC guidelines would only extend to laboratories that are funded by the National Institutes of Health, and they are therefore required to follow governmental guidelines. A comprehensive approach would take into account the commercial sector as well as increased funding from philanthropic and private foundations like the US-based Howard Hughes Medical Institute, or the Welcome Trust of England, which annually distribute 500 million, and over one billion research dollars, respectively (Aschwanden, 2007). Comprehensive oversight mechanisms would also include governmental laboratories, including those involved in biodefence research. Finally, the biotechnology challenge is inherently global, so an effective research oversight regime would have to be international in scope.\n\nTo this end, John Steinbruner and his colleagues at the Center for International and Security Studies at Maryland (CISSM) have proposed a global system of internationally agreed rules for the oversight of potentially high-consequence pathogens research (Steinbruner et al., 2005). Although dedicated non-state groups would not be likely to be captured by such a system, they are unlikely to conduct forefront research. Instead, they might attempt to co-opt discoveries and techniques that are reported in the scientific literature. By overseeing certain high-risk research and its publication, society might therefore head off some of the worst misuse. A limited model for what oversight of the highest consequence biological research might look like is provided by the World Health Organization’s international advisory committee that oversees smallpox research; it is important that this committee demonstrate that it is capable of real oversight.\n\nThe CISSM model oversight system calls for an International Pathogens Research Authority with administrative structures and legal foundations for participation of its states-parties. It is unlikely that such a system could be negotiated and ratified in the current climate, although plausibility of possible oversight mechanisms could change rapidly subsequent to a laboratory engineered pandemic; better that careful thinking be applied now before the urgency and fear that would become pervasive in the post-attack world. Other international approaches to provide some level of oversight have also been envisioned, including the creation of additional UN bodies, or the establishment of an ‘International Biotechnology Agency’ (IBTA) analogousto theIAEA. The IBTA could be established in a modular way, with initial modest goals of helping BWC states-parties meet their reporting (confidence building measures) requirements, and promoting best practices in laboratory safety. All these approaches require the creation of new international oversight bodies, a politically challenging requirement.\n\n20.6.2 ‘Soft’ oversight\n\nAt the other end of the spectrum from the CISSM oversight model are efforts at what might be called ‘soft’ oversight of high-risk research. Some of the most common among these are efforts to promote codes of ethics (or the more demanding, but rarer, codes of conduct or codes of practice) for scientists working in the relevant fields.⁸ Many national and international groups have made efforts in this direction. If coupled with education about the possible misuse of scientific research, such codes would help provide the scientific community with tools to police itself. To this end, a US National Academy panel has recommended establishing a global internet-linked network of vigilant scientists to better protect against misuse within their community (NRC, 2006).\n\n20.6.3 Multi-stakeholder partnerships for addressing biotechnology risks\n\nThe failure of the negotiations for a compliance protocol to the BWC shows some of the challenges now facing treaty negotiation and ratification. (This protocol, while it would have provided valuable transparency into certain high-end biological facilities, would not have – nor was it meant to – directly addressed the challenge of dual-use biotechnology.) One general result has been increasing interest in alternative policy models such as multistakeholder partnerships. Indeed, the international relations literature has seen an increasing volume of work devoted to the mismatch between important global problems and the absence of international mechanisms to address them in a timely and effective way.⁹ Means of international governance without formal treaties are being sought.\n\nIn the biological security realm, efforts to forge multi-stakeholder partnerships are bringing together the academic science sector, commercial industry, the security community, and civil society, in order to raise awareness and facilitate feasible risk-management solutions to biology’s dualuse problem. The former UN Secretary-General, Kofi Annan, recognized that the increasing distribution of biotechnology requires solutions that have an international dimension and called for a global forum to help extend the benefits of biotechnology and life-science research, while managing its security risks. The Secretary-General’s unique convening power to bring together a diverse number of players from the appropriate sectors is instrumental for a successful bottom-up approach that aims to address biotechnology’s challenges. This combined with other efforts by the Royal Society, the InterAcademy Panel on International Issues, the International Council for the Life Sciences, The International Consortium for Infectious Diseases, and a number of others, are beginning to work towards an international framework in the absence of a formal, government-driven treaty process.\n\nIn addition to recognizing the urgency to address biotechnology’s risks, some of these efforts have also highlighted the importance of risk-management strategies that do not hinder free flow of scientific communication and that do not impose excessively intrusive oversight mechanisms that would hurt scientific progress. An example of an effective risk-management strategy that manages risks without impacting potential benefits is a proposal that specifically addresses de novo DNA synthesis technology. The successful adoption of the proposal in the academic and commercial science sectors merits further attention, as the risk-management strategy might be applicable to some of biology’s other dual-use areas.\n\n20.6.4 A risk management framework for de novo DNA synthesis technologies\n\nCurrently, de novo DNA synthesis technologies capable of making complete pathogen genomes are concentrated in a relatively small number of companies. In 2004, the Harvard biologist and biotechnology developer, George Church, proposed a safeguards strategy to ensure that the technology is not used for the illegitimate synthesis of potentially harmful genomes (Church, 2005). This involves companies agreeing to install automated screening software that ‘reads’ the DNA sequence of incoming customer orders and compares them to genomes of a known list of pathogens (and, potentially, to a list of other potentially dangerous sequences, for example, those for particular genes). An exact match, or more likely a certain degree of sequence similarity, would elicit further inquiry and possibly result in the notification of proper authorities. Software in the synthesis machines could be installed, and updated, to make it impossible for the machines to synthesize certain sequences of particular concern.\n\nThis kind of DNA screening has already been adopted by a number of the DNA providers and has won endorsement within the synthetic biology community, which is a heavy user of DNA synthesis technologies (Declaration of the Second International Meeting on Synthetic Biology, 2006). Successful implementation of the protocol is in large part due to the proposal’s nonintrusive nature; rather than requiring formal oversight structures, which many scientists oppose for fear that progress might be hindered, the screening tool allows the laboratory to go about business as usual as the computer software engages in the invisible detective work. The automated nature of DNA screening is also appealing to industry because it enables the protection of customer information. MIT synthetic biologist Drew Endy, together with George Church and other colleagues such as John Mulligan, CEO of a leading DNA synthesis company, are now working to extend the screening proposal to companies overseas. Indeed, lack of unity among the various DNA providers would risk jeopardizing the entire venture since it only takes a single noncompliant company to provide harmful materials to all interested customers. Possible strategies to address this deficiency include licensing all DNA providers or establishing a centralized international clearinghouse that receives and screens all DNA orders from the various providers (Búgl et al., 2006).\n\n20.6.5 From voluntary codes of conduct to international regulations\n\nWhile adopting safeguard strategies such as DNA screening exemplifies corporate responsibility, implementation of these measures is purely voluntary and without a legal framework. The UN Security Council Resolution 1540 provides the impetus to strengthen and globally extend such measures. Resolution 1540 requires UN member states to strengthen national legislation in order to address a number of issues, including biological terrorism. The legally binding implementation of the DNA screening protocol by countries that are users or providers of the technology could be cast in terms of a step in the implementation of resolution 1540. Alternatively or additionally, other international mechanisms such as the BWC could be adapted to carry out the operations of a centralized international clearinghouse for DNA synthesis screening.\n\n20.6.6 Biotechnology risks go beyond creating novel pathogens\n\nAs biotechnological tools improve, the various methods that can be used to create novel organisms should be assessed further in order to make informed policy decisions regarding the risks. For example, a combination of virology and molecular biology could be used to create novel pathogens like hybrid viruses that are composed of inert laboratory strains loaded with additional toxic genes. Once inside its host, the hybrid virus would enter its target cell population, wherein viral genetic material would be converted into toxic proteins that disrupt normal cellular processes and cause disease (Block, 1999).\n\nSimilarly, viruses can be created that have the ability to shut down essential cellular genes (Block, 1999). Consider small interfering RNA (siRNA) technology, whose beneficial applications were recognized by the 2006 Nobel Prize for physiology or medicine. siRNA molecules turn off a gene by inactivating its RNA product (Sen and Blau, 2006). A highly contagious virus, supplemented with siRNA, or other ‘gene knockdown’ technologies, could shut down essential genes in particular cell populations of its host (Block, 1999). Theoretically, this could set off a novel epidemic for which no known vaccine or cure exists. Even more alarming is the ease with which commercial DNA sources can automate the synthesis of these novel pathogens, relieving even the novice from the laborious and methodical task of splicing genes into viral genomes (Tucker and Zilinskas, 2006). Thus, it is important that DNA screening safeguards encompass more than just naturally occurring pathogenic genomes and gene-encoding toxins.\n\n20.6.7 Spread of biotechnology may enhance biological security\n\nThe spread of novel biotechnologies such as large-DNA synthesizers could, paradoxically, provide an opportunity to decrease the probability of misuse. If costs associated with commercial synthesis of large DNA fragments continue to decline, research laboratories will increasingly seek to outsource the laborious task of manipulating DNA sequences to more centralized, automated sources. Similar trends have been observed for DNA sequencing; laboratories that carried out sequencing operations in-house now outsource their needs to commercial sources that perform the task faster and at a fraction of the cost. A similar outcome for DNA synthesis could eventually replace a large number of diffuse and difficult-to-regulate DNA laboratories with more centralized DNA providers whose technologies are automated and more safeguard-friendly.\n\nNot all dual-use issues will be addressed through technical solutions. But where possible, technologies that can be safeguarded should be promoted. This requires innovators and users of new biotechnologies to identify potential risks and develop appropriate safeguards. Biological security gatherings that bring together scientists and policy makers are useful for creating the right mechanism for this but they cannot replace hours of brainstorming of students searching for technical and feasible risk management solutions. Stronger communication links between the security community and biologists and more formal interdisciplinary education programmes should be fostered. Fortunately, some scientists at the forefront of fields such as synthetic biology have also been at the forefront of addressingthe ethical and security implications of their research (Church, 2005; Endy, 2007).\n\n20.7 Catastrophic biological attacks\n\nIt is difficult to forecast mortality figures resulting from potentially catastrophic bioterrorist incidents – however important such predictions may be for designing defensive public health measures. Unpredictability in human behaviour, for example, would impact morbidity and mortality figures, particularly if contagious agents are involved. For aerosolized pathogens, factors like wind speed and direction, as well as other environmental fluctuations, could result in very different attack outcomes. Limitations in our understanding of the biology of pathogens and their interaction with their hosts (i.e., precise mode of infection and, for transmissible pathogens, mode of spread) also render accurate predictions difficult. And as with any major disaster, it is difficult to know in advance the efficacy of emergency response plans and the competence with which they will be carried out (Clarke, 1999).\n\nMoreover, modern society’s experience with bioterrorism has, fortunately, so far been limited to a small number of events that were neither intended to, nor did result in high mortality figures, so they may not serve as good indicators for what a successful major attack would look like. The 2001 US Anthrax scare that caused five deaths, for instance, involved a non-contagious pathogen, and although milled into a fine powder, the bacterial spores were initially contained within envelopes that resulted in only local dissemination. By contrast, the Aum Shinrikyo cult, seeking to stage a mass-casualty attack in order to realize a prophecy, attempted to disperse Bacillus anthracis, from a building rooftop onto the dense urban population of Tokyo. The Aum, which later succeeded in dispersing Sarin nerve gas in Tokyo subways, was, fortunately, unsuccessful both in efforts to procure a pathogenic strain of Bacillus anthracis, and in its attempts to efficiently disseminate the bacterium. But a more rudimentary dispersal technique was successfully used by another group, the Rajneeshees, whose actions were motivated by a desire to keep a large block of individuals away from voting polls, in order to influence local elections. In 1984, members of the Oregon-based cult successfully spread the enteric bacterium, Salmonella typhimurium, onto salad bars, causing illness in over 750 Oregonians and sending many to hospitals. Had the Rajneeshees used a more virulent pathogen, or had the US Anthrax been more efficiently dispersed, major public health disasters may have ensued. In 1993, estimates from the US Congress’ Office of Technology Assessment found that a single 100 kg load of anthrax spores, if delivered by aircraft over a crowded urban setting, depending on weather conditions, could result in fatalities ranging between 130,000 and 3 million individuals. However, these sort of dramatic results have been viewed as overly alarmist by those claiming that such high casualties would require optimal conditions and execution by the perpetrators, and that there would in fact be a very wide range of possible outcomes (Leitenberg, 2005).\n\nBesides the Rajneeshees and the Aum Shinrikyo, another non-state group¹⁰ that appears to have pursued biological weapons is Al Qaeda, apparently making use of one doctoral-level biologist and perhaps several others with undergraduate degrees. It is difficult from the open literature to determine either the level of sophistication or accomplishment of the programme, but what is available suggests that the programme was more aspirational than effective at the time that Al Qaeda was expelled from Afghanistan (National Commission on Terrorist Attacks Upon the United States, 2004; Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction, 2005).\n\nWhile intentional biological attacks have yet to result in catastrophic scenarios, natural disease outbreaks can serve as proxies for what such events might look like. Consider smallpox, which infected 50 million individuals – annually – even as late as the early 1950s (WHO, 2007). Procuring (or creating), and releasing a vaccine-resistant or more lethal strain of this contagious virus in a dense urban environment might well be a cataclysmic event.\n\nWhereas smallpox kills up to a third of its victims, certain strains of the hemorrhagic fever viruses, like Ebola-Zaire, can kill up to 90% of the infected – within several days after symptoms surface. Since 1976, when Ebola first appeared in Zaire, there have been intermittent outbreaks of the disease, often along Sub-Saharan African rainforests where the virus is transmitted from other primates to humans. The remoteness of these regions, and the rapid pace by which these viruses kill their human host, have thus far precluded a global pandemic. However, if these pathogens were procured, aerosolized, and released in busy urban centres or hubs, a catastrophic pandemic might ensue – particularly because attempts to generate vaccines to Ebola have, thus far, proven unsuccessful. In 1992, the Aum Shinrikyo sent a medical team to Zaire in what is believed to have been an attempt to procure Ebola virus (Kaplan, 2000). While the attempt was unsuccessful, the event provides an example of a terrorist group apparently intending to make use of a contagious virus.\n\nInadditionto the toll onhuman life, biological attacks can inflict serious psychological damage, hurt economies, cause political fallout, and disrupt social order. A 1994 natural outbreak of pneumonic plague in Surat, India, provides a glimpse into what such a scenario might look like.¹¹ Pneumonic plague is an airborne variant, and deadliest form, of the ‘black death’ -the disease caused by the bacterium, Yersinia pestis, that is believed to have wiped out a quarter of Europe’s population in the fourteenth century (Anderson and May, 1991). The 1994 outbreak in Surat resulted in an estimated 300,000 individuals fleeing the city (Hazarika, 1995a), and even led to closure of schools, universities, and movie theatres in cities hundreds of miles away (The New York Times, September 20, 1994). Shock waves were felt globally as India faced increasing international isolation: its exports were banned; its tourism industry drastically declined (Hazarika, 1995b); and its citizens were subjected to scrutiny and surveillance at foreign airports (Altman, 1994).\n\nBut while much attention has been paid to human pathogens, threats to agriculture, livestock, and crops, which can cause major economic damage and loss of confidence in food security, should not be overlooked. In 1997, for example, an outbreak in Taiwan of the highly contagious foot-and-mouth disease, caused the slaughter of 8 million pigs and brought exports to a halt, with estimated costs of $20 billion (Gilmore, 2004). Crops can be particularly vulnerable to an attack; they inhabit large tracts of difficult-to-protect land, and suffer from low levels of disease surveillance, sometimes taking months, even years, before disease outbreaks are detected. In 2001, in an effort to contain a natural outbreak of Xanthomonas axonopodis, a bacterium that threatened Florida’s citrus industry and for which there is no cure, 2 million trees were destroyed (Brown, 2001). There are well-defined steps that may be taken by countries (assuming the resources and capacity are available) to protect against such threats (NRC, 2003b).\n\nIn addition to actual attacks on human health and food security, biological ‘hoaxes’ can also exact an important societal toll. Between 1997 and 1998, as media attention to bioterrorism grew, the number of hoaxes in the United States increased from 1 to 150 (Chyba, 2001). In October and November of 2001, following the US Anthrax attacks, 750 hoax letters were sent worldwide, 550 of which went to US reproductive health clinics by a single group (Snyder and Pate, 2002). The high rate of these hoaxes requires defensive systems that can quickly distinguish a real attack from a fake one. This involves vigilance in disease detection and surveillance, as well as forensic discrimination. The remainder of this chapter explores such public health systems, as well as other strategies to defend against biological outbreaks.\n\n20.8 Strengthening disease surveillance and response\n\nThe need to recognize and respond to disease outbreaks is the same regardless of whether the outbreak occurs naturally, by accident, or through an act of terrorism. Therefore, appropriate defence measures should include a strong public health sector that can react to the full spectrum of risks – whether they are relatively common infectious disease outbreaks or less familiar events like bioterrorist attacks.\n\nDefence against biological attacks requires rapid detection of disease, efficient channels of communication, mechanisms for coordination, treating the infected, and protecting the uninfected. The World Health Organization’s deliberate epidemics division provides specific guidelines in these areas.¹²\n\n20.8.1 Surveillance and detection\n\nEfficient response to a disease outbreak begins with disease surveillance. Early detection can greatly minimize the numbers of infected individuals – particularly when contagious pathogens that can cause secondary infections are involved. Clinicians and medical personnel are indispensable for diagnosing and detecting disease, but they can be complemented by improved surveillance of air, food, and water supplies. The United States employs BioWatch in 30 cities; BioWatch employs a device that concentrates outside air onto filters that are routinely tested for the presence of various bioterrorism agents. More advanced systems include the Autonomous Pathogen Detection System (APDS), an automated diagnostic device that conducts polymerase chain reaction (PCR), a method that amplifies DNA sequences) as well as other forensic analysis within the device itself. In addition to rapidly recognizing disease agents, the effective use of these automated diagnostics also allows human capacity and laboratory resources to be utilized in other areas of need. Although they are partly effective for detecting aerosolized agents, automated diagnostics suffer from high rates of false positives, mistaking background and normal levels of pathogens for biological weapons (Brown, 2004). In addition to focusing future research on improving sensitivity and accuracy of detection devices, the range of pathogens that are surveyed should be broadened beyond the high probability bioterrorism agents – especially because novel technologies allow the synthesis of a growing number of organisms.\n\nAffordability, availability, and proper implementation of better diagnostic tools represent some of biotechnology’s biggest benefits for improving health (Daar et al., 2002). For example, effective diagnosis of acute lower respiratory infection, if followed by proper treatment, would save over 400,000 lives each year (Lim et al., 2006). Equally optimistic predictions can be made for malaria, tuberculosis, and HIV (Girosi et al., 2006). Moreover, new generations of technologies in the pipeline, if they become more affordable, could revolutionize the future of disease detection. These include small nanotechnology based devices that can prepare and process biological samples and determine the nature of the pathogen in real time, effectively serving as an automated laboratory on a small chip (Yager et al., 2006).\n\nDespite technological improvements, traditional methods such as surveillance of blood samples and diagnosis by medical professionals remain of utmost importance, and must not be overlooked in the face of ‘hightech’ approaches of limited applicability. Diagnosis of anthrax in the 2001 bioterrorist attack, for example, was not made by sophisticated technologies, but by a vigilant clinician. Improvements in human diagnostic skills, however, is badly needed, especially for likely bioterrorist agents; a survey of 631 internal medicine residents in 2002–2003 demonstrated that only 47% were able to correctly diagnose simulated cases of smallpox, anthrax, botulism, and plague, better training for which increased the frequency to 79% (Bradbury, 2005; Cosgrove et al., 2005). Better diagnostic training, human capacity, and laboratory infrastructure are particularly important for parts of the developing world that suffer most from disease. Improving domestic and international disease surveillance capacity and infrastructure (in areas of human and animal health, as well as communication between the two communities) lacks the glamour of high-tech solutions, but remains one of the most important steps that needs to be taken (Chyba, 2001; Kahn, 2006).\n\n20.8.2 Collaboration and communication are essential for managing outbreaks\n\nWhile diagnosis of an unusual disease by a single astute physician can sound the alarm, detecting an epidemic for a disease that normally occurs at low frequencies requires the consolidation of regional surveillance data into a single database that is monitored for unusual trends. Once an outbreak is suspected or confirmed, it must be communicated to appropriate individuals and departments whose roles and responsibilities must be delineated in advance. Coordination, transparency, and timely sharing of information are of great importance – procedures which require oversight bodies and a clear chain of command.\n\nThe increasing volume in global trade and travel, and the rapid pace by which transmissible disease can spread, necessitate effective international communication and coordination. Electronic communication tools include the Program for Monitoring Emerging Diseases (ProMed) and the World Health Organization’s Global Public Health Information network (GPHIN). PROMED provides news, updates and discussion regarding global disease outbreaks, whereas the web-based early ‘disease warning system’, GPHIN, scans websites, blogs and media sources, gathers disease information, and reports unusual biological incidents. Coordination between countries is facilitated by the WHO’s Global Outbreak and Response Network (GOARN), which links over 100 different health networks that provide support for disease detection and response. During the 2004 SARS outbreak, the WHO established laboratories to link efforts among different countries, which resulted in rapid identification of the disease agent as a coronavirus. The organizationis in a position to provide similar support in the event of deliberate pandemics.\n\nCoordination within and between countries is also necessary to facilitate sharing of disease samples, which are used for production of vaccines and treatments. Determining whether an outbreak is intentional or natural can be difficult, unless forensic analysis can be performed on the genome or protein composition of the organism. For example, rapid sequencing of a genome may uncover artificially added pieces of DNA that confer antibiotic resistance or enhanced stability to an organism. Such findings might impact the design of appropriate drugs and therapies, but require the prompt availability of disease data and samples; the US Center for Disease Control and Prevention, for instance, has been criticized for not sharing flu data with other scientists (Butler, 2005).\n\n20.8.3 Mobilization of the public health sector\n\nOnce disease outbreaks are detected and communicated to the proper authorities, local agencies and individuals must assemble and respond to the public health crisis. Lack of preparation for responding to large-scale biological outbreaks can overwhelm the health care system, negatively impacting not just the disaster sector, but also the greater public health infrastructure. There have been speculations, for example, that Toronto’s effective response in curbing SARS placed pressure on other critical health care areas that resulted in a number of preventable deaths (IOM, 2004, p. 34). Emergency relief procedures should be established and practiced in advance, particularly for procedures that deal with surge capacity – the ability to expand beyond normal operations in order to deal with emergencies and disasters. Surge capacity involves enlisting medical personnel from other sectors. The ability to co-opt existing networks of local health care workers to perform disaster relief is an important element of a successful surge-capacity strategy. While local health care providers can address general disaster relief functions, more specialized responders are also instrumental for proper isolation and handling of hazardous biological materials, for selection of appropriate decontamination reagents, and for assessing risks to health and to the environment (Fitch et al., 2003).\n\n20.8.4 Containment of the disease outbreak\n\nThe rapid containment of a disease outbreak requires identification of the ‘hot-zone’ – the area that is contaminated. This is exceedingly difficult for contagious agents, particularly ones with long asymptomatic incubation periods during which disease can be transmitted to others and spread over large areas. Also difficult is containing novel agents whose mode of infection or transmissibility is not known. Both epidemiological tools and computer simulations may be used to help identify, isolate, and quarantine affected individuals, and to break the chain of infections.¹³ This was successfully accomplished during the SARS outbreak with the WHO leadership issuing timely and aggressive guidelines concerning quarantine procedures, curfews, and travel advisories.\n\nHalting disease spread also requires provisions to care for a large number of infected individuals, possibly in isolation from others in mobile hospitals or dedicated hospital wings, gymnasiums, or private homes. The public is most likely to respond well if there is effective dispersal of information through responsible media sources and credible internet sites such as the Center for Disease Control and Prevention and the World Health Organization. The use of telephone hotlines also proved to be an effective information dissemination tool during the SARS outbreak.\n\nIn addition to curbing social contacts, implementing curfews and quarantines, and halting public activities, other types of public health protection measures can be implemented in a disease outbreak. These include the decontamination of high-traffic areas like hospitals, schools, and masstransit facilities and implementing personal precautionary measures as regards hand washing and protective clothing, gloves, and masks. Masks should be worn by both the infected and the uninfected; the N-95 masks, so- called for its ability to block particles greater than 0.3 microns in size 95% of the time, is particularly effective and provided protection against the SARS virus; even though that virus is smaller than 0.3 microns, SARS travels in clumps, resulting in larger- sized particles that become trapped (IOM, 2004, p. 18). Personal protection is particularly important for health care workers and first responders who are in the front lines and more at risk of becoming infected; during the early stages of the SARS pandemic, a single patient, the ‘superspreader’, infected every one of 50 health workers who treated him. Fully contained suits and masks, depending on the nature of the pathogen, might be appropriate for health care workers. In addition, these individuals should also receive prophylaxis and immunizations, when available; the United States, for instance, encourages more than 40,000 medical and public health staff personnel to protect themselves against a potential smallpox outbreak by vaccination (Arita, 2005).\n\nBeyond health workers, determining who should receive treatment can be difficult to assess, particularly when supplies are limited or when there are detrimental side-effects of receiving treatment. These difficult choices can be minimized and avoided through aggressive drug and vaccine research, development, and production strategies.\n\n20.8.5 Research, vaccines, and drug development are essential components of an effective defence strategy\n\nDefending against likely disease outbreaks involves the stockpiling of vaccines, antibiotics and antivirals in multiple repositories. But for outbreaks that are less probable, cost and shelf-life considerations may favour last-minute strategies to rapidly produce large quantities of drugs only when they are needed. Drug and therapy development strategies require coordination between public health experts, life scientists, and the commercial sectors. Equally important is investing in basic science research, which is the cornerstone to understanding disease; decades of basic science research on viruses, bacteria and other organisms has been instrumental for rapid identification and characterization of novel biological agents, and for developing appropriate treatments and cures. (It is, of course, also this research that may bring with it the danger of misuse.) Efforts are needed to generate and promote a stronger global research capacity. This requires better funding mechanisms worldwide so that local scientists can address local health needs that are neglected by pharmaceutical companies that focus on expensive drug markets in the industrialized world. Encouraging industry to address infectious diseases includes providing incentives such as advance market commitments. The United States uses the ‘orphan drug legislation’ to provide tax credits to companies to invest in rare diseases that are otherwise not deemed profitable.\n\nEfforts to improve funding for addressing infectious and neglected diseases include those by The Bill and Melinda Gates Foundation, which provides grants with the precondition that any eventual product would be patent-free and publicly available. In the same spirit, the pharmaceutical giant Sanofi-Aventis and a non-profit drug development organization funded by ‘Doctors Without Borders’ have combined to create a cheap, patent-free Malaria pill (New York Times, March 5, 2007).\n\n20.8.6 Biological security requires fostering collaborations\n\nDue to the low availability of drugs and diagnostic tools, high population densities, and pre-existing health issues, developing countries may suffer the greatest consequences of a biological attack. In some places this is exacerbated by inadequate human resources and infrastructure, which contribute to less effective planning; more than 150 countries do not have national strategies to deal with a possible flu pandemic (Bonn, 2005). Based on current public health capabilities, it is estimated that were the 1918 Spanish Influenza to take place today, 95% of deaths would occur in the developing world (Murray et al., 2006). Moreover, global trends of increasing urbanization create high population densities that are breeding grounds for human pathogens and attractive targets for bioterrorists.\n\nImprovements in disease surveillance, together with better communication and coordination, should be a global priority. In an increasingly interconnected world, small-scale outbreaks can rapidly turn into pandemics, affecting lives and resources worldwide. The SARS outbreak, a relatively small pandemic, is estimated to have cost $40 billion in 2003 alone (Murray et al., 2006). Even local or regionally confined outbreaks can result in a decrease in the trade of goods, travel, and tourism, the impact of which is felt globally. Strengthening global tools to fight disease outbreaks, therefore, is sound governmental and intergovernmental policy for humanitarian reasons as well as for national and international security.\n\n20.9 Towards a biologically secure future\n\nInaddition to defensive measures like improved disease detection and response, a comprehensive biological security strategy must safeguard potentially dangerous biotechnologies. Despite some of the current difficulties in identifying and implementing safeguards, there are historical reasons for optimism regarding the response of the scientific community. In the past, when confronted with potentially hazardous research involving recombinant DNA technology,¹⁴ biologists took precautions by adopting guidelines and self-regulatory measures on a particular class of experiment. One reason for this success was that from the beginning biologists enlisted support from prestigious scientific academies (Chyba, 1980). These continue to provide a powerful tool today.\n\nA greater challenge for biotechnology nonproliferation will be the expansion of safeguards throughout the academic, commercial, and governmental scientific sectors, as well as the international implementation of these measures. Traditional nonproliferation conventions and arms control treaties predominantly address nation states and do not provide adequate models for dealing with the non-state aspects of the biotechnology dilemma (Chyba, 2006). But despite these shortcomings, novel and innovative efforts that safeguard biotechnology are beginning to take shape. Together with better disease detection and response, if accompanied by political will, these efforts may provide a multi-pronged approach of preventative and defensive measures that will help to ensure a more biologically secure future.\n\nSuggestions for further reading\n\nChristopher, F.C. (October 2006). Biotechnology and the Challenge to Arms Control. Arms Control Today. Available at http://www.armscontrol.org/act/2006\\_10/BioTech Feature.asp. This article deals with dual-use biotechnology, which, due to its increasingly accessible and affordable nature, provides unprecedented challenges to arms control. The article also reviews a number of strategies that are aimed at managing biotechnology risks, paying particular attention to proposals that have an international dimension.\n\nInstitute of Medicine of the National Academies. (2004). Learningfrom SARS, Preparing for the next disease outbreak. Washington, DC: National Academies Press. Learning from SARS, Preparingfor the next disease outbreak explores the 2002–2003 outbreak of a novel virus, SARS. The rapidly spreading virus, against which there was no vaccine, posed a unique challenge to global health care systems. The report examines the economic and political repercussions of SARS, and the role of the scientific community, public health systems, and international institutions in the halting of its spread.\n\nLederberg, J. (1999). Biological Weapons: Limiting the Threat (Cambridge, MA: The MIT Press). This book is a collection of essays that examine the medical, scientific, and political aspects of the BW threat, and strategies aimed at mitigating these threats. These essays explore the history of the development and use of offensive biological weapons, and policies that might be pursued to contain them.\n\nLeitenberg, M. (2005). Assessing the Biological Weapons and Bioterrorism Threat. U.S. Army War College. The spreading of Anthrax through the US postal system, and discoveries in Afghanistan that Al Qaeda was interested in procuring biological weapons, have contributed to shifting the context within which biological weapons are considered, to one that almost exclusively involves bioterrorism. This transformation in threat perception, together with a $30 billion, 4-year government spending package, arrived with inadequate threat assessments, which this book begins to provide.\n\nNational Research Council. (2006). Committee on Advances in Techology and the Prevention of their Application to Next Generation Biowarfare Threats, Globalization, Biosecurity, and the Future of the Life Sciences. Washington, DC: National Academies Press. Globalization, Biosecurity, and the Future of Life Sciences explores the current status and future projections of biomedical research in areas that can be applied to the production of biological weapons. The report explores and identifies strategies aimed at mitigating such threats.\n\nNational Research Council. (2003). Biotechnology Research in an Age of Terrorism: Confronting the ‘Dual Use’ Dilemma. Washington, DC: National Academies Press. The report addresses dual-use biotechnology and proposes a greater role for self-governance among scientists and journal editors. Other findings include identification of ‘experiments of concern’, which would be subjected to an approval process by appropriate committees. Proposals are put forward for the creation of an international forum aimed at mitigating biotechnology risks.\n\nReferences\n\nAltman, L.K. (15 November 1994). The doctor’s world; was there or wasn’t there a pneumonic plague epidemic? The New York Times.\n\nAnderson, R.M. and May, R.M. (1991). Infectious Diseases of Humans: Dynamics and Control (Oxford: Oxford University Press).\n\nArita, I. (October 2005). Smallpox vaccine and its stockpile in 2005. Lancet Infectious Disease, 5(10), 647–652.\n\nAschwanden, C. (9 February 2007). Freedom to fund. Cell, 128(3), 421–423.\n\nBhattacharjee, Y. (2 January 2004). Infectious diseases: second lab accident fuels fears about SARS. Science, 303(5654), 26–26.\n\nBlock, S.M. (1999). Living nightmares. In Drell, S.D., Sofaer, A.D., and Wilson, G.D. (eds.), The New Terror: Facing the Threat of Biological and Chemical Weapons. pp. 60–71 (Stanford, CA: Hoover Institution Press).\n\nBonn, D. (March 2005). Get ready now for the next flu pandemic. Lancet Infectious Disease, 5(3), 139–139.\n\nBradbury, J. (November 2005). More bioterrorism education needed. Lancet Infectious Disease, 5(11), 678–678.\n\nBrown, K. (22 June 2001). Florida fights to stop citrus canker. Science, 292(5525), 2275–2276.\n\nBrown, K. (27 August 2004). Biosecurity: up in the air. Science, 305(5688), 1228–1229.\n\nBúgl, H., Danner, J.P., Molinari, R.J., Mulligan, J.T., Park, H.O., Reichert, B., Roth, D.A., Wagner, R., Budowle, B., Scripp, R.M., Smith, J.A., Steele, S.J., Church, G., and Endy, D. (4 December 2006). A Practical Perspective on DNA Synthesis and Biological Security. For the proposal’s text, refer to http://pgen.us/PPDSS.htm\n\nButler, D. (22 September 2005). Flu researchers slam US agency for hoarding data. Nature, 437(7058), 458–459.\n\nCarlson, R. (2003). The pace and proliferation of biological technologies. Biosecurity and Bioterrorism: Biodefense Strategy, Practice, and Science, 1(3), 203–214.\n\nCello, J.P., http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&Cmd=Search &Term=A.V., Wimmer, E. (9 August 2002). Chemical synthesis of poliovirus cDNA: generation of infectious virus in the absence of natural template. Science, 297(5583), 1016–1018.\n\nChaudhri, G., Panchanathan, V., Buller, R.M., van den Eertwegh, A.J., Claassen, E., Zhou, J., de Chazal, R., Laman, J.D., and Karupiah, G. (15 June 2004). Polarized type 1 cytokine response and cell-mediated immunity determine genetic resistance to mousepox. Proceedings of the National Academy of Sciences, 101(24), 9057–9062.\n\nChurch, G.M. (21 May 2005). A Synthetic Biohazard Nonproliferation Proposal. Found at http://arep.med.harvard.edu/SBP/Church\\_Biohazard04c.html.\n\nChyba, C.F. (1980). The recombinant DNA debate and the precedent of Leo Szilard. In Lakoff, S.A. (ed.), Science and Ethical Responsibility. pp. 251–264 (London: Addison-Wesley).\n\nChyba, C.F. (2001). Biological terrorism and public health. Survival, 43(1), 93–106.\n\nChyba, C.F. (2002). Toward biological security. Foreign Affairs, 81(3), 121–136.\n\nChyba, C.F. (October 2006). Biotechnology and the challenge to arms control. Arms Control Today, Available at http://www.armscontrol.org/act/2006\\_10/BioTech Feature.asp.\n\nChyba, C.F. and Greninger, A.L. (Summer 2004). Biotechnology and bioterrorism: an unprecedented world. Survival, 46(2), 143–162.\n\nClarke, L. (1999). Mission Improbable: Using Fantasy Documents to Tame Disaster (Chicago: University of Chicago Press).\n\nCommission on the Intelligence Capabilities of the United States Regarding Weapons of mass Destruction. (2005). Report to the President of the United States (Washington, DC: U.S. Government Printing Office).\n\nCosgrove, S.E., Perl, T.M., Song, X., and Sisson, S.D. (26 September 2005). Ability of physicians to diagnose and manage illness due to category A bioterrorism agents. Archives of Internal Medicine, 165(17), 2002–2006.\n\nDaar, A.S., Thorsteinsdóttir, H., Martin, D.K., Smith, A.C., Nast, S., and Singer, P.A. (October 2002). Top ten biotechnologies for improving health in developing countries. Nature Genetics, 32(2), 229–232.\n\nDeclaration of the Second International Meeting on Synthetic Biology (29 May 2006). Berkeley, CA. Available at https://dspace.mit.edu/handle/1721.1 /18185\n\nEndy, D. Presentation available at http://openwetware.org/images/d/de/Enabling.pdf http://openwetware.org/images/d/de/Enabling.pdf Accessed June 2007.\n\nFitch, J.P., Raber, E., and Imbro, D.R. (21 November 2003). Technology challenges in responding to biological or chemical attacks in the civilian sector. Science, 302(5649), 1350 – 1354.\n\nFu, P. (June 2006). A perspective of synthetic biology: assembling building blocks for novel functions. Biotechnology Journal, 1(6), 690–699.\n\nGilmore R. (December 2004). US food safety under siege? Nature Biotechnology, 22(12), 1503–1505.\n\nGirosi, F., Olmsted, S.S., Keeler, E., Hay Burgess, D.C., Lim, Y.W., Aledort, J.E., Rafael, M.E., Ricci, K. A., Boer, R., Hilborne, L., Derose, K.P., Shea, M.V., Beighley, C.M., Dahl, C.A., and Wasserman, J. (23 November 2006). Developing and interpreting models to improve diagnostics in developing countries. Nature, 444 (Suppl.), 3–8.\n\nHazarika, S. (14 March 1995a). Plague’s origins a mystery. The New York Times.\n\nHazarika S. (5 January 1995b). Bypassed by plague, and tours. The New York Times.\n\nInstitute of Medicine. (2004). Learning from SARS, Preparing for the Next Disease Outbreak (Washington, DC: National Academies Press).\n\nJackson, R.J., Ramsay, A.J., Christensen, C.D., Beaton, S., Hall, D.F., and Ramshaw, I.A. (February 2001). Expression of mouse interleukin-4 by a recombinant ectromelia virus suppresses cytolytic lymphocyte responses and overcomes genetic resistance to mousepox. Journal of Virology, 75(3), 1205–1210.\n\nJernigan, D.B., Raghunathan, P.L., Bell, B.P., Brechner, R., Bresnitz, E.A., Butler, J.C., Cetron, M., Cohen, M., Doyle, T., Fischer, M., Greene, C., Griffith, K.S., Guarner, J., Hadler, J.L., Hayslett, J.A., Meyer, R., Petersen, L.R., Phillips, M., Pinner, R., Popovic, T., Quinn, C.P., Reefhuis, J., Reissman, D., Rosenstein, N., Schuchat, A., Shieh, W.J., Siegal, L., Swerdlow, D.L., Tenover, F.C., Traeger, M., Ward, J.W., Weisfuse, I., Wiersma, S., Yeskey, K., Zaki, S., Ashford, D.A., Perkins, B.A., Ostroff, S., Hughes, J., Fleming, D., Koplan, J.P., Gerberding, J.L., and National Anthrax Epidemiologic Investigation Team (October 2002). Investigation of bioterrorismrelated anthrax, United States, 2001: epidemiologic findings. Emerging Infectious Diseases, 8(10), 1019–1028.\n\nKahn, L.H. (April 2006). Confronting zoonoses, linking human and veterinary medicine. Emerging Infectious Diseases, 12(4), 556–561.\n\nKaiser, J. (27 April 2007). Biodefense: proposed biosecurity review plan endorses selfregulation. Science, 316(5824), 529–529.\n\nKampala Compact: The Global Bargain for Biosecurity and Bioscience. (1 October 2005). Available at http://www.icsu-africa.org/resourcecentre.htm\n\nKaplan, D.E. and Marshall, A. (1996). The Cult at the End of the World: The Terrifying Story of the Aum Doomsday Cult, from the Subways of Tokyo to the Nuclear Arsenals of Russia. Random House. New York.\n\nLartigue, C., Glass, J.I., Alperovich, N., Pieper, R., Parmar, P.P., Hutchison, C.A. III, Smith, H.O., and Venter, J.C. (28 June 2007). Genome transplantation in bacteria: changing one species to another. Science (electronic publication ahead of print).\n\nLeitenberg, M. (2005). Assessing the Biological Weapons and Bioterrorism Threat. U.S. Army War College.\n\nLim, Y.W., Steinhoff, M., Girosi, F., Holtzman, D., Campbell, H., Boer, R., Black, R., and Mulholland, K. (23 November 2006). Reducing the global burden of acute lower respiratory infections in children: the contribution of new diagnostics. Nature, 444 (Suppl. 1), 9–18.\n\nLundstrom, M. (January 2003). Enhanced: Moore’s law forever? Science, 299(5604), 210–211.\n\nMoore, G. (19 April 1965). Cramming more components onto integrated circuits. Electronics Magazine, 114–117.\n\nMurray, C.J., Lopez, A.D., Chin, B., Feehan, D., and Hill, K.H. (23 December 2006). Estimation of potential global pandemic influenza mortality on the basis of vital registry data from the 1918–20 pandemic: a quantitative analysis. The Lancet, 368(9554), 2211–2218.\n\nNational Commission on Terrorist Attacks Upon the United States. (2004). The 9/11 Commission Report (New York: W.W. Norton).\n\nNational Research Council. (2003a). Biotechnology Research in an Age of Terrorism: Confronting the ‘Dual Use’ Dilemma. (Washington, DC: National Academies Press).\n\nNational Research Council. (2003b). Countering Agricultural Bioterrorism (Washington, DC: National Academies Press).\n\nNational Research Council. (2006) Committee on Advances in Techology and the Prevention of their Application to Next Generation Biowarfare Threats, Globalization, Biosecurity, and the Future of the Life Sciences.\n\nNormile, D. (3 November 2006). Bioterrrorism agents: panel wants security rules applied to genomes, not pathogens. Science, 314(5800), 743a.\n\nReinicke, W. (1998). Global Public Policy: Governing Without Government? (Washington, DC: Brookings Institution).\n\nRischard, J.F. (2002). High Noon: Twenty Global Problems, Twenty Years to Solve Them (New York: Basic Books).\n\nRoffey, R., Hart, J., and Kuhlau, F. (September 2006). Crucial guidance: a code of conduct for biodefense scientists. Arms Control Today.\n\nRubins, K.H., Hensley, L.E., Jahrling, P.B., Whitney, A.R., Geisbert, T.W., Huggins, J.W., Owen, A., Leduc, J.W., Brown, P.O., and Relman, D.A. (19 October 2004). The host response to smallpox: analysis of the gene expression program in peripheral blood cells in a nonhuman primate model. Proceedings of the National Academy of Sciences, 101(42), 15190–15195.\n\nSen, G.L. and Blau, H.M. (July 2006). A brief history of RNAi: the silence of the genes. FASEB Journal, 20(9), 1293–1299.\n\nService, R.F. (17 March 2006). Gene sequencing: the race for the $1000 genome. Science, 311(5767), 1544–1546.\n\nShivaji, S., Bhanu, N.V., and Aggarwal, R.K. (15 August 2000). Identification of Yersinia pestis as the causative organism of plague in India as determined by 16S rDNA sequencing and RAPD-based genomic fingerprinting. FEMS Microbiology Letters, 189(2), 247–252.\n\nSlaughter, A.M. (2004). A New World Order (Princeton: Princeton University Press).\n\nSnyder, L. and Pate, J. (2002). Tracking Anthrax Hoaxes and Attacks. Available at http://cns.miis.edu/pubs/week/020520.htm\n\nSteinbruner, J., Harris, E.D., Gallagher, N., and Okutani, S.M. (2005). Controlling Dangerous Pathogens: A Prototype Protective Oversight System. Available at http://www.cissm.umd.edu/papers/files/pathogens\\_ project\\_monograph.pdf\n\nStone, R. (24 May 2002). World health body fires starting gun. Science, 296(5572), 1383.\n\nTucker, J.B. (2001). Scourge: The Once and Future Threat of Smallpox (New York: Grove Press).\n\nTucker, J.B. and Zilinskas, R.A. (Spring 2006). The promise and perils of synthetic biology. The New Atlantis, 12, 25–45.\n\nTumpey, T.M., Basler, C.F., Aguilar, P.V., Zeng, H., Solórzano, A., Swayne, D.E., Cox, N.J., Katz, J.M., Taubenberger, J.K., Palese, P., and García-Sastre, A. (7 October 2005). Characterization of the reconstructed 1918 Spanish influenza pandemic virus. Science, 310(5745), 77–80.\n\nUnit 731 Criminal Evidence Museum. (2005). Unit 731: Japanese Germ Warfare Unit in China. China Intercontinental Press.\n\nWheelis, M. (September 2002). Biological warfare at the 1346 Siege of Caffa. Emerging Infectious Diseases, 8(9), 971–975.\n\nWord Health Organization (WHO). (2004). World Health Report 2004, Statistical Annex. pp. 120–121.\n\nWHO smallpox fact sheet. http://www.who.int/mediacentre/factsheets/smallpox/en/ http://www.who.int/mediacentre/factsheets/smallpox/en/ Accessed on 7 July 2007.\n\nYager, P., Edwards, T., Fu, E., Helton, K., Nelson, K., Tam, M.R., and Weigl, B.H. (27 July 2006). Microfluidic diagnostic technologies for global public health. Nature, 442(7101), 412–418.\n\n• 21 • Nanotechnology as global catastrophic risk\n\nChris Phoenix and Mike Treder\n\nThe word ‘nanotechnology’ covers a broad range of scientific and technical disciplines. Fortunately, it will not be necessary to consider each separately in order to discuss the global catastrophic risks of nanotechnology, because most of them, which we will refer to collectively as nanoscale technologies, do not appear to pose significant global catastrophic risks. One discipline, however, which we will refer to as molecular manufacturing, may pose several risks of global scope and high probability.\n\nThe ‘nano’ in nanotechnology refers to the numeric prefix, one-billionth, as applied to length: most structures produced by nanotechnology are conveniently measured in nanometres. Because numerous research groups, corporations, and governmental initiatives have adopted the word to describe a wide range of efforts, there is no single definition; nanotechnology fits loosely between miniaturization and chemistry. In modern usage, any method of making or studying sufficiently small structures can claim, with equal justice, to be considered nanotechnology. Although nanoscale structures and nanoscale technologies have a wide variety of interesting properties, most such technologies do not pose risks of a novel class or scope.\n\nInterest in nanotechnology comes from several sources. One is that objects smaller than a few hundred nanometres cannot be seen by conventional microscopy, because the wavelength of visible light is too large. This has made such structures difficult to study until recently. Another source of interest is that sufficiently small structures frequently exhibit different properties, such as colour or chemical reactivity, than their larger counterparts. A third source of interest, and the one that motivates molecular manufacturing, is that a nanometre is only a few atoms wide: it is conceptually (and often practically) possible to specify and build nanoscale structures at the atomic level.\n\nMost nanoscale technologies involve the use of large machines to make tiny and relatively simple substances and components. These products are usually developed to be integral components of larger products. As a result, the damage that can be done by most nanoscale technologies is thus limited by the means of production and by the other more familiar technologies with which it will be integrated; most nanoscale technologies do not, in and of themselves, appear to pose catastrophic risks, though the new features and augmented power of nano-enabled products could exacerbate a variety of other risks.\n\nMolecular manufacturing aims to exploit the atomic granularity and precision of nanoscale structures, not only to build new products, but also to build them by means of intricate nanoscale machines, themselves the products of molecular manufacturing techniques. In other words, precisely specified nanoscale machines would build more nanoscale machines by guiding molecular (chemical) processes. This implies that once a certain level of functionality is reached (specifically, the level at which a generalpurpose machine can build its physical duplicate and variants thereof), the nanoscale can become far more accessible, products can become far more intricate, and development of further capabilities could be rapid. The molecular manufacturing approach to nanotechnology may unleash the full power of the nanoscale on the problems of manufacturing – creating products of extreme power in unprecedented abundance. As we will see, the success of this approach could present several global catastrophic risks.\n\nIt should not be overlooked that molecular manufacturing could create many positive products as well. Stronger materials and more efficient mechanisms could reduce resource usage, and non-scarce manufacturing capacity could lead to rapid replacement of inefficient infrastructures. It is also plausible that distributed (even portable), general-purpose, lower-cost manufacturing could allow less-developed and impoverished populations to bootstrap themselves out of deprivation. Reductions in poverty and resource constraints could work to reduce some of the risks described in this chapter. Although the focus here is on the most severe dangers, we support responsible development of molecular manufacturing technology for its benefits, and we promote study and understanding of the technology as an antidote to the threats it poses.\n\n21.1 Nanoscale technologies\n\n21.1.1 Necessary simplicity of products\n\nA common characteristic of nanoscale technologies is that they are not suitable for manufacturing finished products. In general, the method of production for nanoscale components is simple and special-purpose, and can only build a material or component that subsequently must be included in a larger product through other manufacturing methods. For example, a carbon nanotube may be deposited or grown by an innovative process on a computer chip, but the chip then must be packaged and installed in the computer by traditional means. Ultraviolet-blocking nanoparticles may be manufactured by an innovative process, but then must be mixed into sunscreen with mostly ordinary ingredients. A related point is that nanoscale technologies make use of machines far larger in scale than their output.\n\nAccess to the nanoscale is typically indirect, painstaking, or both, and most manufacturing processes are unable to convey detailed information to the nanometre realm. The resulting nanoscale products, therefore, cannot be highly structured and information-rich, even by comparison with today’s manufactured products; every feature must be either miniscule, highly repetitive, or random. Large quantities of identical or nearly identical particles and molecular constructs can be built, but they will be either amorphous, highly ordered to the point of simplicity (like a crystal), organized by a prebuilt template, or have a partially random structure influenced by only a few parameters. Computer chips, which now contain millions of nanoscale features, are an exception to this, but they can only be built with the aid of ‘masks’ that are immensely expensive, slow, and difficult to produce. This is another reason why nanoscale technologies generally are not suitable for manufacturing finished products.\n\nBecause the outputs of nanoscale technologies can typically be used only as inputs to traditional manufacturing or material-use steps, their scope and impact necessarily will be limited by the non-nanotech components of the products and the non-nanotech manufacturing steps. For the most part, nanoscale technologies can be viewed as an addition to the existing array of industrial technologies. In some cases, nanoscale structures including nanoparticles may be released into the environment. This could happen deliberately as part of their use, due to accidents including unintended product destruction, or as part of the product lifecycle (including manufacturing processes and product disposal).\n\n21.1.2 Risks associated with nanoscale technologies\n\nThe risks of nanoscale technologies appear in two broad classes, which are analogous to existing industrial risks. The first class is risks resulting from the availability and use of the eventual products. In this way, the nanoscale technology contributes only indirectly, by making products more powerful, more dangerous, more widely used, and so on. The second class is risks resulting from new materials that may cause inadvertent harm.\n\nDepending on the application, risks may be increased by making improved products. Improved medical technology frequently must confront ethical issues. Some applications of nanoscale technology (such as using nanoparticles to treat cancer) will have strong demand and little downside beyond the standard medical issues of creating good treatment protocols. The use of nanoscale technologies for military medicine (ISN, 2005) or advanced goals such as anti-aging may be more controversial. But in any case, these risks and issues are only incremental over existing risks; they are not new classes of risk, and there do not appear to be any global catastrophic risks in nanoscale health care technology.\n\nNanoscale technologies may contribute indirectly to the development of weaponized pathogens, which arguably could lead to an existential risk. Smaller and more powerful research tools could be used to fine-tune or accelerate the development process. However, the same tools also would be useful to counter a biological attack. The SARS virus was sequenced in only six days in 2003 (Bailey, 2003). The development of sufficiently fast, sensitive, and inexpensive tests could greatly reduce the threat of almost any infectious disease. Thus, it is not yet clear whether nanoscale technologies will increase or reduce the risk of globally catastrophic pandemics.\n\nImproved computers may produce new classes of risk, some of which arguably might be existential. Surveillance technology is improving rapidly, and will continue to improve as data mining, data processing, and networking become cheaper. A state that knows every movement and action of its populace would have power unequalled by history’s most repressive regimes. Misuse of that power on a global scale could pose a catastrophic risk. However, this risk cannot be blamed entirely on nanoscale technologies, since it seems likely that sufficiently powerful computers could be developed anyway. Another possible risk stemming from improved computers is artificial intelligence (see Chapter 15, this volume).\n\nEnvironmental or health risks from inadvertent releases of nanoscale materials will not be existential. That is not to say that there is no risk. Just like any industrial material, new nanoscale materials should be evaluated for toxicology and environmental impacts. Some nanoparticles may have high stability and environmental persistence, may migrate through soils or membranes, and may be chemically active; these are all reasons to study them closely. Information about the health and environmental impacts of nanoparticles is still far from complete, but at this writing their risks seem to fall into the familiar range of industrial chemicals. Some nanoparticles will be mostly harmless, while others may be quite toxic. Although it is possible to imagine scenarios in which the manufacture of sufficient quantities of sufficiently toxic particles would threaten the world in the event of accidental release, such scenarios seem unlikely in practice.\n\n21.2 Molecular manufacturing\n\nAs nanotechnology develops, it is becoming possible to plot a continuum between near-term nanoscale technologies and molecular manufacturing. It begins with a convergence of increasingly complicated molecules, improved facility in mechanical (e.g., scanning probe) chemistry, and more powerful and reliable simulations of chemistry. It is now possible for mainstream nanotechnology researchers to imagine building machine-like constructions with atomic precision. Once this is achieved, the next step would be to harness these machines to carry out an increasing fraction of manufacturing operations. Further advances in machine design and construction could lead to integrated large-scale manufacturing systems building integrated largescale products, with both the manufacturing systems and the products taking advantage of the (expected) high performance of atomically precise nanoscale machinery and materials.\n\nThe molecular manufacturing approach is expected by several researchers, including Robert Freitas, Ralph Merkle, and Eric Drexler, to lead to ‘nanofactories’ weighing a kilogram or more, capable of producing their own weight in product from simple molecules in a matter of hours. Because of the immense number of operations required, the factory is expected to be entirely computer-controlled; this is thought to be possible without advanced error-correcting software because of the high precision inherent in molecular construction (Phoenix, 2006). And because the manufacturing process would be general-purpose and programmable, it is expected to be able to produce a range of products including nanofactories and their support structure.\n\nAny step along the pathway to molecular manufacturing would represent an advance in nanotechnology. But the point at which nanofactories become able to build more nanofactories seems particularly noteworthy, because it is at this point that high-tech manufacturing systems could become, for the first time in history, non-scarce. Rather than requiring cutting-edge laboratory or industrial equipment to produce small amounts of nanoscale products, a nanofactory would be able to build another nanofactory as easily as any other product, requiring only blueprints, energy, and feedstock. Thus, the cost of nanofactorybuilt products would owe very little to either labour or (physical) capital.\n\nThe supply of nanofactory-built products, as well as their cost, would depend on which resource – information, feedstock, or energy – was most difficult to obtain at point of use (the nanofactory itself).\n\n• Information costs very little to store and copy; although it might be limited by regulations such as intellectual property laws, this would not represent a natural limit to the use of nanofactory-based manufacturing.\n\n• Feedstock would be a small molecule, used in bulk. The molecule has not been specified yet, so its cost and availability cannot be determined, but the relatively low cost and high production of manufactured gases such as acetylene and ammonia indicates that nanofactory feedstock may contribute only a few dollars per kilogram to the price of the product.\n\n• The largest cost of product fabrication may be energy. A preliminary analysis of a primitive nanofactory architecture was carried out recently (Phoenix, 2005), which calculated that building a product might require approximately 200 kWh/kg. Although this is significant, it is comparable with the energy cost of refining aluminum, and the material fabricated by the nanofactory would probably have significantly greater strength than aluminum (comparable to carbon nanotubes), resulting in lower weight products.\n\nAs general-purpose manufacturing systems, it is quite possible that nanofactories would be able to build feedstock processing plants and solar collectors. In that case, the quantity of products built would seem to be potentially unlimited by any aspect of the present-day industrial infrastructure, but rather by the resources available from the environment: sunlight and light elements such as carbon, none of which is in short supply. The time required for a nanofactory to build another nanofactory might be measured in days or perhaps even hours (Phoenix, 2005). With strong, lightweight materials, the time required to build another nanofactory and all its supporting infrastructure, thus exponentially doubling the available manufacturing capacity, might be as little as a few days (though without detailed designs this can be only speculation).\n\n21.2.1 Products of molecular manufacturing\n\nNanofactory-built products (including nanofactories) would potentially enjoy a number of advantages over today’s products. Certain atomically precise surfaces have been observed to have extremely low friction and wear (Dienwiebel et al., 2004), and it is hoped that these attributes could be constructed into nanomachines. Smaller machines work better in several respects, including greater power density, greater operating frequency, far greater functional density, and greater strength with respect to gravity (Drexler, 1992). Being built at the scale of atoms and molecules, the machines would be able to perform a number of medically significant procedures (Freitas, 1999). Atomically precise materials in appropriate structures could be far stronger than today’s building materials (Drexler, 1992).\n\nBuilding on the basic set of capabilities analysed by Drexler (1992), a number of researchers have worked out designs for products in varying degrees of detail. Freitas has analysed a number of basic capabilities relevant to medical devices, and has described several medical nanorobots in detail. Freitas has proposed devices to collect and neutralize harmful microbes (Freitas, 2005), to supplement the gas-transport function of blood (Freitas, 1998), and to replace damaged chromosomes (Freitas, 2007), among others. For everyday use, masses of relatively simple robots a few microns wide could cooperate to reconfigure themselves in order to simulate a wide range of shapes and conditions (Hall, 1996). Hall has also undertaken preliminary investigation of small aircraft with several innovative features.\n\nDetailed designs for larger products have been scarce to date. An exception is the architectural and scaling study of nanofactory design mentioned earlier (Phoenix, 2005); this 85-page paper considered numerous factors including energy use and cooling, physical layout, construction methods, and reliability, and concluded that a kilogram-scale, desktop-size, monolithic nanofactory could be built and could build duplicates in a few hours. Larger nanofactories building larger products, up to ton-scale and beyond, appear feasible. But in the absence of detailed plans for building and assembling nanomachines, it has not generally been feasible to design other products at high levels of detail. Nevertheless, analyses of extreme high performance at the nanoscale, as well as expectations of being able to integrate nearly unlimited numbers of functional components, have led to a variety of proposed applications and products, including the rapid construction of light weight aerospace hardware (Drexler, 1986).\n\nGeneral-purpose manufacturing would allow the rapid development of new designs. A manufacturing system that was fully automated and capable of making complete products from simple feedstock could begin making a new product as soon as the design was ready, with no need for retraining, retooling, or obtaining components. A product designer developing a new design could see it produced directly from blueprints in a matter of hours. The cost of manufacture would be no higher for a prototype than for full-scale manufacturing. This would benefit designers in several ways. They would be able to see and evaluate their products at several stages in the development process. They would not have to spend as much effort on getting the design right before each prototype was produced. They would not have to make an additional design for full-scale manufacture. With these constraints removed, they could be considerably more aggressive in the technologies and techniques they chose to include in a product; if an attempted design failed, they would lose relatively little time and money.\n\nOnce a design was developed, tested, and approved, its blueprint could be distributed at minimal cost (as computer files on the Internet), ready for construction as and where needed. This means that the cost to deliver a new product also would be minimal. Rather than having to manufacture, ship, and warehouse many copies of a design whose success is not assured, it could be built only when purchased. This would further reduce the risk associated with developing innovative products. If a design was successful, it could be manufactured immediately in as many copies as desired.\n\n21.2.2 Nano-built weaponry\n\nBecause weapons figure in several global catastrophic risks, it is necessary to discuss briefly the kinds of weapons that might be built with molecular manufacturing. Increased material strength could increase the performance of almost all types of weapons. More compact computers and actuators could make weapons increasingly autonomous and add new capabilities. Weapons could be built on a variety of scales and in large quantities. It is possible, indeed easy, to imagine combining such capabilities: for example, one could imagine an uncrewed airplane in which 95% of the dry weight is cargo, the said cargo consisting of thousands of sub-kilogram or even sub-gram airplanes that could, upon release, disperse and cooperatively seek targets via optical identification, and then deploy additional weapons capabilities likewise limited mainly by imagination.\n\nThe size of the gap between such speculation and actual development is open to debate. Smart weapons presumably would be more effective in general than uncontrolled weapons. However, it will be a lot easier to cut-and-paste a motor in a computer-aided design programme than to control that motor as part of a real-world robot. It seems likely, in fact, that software will require the lion’s share of the development effort for ‘smart’ weapons that respond to their environment. Thus, the development of novel weapon functionality may be limited by the speed of software development.\n\nTo date, there does not appear to have been a detailed study of molecular manufacturing-built weapons published, but it seems plausible that a single briefcase full of weaponry could kill a large percentage of a stadium full of unprotected people (to take one scenario among many that could be proposed). Small robots could implement some of the worst properties of land mines (delayed autonomous action), cluster bombs (dispersal into small lethal units), and poison gas (mobile and requiring inconvenient degrees of personal protection). A wide variety of other weapons may also be possible, but this will suffice to put a lower bound on the apparent potential destructive power of molecular manufacturing-built products.\n\nAn idea that has caused significant concern (Joy, 2000) since it was introduced two decades ago (Drexler, 1986) is the possibility that small, self-contained, mobile, self-copying manufacturing systems might be able to gain sufficient resources from the ecosphere to replicate beyond human control. Drexler’s original concern of accidental release was based on a now-obsolete model of manufacturing systems (Phoenix and Drexler, 2004). However, there is at least the theoretical possibility that someone will design and release such a thing deliberately, as a weapon (though for most purposes it would be more cumbersome and less effective than non-replicating weapons) or simply as a hobby. Depending on how small such a device could be made, it might be quite difficult to clean up completely; furthermore, if made of substances not susceptible to biological digestion, it might not have to be very efficient in order to perpetuate itself successfully.\n\n21.2.3 Global catastrophic risks\n\nMolecular manufacturing, if it reaches its expected potential, may produce three kinds of risk: (1) as with nanoscale technologies, molecular manufacturing may augment other technologies and thus contribute to the risks they present; (2) molecular manufacturing may be used to build new products that may introduce new risk scenarios depending on how they are used by people; (3) molecular manufacturing may lead to self-perpetuating processes with destructive side effects. At the same time, however, molecular manufacturing may help to alleviate several other catastrophic risks.\n\nRapid prototyping and rapid production of high-performance nanoscale and larger products could speed up innovation and R&D in a wide variety of technologies. Medicine is an obvious candidate. Aerospace is another; constructing new airframes and spacecraft tends to be extremely labourintensive and expensive, and the ability to rapid-prototype finished test hardware at relatively low cost may allow significantly more aggressive experimentation. If molecular manufacturing is developed within the next 20 years, then it would be capable of building computers far more advanced than Moore’s Law would predict (Drexler, 1992). Each of these technologies, along with several others, is associated with catastrophic risks; conversely, medicine and aerospace may help to avert risks of plague, asteroid impact, and perhaps climate change.\n\nIf molecular manufacturing fulfils its promise, the products of molecular manufacturing will be inexpensive and plentiful, as well as unprecedentedly powerful. New applications could be created, such as fleets of high-altitude uncrewed aircraft acting as solar collectors and sunshades, or dense sensor networks on a planetary scale. General-purpose manufacturing capacity could be stockpiled and then used to build large volumes of products quite rapidly. Robotics could be advanced by the greatly increased functional density and decreased cost per feature associated with computer-controlled nanoscale manufacturing. In the extreme case, it may even make sense to speak of planet-scale engineering, and of modest resources sufficing to build weapons of globally catastrophic power.\n\n21.2.3.1 Global war\n\nIf molecular manufacturing works at all, it surely will be used to build weapons. A single manufacturing system that combines rapid prototyping, mass manufacturing, and powerful products could provide a major advantage to any side that possessed it. If more than one side had access to the technology, a fast-moving arms race could ensue. Unfortunately, such a situation is likely to be unstable at several different points (Altmann, 2006). A number of players would want to enter the race. Uncertainty over the future, combined with a temporary perceived advantage, could lead to preemptive strikes. And even if no one deliberately launched a strike, interpenetrating forces with the necessary autonomy and fast reaction times could produce accidental escalation.\n\nDuring the Cold War period, the world’s military power was largely concentrated in two camps, one dominated by the United States and the other by the Soviet Union. As both sides continued to develop and stockpile massive amounts of nuclear weapons, the doctrine of Mutually Assured Destruction (MAD) emerged. Full-scale war could have resulted in the annihilation of both powers, and so neither one made the first move.\n\nUnfortunately, many of the factors that allowed MAD to work in deterring World War III may not be present in an arms race involving nano-built weapons:\n\n1. The Cold War involved only two primary players; once a rough parity was achieved (or perceived), the resulting standoff was comparatively stable. Unless the ability to make nano-weapons is somehow rigorously restricted, a large number of nations can be expected to join the nanotech arms race, and this could create a situation of extreme instability.\n\n2. Acquiring the capability to use nuclear weapons is an expensive, slow, and difficult process. It is therefore relatively easy to track nations that are seeking to gain or expand a nuclear fighting potential. By contrast, the capability to make weapons with molecular manufacturing will be very inexpensive, easy to hide (in the absence of near-total surveillance), and can be expanded rapidly. A ‘starter’ nanofactory could be smuggled from place to place more easily than a stick of gum, then used to build more and larger nanofactories.\n\n3. Rapidly shifting balances of military power may create an atmosphere of distrust. Greater uncertainty of the capabilities of adversaries could foster caution – but it also could increase the temptation for preemptive strikes to prevent proliferation.\n\nFollowing the collapse of the Soviet Union, another factor emerged to keep the likelihood of full-scale war at a low level. This was the growing economic interdependence of nations: the global economy. It is said that democracies rarely attack one another, and that’s also true for trading partners (Lake, 1996; Orr, 2003).\n\nBut the proliferation of civilian molecular manufacturing capability could reduce global trade (McCarthy, 2005), at least in physical goods. Any nation with nanofactories would be able to provide virtually all their own material needs, using inexpensive, readily available raw materials. As economic interdependence disappears, a major motivation for partnership and trust also may be substantially reduced (Treder, 2005). Trade in information may represent an important economic exchange between countries that have compatible intellectual property systems but cannot be counted on to stabilize relations between any given pair of nations; indeed, the ease of copying information may lead to increased tensions due to ‘theft’ of potential earnings.\n\nToday, the destruction wrought by war is an incentive not to engage in it; a nation may very well make more profit by trading with an intact neighbour than by owning a shattered neighbour. But if molecular manufacturing enables rapid inexpensive manufacture, it might be possible to rebuild quickly enough that war’s destruction would represent less of an economic penalty for the victor.\n\nBetter sensors, effectors, communications, and computing systems – made at very low cost – may enable deployment of teleoperated robot ‘soldiers’ able to occupy territory and carry out missions without risk to human soldiers. There could be a wide variety of such ‘soldiers’, including ground-, water-, and airbased robots in a wide range of sizes from a few grams to many tons. Each robot might be directly teleoperated or partially autonomous (e.g., able to navigate to a programmed location). It is important to realize that, while stronger materials and greater power efficiency would increase the performance of the robot somewhat, computers many orders of magnitude, more compact and powerful than today’s, would enable algorithms that are barely in research today to be deployed in even the smallest robots. (Robots smaller than a gram might have difficulty in locomotion, and small robots would also be limited in the payloads they could carry.)\n\nA consequence of removing humans from the battlefield is to make war potentially less costly to the aggressor, and therefore more likely. Removal of humans from the battlefield (at least on one side), and the likely advances in ‘less lethal’ weapons aided by advanced nanotechnology, could reduce the moral sanctions against wars of aggression. It is tempting to think that removal of humans from the battlefield would make wars less destructive, but history shows that this argument is at best overly simple. Automated or remotecontrolled weapons, rather than removing humans from the field of battle, instead may make it easier to take the battlefield to the humans. Although these new weapons might shift the focus of conflict away from conventional battlefields, new battlefields would likely develop, and many of them could overlap and overwhelm civilian populations.\n\nFinally, big wars often arise from small wars: with trust factors rapidly declining, the potential for escalation of internecine or regional wars into larger conflagrations will be substantially higher. A significant (and sustainable) imbalance of power, wherein some government (whether national or international) has access to far more force than any of the combatants, could prevent small wars and thus their growth into big wars. However, recent history shows that it is quite difficult for even a powerful government to prevent local conflict or civil war, and it is far from certain that even the technological power of molecular manufacturing, as used by fallible political institutions, would be able to prevent small wars.\n\nThe desire to avoid unexpected destructive conflict would in theory provide a counterbalance to these destabilizing factors. But recognition of the danger probably will not be sufficient to avoid it, particularly when failing to develop and deploy advanced weapon systems may be tantamount to unilateral disarmament. An alternative, which might be especially attractive to an actor perceiving that it has a definite lead in nano-based military technology, could be consolidation of that lead by force. Attempts to do so might succeed, but if they did not, the outcome may well be exactly the destructive escalating conflict that the pre-emptive actor wanted to avoid.\n\n21.2.3.2 Economic and social disruption\n\nIt is unclear at this point how rapidly molecular manufacturing might displace other kinds of manufacturing, and how rapidly its products might displace established infrastructures and sources of employment. A sufficiently general manufacturing technology, combined with advanced inexpensive robotics (even without major Artificial Intelligence advances), could in theory displace almost all manufacturing, extraction, and transportation jobs, and many service jobs as well. If this happened slowly, there might be time to find new sources of employment and new social systems. But if it happened quickly, large numbers of people might find themselves economically superfluous. This would tend to facilitate their oppression as well as reducing their motivation to produce and innovate. In an extreme scenario, the resulting loss of human potential might be considered catastrophic.\n\nAnother potential problem raised by distributed general-purpose manufacturing is a variety of new forms of crime. Even something as simple as a portable diamond saw capable of quickly cutting through concrete could facilitate breaking and entering. Medical devices might be used for illegal psychoactive purposes. Sensors could be used to violate privacy or gather passwords or other information. New kinds of weapons might enable new forms of terrorism. These are not new classes of problems, but they might be exacerbated if nanofactory technology were available to the general public or to organized crime. Although it seems unlikely that crime by itself could constitute a catastrophic risk, sufficient levels of crime could lead to social breakdown and/or oppressive governance, which might result in significant risk scenarios.\n\n21.2.3.3 Destructive global governance\n\nAny structure of governance is limited in scope by the technology available to it. As technology becomes increasingly powerful, it raises the possibility of effective governance on a global scale. There are a number of possible reasons why such a thing might be tried, a number of forms it could take, and likewise a number of pathways leading up to it and a number of potential bad effects. Molecular manufacturing itself, in addition to supplying new tools of governance, may supply several new incentives which might tend to promote attempts at global governance.\n\nAs discussed above, molecular manufacturing may enable the creation of new forms of weapons and/or newly powerful weapons on an exceptionally large scale. Along with other kinds of nanotechnology and miniaturization, it also may produce small and capable sensor networks. It should be noted that a single unrestricted nanofactory, being fully automated, would be able to build any weapon or other device in its repertoire with no skill on the part of the user (use of the weapon might or might not require special skills). Building large weapons would simply require building a bigger nanofactory first. Most governments would have a strong incentive to keep such capabilities out of the hands of their citizens; in addition, most governments would not want potentially hostile foreign populations to have access to them.\n\nIn addition to keeping excessively destructive capabilities out of private hands, both at home and abroad, many governments will be concerned with avoiding the possibility of a high-tech attack by other governments. If an arms race is seen as unstable, the alternative would seem to be the disarming of rival governments. This in turn would imply a global power structure capable of imposing policy on nations.\n\nThe analysis to this point suggests a winner-take-all situation, in which the first nation (or other group) to get the upper hand has every incentive to consolidate its power by removing molecular manufacturing from everyone else. Gubrud (1997) argues that development of molecular manufacturing will not be so uneven and that no one will have a clear advantage in the race, but at this point that is unclear. Given the potential for rapid exponential growth of manufacturing capacity, and given foresighted preparation for rapid design, it seems plausible that even a few months’ advantage could be decisive. Unless some foolproof safeguard can be devised, a policy of ruthless pre-emption may be adopted by one or more parties. If this calculation were generally accepted, then the first to develop the capability would know that they were first, because otherwise they would have been pre-empted; this temporary certainty would encourage an immediate strike. The ensuing struggle, if it did not result in a terminal catastrophic war, could quite possibly produce a global dictator.\n\nOne pathway to global governance, then, is a pre-emptive strike to consolidate a temporary advantage. Another pathway, of course, is consolidation of advantage after winning a more symmetrical (and presumably more destructive) war. But there are also pathways that do not involve war. One is evolutionary: as the world becomes more tightly interconnected, international infrastructures may expand to the point that they influence or even supersede national policy. This does not seem catastrophically risky in and of itself; indeed, in some respects it is going on today. Deliberate molecular manufacturing-related policy also may be implemented, including arms control regimes; the closest present-day analogy is perhaps the International Atomic Energy Agency (IAEA).\n\nGlobal governance appears to present two major risks of opposite character. The first is that it may create stagnation or oppression causing massive loss of human potential. For example, an arms-control regime might choose not to allow space access except under such tight restrictions as to prevent the colonization of space. The second risk is that, in attempting to create a safe and predictable (static) situation, policy inadequately planned or implemented may instead lead to instability and backlash. The effects of the backlash might be exacerbated by the loss of distributed experience in dealing with hostile uses of the technology that would have been a result of restrictive policies.\n\nWe cannot know in advance whether a dictatorial regime, in order to ensure security and stability, would choose to engage in genocide, but if the desire existed, the means would be there. Even if it were simply a matter of permanent ‘humane’ subjugation, the prospect of a world hopelessly enslaved can be regarded as a terminal catastrophe. (Chapter 22 from this volume has more on the threat of totalitarianism.)\n\n21.2.3.4 Radically enhanced intelligences\n\nNanotechnology, together with bioengineering, may give us the ability to radically enhance our bodies and brains. Direct human-computer connections almost certainly will be employed, and it is possible to imagine a situation in which one or more enhanced humans continue the process of augmentation to an extreme conclusion. Considering the power that merely human demagogues, tycoons, and authorities can wield, it is worth considering the potential results of enhanced post-humans who retain human competitive drives. Fully artificial intelligences – a likely consequence of nano-built computers – may pose similar issues, and Eliezer Yudkowsky (Chapter 15, this volume) argues that they may also cause existential risks due to unwisely specified goals.\n\n21.2.3.5 Environmental degradation\n\nExtensive use of molecular manufacturing could produce environmental impacts in any of several ways. Some types of environmental impact might represent disastrous, even existential, risks. Deliberate destruction of the environment on a globally significant scale would probably happen only as a side effect of a war that would do greater damage in other ways. It is conceivable that unwise development could do enough damage to threaten planetary ecology, but this is already a risk today.\n\nThe ease and speed with which planet-scale engineering could be carried out may tempt huge well-intended projects that backfire in ecological catastrophe. If the earth’s climate exists at a delicate balance between a runaway greenhouse effect and a sudden ice age, as some experts think (FEASTA, 2004), then large-scale releases of waste heat from overuse of billions of nanofactories and other nanomachinescould inadvertently tip the scale. Or, a seemingly benign effort such as the creation of huge areas of land or sea covered with solar collecting material might change our planet’s albedo enough to drastically alter the climate. (Climate change risk is covered in more depth in Chapter 13, this volume.)\n\nAnother possibility is ecosystem collapse as a secondary effect of massive infrastructure change. Such change could be rapid, perhaps too rapid to notice its effects until they are well underway. Computer models (Sinclair and Arcese, 1995) have shown that declining biodiversity, for example, can reach a tipping point and slide rapidly into widespread devastation, affecting whole food chains, including perhaps our own.\n\n21.2.3.6 Ecophagy\n\nWhen nanotechnology-based manufacturing was first proposed (Drexler, 1986), a concern arose that tiny manufacturing systems might run amok and ‘eat’ the biosphere, reducing it to copies of themselves. However, current research by Drexler (Burch, 2005) and others (Phoenix, 2003) makes it clear that small self-contained ‘replicating assemblers’ will not be needed for manufacturing – nanofactories will be far more efficient at building products, and a nanofactory contains almost none of the functionality of a freerange replicating robot. Development and use of molecular manufacturing appears to pose no risk of creating free-range replicators by accident at any point.\n\nDeliberately designing a functional self-replicating free-range nanobot would be no small task. In addition to making copies of itself, the robot also would have to survive in the environment, move around (either actively or by drifting – if it were small enough), find usable raw materials, and convert what it finds into feedstock and power, which entails sophisticated chemistry. The robot also would require a relatively large computer to store and process the full blueprint of such a complex device. A nanobot or nanomachine missing any part of this functionality could not function as a free-range replicator (Phoenix and Drexler, 2004). Despite this, there is no known reason why such a thing would be theoretically impossible.\n\nAlthough free-range replicators have no apparent commercial value, no significant military value, and only limited terrorist value, they might someday be produced by irresponsible hobbyists or by sects with apocalyptic creeds (compare Chapter 4, this volume), or might even be used as a tool for largescale blackmail. Cleaning up an outbreak would likely be impossible with today’s technology, and would be difficult or perhaps impossible even with molecular manufacturing technology. At the least, it probably would require severe physical disruption in the area of the outbreak (airborne and waterborne devices deserve special concern for this reason).\n\nPossible ways to cope with outbreaks include irradiating or heating the infested area, encapsulating the area, spreading some chemical that would gum up the replicator’s material intakes (if such a weakness can be discovered), or using a large population of robots for cleanup. Although cleanup robots would require advanced designs, they might be the least disruptive alternative, especially for infestations that are widespread by the time corrective measures are deployed. An exponentially replicating free-range replicator population would not require an exponentially replicating population of robots to combat it. Since the time to nullify a robot is dominated by the time to find it in the first place, which is inversely proportional to the concentration in the environment, a constant population of cleanup robots would exert a constant pressure on the replicator population regardless of replicator concentration (see Freitas, 2000 for further discussion of these issues).\n\nIn theory, a replicator that was built of materials indigestible to biology (such as diamond), and was not combated by a technological cleanup method, might be able to sequester or destroy enough biomass to destroy the planetary ecology. A hypothetical airborne replicator might block significant amounts of sunlight, and a widespread replicator operating at full speed could produce destructive amounts of heat even before it consumed most of the biomass (Freitas, 2000). Although much more research will be needed to understand the possibilities and countermeasures, at this point the future possibility of a global catastrophe cannot be ruled out.\n\n21.3 Mitigation of molecular manufacturing risks\n\nIn the preceding discussion of nanotech-related global catastrophic risks, the greatest near-term threats appear to be war and dictatorship. As we have seen, the two are closely related, and attempts to prevent one may lead to the other. Although the focus of this chapter is on understanding risk, we will take a brief look at some potential mitigating solutions.\n\nOne of the biggest unanswered questions is the balance between offence and defence in a nano-enabled war. If it turns out that defence is relatively easy compared to offence, then multiple coexisting agents could be stable even if not all of them are reliable. Indeed, preparing to defend against unreliable or hostile states might create an infrastructure that could mitigate the impacts of private misuse of molecular manufacturing. Conversely, if it turns out that resisting offensive weaponry requires a lot more resources than deploying the weaponry, there will be a strong temptation to maintain lopsided concentrations of power, comparable to or even more extreme than today’s imbalance between states and citizens, or between nuclear superpowers and non-nuclear nations. Such power imbalances might increase the risk of oppression.\n\nOne possible solution to maintaining a stable defence in a situation where defence is difficult is to design new institutions that can maintain and administer centralized global power without becoming oppressive, and to plan and implement a safe evolution to those institutions. Another possibility would be to devise safe and predictable versions of today’s untrustworthy institutions, and transition to them. Each of these solutions has serious practical problems. Some kind of limits will have to be set in order to prevent runaway processes, including human economic activity, from doing excessive amounts of damage. Self-regulation may not be sufficient in all cases, implying at least some need for central coordination or administration.\n\nA research organization that focused on developing defensive technologies and disseminated its work freely to everyone might help to improve stability. It is admittedly unusual in today’s military or commercial systems to have an organization that is completely transparent; however, transparency could lead to trust, which would be a key factor in the success of this approach. If defence could be made easier relative to offence, then stability may be increased. Groups that wanted to defend themselves would be able to do so without engaging in intensive military research. Some researchers who would have worked on offensive weaponry might be attracted to the defence organization instead. Ideally, most nations and other organizations should support the goals of such an organization, especially if the alternative is known to be instability leading to war, and this level of support might be sufficient to protect against efforts to subvert or destroy the organization.\n\nFor private or civilian use of molecular manufacturing, it might be a good idea to make more convenient and less powerful capabilities than nanofactories available. For example, nanofactories could be used to make micron-scale blocks containing advanced functionality that could be rapidly assembled into products. The block-assembly operation would likely be far faster and less energy-intensive than the manufacture of the blocks. Product design would also be simpler, since products would contain far fewer blocks than atoms, and the designers would not have to think about nanoscale physics. Meanwhile, especially dangerous capabilities could be omitted from the blocks.\n\nAnother unanswered question is how quickly the technological components of molecular manufacturing will be developed and adopted. Competition from other technologies and/or a widespread failure to recognize molecular manufacturing’s full potential may blunt development efforts. In the absence of targeted and energetic development, it seems likely that preparation of a supporting infrastructure for molecular manufacturing – from CAD software to feedstock supply – would be uneven, and this could limit the consequences of rapid development and deployment of new products. On the other hand, slower and more limited development would retard the ability of molecular manufacturing technology to mitigate risks, including counteracting ecophagic devices.\n\nThese suggestions are admittedly preliminary. The primary need at this point is for further studies to understand the technical capabilities that molecular manufacturing will enable, how rapidly those capabilities may be achieved, and what social and political systems can be stable with those capabilities in the world.\n\n21.4 Discussion and conclusion\n\nVast parallel arrays of precise molecular tools using inexpensive, readily available feedstock and operating under computer control inside a nanofactory could be capable of manufacturing advanced, high-performance products of all sizes and in large quantity. It is this type of nanotechnology – molecular manufacturing – that offers the greatest potential benefits and also poses the worst dangers. None of the many other nanoscale technologies currently in development or use appear likely to present global catastrophic risks.\n\nA nanofactory, as currently conceived, would be able to produce another nanofactory on command, leading to rapid exponential growth of the means of production. In a large project requiring scale-up to large amounts of fabrication capacity, the nanofactories could still weigh a fraction of the final product, thanks to their high throughput. This implies that the resources required to produce the product – not the nanofactories – would be the limiting factor. With nanofactory-aided rapid prototyping, the development and production of revolutionary, transformative products could appear very quickly. If solar cells and chemical processing plants are within the range of nanofactory products, then energy and feedstock could be gathered from abundant sources; with no shortage of manufacturing capital, projects of almost any scale could be undertaken.\n\nAs a general-purpose fully automated manufacturing technology, molecular manufacturing could become extremely pervasive and significant. Among the risks examined in this chapter, two stand out well above the others: global war fought with nano-weapons, and domination by nano-enabled governments. The probability of one or the other of these (or both) occurring before the end of the twenty-first century appears to be high, since they seem to follow directly from the military potential of molecular manufacturing. (The argument for instability of a nano-driven arms race in particular deserves careful scrutiny, since if it is incorrect but accepted as plausible, it may lead to unnecessary and destructive attempts at pre-emption.)\n\nIn the absence of some type of preventive or protective force, the power of molecular manufacturing products could allow a large number of actors of varying types – including individuals, groups, corporations, and nations – to obtain sufficient capability to destroy all unprotected humans. The likelihood of at least one powerful actor being insane is not small. The likelihood that devastating weapons will be built and released accidentally (possibly through overly sensitive automated systems) is also considerable. Finally, the likelihood of a conflict between two MAD-enabled powers escalating until one feels compelled to exercise a doomsday option is also non-zero. This indicates that unless adequate defences can be prepared against weapons intended to be ultimately destructive – a point that urgently needs research – the number of actors trying to possess such weapons must be minimized.\n\nIn assessing the above risks, a major variable is how long it will take to develop nanotechnology to the point of exponential molecular manufacturing (nanofactories building nanofactories). Opinions vary widely, but nanotechnologists who have studied molecular manufacturing most closely tend to have the shortest estimates. It appears technically plausible to us that molecular manufacturing might be developed prior to 2020, assuming that increasing recognition of its near-term feasibility and value leads to a substantial ($100-1000 million) programme starting around 2010.\n\nIf molecular manufacturing were to be delayed until 2040 or 2050, many of its strengths would be achieved by other technologies, and it would not have as much impact. However, this much delay does not seem plausible. Well before 2030, rapid advances in nanoscale technologies and computer modelling will likely make molecular manufacturing development relatively straightforward and inexpensive. It should be noted that once the first nanofactory is developed, the cost of further development may drop sharply while the payback may increase dramatically, thanks to the convergence of high performance, atomic precision, exponential manufacturing, and full automation. As the cost of nanofactory development falls, the number of potential investors increases very rapidly. This implies that the development of capabilities and products based on molecular manufacturing may come quite soon. Given the present lack of interest in related policy research, it seems likely that risks which might be mitigated by wise policy could in fact be faced unprepared.\n\nSuggestions for further reading\n\nElementary level\n\nEngines of Creation by Drexler, K.E. (1986) – While outdated in some respects, this book remains a classic introduction to the potential of nanotechnology. The manufacturing mechanism described, a collection of semi-autonomous cooperating ‘assemblers’, has been superseded by a model far more similar to conventional factories; this development obsoletes the book’s most famous warning, about runaway self-replicators or ‘grey goo’. Although Engines was written in the political context of the US-Soviet conflict, its broader warnings about military implications and the need for wise policy still have great value. Many of the author’s predictions and recommendations are relevant and forward-looking two decades after publication (entire book online at http://www.e-drexler.com/d/06/00/EOC/EOC\\_Table\\_of\\_Contents.html).\n\nNanofuture by Hall, J.S. (2005)- Nanofuture combines a layman’s introduction to molecular manufacturing with a description of some of its possible applications. The discussion of implications is scanty and rather optimistic, but the reader will get a good sense of how and why molecular manufactured products could transform – and often revolutionize – many of the technologies we use in daily life.\n\nOur Molecular Future by Mulhall, D. (2002) – This book explores some of the implications and applications of molecular manufacturing, including its applicability to mitigating a variety of natural disasters. The proposal to deal with threatening asteroids by breaking them apart with self-replicating robots appears both unworkable and unwise, but aside from this, the book provides interesting food for thought as to how molecular manufacturing might be used.\n\nProductive Nanosystems: From Molecules to Superproducts by Burch, J. and Drexler, K.E. (2005) – This remarkable animation shows clearly how a nanofactory might work to combine vast numbers of atoms into a single integrated product. The molecular manipulation steps shown have been simulated with advanced quantum chemistry methods (online at http://tinyurl.com/9xgs4, http://ourmolecularfuture.com).\n\n‘Safe utilization of advanced nanotechnology’ by Phoenix, C. and Treder, M. (2003) – Early descriptions of molecular manufacturing raised fears that the manufacturing systems might somehow get loose and start converting valuable biomass into pointless copies of themselves. This paper describes a manufacturing system made of components that would all be entirely inert if separated from the factory. In addition, the paper describes some ways to make the factory system less prone to deliberate misuse or abuse (online at http://crnano.org/safe.htm).\n\nIntermediate level\n\nKinematic Self-replicating Machines by Freitas, R.A., Jr and Merkle, R.C. (2004)-In its broadest sense, ‘replication’ means simply the building of a copy; the replicator need not be autonomous or even physical (it could be simulated). KSRM is worth reading for several reasons. First, it contains a wide-ranging and near-encyclopedic survey of replicators of all types. Second, it contains a taxonomy of 12 design dimensions containing 137 design properties, most of which can be varied independently to describe new classes of replicators. Third, it includes a discussion on the design and function of several molecular manufacturing-style replicators (again, in the broad sense; these are not the dangerous autonomous kind). The book puts molecular manufacturing in a broader context and is a rich source of insights (entire book online at http://molecularassembler.com/KSRM.htm).\n\n‘Design of a primitive nanofactory’ by Phoenix, C. (2003) – This lengthy paper explores many of the design issues that will have to be solved in order to build a nanofactory. Design concepts have improved somewhat since the paper was written, but this only strengthens its conclusion that nanofactories will be able to be pressed into service rapidly in a wide variety of applications (online at http://jetpress.org/volume13/Nanofactory.htm).\n\n‘Three systems of action: a proposed application for effective administration of molecular nanotechnology’ by Phoenix, C. and Treder, M. (2003) – Different types of issues need to be addressed in different ways. As explored in this paper, the approaches required for security issues, economic activity, and easily copied information are so dissimilar that an organization would have a very hard time encompassing them all without ethical conflicts. Molecular manufacturing will create issues of all three types, which will require a delicate and carefully planned interplay of the three approaches (online at http://crnano.org/systems.htm).\n\n‘Thirty essential studies’ by Phoenix, C. (2003) – Molecular manufacturing needs to be studied from many different angles, including technological capability, strategies for governance, and interactions with existing world systems. These studies raise a broad spectrum of necessary questions, and provide preliminary answers – which are not especially reassuring (online at http://crnano.org/studies.htm).\n\nAdvanced level\n\nNanomedicine, Volume I: Basic Capabilities by Freitas, R.A., Jr (1999)-This book was written to lay a foundation for subsequent volumes applying molecular manufacturing to medicine. As a fortunate side-effect, it also lays the foundation for many other applications of the technology. Although some of the chapters are heavily medical, much of the book deals with technical capabilities such as sensing, power transmission, and molecular sorting. Scattered throughout are many useful physics formulas applied to real-world nanoscale, microscale, and macroscale problems. As such, it is useful on its own or as a companion to Nanosystems (entire book online at http://nanomedicine.com/NMI.htm).\n\nNanosystems: Molecular Machinery, Manufacturing and Computation by Drexler, K.E. (1992) – This book is the foundational textbook and reference book of the field. Although not accessible to non-technical readers, it makes a clear technical case, grounded in well-established physics and chemistry, that molecular manufacturing can work as claimed. Several of its key theoretical extrapolations, such as superlubricity, have since been demonstrated by experiment. Although others remain to be confirmed, no substantial error has yet been found (partially available online at www.edrexler.com/d/06/00/Nanosystems/toc.html).\n\nReferences\n\nAltmann, J. (2006). Military Nanotechnology: Potential Applications and Preventive Arms Control (Hampshire: Andover).\n\nBailey, R. (2003). SARS Wars. ReasonOnline, Apr. 30. http://www.reason.com/news/show/34803.html\n\nBurch, J. (Lizard Fire Studios) and Drexler, E. (Nanorex) (2005). Productive Nanosystems: From Molecules to Superproducts (animated film). http://lizardfire.com/html\\_nano/ nano.html\n\nDienwiebel et al. (2004). Superlubricity of graphite. Physical Review Letters, 92, 126101.\n\nDrexler, K.E. (1986). Engines of Creation (New York, NY: Anchor Books).\n\nDrexler, K.E. (1992). Nanosystems: Molecular Machinery, Manufacturing, and Computation (New York: John Wiley &Sons).\n\nFEASTA (Foundation for the Economics of Sustainability) (2004). World climate liable to sudden, rapid change. http://www.feasta.org/documents/feastareview/climate changepanel.htm\n\nFreitas, R.A (1998). Exploratory design in medical nanotechnology: a mechanical artificial red cell. Artificial Cells, Blood Substitutes, and Immobilization Biotechnology, 26, 411–430. http://www.foresight. org/Nanomedicine/Respirocytes.html\n\nFreitas, R.A. (1999). Nanomedicine, Volume I: Basic Capabilities (Austin, TX: Landes Bioscience).\n\nFreitas, R.A (2000). Some limits to global ecophagy by biovorous nanoreplicators, with public policy recommendations. http://www.rfreitas.com/Nano/Ecophagy.htm\n\nFreitas, R.A. (2005). Microbivores: artificial mechanical phagocytes using digest and discharge protocol. J. Evol. Technol., 14. http://www.jetpress.org/volume14/freitas.html\n\nFreitas, R.A. (2007). The ideal gene delivery vector: chromallocytes, cell repair nanorobots for chromosome replacement therapy. J.Evol. Technol., 16(1), 1–97. http://jetpress.org/volume16/freitas.html\n\nGubrud, M. (1997). Nanotechnology and international security. http://www.foresight. org/conference/MNT05/Papers/Gubrud/\n\nHall, J.S. (1996). Utility fog: the stuff that dreams are made of. In Crandall, B.C. (ed.), Nanotechnology: Molecular Speculations on Global Abundance, pp. 161–184 (Cambridge, MA: MIT Press).\n\nISN (Institute for Soldier Nanotechnologies, Massachusetts Institute of Technology) (2005). Biomaterials and nanodevices for soldier medical technology. http://web.mit.edu/isn/research/team04/index.html\n\nJoy, B. (2000). Why the future doesn’t need us. Wired Magazine, April. http://www.wired.com/wired/archive/8.04/joy.html\n\nLake, A. (1996). Defining Missions, Setting Deadlines. Defense Issues, 11(14), 1–4.\n\nMcCarthy, T. (2005). Molecular nanotechnology and the world system. www.mccarthy.cx/WorldSystem/\n\nOrr, S.D. (2003). Globalization and democracy. A World Connected. http://aworld connected.org/article.php/527.html\n\nPhoenix, C. (2003). Design of a primitive nanofactory. J. Evol. Technol., 13. http://www.jetpress.org/volume13/Nanofactory.htm\n\nPhoenix, C. (2005). Molecular manufacturing: what, why and how. http://wisenano.org/w/Doing\\_MM\n\nPhoenix, C. (2006). Preventing errors in molecular manufacturing. http://www.crnano.org/essays06.htm#11, Nov\n\nPhoenix, C. and Drexler, E. (2004). Safe exponential manufacturing. Nanotechnology, 15, 869–872.\n\nSinclair, A.R.E. and Arcese P. (eds.) (1995). Serengeti II: Dynamics, Management and Conservation ofan Ecosystem (Chicago, IL: Chicago University Press).\n\nTreder, M. (2005). War, interdependence, and nanotechnology. http://www.futurebrief.com/miketrederwar002.asp\n\n• 22 • The totalitarian threat\n\nBryan Caplan\n\nIf you want a picture of the future, imagine a boot stamping on a human face – forever.\n\n-George Orwell, 1984 (1983, p. 220)\n\n22.1 Totalitarianism: what happened and why it (mostly) ended\n\nDuring the twentieth century, many nations – including Russia, Germany, and China – lived under extraordinarily brutal and oppressive governments. Over 100 million civilians died at the hands of these governments, but only a small fraction of their brutality and oppression was necessary to retain power. The main function of the brutality and oppression, rather, was to radically change human behaviour, to transform normal human beings with their selfish concerns into willing servants of their rulers. The goals and methods of these governments were so extreme that they were often described – by friend and foe alike – as ‘total’ or ‘totalitarian’ (Gregor, 2000).\n\nThe connection between totalitarian goals and totalitarian methods is straightforward. People do not want to radically change their behaviour. To make them change anyway requires credible threats of harsh punishment – and the main way to make such threats credible is to carry them out on a massive scale. Furthermore, even if people believe your threats, some will resist anyway or seem likely to foment resistance later on. Indeed, some are simply unable to change. An aristocrat cannot choose to have proletarian origins, or a Jew to be an Aryan. To handle these recalcitrant problems requires special prisons to isolate dangerous elements, or mass murder to eliminate them.\n\nTotalitarian regimes have many structural characteristics in common. Richard Pipes gives a standard inventory: ‘[A]n official all-embracing ideology; a single party of the elect headed by a “leader” and dominating the state; police terror; the ruling party’s control of the means of communication and the armed forces; central command of the economy’ (1994, p. 245). All of these naturally flow from the goal of remaking human nature. The official ideology is the rationale for radical change. It must be ‘all-embracing’ – that is, suppress competing ideologies and values – to prevent people from being side-tracked by conflicting goals. The leader is necessary to create and interpret the official ideology, and control of the means of communication to disseminate it. The party is comprised of the ‘early-adopters’ – the people who claim to have ‘seen the light’ and want to make it a reality. Police terror and control of the armed forces are necessary to enforce obedience to the party’s orders. Finally, control of the economy is crucial for a whole list of reasons: to give the party the resources it needs to move forward; to suppress rival power centres; to ensure that economic actors do not make plans that conflict with the party’s; and to make citizens dependent on the state for their livelihood.\n\nThis description admittedly glosses over the hypocrisy of totalitarian regimes. In reality, many people join the party because of the economic benefits of membership, not because they sincerely share the party’s goals. While it is usually hard to doubt the ideological sincerity of the founding members of totalitarian movements, over time the leadership shifts its focus from remaking human nature to keeping control. Furthermore, while totalitarian movements often describe their brutality and oppression as transitional measures to be abandoned once they purify the hearts and minds of the people, their methods usually severely alienate the subject population. The ‘transition’ soon becomes a way of life.\n\nThe Soviet Union and Nazi Germany are the two most-studied totalitarian regimes. By modern calculations, the Soviets killed approximately 20 million civilians, the Nazis 25 million(Courtois et al., 1999, pp. 4–5, 14–15; Payne, 1995). However, these numbers are biased by the relative difficulty of data collection. Scholars could freely investigate most Nazi atrocities beginning in 1945, but had to wait until the 1990s to document those of the Soviets. In all likelihood, the death toll of the Soviets actually exceeded those of the Nazis.\n\nOne of the main differences between the Soviet Union and Nazi Germany was that the former became totalitarian very rapidly. Lenin embarked upon radical social change as soon as he had power (Malia, 1994). In contrast, totalitarianism developed gradually in Nazi Germany; only in the last years of World War II did the state try to control virtually every area of life (Arendt, 1973). The other main difference is that most of the atrocities of the Soviet Union were directed inwards at its own citizens, whereas most Nazi atrocities were directed outwards at the citizens of occupied countries (Noakes and Pridham, 2001; Friedlander, 1995).\n\nBut despite historians’ focus on Russia and Germany, Maoist China was actually responsible for more civilian killings than the Soviet Union and Nazi Germany put together. Modern estimates put its death toll at 65 million (Margolin, 1999a). The West is primarily familiar with the cruelties inflicted on Chinese intellectuals and Party members during the Cultural Revolution, but its death toll was probably under one million. The greatest of Mao’s atrocities was the Great Leap Forward, which claimed 30 million lives through man-made starvation(Becker, 1996).\n\nBesides mass murder, totalitarian regimes typically engage in a long list of other offences. Slave labour was an important part of both the Soviet and Nazi economies. Communist regimes typically placed heavy restrictions on migration – most notably making it difficult for peasants to move to cities and for anyone to travel abroad. Freedom of expression and religion were heavily restricted. Despite propaganda emphasizing rapid economic growth, living standards of non-party members frequently fell to the starvation level. Totalitarian regimes focus on military production and internal security, not consumer well-being.\n\nAnother notable problem with totalitarian regimes was their failure to anticipate and counteract events that even their leaders saw as catastrophic. Stalin infamously ignored overwhelming evidence that Hitler was planning to invade the Soviet Union. Hitler ensured his own defeat by declaring war on the United States. Part of the reason for these lapses of judgement was concentration of power, which allowed leaders’ idiosyncrasies to decide the fates of millions. But this was amplified by the fact that people in totalitarian regimes are afraid to share negative information. To call attention to looming disasters verges on dissent, and dissent is dangerously close to disloyalty.\n\nFrom the viewpoint of the ruling party, this may be a fair trade: More and worse disasters are the price of social control. From the viewpoint of anyone concerned about global catastrophic risks, however, this means that totalitarianism is worse than it first appears. To the direct cost of totalitarianism we must add the indirect cost of amplifying other risks. It is important not to push this argument too far, however. For goals that can be achieved by brute force or mobilizing resources, totalitarian methods have proven highly effective. For example, Stalin was able to develop nuclear weapons with amazing speed simply by making this the overarching priority of the Soviet economy (Holloway, 1994). Indeed, for goals that can only be achieved by radically changing human behaviour, nothing but totalitarian methods have proven highly effective. Overall, totalitarian regimes are less likely to foresee disasters, but are in some ways better-equipped to deal with disasters that they take seriously.\n\n22.2 stable totalitarianism\n\nThere are only four ways in which a ruling group can fall from power. Either it is conquered from without, or it governs so inefficiently that the masses are stirred to revolt, or it allows a strong and discontented Middle Group to come into being, or it loses its own self-confidence and willingness to govern … A ruling class which could guard against all of them would remain in power permanently. Ultimately the determining factor is the mental attitude of the ruling class itself.\n\n-George Orwell, 1984(1983, p. 170;\n\nThe best thing one can say about totalitarian regimes is that the main ones did not last very long.¹ The Soviet Union greatly reduced its level of internal killing after the death of Stalin, and the Communist Party fell from power in 1991. After Mao Zedong’s death, Deng Xiaoping allowed the Chinese to resume relatively normal lives, and began moving in the direction of a market economy. Hitler’s Thousand-Year Reich lasted less than 13 years, before it ended with military defeat in World War II.\n\nThe deep question, however, is whether this short duration was inherent or accidental. If the short lifespan of totalitarianism is inherent, it probably does not count as a ‘global catastrophic risk’ at all. On the other hand, if the rapid demise of totalitarianism was a lucky accident, if future totalitarians could learn from history to indefinitely prolong their rule, then totalitarianism is one of the most important global catastrophic risks to stop before it starts.\n\nThe main obstacle to answering this question is the small number of observations. Indeed, the collapse of the Soviet bloc was so inter-connected that it basically counts as only one data point. However, most of the historical evidence supports the view that totalitarianism could have been much more durable than it was.\n\nThis is clearest in the case of Nazi Germany. Only crushing military defeat forced the Nazis from power. Once Hitler became dictator, there was no serious internal opposition to his rule. If he had simply pursued a less aggressive foreign policy, there is every reason to think he would have remained dictator for life. One might argue that grassroots pressure forced Hitler to bite off more than he could militarily chew, but in fact the pressure went the other way. His generals in particular favoured a less aggressive posture (Bullock, 1993, pp. 393–394, 568–574, 582).\n\nThe history of the Soviet Union and Maoist China confirms this analysis. They were far less expansionist than Nazi Germany, and their most tyrannical leaders – Stalin and Mao – ruled until their deaths. But at the same time, the demise of Stalin and Mao reveals the stumbling block that the Nazis would have eventually faced too: succession. How can a totalitarian regime ensure that each generation of leaders remains stridently totalitarian? Both Stalin and Mao fumbled here, and perhaps Hitler would have done the same.\n\nA number of leading Communists wrestled for Stalin’s position, and the eventual winner was Nikita Khrushchev. Khrushchev kept the basic structure of Stalinist Russia intact, but killed far fewer people and released most of the slave labourers. He even readmitted many of Stalin’s victims back into the Party, and allowed anti-Stalinists like Solzhenitsyn to publish some of their writings. The result was a demoralization of the Party faithful, both inside the Soviet Union and abroad: Stalin was a tyrant not merely according to the West, but to the new Party line as well (Werth, 1999, pp. 250–260).\n\nKhrushchev was eventually peacefully removed from power by other leading Communists who might be described as ‘anti-anti-Stalinists’. While they did not restore mass murder of Soviet citizens or large-scale slave labour, they squelched public discussion of the Party’s ‘mistakes’. As the Party leadership aged, however, it became increasingly difficult to find a reliable veteran of the Stalin years to take the helm. The 54-year-old Mikhail Gorbachev was finally appointed General Secretary in 1985. While it is still unclear what his full intentions were, Gorbachev’s moderate liberalization measures snowballed. The Soviet satellite states in Eastern Europe collapsed in 1989, and the Soviet Union itself disintegrated in 1991.\n\nThe end of totalitarianism in Maoist China happened even more quickly. After Mao’s death in 1976, a brief power struggle led to the ascent of the pragmatist Deng Xiaoping. Deng heavily reduced the importance of Maoist ideology in daily life, de facto privatized agriculture int his still largely agricultural economy, and gradually moved towards more free-market policies, under the guise of ‘socialism with Chinese characteristics’. China remained a dictatorship, but had clearly evolved from totalitarian to authoritarian (Salisbury, 1992).\n\nIt is tempting for Westerners to argue that the Soviet Union and Maoist China changed course because their systems proved unworkable, but this is fundamentally incorrect. These systems were most stable when their performance was worst. Communist rule was very secure when Stalin and Mao were starving millions to death. Conditions were comparatively good when reforms began. Totalitarianism ended not because totalitarian policies were unaffordable, but because new leaders were unwilling to keep paying the price in lives and wealth.\n\nPerhaps there was no reliable way for totalitarian regimes to solve the problem of succession, but they could have tried a lot harder. If they had read George Orwell, they would have known that the key danger to the system is ‘the growth of liberalism and scepticism in their own ranks’ (1983, p. 171). Khrushchev’s apostasy from Stalinism was perhaps unforeseeable, but the collapse of the Soviet Union under Gorbachev could have been avoided if the Politburo considered only hard-line candidates. The Soviet Union collapsed largely because a reformist took the helm, but a reformist was able to take the helm only because his peers failed to make holding power their top priority. Mao, similarly, could have made continuity more likely by sending suspected ‘capitalist-roaders’ like Deng to their deaths without exception.\n\nProbably the most important reason why a change in leaders often led totalitarian regimes to moderate their policies is that they existed side by side with non-totalitarian regimes. It was obvious by comparison that people in the non-totalitarian world were richer and happier. Totalitarian regimes limited contact with foreigners, but news of the disparities inevitably leaked in. Even more corrosively, party elites were especially likely to see the outside world firsthand. As a result, officials at the highest levels lost faith in their own system.\n\nThis problem could have been largely solved by cutting off contact with the non-totalitarian world, becoming ‘hermit kingdoms’ like North Korea or Communist Albania. But the hermit strategy has a major drawback. Totalitarian regimes have trouble growing and learning as it is; if they cannot borrow ideas from the rest of the world, progress slows to a crawl. But if other societies are growing and learning and yours is not, you will lose the race for political, economic, and military dominance. You may even fall so far behind that foreign nations gain the ability to remove you from power at little risk to themselves.\n\nThus, a totalitarian regime that tried to preserve itself by turning inwards could probably increase its life expectancy. For a few generations the pool of potential successors would be less corrupted by alien ideas. But in the long run the non-totalitarian neighbours of a hermit kingdom would overwhelm it.\n\nThe totalitarian dilemma, then, is that succession is the key to longevity. But as long as totalitarian states co-exist with non-totalitarian ones, they have to expose potential successors to demoralizing outside influences to avoid falling dangerously behind their rivals.²\n\nTo understand this dilemma, however, is also to understand its solution: totalitarianism would be much more stable if there were no non-totalitarian world. The worse-case scenario for human freedom would be a global totalitarian state. Without an outside world for comparison, totalitarian elites would have no direct evidence that any better way of life was on the menu. It would no longer be possible to borrow new ideas from the non-totalitarian world, but it would also no longer be necessary. The global government could economically and scientifically stagnate without falling behind. Indeed, stagnation could easily increase stability. The rule of thumb ‘Avoid all change’ is easier to correctly apply than the rule, ‘Avoid all change that makes the regime less likely to stay in power.’\n\nIt is fashionable to paint concerns about world government as xenophobic or even childish. Robert Wright maintains that the costs would be trivial and the benefits large:\n\nDo you cherish the freedom to live without fear of dying in a biological weapons attack? Or do you prefer the freedom to live without fear of having your freezer searched for anthrax by an international inspectorate in the unlikely event that evidence casts suspicion in your direction? … Which sort of sovereignty would you rather lose? Sovereignty over your freezer, or sovereignty over your life? (2000, p. 227)\n\nBut this is best-case thinking. In reality, a world government might impose major costs for minor gains. If mankind is particularly unlucky, world government will decay into totalitarianism, without a non-totalitarian world to provide a safety valve.\n\nTotalitarianism would also be more stable than it was in the twentieth century if the world were divided between a small number of totalitarian states. This is Orwell’s scenario in 1984: Oceania, Eurasia, and Eastasia control the earth’s surface and wage perpetual war against one another. In a world like this, elites would not be disillusioned by international comparisons, because every country would feature the poverty and misery typical of totalitarian societies. Deviating from the hermit strategy would no longer taint the pool of successors. Orwell adds the interesting argument that the existence of external enemies helps totalitarian regimes maintain ideological fervour: ‘So long as they remain in conflict they prop one another up, like three sheaves of corn’ (1983, p. 162).\n\nFor this scenario to endure, totalitarian regimes would still have to make sure they did not fall far behind their rivals. But they would only have to keep pace with other relatively stagnant societies. Indeed, if one state fell so far behind its rivals that it could be conquered at little risk, the world would simply be one step closer to a unified totalitarian government. The greater danger would be an increase in the number of states via secession. The more independent countries exist, the greater the risk that one will liberalize and begin making the rest of the world look bad.\n\n22.3 Risk factors for stable totalitarianism\n\nOn balance, totalitarianism could have been a lot more stable than it was, but also bumped into some fundamental difficulties. However, it is quite conceivable that technological and political changes will defuse these difficulties, greatly extending the lifespan of totalitarian regimes. Technologically, the great danger is anything that helps solve the problem of succession. Politically, the great danger is movement in the direction of world government.\n\n22.3.1 Technology\n\nOrwell’s 1984 described how new technologies would advance the cause of totalitarianism. The most vivid was the ‘telescreen’, a two-way television set. Anyone watching the screen was automatically subject to observation by the Thought Police. Protagonist Winston Smith was only able to keep his diary of thought crimes because his telescreen was in an unusual position which allowed him to write without being spied upon.\n\nImproved surveillance technology like the telescreen would clearly make it easier to root out dissent, but is unlikely to make totalitarianism last longer. Even without telescreens, totalitarian regimes were extremely stable as long as their leaders remained committed totalitarians. Indeed, one of the main lessons of the post-Stalin era was that a nation can be kept in fear by jailing a few thousand dissidents per year.\n\nBetter surveillance would do little to expose the real threat to totalitarian regimes: closet sceptics within the party. However, other technological advances might solve this problem. In Orwell’s 1984, one of the few scientific questions still being researched is ‘how to discover, against his will, what another human being is thinking’ (1983, p. 159). Advances in brain research and related fields have the potential to do just this. Brain scans, for example, might one day be used to screen closet sceptics out of the party. Alternately, the new and improved psychiatric drugs of the future might increase docility without noticeably reducing productivity.\n\nBehavioural genetics could yield similar fruit. Instead of searching for sceptical thoughts, a totalitarian regime might use genetic testing to defend itself. Political orientation is already known to have a significant genetic component (Pinker, 2002, pp. 283–305). A ‘moderate’ totalitarian regime could exclude citizens with a genetic predisposition for critical thinking and individualism from the party. A more ambitious solution – and totalitarian regimes are nothing if not ambitious – would be genetic engineering. The most primitive version would be sterilization and murder of carriers of ‘anti-party’ genes, but you could get the same effect from selective abortion. A technologically advanced totalitarian regime could take over the whole process of reproduction, breeding loyal citizens of the future in test tubes and raising them in state-run ‘orphanages’. This would not have to go on for long before the odds of closet sceptics rising to the top of their system and taking over would be extremely small.\n\nA very different route to totalitarian stability is extending the lifespan of the leader so that the problem of succession rarely if ever comes up. Both Stalin and Mao ruled for decades until their deaths, facing no serious internal threats to their power. If life extension technology had been advanced enough to keep them in peak condition forever, it is reasonable to believe that they would still be in power today.\n\nSome of Or well’s modern critics argue that he was simply wrong about the effect of technology (Huber, 1994), In practice, technology – from fax machines to photocopiers – undermined totalitarianism by helping people behind the Iron Curtain learn about the outside world and organize resistance. Whatever the effect of past technology was, however, the effect of future technology is hard to predict. Perhaps genetic screening will be used not to prevent the births of a future Alexandr Solzhenitsyn or Andrei Sakharov, but a future Stalin. Nevertheless, future technology is likely to eventually provide the ingredients that totalitarianism needs to be stable.\n\nAt the same time, it should be acknowledged that some of these technologies might lead totalitarianism to be less violent than it was historically. Suppose psychiatric drugs or genetic engineering created a docile, homogeneous population. Totalitarian ambitions could then be realized without extreme brutality, because people would want to do what their government asked – a possibility explored at length in the dystopiannovel Brave New World (Huxley, 1932).\n\n22.3.2 Politics\n\nTo repeat, one of the mainchecks on totalitarian regimes has been the existence of non-totalitarian regimes. It is hard to maintain commitment to totalitarian ideologies when relatively free societies patently deliver higher levels of wealth and happiness with lower levels of brutality and oppression. The best way for totalitarian societies to maintain morale is the hermit strategy: Cut off contact with the non-totalitarian world. But this leads to stagnation, which is only viable in the long run if the rest of the world is stagnant as well.\n\nFrom this perspective, the most dangerous political development to avoid is world government.³ A world totalitarian government could permanently ignore the trade-off between stability and openness.\n\nHow likely is the emergence of world government? Recent trends towards secession make it improbable in the immediate future (Ales ina and Spolaore, 2003). At the same time, however, nominally independent countries have begun to surrender surprising amounts of sovereignty (Barrett, 2003; Wright, 2000). The most striking example is the European Union. Without military conquest, the long-quarrelling peoples of Western and Central Europe have moved perhaps half-way to regional government. Membership is attractive enough that many neighbouring countries want to join as well. It is quite conceivable that within a century the continent of Europe will be as unified as the United States is today.\n\nEuropean unification also increases the probability of copycat unions in other parts of the world. For example, if the EuropeanUnionadopts free trade internally and protectionism externally, other nations will be tempted to create trading blocs of their own. Once economic unions take root, moreover, they are likely to gradually expand into political unions – just as the European Economic Community became the European Union. If it seems fanciful to imagine North and South America becoming a single country in one century, consider how improbable the rise of the European Union would have seemed in 1945. Once the idea of gradual peaceful unification takes root, moreover, it is easy to see how rival super-nations might eventually merge all the way to world government.\n\nThe growing belief that the world faces problems that are too big for any one country to handle also makes the emergence of world government more likely. Robert Wright (2000, p. 217) observes that an extraterrestrial invasion would make world government a respectable idea overnight, and argues that other, less fanciful, dangers will gradually do the same:\n\n[T]he end of the second millennium has brought the rough equivalent of hostile extraterrestrials – not a single impending assault, but lots of new threats that, together, add up to a big planetary security problem. They range from terrorists (with their menu of increasingly spooky weapons) to a new breed of transnational criminals (many of whom will commit their crimes in that inherently transnational realm, cyberspace) to environmental problems (global warming, ozone depletion, and lots of merely regional but supranational issues) to health problems (epidemics that exploit modern thoroughfares).\n\nSo far, environmental concerns have probably gotten the most international attention. Environmentalists have argued strongly on behalf of global environmental agreements like the Kyoto Treaty, which has been ratified by much of the world (Barrett, 2003, p.373-374). But environmentalists often add that even if it were ratified, it would only address one of the environment’s numerous problems. A natural inference is that it is hopeless to attack environmental problems one at a time with unanimous treaties. It seems more workable to have an omnibus treaty that binds signatories to a common environmental policy. The next step would be to use economic and other pressures to force hold-outs to join (Barrett, 2003). Once a supranational ‘Global Environmental Protection Agency’ was in place, it could eventually turn into a full-fledged political union. Indeed, since environmental policy has major implications for the economy, a world environmental agency with the power to accomplish its goals would wind up running a great deal of the traditionally domestic policy of member nations. World government would not inevitably follow, but it would become substantially more likely.\n\nAnother political risk factor for totalitarianism is the rise of radical ideologies. At least until they gain power, a tenet common to all totalitarians is that the status quo is terribly wrong and must be changed by any means necessary. For the Communists, the evil to abolish was private ownership of the means of production; for the Nazis, it was the decay and eventual extinction of the Aryan race; for totalitarian religious movements, the great evils are secularization and pluralism. Whenever large movements accept the idea that the world faces a grave danger that can only be solved with great sacrifices, the risk of totalitarianism goes up.\n\nOne disturbing implication is that there may be a trade-off between preventing totalitarianism and preventing the other ‘global catastrophic risks’ discussed in this volume. Yes, in practice, totalitarian regimes failed to spot some major disasters until it was too late. But this fact is unlikely to deter those who purport to know that radical steps are necessary to save humanity from a particular danger. Extreme pessimism about the environment, for example, could become the rationale for a Green totalitarianism.\n\nOf course, if humanity is really doomed without decisive action, a small probability of totalitarianism is the lesser evil. But one of the main lessons of the history of totalitarianism is that moderation and inaction are underrated. Few problems turned out to be as ‘intolerable’ as they seemed to people at the time, and many ‘problems’ were better than the alternative. Countries that ‘did nothing’ about poverty during the twentieth century frequently became rich through gradual economic growth. Countries that waged ‘total war’ on poverty frequently not only choked off economic growth, but starved.\n\nAlong these lines, one particularly scary scenario for the future is that overblown doomsday worries become the rationale for world government, paving the way for an unanticipated global catastrophe: totalitarianism. Those who call for the countries of the world to unite against threats to humanity should consider the possibility that unification itself is the greater threat.\n\n22.4 Totalitarian risk management\n\n22.4.1 Technology\n\nDwelling on technology-driven dystopias can make almost anyone feel like a Luddite. But in the decentralized modern world, it is extremely difficult to prevent the development of any new technology for which there is market demand. Research on the brain, genetics, and life extension all fit that description. Furthermore, all of these new technologies have enormous direct benefits. If people lived forever, stable totalitarianism would be a little more likely to emerge, but it would be madness to force everyone to die of old age in order to avert a small risk of being murdered by the secret police in a 1000 years.\n\nIn my judgement, the safest approach to the new technologies I have discussed is freedom for individuals combined with heavy scrutiny for government. In the hands of individuals, new technology helps people pursue their diverse ends more effectively. In the hands of government, however, new technology risks putting us on the slippery slope to totalitarianism.\n\nTake genetic engineering. Allowing parents to genetically engineer their children would lead to healthier, smarter, and better-looking kids. But the demand for other traits would be about as diverse as those of the parents themselves. On the other hand, genetic engineering in the hands of government is much more likely to be used to root out individuality and dissent. ‘Reproductive freedom’ is a valuable slogan, capturing both parents’ right to use new technology if they want to, and government’s duty not to interfere with parents’ decisions.\n\nCritics of genetic engineering often argue that both private and government use lie on a slippery slope. In one sense, they are correct. Once we allow parents to screen for genetic defects, some will want to go further and screen for high IQ, and before you know it, parents are ordering ‘designer babies’. Similarly, once we allow government to genetically screen out violent temperaments, it will be tempting to go further and screen for conformity. The difference between these slippery slopes, however, is where the slope ends. If parents had complete control over their babies’ genetic makeup, the end result would be a healthier, smarter, better-looking version of the diverse world of today. If governments had complete control over babies’ genetic makeup, the end result could easily be a population docile and conformist enough to make totalitarianism stable.\n\n22.4.2 Politics\n\nWorld government hardly seems like a realistic threat today. Over the course of a few centuries, however, it seems likely to gradually emerge. Nationalism prevents it from happening today, but the emerging global culture has already begun to dilute national identities (Wright, 2000). Cultural protectionism is unlikely to slow this process very much, and in any case the direct benefits of cultural competition are large (Caplan and Cowen, 2004).\n\nIn the long run, it is better is to try to influence global culture instead of trying to fragment it. Xenophobia may be the main barrier to world government for the time being, but it is a weak argument. In fact, there would be good reasons to avoid political centralization even if the world were culturally homogeneous.\n\nTwo of the most visible reasons to preserve competition between governments are to (1) pressure governments to treat their citizens well in order to retain population and capital; and (2) make it easier to figure out which policies work best by allowing different approaches to exist side by side (Inman and Rubinfeld, 1997). Yes, there are large benefits of international economic integration, but economic integration does not require political integration. Economists have been pointing out for decades that unilateral free trade is a viable alternative to cumbersome free trade agreements (Irwin, 1996).\n\nReducing the risk of totalitarianism is admittedly one of the less visible benefits of preserving inter-governmental competition. But it is an important benefit, and there are ways to make it more salient. One is to publicize the facts about totalitarian regimes. The extent of Hitler’s crimes is well known to the general public, but Lenin’s, Stalin’s, and Mao’s are not. Another is to explain how these terrible events came to pass. Even though the risk of genocide is very small in most of the world, historians of the Holocaust have helped the world to see the connection between racial hatred and genocide. Historians could perform a similar service by highlighting the connection between political centralization and totalitarianism. Perhaps such lectures on the lessons of history would fall on deaf ears, but it is worth a try.\n\n22.5 ‘What’s your p?’\n\nI am an economist, and economists like to make people assign quantitative probabilities to risks. ‘What’s your p of X ?’ we often ask, meaning ‘What probability do you assign to X happening?’ The point is not that anyone has the definitive numbers. The point, rather, is that explicit probabilities clarify debate, and impose discipline on how beliefs should change as new evidence emerges (Tetlock, 2005). If a person says that two risks are both ‘serious’, it is unclear which one he sees as the greater threat; but we can stop guessing once he assigns a probability of 2%tooneand 0.1% to another. Similarly, if a person says that the probability of an event is 2%, and relevant new information arrives, consistency requires him to revise his probability.\n\nHow seriously do I take the possibility that a world totalitarian government will emerge during the next 1000 years and last for a 1000 years or more? Despite the complexity and guesswork inherent in answering this question, I will hazard a response. My unconditional probability – that is, the probability I assign given all the information I now have – is 5%. I am also willing to offer conditional probabilities. For example, if genetic screening for personality traits becomes cheap and accurate, but the principle of reproductive freedom prevails, my probability falls to 3%. Given the same technology with extensive government regulation, my probability rises to 10%. Similarly, if the number of independent countries on earth does not decrease during the next 1,000 years, my probability falls to 0.1%, but if the number of countries falls to one, my probability rises to 25%.\n\nIt is obviously harder to refine my numbers than it is to refine estimates of the probability of an extinction-level asteroid impact. The regularities of social science are neither as exact nor as enduring as the regularities of physical science. But this is a poor argument for taking social disasters like totalitarianism less seriously than physical disasters like asteroids. We compare accurately measured to inaccurately measured things all the time. Which is worse for a scientist to lose: 1 point of IQ, or his ‘creative spark’? Even though IQ is measured with high accuracy, and creativity is not, loss of creativity is probably more important.\n\nFinally, it is tempting to minimize the harm of a social disaster like totalitarianism, because it would probably not lead to human extinction. Even in Cambodia, the totalitarian regime with the highest death rate per-capita, 75% of the population remained alive after 3 years of rule by the Khmer Rouge (Margolin, 1999b). But perhaps an eternity of totalitarianism would be worse than extinction. It is hard to read Orwell and not to wonder:\n\nDo you begin to see, then, what kind of world we are creating? It is the exact opposite of the stupid hedonistic Utopias that the old reformers imagined. A world of fear and treachery and torment, a world of trampling and being trampled upon, a world which will grow not less but more merciless as it refines itself. Progress in our world will be progress towards more pain. The old civilizations claimed that they were founded on love or justice. Ours is founded upon hatred. In our world there will be no emotions except fear, rage, triumph and self-abasement. Everything else we shall destroy-everything … There will be no loyalty, except loyalty towards the Party. There will be no love, except the love of Big Brother. There will be no laughter, except for the laugh of triumph over a defeated enemy. There will be no art, no literature, no science. When we are omnipotent we shall have no more need of science. There will be no distinction between beauty and ugliness. There will be no curiosity, no enjoyment of the process of life. All competing pleasures will be destroyed. (1983, p. 220)\n\nAcknowledgements\n\nFor discussion and useful suggestions I would like to thank Tyler Cowen, Robin Hanson, Alex Tabarrok, Dan Houser, Don Boudreaux and Ilia Rainer. Geoffrey Lea provided excellent research assistance.\n\nSuggestions for further reading\n\nBecker, J. (1996). Hungry Ghosts: Mao’s Secret Famine (New York: The Free Press). An eye-opening history of Chinese Communism’s responsibility for the greatest famine in human history.\n\nCourtois, S., Werth, N., Panné, J.-L., Paczkowski, A., Bartosek, K., and Margolin, J.-L. The Black Book of Communism: Crimes, Terror, Repression. The most comprehensive and up-to-date survey of the history of Communist regimes around the world.\n\nGregor, A.J. The Faces of Janus: Marxism and Fascism in the Twentieth Century. An excellent survey of the parallels between ‘left-wing’ and ‘right-wing’ totalitarianism, with an emphasis on intellectual history.\n\nNoakes, J., and Pridham, G. (2001). Nazism 1919–1945, volumes 1–4 (Exeter, UK: University of Exeter Press). A comprehensive study of Nazism, with insightful commentary interspersed between critical historical documents.\n\nOrwell, G. 1984. The greatest and most insightful of the dystopian novels.\n\nPayne, S. A History of Fascism 1914–1945. A wide-ranging comparative study of Fascist Italy, Nazi Germany, and their numerous imitators in Europe and around the globe.\n\nReferences\n\nAlesina, A. and Enrico, S. (2003). The Size of Nations (Cambridge: MIT Press). Arendt, H. (1973). The Origins of Totalitarianism (New York: Harcourt, Brace &World Press).\n\nBarrett, S. (2003). Environment and Statecraft: The Strategy of Environmental Treaty-Making (Oxford: Oxford University Press).\n\nBecker, J. (1996). Hungry Ghosts: Mao’s Secret Famine (New York: The Free Press).\n\nBostrom, N. (2006). What is a Singleton? Linguistic and Philosophical Investigations. 5(2), 48–54.\n\nBullock, A. (1993). Hitler and Stalin: Parallel Lives (New York: Vintage Books).\n\nCaplan, B. and Cowen, T. (2004). Do We Underestimate the Benefits of Cultural Competition? Am Econ Rev, 94(2), 402–407.\n\nCourtois, S., Werth, N., Panné, J.-L., Paczkowski, A., Bartosek, K., and Margolin, J.-L. (1999). The Black Book ofCommunism: Crimes, Terror, Repression (Cambridge: Harvard University Press).\n\nFriedlander, H. (1995). The Origins of Nazi Genocide: From Euthanasia to the Final Solution (Chapel Hill, NC: University of North Carolina Press).\n\nGregor, A.J. (2000). The Faces of Janus: Marxism and Fascism in the Twentieth Century (New Haven, CT: Yale University Press).\n\nHolloway, D. (1994). Stalin andthe Bomb: The Soviet Union and Atomic Energy, 1939–1956 (New Haven, CT: Yale University Press).\n\nHuber, P. (1994). Orwell’s Revenge: The 1984 Palimpsest (New York: The Free Press).\n\nHuxley, A. (1996 [1932]). Brave New World (New York: Chelsea House Publishers).\n\nInman, R. and Daniel, R. (1997). Rethinking federalism. J Econ Perspect, 11(4), 43–64.\n\nIrwin, D. (1996). Against the Tide: An Intellectual History of Free Trade (Princeton, NJ: Princeton University Press).\n\nMalia, M. (1994). The Soviet Tragedy: A History of Socialism in Russia, 1917–1991 (NY: The Free Press).\n\nMargolin, J.-L. (1999a). China: a long march into night. In Courtois, S., Werth, N., Panné, J.-L., Paczkowski, A., Bartošek, K., and Margolin, J.-L. The Black Book of Communism: Crimes, Terror, Repression, pp. 463–546 (Cambridge: Harvard University Press).\n\nMargolin, J.-L. (1999b). Cambodia: the country of disconcerting crimes. In Courtois, S., Werth, N., Panné, J.-L., Paczkowski, A., Bartošek, K., and Margolin, J.-L. The Black Book of Communism: Crimes, Terror, Repression, pp. 577–635 (Cambridge: Harvard University Press).\n\nNoakes, J. and Pridham, G. (2001). Nazism 1919–1945, vol. 3: Foreign Policy, War and Racial Extermination (Exeter, UK: University of Exeter Press).\n\nOrwell, G. (1983). 1984(New York: Signet Classic).\n\nPayne, S. (1995). A History of Fascism 1914–1945 (Madison, WI: University of Wisconsin Press).\n\nPinker, S. (2002). The Blank Slate: The Modern Denial of Human Nature (New York: Viking).\n\nPipes, R. (1994). Russia Under the Bolshevik Regime (New York: Vintage Books).\n\nSalisbury, H. (1992). The New Emperors: China in the Era of Mao and Deng (New York: AvonBooks).\n\nTetlock, P. (2005). Expert Political Judgment: How Good is It? How Can We Know? (Princeton, NJ: Princeton University Press).\n\nWerth, N. (1999). A state against its people: violence, repression, and terror in the Soviet Union. In Courtois, S., Werth, N., Panné, J.-L., Paczkowski, A., Bartošek, K., and Margolin, J.-L. The Black Book of Communism: Crimes, Terror, Repression, pp.33-628 (Cambridge: Harvard University Press).\n\nWright, R. (2000). Nonzero: The Logic of Human Destiny (New York: Pantheon Books).\n\nAuthors’ biographies\n\nGary Ackerman is research director of the National Consortium for the Study of Terrorism and Responses to Terrorism (START), a US Department of Homeland Security Center of Excellence. Ackerman concurrently also holds the post of director of the Center for Terrorism and Intelligence Studies, a private research and analysis institute. Before taking up his current positions, Ackerman was director of the Weapons of Mass Destruction Terrorism Research Program at the Center for Nonproliferation Studies in Monterey, California, and earlier he served as the chief of operations of the South Africa-based African-Asian Society. He received his M.A. in International Relations (Strategic Studies – Terrorism) from Yale University and his Bachelors (Law, Mathematics, International Relations) and Honors (International Relations) degrees from the University of the Witwatersrand in Johannes burg, South Africa. Originally hailing from South Africa, Ackerman possesses an eclectic academic background, including past studies in the fields of mathematics, history, law, and international relations, and has won numerous academic awards. His research encompasses various areas relating to terrorism and counterterrorism, including terrorist threat assessment, terrorist technologies and motivations, terrorism involving chemical, biological, radiological, and nuclear (CBRN) weapons, terrorist financing, environmental extremism, and the modelling and simulation of terrorist behaviour.\n\nFred Adams was born in Redwood City and received his undergraduate training in mathematics and physics from Iowa State University in 1983 and his Ph.D. in Physics from the University of California, Berkeley, in 1988. For his Ph.D. dissertation research, he received the Robert J. Trumpler Award from the Astronomical Society of the Pacific. After a post-doctoral fellowship at the Harvard-Smithsonian Center for Astrophysics, he joined the faculty in the Physics Department at the University of Michigan in 1991. Adams is the recipient of the Helen B. Warner Prize from the American Astronomical Society and the National Science Foundation Young Investigator Award. He has also been awarded both the Excellence in Education Award and the Excellence in Research Award from the College of Literature, Arts, and Sciences (at Michigan). In 2002, he was given The Faculty Recognition Award from the University of Michigan. Adams works in the general area of theoretical astrophysics with a focus on star formation and cosmology. He is internationally recognized for his work on the radiative signature of the star formation process, the dynamics of circum stellar disks, and the physics of molecular clouds. His recent work in star formation includes the development of a theory for the initial mass function for forming stars and studies of extra-solar planetary systems. In cosmology, he has studied many aspects of the inflationary universe, cosmological phase transitions, magnetic monopoles, cosmic rays, anti-matter, and the nature of cosmic background radiation fields. His work in cosmology includes a treatise on the long-term fate and evolution of the universe and its constituent astrophysical objects.\n\nMyles R. Allen is joint head of the Climate Dynamics Group, Atmospheric, Oceanic, and Planetary Physics, Department of Physics, University of Oxford. His research focuses on the attribution of causes of recent climate change, particularly changing risks of extreme weather, and assessing what these changes mean for the future. He has worked at the Energy Unit of the United Nations Environment Programme, the Rutherford Appleton Laboratory, and the Massachusetts Institute of Technology. He has contributed to the Intergovernmental Panel on Climate Change as lead author on detection of change and attribution of causes for the 2001 Assessment, and as review editor on global climate projections for the 2007 Assessment. He is principal investigator of the climateprediction.net project, also known as the BBC Climate Change Experiment, using public resource distributed computing for climate change modelling. He is married to Professor Irene Tracey and has three children.\n\nNick Bostrom is director of the Future of Humanity Institute at Oxford University. He previously taught in the Faculty of Philosophy and in the Institute for Social and Policy Studies at Yale University. He has a background in physics and computational neuroscience as well as philosophy. Bostrom’s research covers the foundations of probability theory, scientific methodology, and risk analysis, and he is one of the world’s leading experts on ethical issues related to human enhancement and emerging technologies such as artificial intelligence and nanotechnology. He has published some 100 papers and articles, including papers in Nature, Mind, Journal of Philosophy, Bioethics, Journal of Medical Ethics, Astrophysics &Space Science, one monograph, Anthropic Bias (Routledge, New York, 2002), and two edited volumes with Oxford University Press. One of his papers, written in 2001, introduced the concept of an existential risk. His writings have been translated into more than 14 languages. Bostrom has worked briefly as an expert consultant for the European Commission in Brussels and for the Central Intelligence Agency in Washington, DC. He is also frequently consulted as a commentator by the media. Preprints of many of his papers can be found on his website, http://www.nickbostrom.com.\n\nBryan Caplan received his Ph.D. in Economics in 1997 from Princeton University, and is now an associate professor of Economics at George Mason University. Most of his work questions the prevailing academic assumption of voter rationality; contrary to many economists and political scientists, mistaken voter beliefs do not harmlessly balance each other out. Caplan’s research has appeared in the American Economic Review, the Economic Journal, the Journal of Law and Economics, Social Science Quarterly, and numerous other outlets. He has recently completed The Logic of Collective Belief, a book on voter irrationality. Caplan is a regular blogger at Econlog, http://www.econlog.econlib.org; his website is http://www.bcaplan.com.\n\nChristopher F. Chyba is professor of astrophysical sciences and international affairs at Princeton University, where he also directs the Program on Science and Global Security at the Woodrow Wilson School of Public and International Affairs. Previously, he was co-director of the Center for International Security and Cooperation at Stanford University. He was a member of the White House staff from 1993 to 1995, entering as a White House Fellow, serving on the National Security Council staff and then in the Office of Science and Technology Policy’s National Security Division. He is a member of the Committee on International Security and Arms Control of the US National Academy of Sciences. His degrees are in physics, mathematics, history and philosophy of science, and astronomy and space sciences. His work addresses international security (with a focus on nuclear and biological weapons proliferation and policy) as well as solar system physics and astrobiology. He has published in Science, Nature, Icarus, Foreign Affairs, Survival, International Security, and elsewhere. In October 2001 he was named a Mac Arthur Fellow for his work in both planetary sciences and international security. Along with Ambassador George Bunn, he is co-editor of the recent volume US Nuclear Weapons Policy: Confronting Today’s Threats.\n\nJoseph Cirincione is a senior fellow and director for nuclear policy at the Center for American Progress and the author of Bomb Scare: The History and Future of Nuclear Weapons (Columbia University Press, Spring 2007). Before joining the Center in May 2006, he served as director for non-proliferation at the Carnegie Endowment for International Peace for eight years. He teaches a graduate seminar at the Georgetown University School of Foreign Service, has written over 200 articles on defence issues, produced two DVDs on proliferation, appears frequently in the media, and has given over 100 lectures around the world in the past 2 years. Cirincione worked for nine years in the US House of Representatives on the professional staff of the Committee on Armed Services and the Committee on Government Operations and served as staff director of the Military Reform Caucus. He is the co-author of Contain and Engage: A New Strategy for Resolving the Nuclear Crisis with Iran (March 2007), two editions of Deadly Arsenals: Nuclear, Biological and Chemical Threats (2005 and 2002), Universal Compliance: A Strategy for Nuclear Security (March 2005) and WMD in Iraq, (January 2004). He was featured in the 2006 award-winning documentary Why We Fight. He is a member of the Council on Foreign Relations and the International Institute for Strategic Studies.\n\nMilan M. Ćirković is a research associate of the Astronomical Observatory of Belgrade, (Serbia) and a professor of cosmology at the Department of Physics, University of Novi Sad (Serbia). He received his Ph. D. in Physics from the State University of New York at Stony Brook (USA), M.S. in Earth and Space Sciences from the same university, and his B.S. in Theoretical Physics from the University of Belgrade. His primary research interests are in the fields of astrophysical cosmology (baryonic dark matter, star formation, future of the universe), astrobiology (anthropic principles, SETI studies, catastrophic episodes in the history of life), as well as philosophy of science (risk analysis, foundational issues in quantum mechanics and cosmology). A unifying theme in these fields is the nature of physical time, the relationship of time and complexity, and various aspects of entropy-increasing processes taking place throughout the universe. He wrote one monograph (QSO Absorption Spectroscopy and Baryonic Dark Matter ; Belgrade, 2005) and translated several books, including titles by Richard P. Feynman and Roger Penrose. In recent years, his research has been published in Monthly Notices of the Royal Astronomical Society, Physics Letters A, Astrobiology, New Astronomy, Foundations of Physics, Philosophical Quarterly and other major journals.\n\nArnon Dar is a professor of physics at the Department of Physics and the Asher Space Research Institute of the Technion, Israel Institute of Technology, Haifa and is the incumbent of the Naite and Beatrice Sherman Chair in physics. He received his Ph.D. in 1964 from the Weizmann Institute of Science in Rehovot for inventing the Diffraction Model of direct nuclear reactions. After its generalization to high energy particle reactions, he worked on the quark model of elementary particles at the Weizmann Institute and MIT. In the late 1960s and early 1970s, he applied the quark model to the interaction of high energy elementary particles, nuclei and cosmic rays with atomic nuclei, while working at the Technion, MIT, the University of Paris at Orsay, and Imperial College, London. In the late 1970s, he became interested in neutrino physics and neutrino astronomy. Since the early 1980s his main research interest has been particle astrophysics and cosmology, particularly astrophysical and cosmological tests of the standard particle-physics model, the standard Big Bang model, and general relativity. These included studies of cosmic puzzles such as the solar neutrino puzzle, the origin of cosmic rays and gamma-ray bursts, dark matter and dark energy. The research was done at the Technion, the University of Pennsylvania, the Institute of Advanced Study in Princeton, NASA’s Goddard Space Flight Center, the Institute of Astronomy, Cambridge, UK, and the European research centre, CERN in Geneva. In collaboration with various authors, he suggested the existence of cosmic backgrounds of energetic neutrinos from stellar evolution, supernova explosions, and cosmic ray interactions in external galaxies, the day-dight Effect in solar neutrinos, tests of neutrino oscillations with atmospheric neutrinos, gravitational lensing tests of general relativity at very large distances and the supernova-gamma ray burst-cosmic rays-mass extinction connection. His most recent work has been on a unified theory of cosmic accelerators, gamma-ray bursts, and cosmic rays. He published more than 150 scientific papers in these various fields in professional journals and gave more than 100 invited talks, published in the proceedings of international conferences. He won several scientific awards and served the Technion, Israel Defense Ministry, Israel Atomic Energy Committee, and numerous national and international scientific committees and advisory boards.\n\nDavid Frame holds a Ph.D. in Physics and a bachelors degree in philosophy and physics from the University of Canterbury, in New Zealand. He spent two years working in the Policy Coordination and Development section of the New Zealand Treasury as an economic and social policy analyst, followed by a stint in the Department of Meteorology at the University of Reading, working on the PREDICATE project. In 2002 he moved to the Climate Dynamics group in the Atmospheric, Oceanic and Planetary Physics, sub-department of Physics at the University of Oxford, where he managed the climateprediction.net experiment.\n\nYacov Y. Haimes is the Lawrence R. Quarles Professor of Systems and Information Engineering, and founding director (1987) of the Center for Risk Management of Engineering Systems at the University of Virginia. He received his M.S. and Ph.D. (with Distinction) degrees in Systems Engineering from UCLA. On the faculty of Case Western Reserve University for 17 years (1970-1987), he served as chair of the Systems Engineering Department. As AAAS-AGU Congressional Science Fellow (1977-1978), Haimes served in the Office of Science and Technology Policy, Executive Office of the President, and on the Science and Technology Committee, US House of Representatives. He is a fellow of seven societies, including the IEEE, INCOSE, and the Society for Risk Analysis (where he is a past President). The second edition of his most recent book, Risk Modeling, Assessment, and Management, was published by Wiley &Sons in 2004 (the first edition was published in 1998). Haimes is the recipient of the 2001 Norbert Weiner Award, the highest award presented by the Institute of Electrical and Electronics Engineers; Systems, Man, and Cybernetics Society, Systems, Man, and Cybernetics Society, the 2000 Distinguished Achievement Award, the highest award presented by the Society for Risk Analysis, the 1997 Warren A. Hall Medal, the highest award presented by Universities Council on Water Resources, the 1995 Georg Cantor Award, presented by the International Society on Multiple Criteria Decision Making, and the 1994 Outstanding Contribution Award presented by the Institute of Electrical and Electronics Engineers; Systems, Man, and Cybernetics Society, among others. He is the Engineering Area Editor of Risk Analysis: An International Journal, member of the Editorial Board of Journal of Homeland Security and Emergency Management, and Associate Editor of Reliability Engineering and Systems Safety. He has served on and chaired numerous national boards and committees, and as a consultant to public and private organizations. He has authored (and co-authored) six books and over 250 editorials and technical publications, edited 20 volumes, and has served as dissertation/thesis advisor to over 30 Ph.D. and 70 M.S. students. Under Haimes’ direction, the Center for Risk Management of Engineering Systems has focused most of its research during the last decade on risks to infrastructures and safety-critical systems.\n\nRobin Hanson is an assistant professor of Economics, and received his Ph.D. in 1997 in social sciences from Caltech. He joined George Mason’s economics faculty in 1999 after completing a two year postdoc at University of California, Berkeley. His major fields of interest include health policy, regulation, and formal political theory. He is known as an expert on idea futures markets and was involved in the creation of the Foresight Exchange and DARPA’s Future MAP project. He is also known for inventing Market Scoring Rules such as LMSR (Logarithmic Market Scoring Rule) used by prediction markets such as Inkling Markets and Washington Stock Exchange, and has conducted research on signalling.\n\nJames J. Hughes is a bioethicist and sociologist at Trinity College in Hartford, Connecticut, where he teaches health policy. He holds a doctorate in sociology from the University of Chicago, where he also taught bioethics and health policy at the MacLean Center for Clinical Medical Ethics. Hughes serves as the Executive Director of the Institute for Ethics and Emerging Technologies and its affiliated World Transhumanist Association. He is the author of Citizen Cyborg: Why Democratic Societies Must Respond to the Redesigned Human of the Future (Westview Press, 2004) and produces a syndicated weekly programme, Changesurfer Radio. In the 1980s, while working in Sri Lanka, Hughes was briefly ordained as a Buddhist monk, and he is working on a second book ‘Cyborg Buddha: Spirituality and the Neurosciences’. He is a fellow of the World Academy of Arts and Sciences, and a member of the American Society of Bioethics and Humanities and the Working Group on Ethics and Technology at Yale University. Hughes lives in rural eastern Connecticut with his wife, the artist Monica Bock, and their two children.\n\nEdwin Dennis Kilbourne has spent his professional lifetime in the study of infectious diseases, with particular reference to virus infections. His early studies of coxsackieviruses and herpes simplex preceded intensive study of influenza in all of its manifestations. His primary contributions have been to the understanding of influenza virus structure and genetics and the practical application of these studies to the development of influenza vaccines and to the understanding of the molecular epidemiology and pathogenesis of influenza. His studies of influenza virus genetics resulted in the first genetically engineered vaccine of any kind for the prevention of human disease. The approach was not patented, and recombinant viruses from his laboratory have been used by all influenza vaccine manufacturers since 1971. A novel strategy for infection permissive influenza immunization has received two US Patents. Following his graduation from Cornell University Medical College in 1944, and an internship and residency in medicine at the New York Hospital, he served 2 years in the Army of the United States. After 3 years at the former Rockefeller Institute, he served successively as associate professor of Medicine at Tulane University, as Professor of Public Health at Cornell University Medical College, and as founding Chairman of the Department of Microbiology at Mount Sinai School of Medicine at which he was awarded the rank of Distinguished Service Professor. His most recent academic positions were as Research Professor and then as Emeritus Professor at New York Medical College. He is a member of the Association of American Physicians and the National Academy of Sciences and was elected to membership in the American Philosophical Society in 1994. He is the recipient of the Borden Award of the Association of American Medical Colleges for Outstanding Research in Medical Sciences, and an honorary degree from Rockefeller University in addition to other honors and lectureships. As an avocation, Kilbourne has published light verse and essays and articles for the general public on various aspects of biological science – some recently collected in a book on Strategies of Sex.\n\nWilliam Napier is an astronomer whose research interests are mostly to do with the interaction of comets and asteroids with the Earth. He co-authored the first paper (Napier &Clube, 1979) to point out that the impact rates then being found were high enough to be relevant on timescales from the evolutionary to the historical, and has co-authored 3 books on the subject and written about 100 papers. His career covers 25 years at the Royal Observatory, Edinburgh, 2 at Oxford and 9 at Armagh Observatory, from which he retired in March 2005. He now writes novels with a scientific background (Nemesis was referred to in a House of Lords debate on the impact hazard). This allows him to pursue a peripatetic research career, working with colleagues in La Jolla, Armagh and Cardiff. He is an honorary professor at Cardiff University.\n\nAli Nouri is a post-doctoral fellow at Princeton’s Program on Science and Global Security where he works on issues related to biological security. His interests include designing non-proliferation schemes to curb the potential misuse of biotechnology. Before joining the program, Nouri was engaged with biotechnology-related activities at the United Nations office of the Secretary General. Nouri holds a Ph.D. in molecular biology from Princeton University, where he studied the role of various tumour suppressor genes during animal development. Before Princeton, he was a research assistant at the Oregon Health Sciences University and studied the molecular basis of retroviral entry into cells. He holds a B.A. in Biology from Reed College.\n\nChris Phoenix, director of research at the Center for Responsible Nanotechnology (CRN), has studied nanotechnology for more than 15 years. He obtained his B.S. in Symbolic Systems and M.S. in Computer Science from Stanford University in 1991. From 1991 to 1997, Phoenix worked as an embedded software engineer at Electronics for Imaging. In 1997, he left the software field to concentrate on dyslexia correction and research. Since 2000, he has focused exclusively on studying and writing about molecular manufacturing. Phoenix, a published author in nanotechnology and nanomedical research, serves on the scientific advisory board for Nanorex, Inc., and maintains close contacts with many leading researchers in the field. He lives in Miami, Florida.\n\nRichard A. Posner is a judge of the U.S. Court of Appeals for the Seventh Circuit, in Chicago, and a senior lecturer at the University of Chicago Law School. He is the author of numerous books and articles, mainly dealing with the application of economics to law and public policy. His recent books include the sixth edition of Economic Analysis of Law (Aspen Publishers, 2003); Catastrophe: Risk and Response (Oxford University Press, 2004); and Preventing Surprise Attacks: Intelligence Reform in the Wake of 9/11 (Hoover Institution and Rowman; Littlefield, 2005). He is a former president of the American Law and Economics Association, a former editor of the American Law and Economics Review, and a Corresponding Fellow of the British Academy. He is the recipient of a number of honorary degrees and prizes, including the Thomas C. Schelling Award for scholarly contributions that have had an impact on public policy from the John F. Kennedy School of Government at Harvard University.\n\nWilliam Potter is institute professor and director of the Center for Nonproliferation Studies at the Monterey Institute of International Studies (MIIS). He is the author or editor of 14 books, the most recent of which is The Four Faces of Nuclear Terrorism (2005). Potter has been a member of numerous committees of the National Academy of Sciences and currently serves on the Non-proliferation Panel of the Academy’s Committee on International Security and Arms Control. He is a member of the Council on Foreign Relations, the Pacific Council on International Policy, and the International Institute for Strategic Studies, and has served for five years on the UN Secretary-General’s Advisory Board on Disarmament Matters and the Board of Trustees of the UN Institute for Disarmament Research. He currently serves on the International Advisory Board of the Center for Policy Studies in Russia (Moscow). He was an advisor to the delegation of Kyrgyzstan to the 1995 NPT Review and Extension Conference and to the 1997, 1998, 1999, 2002, 2003 and 2004 sessions of the NPT Preparatory Committee, as well as to the 2000 and 2005 NPT Review Conferences.\n\nMichael R. Rampino is an associate professor of Biology working with the Environmental Studies Program at New York University (New York City, USA), and a research associate at the NASA, Goddard Institute for Space Studies in New York City. He has visiting appointments at the Universities of Florence and Urbino (Italy), the University of Vienna (Austria), and Yamaguchi University (Japan). He received his Ph.D. in Geological Sciences from Columbia University (New York City, USA), and completed a post-doctoral appointment at the Goddard Institute. His research interests are in the fields of the geological influences on climatic change (such as explosive volcanism), astrobiology (evolution of the Universe, planetary science, catastrophic episodes in the history oflife) as well as the history and philosophy of science. He has edited or co-edited several books on climate (Climate: History, Periodicity and Predictability; Van Nostrand Reinhold, 1987); catastrophic events in Earth history (Large Ecosystem Perturbations; The Geological Society of America, 2007; and K-T Boundary Events (Special Issue); Springer, 2007), and co-authored a text in astrobiology with astrophysicist Robert Jastrow (Stars, Planets and Life: Evolution of the Universe; Cambridge University Press, 2007). He is also the co-editor of the Encyclopedia of Earth Sciences Series for Springer. His research has been published in Nature, Science, Proceedings of the National Academy of Sciences (USA), Geology and other major journals.\n\nSir Martin J. Rees is professor of cosmology and astrophysics and Master of Trinity College at the University of Cambridge. He holds the honorary title of Astronomer Royal and is also visiting professor at Imperial College London and at Leicester University. He has been director of the Institute of Astronomy and a research professor at Cambridge. He is a foreign associate of the National Academy of Sciences, the American Philosophical Society, and the American Academy of Arts and Sciences, and is an honorary member of the Russian Academy of Sciences, the Pontifical Academy, and several other foreign academies. His awards include the Balzan International Prize, the Bower Award for Science of the Franklin Institute, the Cosmology Prize of the Peter Gruber Foundation, the Einstein Award of the World Cultural Council and the Crafoord Prize (Royal Swedish Academy). He has been president of the British Association for the Advancement of Science (1994-1995) and the Royal Astronomical Society (1992-1994). In 2005 he was appointed to the House of Lords and elected President of the Royal Society. His professional research interests are in high energy astrophysics and cosmology He is the author or co-author of more than 500 research papers, and numerous magazine and newspaper articles on scientific and general subjects. He has also written eight books – including Our Final Century?, which highlighted threats posed by technological advances.\n\nPeter Taylor is a research associate at the Future of Humanity Institute in Oxford, with a background in science, mathematics, insurance, and risk analysis. Following a B.A. in Chemistry and a D.Phil. in Physical Science, Taylor spent 25 years in the City of London first as a management consultant and then as a director of insurance broking, underwriting, and market organizations in the London insurance market. During this time, he was responsible for IT, analysis, and loss modelling departments and led and participated in many projects. Taylor is also Deputy Chairman of the Lighthill Risk Network (www.lighthillrisknetwork.org), created in 2006 to link the business and science communities for their mutual benefit.\n\nMike Treder, executive director of the Center for Responsible Nanotechnology (CRN), is a professional writer, speaker, and activist with a background in technology and communications company management. In addition to his work with CRN, Treder is a consultant to the Millennium Project of the American Council for the United Nations University, serves on the Scientific Advisory Board for the Lifeboat Foundation, is a research fellow with the Institute for Ethics and Emerging Technologies, and is a consultant to the Future Technologies Advisory Group. As an accomplished presenter on the societal implications of emerging technologies, he has addressed conferences and groups in the United States, Canada, Great Britain, Germany, Italy, and Brazil. Treder lives in New York City.\n\nFrank Wilczek is considered one of the world’s most eminent theoretical physicists. He is known, among other things, for the discovery of asymptotic freedom, the development of quantum chromodynamics, the invention of axions, and the discovery and exploitation of new forms of quantum statistics (anyons). When he was only 21 years old and a graduate student at Princeton University, working with David Gross he defined the properties of colour gluons, which hold atomic nuclei together. He received his B.S. degree from the University of Chicago and his Ph.D. from Princeton University. He taught at Princeton from 1974 to 1981. During the period 1981-1988, he was the Chancellor Robert Huttenback Professor of Physics at the University of California at Santa Barbara, and the first permanent member of the National Science Foundation’s Institute for Theoretical Physics. In the fall of 2000, he moved from the Institute for Advanced Study, where he was the J.R. Oppenheimer Professor, to the Massachusetts Institute of Technology, where he is the Herman Feshbach Professor of Physics. Since 2002, he has been an Adjunct Professor in the Centro de Estudios Científicos of Valdivia, Chile. In 2004 he received the Nobel Prize in Physics and in 2005 the King Faisal Prize. He is a member of the National Academy of Sciences, the Netherlands Academy of Sciences, and the American Academy of Arts and Sciences, and is a Trustee of the University of Chicago. He contributes regularly to Physics Today and to Nature, explaining topics at the frontiers of physics to wider scientific audiences. He received the Lilienfeld Prize of the American Physical Society for these activities.\n\nChristopher Wills is a professor of biological sciences at the University of California, San Diego. Trained as a population geneticist and evolutionary biologist, he has published more than 150 papers in Nature, Science, the Proceedings of the U.S. National Academy of Sciences and elsewhere, on a wide diversity of subjects. These include the artificial selection of enzymes with new catalytic capabilities, the distribution and function of micro satellite repeat DNA regions, and most recently the maintenance of diversity in tropical forest ecosystems. He has also written many popular articles for magazines such as Discover and Scientific American and has published six popular books on subjects ranging from the past and future evolution of our species to (with Jeff Bada) the origin of life itself. In 1999 he received the Award for Public Understanding of Science from the American Association for the Advancement of Science, and in 2000 his book Children of Prometheus was a finalist for the Aventis Prize. He lives in La Jolla, California, and his hobbies include travel, SCUBA diving, and photography.\n\nEliezer Yudkowsky is a research fellow of the Singularity Institute for Artificial Intelligence, a non-profit organization devoted to supporting full-time research on very-long-term challenges posed by Artificial Intelligence. Yudkowsky’s current focus as of2006 is developing a reflective decision theory, a foundation for describing fully recursive self-modifying agents that retain stable preferences while rewriting their source code. He is the author of the papers, ‘Levels of organization in general intelligence’ and ‘Creating friendly AI’, and assorted informal essays on human rationality.\n\nIndex\n\nNote: page numbers in italics refer to Figures and Tables.\n\n2-4-6 task 98–9\n\n1984, Orwell, G. 506–7, 508, 510, 511, 517\n\nAbbott, D. et al. 234\n\nabnormal spindle-like microcephaly-associated (ASPM ) gene 61\n\n      global frequencies 60\n\nabrupt global warming 192, 193, 197–8\n\naccelerating returns, law of 79\n\nAcceleration Studies Foundation 80\n\naccelerator risks 5, 18–19, 347–8, 356–7\n\n      black holes 348–50\n\n      RHIC, cost-benefit analysis 189–90\n\n      strangelets 350–4\n\n      vacuum instability 354–6\n\nAdams, F.C. and Laughlin, G. 45\n\nadaptation to climate change 277, 278–9\n\nadjustment of estimates 102\n\naerosol formation, volcanic eruptions 206–7\n\naerospace industry, impact of molecular manufacturing 489\n\naffect heuristic 104–5, 106\n\nAfrica, mortality from infectious diseases 288\n\nage distributions, impact craters 223–4\n\nageing 27–8\n\naggregate exceedance probability (AEP) curve 175\n\naggregates, insurance industry 173\n\naggregative consequentialism 4\n\nagriculture\n\n      artificial selection 64–5\n\n      development 365\n\n      effects of asteroid impact 231\n\n      effects of volcanic super-eruptions 213\n\n      technological changes 67\n\nAIDS see HIV/AIDS\n\nalbedo 240–1, 495\n\nAlbright, D. 423\n\naleatory uncertainty 172\n\nAleph 420\n\nAllen, C. et al. 340\n\nAllison, G. 423\n\nAlpert, M. and Raiffa, H. 107–8\n\nal Qaeda 77, 85, 413\n\n      interest in biological weapons 467\n\n      nuclear ambitions 417–18, 420–2\n\nAlternating Gradient Synchrotron (AGS) 353\n\nAlvarez, L. 413\n\nAmazonian dieback 274\n\nAmbrose, S.H. 212\n\namillenialism 75–6\n\nancestor-simulation 138–40\n\nanchoring effects 102–4\n\nAnderson, H. 324, 325\n\nAndromeda, collision with Milky Way 37–8\n\nAnnan, K. 463\n\nanthrax 300, 456, 466\n\n      diagnosis 470\n\nanthropic bias 10, 120–2, 124–6\n\n      cometary impacts 127\n\n      Doomsday Argument 129–31\n\n      vacuum phase transition 123–4\n\nAnthropic Bias: Observation Selection Effects, Bostrom, N. 141\n\nanthropogenic climate forcing 271–3\n\nanthropomorphic bias 308–11, 312, 326\n\nApgar, D., Risk Intelligence: Learning to Manage What we Don’t Know 162\n\nApocalypse Pretty Soon: Travels in End-time America, Heard, A. 86\n\napocalypticism 77, 78, 84, 409, 417\n\n      techno-apocalypticism 81–3\n\na posteriori distribution function 124–6, 125\n\napplicability class of risks 126–8\n\na priori distribution function 124–6, 125\n\nApplied Insurance Research (AIR) 174, 175\n\narbovirus, as cause of yellow fever 289, 290\n\nArdipithecus ramidus 56\n\narea denial 408\n\narming sequences, nuclear weapons 415\n\nArm of the Lord (religious cult) 409\n\nartificial intelligence (AI) x, 17–18, 79, 308, 341–2, 494\n\n      anthropomorphic bias 310–11\n\n      capability and motive 314–18\n\n      current progress 339–40\n\n      failure 318–19\n\n         philosophical 319–20\n\n         technical 320–3\n\n      first use of term 338–9\n\n      Friendly 317–18, 326–7\n\n      hardware 328–9\n\n      interaction with other technologies 337–8\n\n      prediction and design 311–13\n\n      rates of intelligence increase 323–8, 335, 336–7\n\n      risk mitigation strategies 333–7\n\n      risks and promises 329–33\n\n      underestimation of impact 313–14\n\nArtificial Intelligence: A Modern Approach, S.J. Russell and P. Norvig 340\n\nartificial organisms 357–8\n\nartificial selection 64–5\n\nAsbeck, E.L. and Haimes, Y.Y. 157\n\nasbestos, latent health risk 168\n\nAsimov, I., ‘Three Laws of Robotics’ 340\n\nAssessing the Biological Weapons and Bioterrorism Threat, M. Leitenberg 452, 475\n\nasteroid defence, cost-benefit analysis 187, 193\n\nasteroids 14–15\n\n      as source of dust showers 231–3\n\nasteroid strikes 5–6, 51–2, 184, 214–15, 258–9\n\n      as cause of mass extinctions 255\n\n      contemporary risk 233–4\n\n      distribution functions 125\n\n      effects 229–31\n\n      frequency\n\n         dynamical analysis of near-Earth objects 226–9\n\n         impact craters 223–5\n\n         near-Earth object searches 226\n\n      on Moon 127\n\n      mythological accounts 222\n\n      uncertainties 234–5\n\nastrobiology 128–9\n\nastroengineering 133\n\nastronomy, Malquist bias 120\n\nAtlantic meridional overturning circulation disruption 274, 281\n\natmosphere\n\n      carbon dioxide content 243, 244\n\n      effects of volcanic eruptions 206–7, 208\n\n      protection from extraterrestrial radiation 239\n\natmospheric Atomic Energy Commission (AEC) 404\n\natomic fetishism, as motivation for nuclear terrorism 410, 417\n\nattitude polarization 100\n\nattitude strength effect 100\n\naudience impact, nuclear terrorism 407\n\nAum Shinrikyo 85, 406, 409, 410, 467\n\n      attempted anthrax attack 466\n\n      interest in Ebola virus 467\n\n      nuclear ambitions 417, 419–20, 422\n\nauthoritarianism 507, 509\n\nAutonomous Pathogen Detection System (APDS) 469\n\navailability heuristic, biases 92–3\n\navoidable risk 176\n\nAztecs, effect of smallpox 296\n\nBacillus anthracis 456\n\nbacteria, drug resistance 302–3\n\nbacterial toxins 300\n\nBacteriological (Biological) and Toxins Weapons Convention (BWC), 1972 453–4, 462, 463\n\nBaillie, M.G.L. 233\n\n      Exodus to Arthur 235\n\nbalancing selection 50, 63\n\nballistic nuclear weapons 396\n\nBandura, A. 408\n\nBanks, E., Catastrophic Risk 165, 182\n\nBaron, J. and Greene, J. 106\n\nBarrett, J.L. and Keil, F. 310\n\nBatcher, R.T. 390\n\nBaumgartner, F.J., Longing for the End: A History of Millenialism in Western Civilization 86\n\nBaxter, S. 140\n\nBayes theorem 122–3\n\nBear, G. 358\n\nBecker, J., Hungry Ghosts 518\n\nBecker, S.M. 430\n\nBefore the Beginning: Our Universe and Others, Rees, M.J 45\n\nbehavioural evolution\n\n      ongoing 61–3\n\n      as response to climate change 66–7\n\nbehavioural genetics, role in totalitarianism 511\n\nbehavioural responses to climate change 279\n\nBensimon, C.M. and Upshur, R.E.G. 472\n\nBenton, M.J. and Twitchett, R.J. 273\n\nbiases 10, 91–2, 113–15\n\n      affect heuristic 104–5\n\n      anchoring, adjustment and contamination effects 101–4\n\n      anthropomorphism 308–11, 312, 326\n\n      in availability heuristic 92–3\n\n      Black Swans 94–5\n\n      bystander apathy 109–11\n\n      cautions 111–12\n\n      confirmation bias 98–101\n\n      conjunction fallacy 95–8\n\n      hindsight bias 93–4\n\n      overconfidence 107–9\n\n      scope neglect 105–7\n\n      technical experts 19\n\n      see also observation selection effects\n\nbig bang, laws of physics 355\n\nBill and Melinda Gates Foundation 473\n\nBindeman, I.N. 216\n\nBin Laden, O. 417–18\n\nbiodiversity ix\n\nbiodiversity cycles 256\n\nbiodiversity preservation 20\n\nbiological weapons 453–4\n\n      difference from other weapons of mass destruction 454–5\n\n      use of nanoscale technology 484\n\nBiological Weapons: Limiting the Threat, Lederberg, J. 475\n\nbiosecurity threats ix, 22–4\n\nbiosphere, destruction of 34, 35\n\nbiotechnology 450–2\n\n      dual-use challenges 455–8\n\n      Luddite apocalypticism 81–2\n\n      micro- and molecular biology 458–60\n\n      rapidity of progress 454–5\n\n      risk management 460, 474–5\n\n         DNA synthesis technology 463–4, 465\n\n         international regulations 464\n\n         multi-stakeholder partnerships 462–3\n\n         novel pathogens 464–5\n\n         oversight of research 460–2\n\n         ‘soft’ oversight of research 462\n\n      use of nanoscale technology 484\n\nbioterrorism 82, 407, 451–2, 456–7\n\n      catastrophic attacks 466–8\n\n      infectious disease surveillance 469–70\n\n      prevention, inverse cost-benefit analysis 188–9\n\nBioWatch 469\n\nBird, K. and Sherwin, M.J. 403\n\nBlack, F. 301\n\nThe Black Book of Communism: Crimes, Terror, Repression, Courtois, S. et al. 518\n\nBlack Death 290, 294, 295\n\nblack holes 41\n\n      risk from particle accelerators 348–50\n\nThe Black Swan: The Impact of the Highly Improbable, Taleb, N. 162\n\nBlack Swans 94–5, 180\n\nBlair, B. 383–4\n\nblast, nuclear explosions 386\n\nblock-assembly operation, nanofactories 497\n\nBlood Music, G. Bear 358\n\nBomb Scare: The History and Future of Nuclear Weapons, Cirincione, J. 401\n\nbonobos, evolution 56\n\nBostrom, N. 103, 121, 130, 136, 138–9, 318, 512\n\n      Anthropic Bias: Observation Selection Effects 141\n\nbottom up climate models 266\n\nbotulinum toxin 456\n\nBovine Spongiform Encephalitis (BSE) 303\n\nbrain\n\n      acceleration of 331\n\n      evolution 56–7\n\n      gene regulation 58\n\nbrain function, gene changes 58, 61\n\nbrain scans, value in totalitarianism 511\n\nbrain size, increase 365\n\nBrave New World, Huxley, A. 512\n\n‘breakout’, technological 360\n\nBrenner, L.A. et al. 99\n\nBrillouin, L. 139\n\nBrin, G.D. 137\n\nBrock, J.C. and McClain, C.R. 211\n\nBrookhaven Relativistic Heavy Ion Collider report (2000) 18\n\nBrooks N. 283\n\nBrown, D.E. 312\n\nbrown dwarf stars 39\n\n      collisions 40\n\nBruce, G.B. et al. 386, 387\n\nbubonic plague 290, 291, 294, 295\n\nBuchanan, M., Ubiquity 181, 182\n\nBuddhism\n\n      messianism 77\n\n      post-millenialism 76\n\n      premillenialism 75\n\nBuehler, R. et al. 108–9\n\nBulletin of Atomic Scientists, doomsday clock vii, viii\n\nBunn, M. 423\n\nBurnet, F.M., Virus as Organism:\n\nEvolutionary and Ecological Aspects of Some Human Virus Diseases 304\n\nBurton, I. et al. 93\n\nbusiness world, perceived risks 168–9\n\nbusy conditions, increase of contamination effect 103\n\nbystander apathy 109–11\n\ncalibration of confidence intervals 107\n\nCalifornia, earthquake insurance 173\n\nCambodia, totalitarianism 517\n\nCameron, G. 406\n\nCampanian eruption 209\n\nCampbell, K. et al., The Nuclear Tipping Point: Why States Reconsider Their Nuclear Choices 401\n\nCannonball Model, GRBs 252\n\ncapability, definition 149\n\ncapital 363–4\n\n      effect of crises 367\n\ncarbon,¹⁴C production 250–1\n\ncarbon cycle feedbacks 273–5\n\ncarbon dioxide\n\n      atmospheric content viii, 243, 244\n\n      climate forcing 271–3\n\ncarbon dioxide emission\n\n      Kyoto Protocol 190–1\n\n      public policy 193–6, 280–2\n\ncarbon dioxide trapping 191\n\ncarbon nanotubes 482\n\ncardiovascular disease 27\n\nCarnegie Endowment for International Peace, Universal Compliance 394\n\nCarrington, R.C. 242\n\nCarson, R.T. and Mitchell, R.C. 106\n\nCarter, B. 129\n\nCassiopeia A supernova remnant 241\n\nCastle Bravo nuclear test 112\n\ncatastrophe loss models 173\n\nCatastrophic Risk, Banks, E. 165, 182\n\ncatastrophic risks, classification by insurance industry 166–7\n\ncauses of death\n\n      infectious diseases 289\n\n      probability judgements 92, 107\n\nCBRN (chemical, biological, radiological, nuclear) weapons 410, 418\n\n      see also biological weapons; nuclear terrorism\n\nCenter for International and Security Studies at Maryland (CISSM) 461\n\ncereals, artificial selection 64–5\n\nCERN Large Hadron Collider 18, 347–8\n\n      probability of strangelet production 353\n\n      see also accelerator disasters\n\nChallenger disaster 94\n\nChandrasekhar limit 245\n\nChapman, C.R. and Morrison, D. 215\n\nChari, P.R. 393\n\nchemical stockpiles, monitoring 454\n\nChesley, S.R. and Ward, S.N. 184\n\nchimpanzees\n\n      evolution 56\n\n      genome, comparison with human genome 57–8, 61\n\nChina\n\n      ballistic nuclear weapons 396\n\n      nuclear war risk 390, 393\n\nChoices, Values, and Frames, Kahneman, D and Tversky, A. 115\n\ncholera 291, 295\n\n      pandemic 1829–1851 290\n\n      treatment 294\n\ncholera Vibrio 289\n\nChristensen-Szalanski, J.J.J. and Bushyhead, J.B. 108\n\nChristianity\n\n      amillenialism 75–6\n\n      post-millenialism 76\n\n      premillenialism 73\n\n      utopianism 78\n\nChristopher, F.C. 475\n\nchromosome duplications 49, 53\n\nchronic disease risk, developed world 12\n\nChurch, G. 463, 464\n\nCialdini, R.B. 110\n\nCirincione, J., Bomb Scare: The History and Future of Nuclear Weapons 401\n\nĆirković, M.M. 45, 93, 135\n\nclathrate gun hypothesis 273\n\nCleaver, J. 112\n\nclimate\n\n      effect of asteroid impact 230, 231\n\n      effects of cosmic rays 248–9\n\n      effects of dust showers 233\n\n      effect of volcanic eruption 13–14, 205, 207–9, 213\n\n      nuclear winter 208, 381, 390–2\n\nclimate change 265–6, 281–2\n\n      agricultural consequences 65\n\n      as cause of mass extinctions 256\n\n      as cause of social collapse 366\n\n      dangerous risk 276–7\n\n      evolutionary responses 66–7\n\n      global warming viii–ix, 5, 15\n\n      historical 191–2\n\n      limits to current knowledge 273–6\n\n      mitigation policy 279–81\n\n      regional risks 278–9\n\n      risk from molecular manufacturing 494–5\n\n      see also global warming\n\nClimate Change Policy: A Survey, Schneider, S.H. et al. 283\n\nclimate forcing 267\n\n      anthropogenic 271–3\n\n      current forcing factors 268\n\n      mechanisms of amplification 273–5\n\n      solar 268–9\n\n      volcanic 269–71\n\nclimate modelling 266–7\n\n      uncertainty 275–6\n\ncloning 64\n\nClostridium botulinum 456\n\ncloud cover, relationship to cosmic ray flux 249\n\ncloud feedbacks, global warming 275\n\nClube, S.V.M. and Napier, W.M. The Cosmic Winter 235\n\ncodes of conduct, biotechnology 462\n\ncognitive biases 10\n\nCohn, N., The Pursuit of Millenialism: Revolutionary Millenarians and Mystical Anarchists of the Middle Ages 86\n\n‘cold’ confirmation bias 99\n\ncold degenerate stars, long-term evolution 42\n\nCold War vii, 20, 381–2, 490\n\ncollateral damage 173\n\ncolonization\n\n      minimum population 369–70\n\n      of other planets 67–8\n\n      as source of evolutionary change 55\n\nCombs, B. and Slovic, P. 92\n\ncomets 14–15, 226–7, 229\n\n      anthropic bias 127\n\n      as cause of mass extinctions 255, 256\n\n      Comet Shoemaker-Levy, collision with Jupiter 240\n\n      contemporary risk 233–4\n\n      Oort cloud 225\n\n      as source of dust showers 232, 233\n\n      as source of meteor showers 228\n\n      uncertainties 234–5\n\ncommunism 319\n\n      amillenialism 76\n\n      ideology 514\n\nCommunist rule, Soviet Union 505, 506\n\n      stability 507–8\n\ncomplexity science 181\n\ncompound failure modes 161–2\n\nComprehensive Test Ban Treaty (1996) 394\n\ncomputer-based catastrophes, perceived risk 168\n\ncomputer chips\n\n      manufacture 482, 483\n\n      proof of validity 317–18\n\ncomputers\n\n      impact of molecular manufacturing 489\n\n      reversible 139, 359\n\ncomputing hardware, AI 328–9\n\nconditional expectations, PMRM 156, 157\n\nconditional expected value, PMRM 158\n\nconfirmation bias 98–101\n\nconflict resolution 400\n\nconjunction fallacy 95–8, 329\n\nconsumption 297\n\n      see also tuberculosis\n\ncontainment of epidemics 472–3\n\ncontamination effects, cognitive judgement 102–4\n\nConvention on the Physical Protection of Nuclear Material 438–9\n\ncopulas 173\n\nCordesman, A. 407\n\ncore-collapse supernovae 245, 251\n\ncoronal mass ejections (CMEs) 239, 243\n\ncosmic hazards 14–15, 238, 258–9\n\n      as cause of mass extinctions 256–7\n\n      changes in solar luminosity 238–42\n\n         solar extinction 245\n\n         solar flares 242–3\n\n      Earth magnetic field reversals 250\n\n      passage through Galactic spiral arms 251–2\n\n       see also asteroid strikes; comets; cosmic rays; gamma-ray bursts; supernovae\n\ncosmic perspective xi\n\ncosmic rays 18, 243, 247, 248–50, 259, 356–7\n\n      from gamma-ray bursts 252–5\n\n      from nearby supernovae 252\n\n      in Galactic spiral arms 251\n\ncosmic risk 3, 4\n\ncosmic sterilization rate 128\n\nThe Cosmic Winter, Clube, S.V.M. and Napier W.M. 235\n\ncosmology, anthropic bias 120–1\n\ncost-benefit analysis 185–7\n\n      global warming 192–200\n\n      inverse 187–9\n\n      Relativistic Heavy Ion Collider 189–90\n\n‘counter-force’ nuclear attack 388\n\ncounterterrorism efforts 439–40\n\n‘counter-value’ nuclear attack 388–9\n\nCourtois, The Black Book of Communism: Crimes, Terror, Repression 518\n\nCourty, M.-A. et al. 234\n\nCovenant (religious cult) 409\n\nCovey, C. et al. 215\n\nCretaceous-Tertiary extinction 52, 53\n\ncrime, as consequence of molecular cultural changesmanufacturing 492\n\ncrops, biological attack 468\n\nCrosby, A.W., Epidemic and Peace, 1918 304\n\nCuban missile crisis 21, 84\n\ncults ix–x, 409–10\n\ncultural changes, evolutionary consequences 56–61\n\ncultural diversity 17\n\ncultural homogeneity 301–2\n\nCultural Revolution, Maoist China 505\n\ncyber terror attacks 426\n\ncyclic processes, relationship to mass extinctions 256\n\nDado, S. et al. 242\n\nDamocloids 227, 229\n\ndangerous anthropogenic interference (DAI), climate system 276–7\n\nDar, A. 190, 247\n\ndark comets, impact risk 227, 229, 233, 235\n\ndark energy 33\n\nDark Era of universe 41–3\n\ndark matter annihilation 40\n\nDartmouth Proposal, AI (1955) 338–9\n\nDarwin, C. x, 49–50\n\ndata uncertainty, climate change 275\n\nDawes, R.M. 98, 101\n\n      Rational Choice in an Uncertain World: The Psychology of Intuitive Judgment 115\n\n‘deadly probes’ scenario, Fermi’s paradox 137\n\ndeath power of disasters 368–9\n\ndeath toll\n\n      from totalitarianism 505–6\n\n      global nuclear war 388, 389\n\n      limited nuclear war 387–8\n\n      South Asian Nuclear war 388, 390\n\nDecker, R.W. 212\n\ndefensive nano-technologies 496–7\n\nDegenerate Era of universe 39–41\n\nDeLong, J.B. 363\n\nDelta submarine strike, consequences 386, 387\n\ndemographic transitions 66\n\nDeng Xiaoping 507, 508\n\ndependency damage 173\n\ndesensitization to terrorism 430\n\nDesvouges, W.H. et al. 105, 106\n\ndeterministic systems, probability estimates 6\n\ndeveloping countries, vulnerability to biological attack 473–4\n\ndevelopmental period, artificial intelligence 322\n\ndiagnosis, infectious disease 469–70\n\nDiamond, J. 66, 357\n\nDick, S.J. 133\n\ndie rolling, conjunction fallacy 96–7\n\ndiffusion of responsibility 110\n\ndinosaurs, extinction 51\n\ndisaster policy 372–5\n\ndisconfirmation bias (motivated scepticism) 99, 100\n\ndiscount rates, global warming 192–3, 198, 200\n\ndisjunctive probability estimation 98\n\ndispensationalism 74\n\ndisruptive technologies 432\n\ndistribution of disaster 367–9\n\ndistribution tails 156–7\n\nDNA synthesis technology 458–60\n\n      increasing availability 450\n\n      outsourcing 465\n\n      risk management 463–4\n\ndollar-loss power of disasters 368–9\n\nDoomsday Argument 129–31\n\ndoomsday clock, Bulletin of Atomic Scientists vii, viii\n\ndotcom bubble burst, insurance costs 173\n\nDrake equation 214–15\n\nDrexler, K.E. 331, 485, 486, 488, 495\n\n      Engines of Creation 499–500\n\n      Nanosystems: Molecular Machinery, Manufacturing and Computation 501–2\n\nDrosophila melanogaster,\n\n         frequency-dependent balancing\n\n         selection 63\n\ndual-use challenge, biotechnology 451–2, 455–8\n\nDubos, R., Man Adapting 304\n\nduration, totalitarianism 506–10\n\ndust showers, cosmic 231–3\n\nDWIM (Do-What-I-Mean) instruction 322–3\n\nDynamics of Populations of Planetary Systems, Kneević, Z. and Milani, A. 235\n\ndysgenic pressures 62\n\nDyson, F, scaling hypothesis 43–4, 45\n\nearly warning systems\n\n      nuclear attack 384\n\n      terrorist-initiated false alarms 426–7\n\nEarth\n\n      ejection from solar system 35–6\n\n      end of complex life 8\n\n      fate of 34–5, 44\n\n      magnetic field reversals 250\n\n      variation in eccentricity 239\n\nEarth-crossing bodies, search for 226\n\nearthquake insurance, California 173\n\nearthquakes 7\n\n      risk mitigation 372\n\n      energy power 368\n\nEarth’s axis, wobbles 268\n\nEarth system models of intermediate complexity (EMICs) 266–7\n\nEbola virus 458\n\n      bioterrorism threat 467\n\neconomic consequences 20\n\n      of bioterrorism 467–8\n\n      of molecular manufacturing 492\n\n      of nuclear terrorism 428, 429\n\n      of SARS 474\n\neconomic interdependence, stabilizing action 490\n\neconomic loss power of disasters 368–9\n\necophagy 25, 495–6\n\necosystem collapse, as result of molecular manufacturing 495\n\nEddy, L. 386\n\neducation, role in nuclear disarmament 440–1\n\nThe Effects of Nuclear War, Office of Technology Assessment (1979) 389, 401\n\nElbaradei, M. 401\n\nEl Chichón eruption (1982)\n\n      effect on climate 270\n\n      effects on ocean productivity 211\n\nelectroweak theory 354–5\n\nEl Nino Southern Oscillation 278\n\nemerging diseases 16, 82\n\nemissions targets 277\n\nemissions taxes 194–6, 197, 198\n\nemotions, artificial intelligence 320–1\n\nempty space transitions 355–7\n\nEncke, comet 232\n\nEnd Cretaceous extinction 255\n\nend game issues 367\n\nThe End of the World: The Science and Ethics of Human Extinction, Leslie, J. 86\n\nendotoxins 300\n\nendurable risk 3, 4\n\nEndy, D. 464\n\nenergy budget climate model 267\n\nenergy costs\n\n      ancestor-simulation 139–40\n\n      nanofactories 485–6\n\nenergy density, cosmic rays 248\n\nenergy power of disasters 368–9\n\nenergy release\n\n      gamma-ray bursts 247, 254\n\n      particle accelerators 348\n\n      solar flares 243\n\nenergy taxes 195\n\nengineering, reliability analysis 160–1\n\nEngines of Creation, Drexler, K.E. 499–500\n\nEnron 173\n\nentrepreneurial ventures, survival rate 98\n\nenvironmental catastrophes 184–5\n\n      as result of molecular manufacturing 494–5\n\nenvironmental change, evolutionary consequences 50–1\n\n      effect of cultural changes 56–61\n\n      extreme evolutionary changes 51–2\n\n      ongoing evolutionary changes 53–5\n\nenvironmental concerns, global agreements 513–14\n\nEpidemic and Peace, 1918, Crosby, A.W 304\n\nepidemics 293–4\n\n      containment 472–3\n\n      coordinated management 470–1\n\n      public health sector mobilization 471\n\n      see also infectious diseases\n\nepistemic (subjective) risks 5, 176\n\nergodicity 6\n\nescalatory pressure, as motivation for nuclear terrorism 408\n\nethics\n\n      in AI systems 340\n\n      in biotechnology 462\n\n      in nanoscale technology 483\n\nEthiopian Drought insurance bond 166\n\nEuropean Union 513\n\nevolution 8, 48\n\n      consequences of environmental changes\n\n         effect of cultural changes 56–61\n\n         extreme evolutionary changes 51–2\n\n         ongoing evolutionary changes 53–5\n\n      future directions 65–6\n\n         colonization 67–8\n\n         responses to climate change 66–7\n\n      ongoing 53–5, 61\n\n         behavioural 61–3\n\n         genetic engineering 63–4\n\n         non-human species 64–5\n\nevolutionary biology 310–11\n\nevolutionary changes 49–50\n\nevolutionary programming (EP) 340\n\nexceedance probability, PMRM 157\n\nexceedance probability loss curves 175\n\nexistential risks 4, 112–13, 369–72\n\nExodus to Arthur, Baillie, M.G.L. 235\n\nexotoxins 300\n\nexpansion of universe 36, 37, 44\n\nexpected value of risk, limitations 155–6\n\nexpert opinion 91, 109, 112\n\n      overconfidence 108\n\nextinction risk, disaster policy 373\n\nextinctions ix, 8–9\n\n      of hominids 57\n\n      see also mass extinctions\n\nextra dimensions 349\n\nextrasolar planets 67–8, 132\n\nextraterrestrial intelligence\n\n      Drake equation 214–15\n\n      Fermi’s paradox 131–4, 257–8, 359–61\n\n         planetarium hypothesis 140\n\n         relationship to global catastrophic risks 134–5\n\n      as source of risk 135–8\n\nextreme value behaviour 167–8\n\nextreme value statistics 181\n\nextremophiles 132\n\nThe Faces of Janus: Marxism and Fascism in the Twentieth Century, Gregor, A.J. 518\n\nfailure, sources of 147\n\nfanaticism ix–x\n\nfarming, development 365\n\nfascism, millenialism 74\n\nThe Fate of the Earth, J. Schell 381\n\nfeedback loops\n\n      in climate change 273–5\n\n      role in evolution 57\n\nfeedstock, nanofactories 485\n\nFerguson, C.D. and Potter, W.C., The Four Faces of Nuclear Terrorism 442\n\nFermi, E. 112, 324–5, 347\n\nfermions, Pauli exclusion principle 351\n\nFermi’s paradox 131–4, 257–8, 359–61\n\n      ‘deadly probes’ scenario 137\n\n      planetarium hypothesis 140\n\n      relationship to global catastrophic risks 134–5\n\nFetherstonhaugh, D. et al. 106\n\nfinancial losses 164–5\n\nFinucane, M.L. et al. 104\n\nfirst-mover effect, AI 335–6\n\nFischhoff, B. 95\n\nFischhoff, B. and Beyth, R. 93\n\nfissile material, protection 412\n\nFive Ages of the Universe, Adams, F.C. and Laughlin, G. 45\n\nflooding\n\n      New Orleans 184, 185\n\n      risk perception 92–3, 93–4\n\nflood protection levees, design 161\n\nFlorida, Xanthomonas axonopodis outbreak (2001) 468\n\nfood security, bioterrorism threat 468\n\nfoodstuffs, global stockpile 14\n\nfoot-and-mouth disease, economic impact 468\n\nforcing uncertainty, climate change 275\n\nforests, effect of volcanic super-eruption 210\n\nfor locus, Drosophila melanogaster 63\n\nfossil fuel use, public policy 194–5\n\nThe Four Faces of Nuclear Terrorism, Ferguson, C.D. and Potter, W.C. 442\n\nFourier, J. 243\n\nFreakonomics: A Rogue Economist Explores\n\nthe Hidden Side of Everything, Levitt, S.D and Dubner, S.J. 162\n\nFreitas, R.A. 485, 486\n\n      Nanomedicine, Vol I: Basic Capabilities 501\n\nFreitas, R.A. and Merkle, R.C., Kinematic Self-replicating Machines 500–1\n\nfrequency-dependent selection 50, 63\n\nFriendly artificial intelligence 18, 312–13, 317–18, 326–7, 338\n\n      current progress 339–40\n\nFrost, R. 381\n\nfuturological forecasts, conjunction fallacy 97–8\n\nG-8 Global Partnership 436\n\ngalactic collisions 36–8\n\nGalactic spiral arms, passage of solar system 251–2, 256, 258\n\ngalaxies, dynamical destruction 39–40\n\ngamma-ray bursts (GRBs) 246–8, 259 030329 (2003), afterglow 242\n\n      as cause of mass extinctions 256–7\n\n      distribution function estimation 127\n\n      as source of cosmic rays 252–5\n\nGamma-ray Large Area Space Telescope (GLAST) 248\n\nGanzach, Y. 104–5\n\nGardner, H. 62\n\nGarreau, J., Radical Evolution 79, 81\n\nGarrett, L. 82\n\nGehrels, T., Hazards Due to Comets and Asteroids 235\n\ngene banks 65\n\ngene flow 55\n\ngeneral circulation models (GCMs) 266\n\n      forcing effect of carbon dioxide 272\n\ngeneral intelligence 313\n\ngene regulation, brain 58\n\ngene surgery 62\n\ngenetic engineering 515\n\n      likelihood of genetic elite 63–4\n\n      value in totalitarianism 511, 512\n\ngenetic homogeneity 301–2\n\nGenin, A. et al. 211\n\ngenocide 516\n\ngenome comparisons 48\n\n      humans and chimpanzees 57–8, 61\n\ngenome sequencing 458\n\n      human genome 455\n\ngenome transplantation 459\n\ngeo-engineering approaches, climate change 280\n\ngerm warfare 453\n\ng-factor 313\n\ng-factor metaphors, AI 330\n\nGhost Dance, Sioux 78\n\nGiant Cheesecake Fallacy 314–15, 317, 341\n\nGilbert, D.T. et al. 103\n\nGilovich, T. 99\n\nGIRO 174, 182\n\nGISP2 ice core, effect of Toba eruption 207\n\nglaciation cycles 268, 358\n\n      effects on biodiversity 256\n\n      Milankovitch theory 240–1\n\n      relationship to cosmic ray flux 251–2\n\nGlobal Brain scenario 80\n\nglobal catastrophic risk\n\n      study as a single field 1–2\n\n      taxonomy 2–6\n\nglobal cooling\n\n      after asteroid impact 231\n\n      nuclear winter 208, 381, 390–2\n\n      as result of dust shower 233\n\n      volcanic eruptions 205, 207–9, 210, 213, 269–71\n\nglobalization, pandemic risk 17\n\nglobal nuclear war 388–9\n\n      nuclear winter 390–2\n\nGlobal Outbreak and Response Network (GOARN), WHO 471\n\nGlobal Public Health Information Network (GPHIN), WHO 470, 471\n\nglobal totalitarianism 26, 67, 509–10\n\n      probability 516–17\n\n      risk factors\n\n         politics 512–14\n\n         molecular manufacturing 492–4, 498\n\n         technology 511–12\n\n      risk management\n\n         politics 515–16\n\n         technology 514–15\n\nglobal war, risk from molecular manufacturing 489–92, 496–7, 498–9\n\nglobal warming viii–ix, 5, 15, 185, 243–4, 258\n\n      after asteroid impact 231\n\n      agricultural consequences 65\n\n      cost-benefit analysis 192–200\n\n      due to brightening of Sun 34\n\n      historical 191–2\n\n      public policy 191–200\n\n         Kyoto Protocol 190–1\n\n      relationship to solar activity 250–1\n\n      sea level rise, evolutionary consequences 55\n\ngluons 350\n\nGod, anthropomorphic bias 310\n\nGoiania radiological incident (1987), psychological consequences 430\n\nGoleman, D. 62\n\ngood cause dump 106\n\nGood, I.J. 324, 340, 341\n\ngoodness-of-fit tests 156\n\ngood-story bias 103\n\nGorbachev, M. 508\n\nGore, Al ix\n\nGott, J.R. 130\n\ngovernmental collapse, scope for nuclear terrorism 410, 424, 425\n\ngrain, global stockpile 14, 214\n\ngravity, possible effect of particle accelerators 348–50\n\nGreat Filters 131–2\n\nGreat Leap Forward, Maoist China 505–6\n\nGreat Plague of London (1665-1666) 290\n\n‘Great Silence’ see Fermi’s paradox\n\ngreenhouse effect 243\n\n      see also global warming\n\ngreenhouse gases, climate forcing 269, 271–3\n\nGregor, A.J., The Faces of Janus: Marxism and Fascism in the Twentieth Century 518\n\nGriffin, D. and Tversky, A. 100–1\n\nGrinspoon, D., Lonely Planets: The Natural Philosophy of Alien Life 141\n\ngrowth, social 364–6\n\nGubrud, M. 493\n\ngun-type nuclear explosives 412\n\nGurr, N. and Cole, B. 406\n\nH5N1 influenza virus 299\n\nHalley-type comets 227\n\nHall, J.S. 486\n\n      Nanofuture 500\n\nhands, evolution 57\n\nHarpending, H.C. et al. 211–12\n\nHarwell, M.A. et al. 210\n\nHawking radiation 41, 42\n\nHawks, J. et al. 61\n\nHazards Due to Comets and Asteroids, Gehrels, T. 235\n\nhealth, contributions of biotechnology 544–6\n\nhealthcare, uses of nanoscale technology 483–4\n\nhealth workers, protection in epidemics 472–3\n\nHeard, A., Apocalypse Pretty Soon: Travels in End-time America 86\n\nheat death of universe 359\n\nheat release, nuclear explosions 386\n\nHeavens Gate cult ix–x\n\nHelin, E.F. and Shoemaker, E.M. 226\n\n‘hellish’ risk 3, 4\n\n‘hermit kingdoms’ 26, 509\n\nHerz, J. 431\n\nheterozygote advantage 50\n\nheuristics and biases programme 91–2\n\n      see also biases\n\nHibbard, B. 320–1\n\nhierarchical holographic modelling (HHM) 150–1\n\n      framework for identification of sources of risk 152\n\n      historical perspectives 151, 153\n\nHiggs condensate 354, 355\n\nHiggs particles 354–5, 356\n\nhigh-end correlations 167\n\nhighly enriched uranium (HEU) 412–13\n\n      loss from Sukhumi Nuclear Research Center 425\n\n      minimization of stockpiles 437–8\n\n      new enrichment technologies 425\n\nhindsight bias 93–4\n\nHinduism, messianism 77\n\nHiroshima bomb 427, 428\n\nhistorical global catastrophes 3\n\nA History of Fascism, Payne, S. 518\n\nhistory metaphors, AI 330\n\nHitler, A. 506, 507\n\nHIV/AIDS 27, 288, 290, 298–9\n\n      tuberculosis 299–300\n\nhoaxes, bioterrorism 468\n\nholistic epidemiology 287\n\nHolland, dike heights 167\n\nhomeotic genes 311\n\nHomo floresiensis (hobbits) 9, 54–5, 57\n\nhomogeneity, genetic and cultural 301–2\n\nhorizontal gene transfer 49\n\nHorn, S. 395\n\n‘hot’ confirmation bias 99\n\n‘hot-zone’ identification, infectious disease outbreaks 472\n\nH-R diagram 42\n\nHuang, C.-H. et al. 209\n\nHughes, D.W. 234\n\nHulme, M. 266\n\nhuman–computer connections 494\n\nhuman extinction, cost estimate 190\n\nhuman genome, comparison with chimpanzee genome 57–8, 61\n\nhuman genome sequencing 455\n\nhuman intelligence, effect of natural selection 326\n\nhuman mind, acceleration 331\n\nhumans\n\n      future development x\n\n      origins 211–12\n\nhumidity, effect on pathogens 292\n\nHungry Ghosts, Becker, J. 518\n\nhunter-gatherers 365\n\n      after societal collapse 374\n\nhurricane insurance 166\n\nHurricane Katrina 185\n\n      subsequent Vibrio infections 293\n\nhurricanes\n\n      effect of climate change 191, 278\n\n      financial losses 164\n\nHut, P. and Rees, M.J. 123–4, 128\n\nHuxley, A., Brave New World 512\n\nHynes, M.E. and Vanmarke, E.K. 108\n\nhyperons 350–1\n\nhypocrisy, totalitarianism 505\n\niatrogenic diseases 300–1\n\nice ages see glaciation cycles\n\nice-core records\n\n      effect of Toba eruption 207, 208\n\n      evidence of past global temperature changes 244\n\n      impact data 234\n\nideology\n\n      as motivation for nuclear terrorism 408–10, 417\n\n      in totalitarianism 504–5, 514\n\nIllinois Institute of Technology, explosives discovery 404\n\nimmunization 288\n\nimmunology research, dual-use challenge 457–8\n\nimpact craters 223–5\n\nimpact hazards 14–15\n\n      contemporary risk 233–4\n\n      dust effects 231–3\n\n      effects of strikes 229–31\n\n      frequency of strikes 223–9\n\n      uncertainties 234–5\n\n      see also asteroid strikes; comets\n\nimperceptible risk 3, 4\n\nimplied probability 13\n\nimprovised nuclear devices (INDs) 402, 403\n\n      consequences of use, physical and economic 427–9\n\n      implementation challenges 411–12, 413–14\n\n      see also nuclear terrorism\n\nincentives, effect on biases 113\n\nIndia\n\n      nuclear tests 395\n\n      nuclear war risk 390, 392–3, 400, 426\n\nIndian Ocean tsunami (2004) 184\n\nindividual empowerment ix\n\n      acquisition of nuclear weapons 418–19\n\nindustry, development 365–6\n\ninfectious diseases 16–17, 27, 61, 287–8\n\n      after natural catastrophes 293–4\n\n      bubonic plague 295\n\n      cholera 295\n\n      defences 473\n\n      effects of human behaviour 293\n\n      environmental factors 292\n\n      epidemics\n\n         containment 472–3\n\n         coordinated management 470–1\n\n         public health sector mobilization 471\n\n      HIV/AIDS 298–9\n\n      leading global causes of death 289\n\n      malaria 296\n\n      modes of transmission 290–1\n\n      nature of impact 291–2\n\n      pathogens 289\n\n      perceived risk 169\n\n      smallpox 296\n\n      syphilis 297\n\n      tuberculosis 297\n\n      see also influenza; pandemics\n\ninfectious disease surveillance 469–70, 474\n\ninflationary cosmology 355\n\ninfluenza 291, 298\n\n      pandemic, 1918–1919 16, 290, 292, 301–2\n\n         re-synthesis of virus 450, 459–60\n\n      threat of future pandemics 299, 303\n\ninfluenza virus 294\n\n      effect of humidity 292\n\n      genome publication 82\n\ninformation processing, relationship to operating temperature 43–4\n\ninformation value 114\n\ninstability, as result of global governance 494\n\ninsurance industry 11, 164–6, 169–72, 181–2\n\n      catastrophe loss models 173–5\n\n      classification of catastrophic\n\n         risks 166–7\n\n      new techniques 180–1\n\n      price of insurance, relationship to probability of risk 179\n\n      pricing risks 172–3\n\n      risk analysis 176–9\n\n      uncertainty 179–80\n\nintact nuclear weapons, acquisition by terrorists 414–16\n\nintelligence 313–14\n\n      heritability 62\n\n      ongoing evolution 62\n\n      see also artificial intelligence; extraterrestrial intelligence\n\nintelligence enhancement, as result of nanotechnology 494\n\nintelligence explosion 324, 340, 341\n\nintelligence increase rates, AI 323–8\n\nintensity of risk 3, 4\n\nintent, definition 149\n\nintergenomic comparisons 48\n\n      humans and chimpanzees 57–8, 61\n\nIntergovernmental Panel on Climate Change (IPCC) 15, 243, 265, 278, 283\n\n      models of temperature change and sea level rise 198, 199\n\ninterleukin research, dual-use challenge 457\n\nIntermediate Nuclear Forces agreement (1987) 382\n\ninternal power source of Earth 35\n\nInternational Atomic Energy Agency (IAEA) 454\n\nInternational Biotechnology Agency (IBTA) 461–2\n\ninternational relations, consequences of nuclear terrorism 431–2\n\ninterstellar medium (ISM) particles, effect of cosmic rays 252–3\n\ninterstellar travel 132–3\n\nintrinsic uncertainty 172\n\ninverse cost-benefit analysis 187–9\n\nIQ, heritability 62\n\nIran, nuclear programme 397–8\n\nIraq, nuclear weapons programme 396\n\niris effect 274\n\nIslam 73\n\nIslam, J.N., The Ultimate fate of the Universe 45\n\nJaffe, R. 189, 190\n\nJaffe, R. et al. 352, 361\n\nJapan, earthquakes, perceived risk 169\n\nJaynes, E.T. 310\n\nJenkins, B. 405\n\nJenkins, J. and LaHaye, T., Left Behind novels 75\n\njihadhists 418\n\nJohnson, E. et al. 97\n\njoint risk function 135\n\nJones Town suicides 85\n\nJoy, B. 81, 82\n\nJudgement Under Uncertainty: Heuristics and Biases, Kahneman, D. et al 115\n\nJupiter, impact of Comet Shoemaker–Levy 240\n\nKahneman, D. et al. 106\n\n      Judgement Under Uncertainty: Heuristics and Biases 115\n\nKahneman, D and Tversky, A., Choices, Values, and Frames 115\n\nKaiser, J. 461\n\nKamin, K. and Rachlinski, J. 93–4\n\nKampala Compact 452\n\nKaplan, S. and Garrick, B.J. 151, 176\n\nKareiva, P. et al. 68–9\n\nKargil region conflict 392\n\nKates, R. 92–3\n\nKeith, D.W. 280\n\nKelly, K. 80\n\nKelvin, Lord 342\n\n      estimation of age of Earth 112\n\nKennan, G., × article 452\n\nKhan, A.Q. 419, 422, 424\n\nKhrushchev, N. 507–8\n\nKilbourne, E.D. 304\n\nKinematic Self-replicating Machines, Freitas, R.A. Jr and Merkle, R.C. 500–1\n\nKirkby, J. et al. 241\n\nKneević, Z. and Milani, A., Dynamics of Populations of Planetary Systems 235\n\nKrepon, M. 393\n\nKunda, Z. 99\n\nKurchatov Institute 419\n\nKurzweil, R., The Singularity is Near 79, 80, 82, 361\n\nKyoto Protocol 190–1, 193, 277, 513\n\nlaboratory safety, pathogenic organisms 460\n\nLa Garita Caldera 270–1\n\nLandauer, R. 139\n\nLandauer-Brillouin’s limit 331\n\nlanguage, evolution 57\n\nLarcher, W. and Bauer, H. 210\n\nLarge Hadron Collider (LHC), CERN 347–8\n\n      probability of strangelet production 353\n\n      see also particle collider experiments\n\nlaser enrichment of uranium 425\n\nLatane, B. and Darley, J. 109–10\n\nThe Late Great Planet Earth, H. Lindsey 74–5\n\nlatency of risks 168\n\nlaunch status, nuclear weapons 383–4\n\nLederberg, J., Biological Weapons: Limiting the Threat 475\n\nLeft Behind novels, Jenkins, J. and LaHaye, T. 75\n\nlegitimization of terrorism 408\n\nLeitenberg, M., Assessing the Biological Weapons and Bioterrorism Threat 452, 475\n\nLenin 505\n\nLeslie, J. 129\n\n      The End of the World: The Science and Ethics of Human Extinction 86\n\n‘level 2 risk’ 176\n\nLeventhal, P. and Alexander, Y. 405\n\nLevi, M., On Nuclear Terrorism 442\n\nLevitt, S.D and Dubner, S.J., Freakonomics: A Rogue Economist Explores the Hidden Side of Everything 162\n\nliability catastrophes 166, 167\n\nliability insurance 171\n\nLibya, nuclear weapons programme 396\n\nLichtenstein, S. et al. 92, 108\n\nlife\n\n      origin of 132\n\n      survival in evolving universe 43–4\n\n      value of 185–6\n\nlife expectancy 27\n\nlife extension, role in totalitarianism 511\n\nlight intensity, effect of volcanic super-eruption 209–10\n\nlimited nuclear war 386–8\n\nLindsey, H., The Late Great Planet Earth 74–5\n\nLindzen, R.S. et al. 274\n\nlinearity, climate change 280–1\n\nlinear modelling, insurance industry 172–3\n\nLineweaver, C.H. 128, 132\n\nlinkage disequilibrium 59, 61\n\nlivestock, biological attack 468\n\nlocal group of galaxies, isolation 36\n\nlocal risk 3\n\nlocal risk mitigation strategies, AI 334, 335\n\nLonely Planets: The Natural Philosophy of Alien Life, Grinspoon, D. 141\n\nLonging for the End: A History of Millenialism in Western Civilization, Baumgartner, F.J. 86\n\nLong-term Capital Management (LTCM) 94–5\n\nloss costs, insurance industry 171, 172\n\nlow-energy supersymmetry 354\n\nlow probability risk, governmental neglect 186–7\n\nLowrance, W.W. 160\n\nLuddite apocalypticism 81–3\n\nLumb, R.F. 404\n\nMcGuiness, M. 400\n\nmachine ethics 340\n\nMcKinzie, M.G. et al. 388\n\nMcNeill, W., Plagues and Peoples 304, 358\n\nmagic moments, recreation 359\n\nmagnetic field reversals 250\n\nMahley, D. 453\n\nMaier, N.R.F. 101\n\nmajoritarian risk mitigation strategies, AI 334, 337\n\nThe Making of the Atomic Bomb, Rhodes, R. 361, 401\n\nMalamud, et al. B.D. 181\n\nMalmquist bias, astronomy 120\n\nmalaria 289, 291, 294, 296\n\n      environmental factors 292\n\nMan Adapting, Dubos, R. 304\n\nmanagerial responses to climate change 279\n\nman-made viruses 302\n\nManuel, F.E. and Fritzie, P.M., Utopian Thought in the Western World 86\n\nMaoism\n\n      amillenialism 76\n\n      messianism 77\n\n      totalitarianism 505–6\n\n         stability 507, 508\n\nmappo 75\n\nMarburger, J. 189\n\nMarburg virus 303\n\nMark, J.C. et al. 405\n\nMarlo, F.H. 407\n\nMarmot, M. et al. 61–2\n\nMarxism, premillenialism 75\n\nMarxist-Leninism, post-millenialism 76\n\nmasks, in containment of disease outbreaks 472\n\nMaslin, M. et al. 273\n\nMason, B.G. et al. 216\n\nmass casualties, as motivation for nuclear terrorism 407, 417, 418\n\nmass extinctions 8–9, 51–2, 258–9\n\n      and Fermi’s paradox 257–8\n\n      hypotheses 255–7\n\n      role of methane hydrate release 273\n\nmathematical proof 317–18\n\nThe Matrix, Wachowskim A. and Wachowskim, L. 309\n\nmaximum likelihood method 156\n\nMayans, social collapse 366\n\nMCPH 1 gene 58, 61, 63\n\n      global frequencies 59\n\nmeasles 288\n\nmedia, anchoring effects 103–4\n\nmedical genomics 456\n\nmedical nanorobots 486\n\nmedical uses, nanoscale technology 483–4\n\nmemes 66\n\nMerkle, R. 485\n\nmessianism 77, 84\n\nMetaman, G. Stock 80\n\nmeteor impact, as cause of mass extinctions 255, 258–9\n\nmeteor showers 227, 228\n\nmeteor strike risk 14–15\n\nmeteoroids, as source of atmospheric dust 232–3\n\nmethane, release from permafrost 274\n\nmethane hydrate, release from oceans 273\n\nmethod of moments 156\n\nmicrobial toxins 300, 453\n\n      use in bioterrorism 456\n\nmicrocephalin genes 58\n\n      global frequencies 59–60\n\nMiddle East, nuclear programmes 397–9, 400\n\nMilankovitch cycles 239–41\n\nMilky Way, collision with Andromeda 37–8\n\nmillenarianism 77\n\nmillenialism 9, 73–4\n\n      amillenialism 75–6\n\n      apocalypticism 77, 78, 409, 417\n\n      dysfunctional manifestations 84–5\n\n      positive effects 83–4\n\n      post-millenialism 76\n\n      premillenialism 74–5\n\n      techno-apocalypticism 81–3\n\n      techno-millenialism 79–81\n\n      utopianism 77–8\n\nMillenialism, Utopianism, and Progress, Olson, T. 86\n\nMillennium Bug 82–3, 340\n\nMillerism 74–5\n\nmind projection fallacy 310\n\nmini-black holes 349–50\n\nminimal standard model 354\n\nMinuteman ballistic missiles 382\n\n      consequences of strike 389\n\nmistakes, as cause of nuclear war 21, 382, 383, 384, 426–7\n\nmitigation policy 29\n\n      climate change 16, 277, 279–81\n\n      see also risk mitigation\n\nmodel risk 176, 180\n\nmodel uncertainty, climate change 275–6\n\nmolecular manipulators 331\n\nmolecular manufacturing 24–5, 481, 482, 484–6, 498–9\n\n      global catastrophic risks 488–9\n\n         destructive global governance 492\n\n         economic and social disruption 492\n\n         ecophagy 495–6\n\n         enhanced intelligences 494\n\n         environmental degradation 494–5\n\n         war 489–92\n\n      products 486–7\n\n         weapons 487–8\n\n      risk mitigation 496–8\n\nmolecular nanotechnology 331–2\n\nMonod, J. 308\n\nMoon, impact craters 127, 223–4\n\nMoore’s Law 79, 328, 450\n\nMoore’s Law of Mad Science 338\n\nmoral satisfaction, purchase of 106\n\nMoravec, H.P. 137\n\n      Robot: Mere Machine to Transcendent\n\nMind 79–80\n\nmorbidity of infectious diseases 291–2\n\nmortality rates of infectious diseases 292\n\nmosquitoes, disease transmission 289, 296\n\nmotivated cognition (rationalization) 99\n\nmotivated scepticism (disconfirmation bias) 99, 100\n\nmotivation\n\n      artificial intelligence 316, 317\n\n      for nuclear terrorism 406–11\n\nmousepox research 457\n\nMt Pinatubo eruption, effect on climate 270\n\nMt St Helens eruption (1980), cooling effect 208\n\nMulhall, D., Our Molecular Future 500\n\nMulligan, J. 464\n\nmultiple catastrophic events 167\n\nmultiple reliabilities 161–2\n\nmultiplier effects, social collapse 367\n\nmultiregional hypothesis, origin of modern humans 211\n\nmulti-stakeholder partnerships, role in biotechnology risk management 462–3\n\nmuons, effect on life 249, 254\n\nmutation 49\n\nmutual insurance 170\n\nMutually Assured Destruction (MAD) 490\n\nMyers, N. and Knoll A.H. 69\n\nNagasaki bomb 428\n\nnano-built weaponry 487–8, 489–92\n\nnanofactories 485–6, 495, 498\n\n      block-assembly operation 497\n\n      products 486–7\n\nNanofuture, Hall, J.S. 500\n\nnanomachines 486\n\nNanomedicine, Vol I: Basic Capabilities, Freitas, R.A. Jr 501\n\nnanoparticles, environmental and health risks 484\n\nnanorobots 486\n\nnanoscale machines 482\n\nnanoscale technologies 481\n\n      associated risks 483–4\n\n      product simplicity 482–3\n\nNanosystems: Molecular Machinery, Manufacturing and Computation, Drexler, K.E. 501–2\n\nnanotechnology 24–5, 81, 481–2\n\n      interaction with AI 328–9, 331–2, 338\n\n      see also molecular manufacturing\n\nNational Research Council (NRC), guidelines on biotechnological research 460–1\n\nNative Americans, effect of smallpox 296\n\nnatural disasters, perceived risk 168\n\nnatural selection 49–50\n\n      effect on human intelligence 325–6\n\nnaval fuel, HEU 413\n\nNazi Germany, totalitarianism 505, 506\n\n      ideology 514\n\n      stability 507\n\nNazism 1919– 1945, Noakes, J. and Pridham, G. 518\n\nNeanderthals 9, 56, 57\n\nnear-Earth objects\n\n      dynamical analysis 226–9\n\n      see also asteroids; comets\n\nnear-Earth object searches 14, 226\n\nneo-catastrophic explanations, Fermi paradox 134\n\nneural networks 312, 340\n\n      tank classifier problem 321–2\n\nneutron stars 39\n\n      hyperons 351\n\nNewby-Clark, I.R. et al. 109\n\nNewhall, C.A. and Self, S. 205\n\nNew Orleans flood 184\n\n      policy analysis 185\n\nNew World Order 84\n\nNichiren Buddhism 75\n\nnickel, production in supernovae 246\n\n‘Noah’s arks’ 20\n\n      see also refuges\n\nNoakes, J. and Pridham, G., Nazism 1919– 1945 518\n\nNoble, D., The Religion of Technology 86\n\nnon-exceedance probability, PMRM 157\n\nnon-exclusivity requirement, extra-terrestrial intelligence 134\n\nnon-extensional reasoning 114\n\nnon-linearity, insurance costs 172–3\n\nnon-proliferation education 440–1\n\nNon-Proliferation Treaty (NPT) 21, 394, 395, 396, 436\n\nnon-synonymous gene changes 58\n\nNorris, R.S. and Kristensen, H.M. 382\n\nNorse Ragnarok, utopianism 77–8\n\nNorth Atlantic Oscillation 278\n\nNorth Korea, nuclear programme 395, 396, 398\n\nnovel pathogens 464–5\n\nNoyes, J.H. 76\n\nnuclear extortion 404\n\nnuclear facilities, monitoring 454\n\nnuclear free zones 398, 400\n\nnuclear nations 396\n\nnuclear proliferation\n\n      good news 396–7\n\n      prevention 397–9\n\nnuclear reactions, first self-sustaining reaction 324–5\n\nnuclear sanctuaries 374\n\nnuclear terrorism 22, 402–3, 441–2\n\n      consequences\n\n         physical and economic 427–8\n\n         psychological, social and political 429–32\n\n      implementation challenges\n\n         HEU availability 412–13\n\n         improvised nuclear devices 411–12, 414\n\n         intact nuclear weapons 414–16\n\n      infliction by non-nuclear means 426–7, 433\n\n      motivations 406–11\n\n      probabilities\n\n         future supply opportunities 422–6\n\n         interest in nuclear weapons 416–19\n\n         present supply opportunities 419–22\n\n      recognition of risk 403–6\n\n      risk of global catastrophe 432–6\n\n      risk reduction 436–7\n\n         immediate priorities 437–40\n\n         long-term priorities 440–1\n\nnuclear tests 394\n\nNuclear Theft: Risks and Safeguards, Willrich, M. and Taylor, T. 404–5\n\nNuclear Threat Initiative 436\n\nThe Nuclear Tipping Point: Why States Reconsider Their Nuclear Choices, Campbell, K. et al. 401\n\nnuclear war risk vii–viii, 5, 20–1, 381–4, 399–401\n\n      comprehensive approach 397–9\n\n      current nuclear balance 392–5\n\n      global 388–9\n\n      instigation by terrorists 426–7, 433\n\n      limited 386–8\n\n      regional 388, 390\n\nnuclear weapons 347\n\n      acquisition by terrorists 414–16\n\nnuclear weapons stockpiles 381, 382, 395\n\n      Russia 385\n\n      United States 384–5\n\nnuclear winter 208, 381, 390–2\n\nNunn-Lugar Cooperative Threat Reduction Program 406\n\nNunn, S. 384\n\nobjective risk 5\n\nobservation selection effects 10, 120–1, 140\n\n      Doomsday Argument 129–31\n\n      Fermi’s paradox 131–8\n\n      past-future asymmetry 121–2\n\n         anthropic overconfidence bias 124–6\n\n         applicability class of risks 126–8\n\n         simplified model 122–4\n\n         value of astrobiological information 128–9\n\n      Simulation Argument 138–40\n\noccurrence exceedance probability (OEP) 175\n\noceans\n\n      acidification 274\n\n      effect of asteroid impact 230\n\n      effect of volcanic super-eruptions 210–11\n\noil price shock, perceived risk 169\n\nOklahoma City bombing 85, 428\n\nOlson, T., Millenialism, Utopianism, and Progress 86\n\nOlum, K. 135\n\n‘Omega point’ 80\n\nOn the Beach, Shute, N. 381, 401\n\nOneida Community 76\n\n‘one rotten apple’ syndrome 17, 302\n\nOn Nuclear Terrorism, Levi, M. 442\n\nOort comet cloud 225, 256\n\nOppenheimer, J.R. 399–400, 403\n\noppression, as result of global governance 494\n\noptimism, grounds for viii\n\noptimization processes 315–16\n\nOrganization for the Prohibition of Chemical Weapons (OPCW) 454\n\norganized crime, perceived risk 169\n\norphan drug legislation 473\n\nOrwell, G., 1984 506–7, 508, 510, 511, 517\n\nOur Molecular Future, Mulhall, D. 500\n\noutsourcing, DNA sequencing 465\n\noverconfidence 107–9\n\noversight of biotechnology research 460–2\n\nozone layer\n\n      effect of cosmic rays 249, 254\n\n      effect of gamma-ray bursts 247\n\n      effect of nuclear war 392\n\n      effect of supernovae 246\n\nozone layer depletion 28\n\nPaisley, I. 400\n\nPakistan\n\n      nuclear tests 395\n\n      nuclear war risk 390, 392–3, 400, 426\n\n      nuclear weapons security 415\n\nPakistan earthquake, financial losses 164\n\npalaeontology, Signor-Lipps effect 120\n\nPale Horse, Pale Rider, Porter, K.A. 304\n\nPaley, W. 309\n\nPalumbi, S.R. 69\n\npandemics 16–17, 27, 287, 303\n\n      causation 289\n\n      future threats 300–2\n\n      historical 290\n\n      HIV/AIDS 298–9\n\n      perceived risk 169\n\n      see also infectious diseases\n\nparticle collider experiments 5, 18–19, 347–8, 356–7\n\n      black hole risk 348–50\n\n      RHIC, cost-benefit analysis 189–90\n\n      strangelets 350–4\n\n      vacuum instability 123–4, 354–6\n\npartitioned multi-objective risk method (PMRM) 155, 156–9\n\npartitioning values, PMRM 157\n\npast-future asymmetry 121–2, 140\n\n      anthropic overconfidence bias 124–6\n\n      applicability class of risks 126–8\n\n      simplified model 122–4\n\n      value of astrobiological information 128–9\n\npathogens 289\n\npathogens research\n\n      dual-use challenge 458–60\n\n      regulation 460–1\n\nPauli exclusion principle 351\n\nPayne, S., A History of Fascism 518\n\nperceived risk 12, 168–9, 170\n\nperinatal mortality rates 288\n\nperiodicity, impact craters 223, 224, 225\n\nPerkovich, G. et al. 394\n\npermafrost melting, methane release 274\n\nPermian-Triassic extinction 8, 52, 255–6\n\nPermissive Action Links (PALs) 415–16\n\npermits, role in carbon dioxide emission reduction 280\n\nPerry, W. 423\n\npersonal risk 3\n\nphantom system models (PSMs) 154–5\n\nphase transitions 355–7\n\nphilosophical failure, AI 18, 318–20\n\nPhoenix, C. and Treder, M., Productive Nanosystems: From Molecules to Superproducts 500\n\nphotosynthesis, effect of volcanic super-eruption 209–10\n\nphysical catastrophes 166, 167\n\nphysical eschatology 33–4\n\nphysics, catastrophe scenarios\n\n      accelerator disasters 347–8, 356–7\n\n         black holes 348–50\n\n         strangelets 350–4\n\n         vacuum instability 354–6\n\n      anticipation 346–7\n\n      runaway technologies 357–8\n\nphytoplankton, effect of ocean acidification 274\n\npicornaviruses, effect of humidity 292\n\nPierce, W., The Turner Diaries 418\n\nPinatubo eruption (1991), environmental effects 211\n\nPinto, J.P. et al. 206–7\n\nPipes, R. 504\n\nPirsig, R. 111\n\nPittock, A.B. et al. 211\n\nPizer, W.A. 282\n\nplague\n\n      bubonic 290, 291, 294, 295\n\n      environmental factors 292\n\n      pneumonic, Surat outbreak (1994) 468\n\nplague bacillus 289\n\nPlague of Justinian 290, 295\n\nplagues 287\n\n      future threats 300–2\n\nsee also infectious diseases\n\nPlagues and Peoples, W. McNeill 304, 358\n\nplanetarium hypothesis 140\n\nplanetary age distribution 128\n\nplanets, extrasolar 67–8, 132\n\nplanet-saving events 35–6\n\nplanning fallacy 108–9\n\nplants, effect of volcanic super-eruption 209–10\n\nPlasmodia 289, 296\n\nplasmoids 252–3\n\nPleistocene, effect of human hunting 53\n\npluralistic ignorance 110\n\nplutonium\n\n      apparent loss (1965) 404\n\n      world’s supply 21\n\nPneumocystis carinii pneumonia 298\n\npneumonic plague, Surat outbreak (1994) 468\n\nPoint Beach nuclear power plant, explosives discovery 404\n\npoliomyelitis 288\n\n      virus synthesis 23, 450, 459\n\npolitical consequences, bioterrorism 467–8\n\npolitical engagement, effect on events 84–5\n\npolitical impact, nuclear terrorism 431–2\n\npolitical orientation, genetic component 511\n\npolitical unrest, scope for nuclear terrorism 410, 424, 425\n\npolitics, risk factors for stable totalitarianism 512–14, 515–16\n\npolymorphic genes 62–3\n\nPope, K.O. et al. 208–9\n\nPopper, K. 99\n\npopulation control 66\n\npopulation decline, Toba catastrophe theory 13\n\npopulation growth 66\n\nPorter, K.A. Pale Horse, Pale Rider 304\n\npositronium atoms 42\n\npost-collapse populations, minimum numbers 369–70, 374\n\nPost, J. 406\n\npost-millenialism 76\n\nPotter, W. 397\n\npoverty, relationship to infectious diseases 288\n\npower law distribution of risks 19–20, 368\n\n      existential disasters 370–1\n\nprecautionary principle 81\n\nprecipitation patterns, effect of climate change 278\n\nprecocious unification 349\n\npredictability of artificial intelligence 316\n\npredictability of events 168\n\n      asteroid strikes 223–9\n\n      use of past records 125\n\n      volcanic super-eruptions 212–13\n\npremillenialism 74–5\n\npremium calculation, insurance industry 171, 172–3\n\npreparation for crises 346, 358–9, 372–5\n\nPresidential Nuclear Initiatives (1991-1992) 415, 439\n\nprestige, as motivation for nuclear terrorism 407–8\n\nprice controls, value in carbon dioxide emission control 280–1, 282\n\npricing of risks, insurance industry 172–3\n\n      relationship to probability of risk 179\n\nprior attitude effect 100\n\nprobability distributions 156–7\n\nprobability estimates 6\n\n      particle collider catastrophes 18–19\n\nprobability of risk 3\n\nprocess risk 176, 180\n\nProductive Nanosystems: From Molecules to Superproducts, Phoenix, C. and Treder, M. 500\n\nProgram for Monitoring Emerging Diseases (ProMed) 470–1\n\nProject BioShield 188\n\nProperty Claims Service (PCS), definition of catastrophe 166\n\nprospective risk analysis 5\n\nprotein folding problem 331, 332\n\nproton decay 40–1\n\npsychiatric drugs, value in totalitarianism 511, 512\n\npsychic unity of mankind 309\n\npsychological impact\n\n      of bioterrorism 467–8\n\n      of nuclear terrorism 407, 429–30, 440\n\npublic policy 12–13, 84–5\n\n      cost-benefit analysis 185–7\n\n         global warming 192–200\n\n         inverse 187–9\n\n         Relativistic Heavy Ion Collider 189–90\n\n      disaster policy 372–5\n\n      global warming 190–200\n\n      responses to climate change 279, 280–2\n\n         Kyoto Protocol 190–1\n\npurchase of moral satisfaction 106\n\nThe Pursuit of Millenialism: Revolutionary\n\nMillenarians and Mystical Anarchists of\n\nthe Middle Ages, Cohn, N. 86\n\nqualitative loss curves 176, 177, 178\n\nqualitative risk assessment 180–1\n\nqualitative risk assessment charts 177, 178\n\nquantum chromodynamics (QCD) 350\n\nquantum fluctuations 354\n\nquantum mechanical exclusion principle 39\n\nquarantines 472\n\nquark-antiquark condensate 354, 355\n\nquark matter (strange matter) 351\n\nquarks 350\n\nradiation, atmospheric protection 239\n\nRadical Evolution, Garreau, J. 79 81\n\nradical ideologies, risk of totalitarianism 514\n\nradioactive fallout 386–7\n\nradio interference, solar flares 243\n\nradiological dispersal devices (RDDs) 403, 411\n\n      psychological impact 430\n\nRaelians ix\n\nRagnarok, utopianism 77–8\n\nrainfall, possible effects of volcanic super-eruptions 211\n\nRajnshees 467\n\nSalmonella typhimurium attack (1984) 466\n\nRampino, M.R. 216\n\nRampino, M.R. and Ambrose, S.H. 212\n\nRampino, M.R. and Self, S. 212\n\nrapid evolution 53–4\n\nrapid infrastructure 331–2, 336\n\nrare Earth hypothesis 134\n\nRational Choice in an Uncertain World: The\n\nPsychology of Intuitive Judgment, Dawes, R.M 115\n\nrationalization 99, 100\n\nrats, as host of plague bacillus 289\n\nRaup, D. and Sepkoski, J. 256\n\nrecombinant DNA technology 474\n\nrecursive self-improvement, AI 323, 325\n\nred dwarf stars 38\n\nred giant Sun 34, 44, 245, 358\n\n‘Red Mercury’ scam, al Qaeda 420\n\nRees, M.J. 189\n\nBefore the Beginning: Our Universe and\n\nOthers 45\n\nreference class problem, SSA 130–1\n\nrefuges 20, 373–5\n\nregional climate risk 278–9\n\nregional nuclear war 388, 390\n\n      risk of nuclear winter 391–2\n\nregulation\n\n      in biotechnology 454–5, 459–62, 464\n\n      in insurance industry 171\n\n      of supercomputers 329\n\nReinicke, W. 462\n\nRelativistic Heavy Ion Collider (RHIC) 347\n\n      cost-benefit analysis 189–90\n\n      probability of strangelet production 353\n\nsee also particle collider experiments\n\nrelativistic jets 252–3\n\nreliability analysis 159–62\n\nThe Religion of Technology, Noble, D. 86\n\nreligious ideology, as motivation for terrorism 408–10, 417–18\n\nreligious movements, millenialism 73\n\n      amillenialism 75–6\n\n      messianism and millenarianism 77\n\n      post-millenialism 76\n\n      premillenialism 74–5\n\n      utopianism and apocalypticism 78\n\nreplacement hypothesis, origin of modern humans 211\n\nreplicators, ecophagy risk 495–6\n\nrepopulation 369–70, 374–5\n\nrepresentativeness heuristic 97\n\nreproductive decision-making\n\n      effect on evolution 62\n\n      response to climate change 66\n\nresearch and development (R&D), incentives in carbon dioxide emission reduction 193–6\n\nresearch effort 6, 28–9\n\nresistance to disaster 371, 373\n\nresources, value in post-collapse societies 374\n\nRevelation of St John 222\n\nrevenge, as motivation for nuclear terrorism 410\n\nreversible computers 139, 359\n\nRhodes, R., The Making of the Atomic Bomb 361, 401\n\nRifkin, J. 81\n\nRigby, E. et al. 233\n\nRischard, J.F. 462\n\nR.I.S.E. (religious cult) 409\n\nrisk, ‘set of triplets’ definition 151\n\nrisk accumulation 173\n\nrisk analysis vii, 11, 112–14, 146–7\n\n      biotechnology 451–2\n\n      expected value of risk, limitations 155–6\n\n      hierarchical holographic modelling (HHM) 150–3\n\n      insurance industry 165, 176–9\n\n      overconfidence 107–9\n\n      partitioned multi-objective risk method 156–9\n\n      past-future asymmetry 121–4\n\n      phantom system models 154–5\n\n      technical experts 19\n\n      terminology 148–9\n\n      see also biases\n\nrisk analysis versus reliability analysis 159–62\n\nrisk communication 19\n\nrisk diversification, insurance industry 171\n\nRisk Intelligence: Learning to Manage What we Don’t Know, Apgar, D. 162\n\nRisk Management Solutions (RMS) 174, 175\n\nrisk mitigation 341–2\n\n      artificial intelligence 333–7\n\n      biotechnology\n\n         DNA synthesis technology 463–4\n\n         international regulation 464\n\n         multi-stakeholder partnerships 462–3\n\n         oversight of research 460–2\n\n         ‘soft’ oversight of research 462\n\n      molecular manufacturing 496–8\n\n      nuclear terrorism 436–7\n\n         immediate priorities 437–40\n\n         long-term priorities 440–1\n\n      totalitarianism\n\n         politics 515–16\n\n         technology 514–15\n\nRNA interference 23, 450–1, 465\n\nRobertson, P. 75\n\nRobot: Mere Machine to Transcendent Mind, Moravec, H. 79–80\n\nrobots, autonomous 357\n\n      see also artificial intelligence robot soldiers 491\n\nrobust strategies, climate change 279\n\nRoffey, R. et al. 462\n\nrogue states, risk of nuclear terrorism 423–4\n\nRohde, R.A. and Muller, R.A. 256\n\nRosenbaum, D. 405\n\nRose, W.I. and Chesner, C.A. 206\n\nrover allele, Drosophila melanogaster 63\n\nrubella (German measles) 293\n\nrunaway technologies 357–8\n\n      see also artificial intelligence\n\nRussell, P., The Global Brain 80\n\nRussell, S.J. and Norvig, P., Artificial Intelligence: A Modern Approach 340\n\nRussia\n\n      Aum Shinrikyo’s attempts to obtain nuclear weapon 419–20\n\n      nuclear forces 381, 382–3, 385, 395, 396\n\n         tactical nuclear weapons 414–15\n\nRutherford, E. 324\n\nsafeguarding\n\n      biotechnology research 460–2, 474–5\n\n      nuclear weapons 415–16\n\nsafety of systems 160\n\nSagam, C. and Turco, R.P. 391\n\nSagan, S.D. 427\n\nSagan, S.D. and Waltz, K.N. 392, 397\n\nSalmonella typhimurium attack, Rajnshees (1984) 466\n\nsarin attack, Aum Shinrikyo 406, 420\n\nSARS 301, 458, 475\n\n      containment 472\n\n      economic impact 474\n\n      infection of research worker 460\n\n      WHO support 471\n\nSARS virus, genome sequencing 484\n\nsatellites, effect of solar flares 243\n\nscale, benefits of 364, 365\n\nscale of global catastrophes 2–3\n\nscattering interactions, ejection of Earth 35\n\nscenario structuring 151, 153\n\nSchell, J., The Fate of the Earth 381\n\nSchelling, T. 408\n\nSchmidhuber, J. 80\n\nSchneider, S.H. et al., Climate Change Policy: A Survey 283\n\nSchnellnhuber, H.-J. 283\n\nscope neglect 105–7, 115\n\nscope of a risk 3–4\n\nscreening, DNA synthesis 463–4, 465\n\nsea level rise viii\n\n      evolutionary consequences 55\n\n      IPCC estimations 15, 199\n\nsecondary uncertainty, insurance industry 175\n\nsecurity\n\n      biological 451, 459\n\n      nuclear material 438–9\n\nseed banks 20, 65\n\nself-modification, artificial intelligence 317, 318, 323, 325, 327, 335\n\nself-reproducing machines 357–8, 488, 495\n\nSelf-Sampling Assumption (SSA) 130–1\n\nSeptember 11 attacks 95, 406\n\n      consequences to insurance industry 166\n\n      economic consequences 428, 429\n\n      unpredictability 168\n\nSETI (Search for Extra-Terrestrial Intelligence) 133, 134, 137\n\n‘set of triplets’ definition of risk 151\n\nseverity of events, distribution 368\n\nseverity of risk 3–4\n\nsexually transmitted diseases (STDs) 293\n\nShalizi, C.R. 126\n\nShaviv, N. 251, 252\n\nShenkel, T. 101\n\nSherry, S.T. et al. 212\n\nShivaji, S. et al. 468\n\nshock wave, nuclear explosions 386\n\nShoemaker-Levy, Comet, collision with Jupiter 240\n\nShultz, G. et al. 395\n\nShute, N., on the Beach 381, 401\n\nSiberia, Tunguska impact (1908) 184, 222, 229\n\nSiberian basalt flood 255\n\nsickle cell allele, heterozygote advantage 50\n\nSides, A. et al. 96\n\nSignor–Lipps effect, palaeontology 120\n\nSimulation Argument 138–40\n\nSingh-Manoux, A. et al. 61–2\n\nsingle-event toy model 122–4\n\nSingularitarianism 79–80, 340\n\nThe Singularity is Near, Kurzweil, R. 79, 80, 82, 361\n\nSioux, Ghost Dance 78\n\nsitter allele, Drosophila melanogaster 63\n\nSIV (simian immunodeficiency virus) 298\n\nSklar’s theorem 173\n\nSlaughter, A.-M. 462\n\nslave labour, totalitarianism 506\n\nSlovic, P. et al. 104, 107\n\nsmall interfering RNA (siRNA) technology 465\n\nsmallpox 291, 292, 294, 296\n\n      bioterrorism threat 457–8, 467\n\n      pandemic 1520–1527 290\n\nsmallpox vaccination, health workers 473\n\nSmart, J. 80\n\nsmart weapons 488\n\nsocial disruption 1, 19–20, 366–7, 375\n\n      after nuclear war 389\n\n      as consequence of molecular manufacturing 492\n\n      evolutionary consequences 55\n\n      in existential disasters 369–72\n\n      after volcanic super-eruptions 213\n\nsocial growth 364–6\n\nsocial impact\n\n      bioterrorism 467–8\n\n      nuclear terrorism 430–1\n\nsocial intelligence 62\n\nsocialism, millenialism 74\n\nsocieties 363–4\n\n      disaster policy 372–5\n\n‘soft’ oversight of biotechnology research 462\n\nsolar activity, relationship to global warming 250–1, 258\n\nsolar evolution 34, 44\n\nsolar extinction 245\n\nsolar flares 239, 242–3, 258\n\n      damage to ozone layer 246\n\nsolar forcing of climate 268–9\n\nsolar luminosity changes 238–42\n\n      global warming 243–4\n\nsolar wind 250\n\nsophistication effect 100\n\nSouth Asia nuclear war risk 390\n\n      predicted death toll 388\n\nSoviet nuclear weapons, possible terrorist acquisition 421\n\nSoviet Union, totalitarianism 505, 506, 507\n\n      stability 507–8\n\nSoviet Union disintegration, nuclear threat 406\n\nSpaceguard project 14\n\nSpanish flu 16, 290\n\n      re-synthesis of virus 450, 459–60\n\nsparse information 105\n\nSpearman’s g 313\n\nspecies metaphors, AI 330, 333\n\nSprinzak, E. 439\n\nstability, totalitarianism 506–10\n\nstagnation, as result of global governance 494\n\nStalin 506, 507, 508\n\nSTART agreements 382\n\nstarvation, causal factors 7\n\n‘Star Wars’ anti-missile system 381\n\nstate complicity, nuclear terrorism 423–5\n\nSteinbruner, J. et al. 461\n\nstellar evolution 42\n\n      end of 38–9\n\nStelliferous Era 38\n\nStern Review 282–3\n\nStock, G., Metaman 80\n\nStothers, R.B. et al. 207\n\nstrangelet disaster 189, 190, 352–4\n\nstrangelets 351–2\n\nstrange matter (quark matter) 351\n\nstrange quarks 350\n\nsubjective (epistemic) risks 5\n\nsubsidies, role in carbon dioxide emission reduction 194, 195, 196\n\nsuccession problem, totalitarianism 26, 508–9\n\nsulphur content, volcanic eruptions 205, 206\n\nsunspots 251\n\nsupercomputers 329\n\nsuperconductivity 354\n\nsuperintelligence 17–18, 79\n\nsupernovae 37, 39, 40, 245–6, 259\n\n      Cassiopeia A remnant 241\n\n      distribution functions 125, 127\n\n      Galactic distribution 247\n\n      production of gamma-ray bursts 247\n\n      as source of cosmic rays 252\n\nsuperproliferation, as consequence of nuclear terrorism 431\n\nsuperstring theory 349, 355\n\nsuper-volcanism 13–14, 127–8, 205–6\n\n      atmospheric impact 206–7\n\n      as cause of mass extinctions 255–6\n\n      consequences for extraterrestrial life 215\n\n      distribution functions 125\n\n      effects on civilization 213–14\n\n      effects on climate 207–9, 270–1\n\n      effects on human populations 211–12\n\n      environmental impact 209–11\n\n      frequency 212–13\n\nSurat, pneumonic plague outbreak (1994) 468\n\nsurveillance, infectious diseases 469–70, 474\n\nsurveillance technology, value in totalitarianism 511\n\nSwiss Re\n\n      estimation of catastrophic losses 164\n\n      survey of perceived risk 12, 168\n\nSword (religious cult) 409\n\nsynonymous gene changes 58\n\nsynthetic biology 23, 451\n\nsyphilis 297\n\nsystemic catastrophes 166, 167\n\nsystems engineering 146–8\n\nSzent-Györgyi, A. 106\n\nSzilard, L. 112\n\nTaber, C.S. and Lodge, M. 99–100\n\ntactical nuclear weapons (TNW)\n\n      acquisition by terrorists 414–16\n\n      consequences of use 428\n\n      security and reduction 439\n\ntails of distributions 156–7\n\nTaiwan\n\n      foot-and-mouth disease outbreak (1997) 468\n\n      risk of military confrontation 393\n\nTaleb, N. 94, 95\n\nThe Black Swan: The Impact of the Highly Improbable 162\n\nTambora eruption (1815), cooling effect 208\n\ntank classifier problem, neural networks 321–2\n\nTauCeti system 129\n\ntaxation, role in carbon dioxide emission reduction 193–6, 280–1, 282\n\nTaylor, T. 404–5\n\ntechnical failure, AI 18, 318, 320–3\n\ntechno-apocalypticism 81–3\n\ntechnological ‘breakout’ 360\n\ntechnological developments ix, x, 27\n\n      risk of nuclear terrorism 425\n\ntechnologies, disruptive 432\n\ntechnology\n\n      as driver of growth 364–5\n\n      interaction with AI 337–8\n\n      responses to climate change 279\n\n      role in totalitarianism 511–12, 514–15\n\n      runaway 357–8\n\ntechnology-forcing taxes, role in carbon dioxide emission reduction 193–6\n\ntechno-millenialism 79–81\n\n‘Techno-rapture’ 79\n\nTegmark, M. and Bostrom, N. 128\n\ntemperature, relationship to information processing 43–4\n\ntemperature change, IPCC models 198, 199\n\nterminal risk 3, 4\n\nterrorism\n\n      as consequence of molecular manufacturing 492\n\n      definition 402\n\n      desensitization 430\n\n      exclusion from insurance 165\n\n      legitimization 408\n\n      perceived risk 168, 169\n\n      see also bioterrorism; nuclear terrorism; September 11 attacks\n\nterrorist networks 149\n\nterrorists, reduction of numbers 440\n\n‘The Big Down’ 81\n\ntheory of scenario structuring (TSS) 151, 153\n\ntherapsids 52\n\nthermal energy release, nuclear explosions 386\n\nthermohaline circulation disruption 274, 281\n\nthermonuclear supernovae 245\n\nThorsett, S.E. 247\n\nthreat, definition 149\n\ntimescales, AI 331–3, 335\n\ntipping points, climate change 265\n\nToba eruption 13, 93, 127–8, 206, 271\n\n      atmospheric impact 206–7, 208\n\n      cooling effect 209\n\n      effect on human population 212\n\n      environmental impact 209–11\n\nToon, O.B. et al. 391–2\n\ntop down climate models 266\n\ntotalitarianism 25–6, 504–6\n\n      probability 516–17\n\n      risk factors 510\n\n         politics 512–14\n\n         technology 511–12\n\n      risk management\n\n         politics 515–16\n\n         technology 514–15\n\n      risks from molecular manufacturing 492–4, 498\n\n      stability 506–10\n\ntoxins 300, 453\n\n      use in bioterrorism 456\n\nTragopogon spp., tetraploidy 53\n\ntransgenerational risk 3–4\n\ntransgenic crops 65\n\ntransplantation of genomes 459\n\ntransplantation of organs and tissues 300\n\nTreasury Board of Canada, qualitative risk assessment chart 177\n\nTreponema pallidum 297\n\nTRIA (Terrorism Risk Insurance Act) 165–6\n\nTrident submarines 382\n\n      consequences of nuclear strike 389\n\ntropical forest dieback 274\n\ntropical forests, effect of volcanic super-eruption 210\n\ntrusted systems 101\n\ntsunamis\n\n      cost-benefit analysis 185, 186\n\n      Indian Ocean (2004) 184\n\n      as result of asteroid impact 184, 230\n\ntuberculosis 288, 297\n\n      association with HIV/AIDS 299–300\n\nTunguska impact (1908) 184, 222, 229\n\nTurco, R.P. et al. 208, 391\n\nThe Turner Diaries, Pierce, W. 418\n\nTversky, A. and Kahneman, D. 95–6, 102\n\ntyphus 291\n\nUbiquity, Buchanan, M. 181, 182\n\nThe Ultimate fate of the Universe, Islam, J.N. 45\n\nultraviolet-blocking nanoparticles 482–3\n\nultraviolet radiation threat, gamma-ray bursts 247–8\n\nunaging humans 341\n\nunanimous risk mitigation strategies, AI 333, 334, 337\n\nuncertainty 11\n\n      in biological attacks 466\n\n      in climate change 15, 265, 273–6, 280\n\n      impact hazards 234–5\n\n      in insurance 179–80\n\nunfair challenges 341\n\nunification theory 349\n\nuninsured risks 165–6\n\nUnited Nations, Security Council Resolution 1540 437, 438, 464\n\nUnited Nations Framework Convention on Climate Change (UNFCCC)\n\n276, 277\n\nUnited States\n\n      current account deficit 169\n\n      and Kyoto Protocol 190, 193\n\n      nuclear forces 381, 382, 383, 384–5, 394, 395\n\n         tactical nuclear weapons 415\n\n      nuclear tests 395\n\n      risk of confrontation over Taiwan 393\n\nUniversal Compliance, Carnegie Endowment for International Peace 394\n\nuniverse\n\n      end of 8\n\n      evolution 33, 44–5\n\n         Dark Era 41–3\n\n         end of stellar evolution 38–9\n\n         era of black holes 41\n\n         era of degenerate remnants 39–41\n\n         survival of life 43–4\n\n      expansion 36, 37\n\n      heat death 359\n\nupright posture, evolution 56\n\nuranium enrichment, danger of new technologies 425\n\nuranium mining efforts, Aum Shinrikyo 420\n\nUtopian Thought in the Western World, Manuel, F.E. and Fritzie, P.M. 86\n\nutopianism 77–8, 84\n\nvaccination 288, 293\n\n      against influenza 298\n\nvaccine stockpiling 473\n\nvacuum instability 43, 354–6\n\n      anthropic bias 123–4\n\n      extraterrestrial risks 137–8\n\nVavilov, N. 65\n\nvertical transmission 291\n\nVesuvius, prediction of next eruption 213\n\nVibrio cholerae 289, 290, 295\n\nvigilance, biotechnology 462\n\nVinge, V. 79, 314, 331, 333\n\nvirtual particles 354\n\nVirus as Organism: Evolutionary and\n\nEcological Aspects of Some Human\n\nVirus Diseases, Burnet, F.M. 304\n\nviruses\n\n      bioterrorism threat 456–7\n\n      genetic manipulation 450\n\n      genome synthesis 459\n\n      man-made 302\n\nvirus genome synthesis 23\n\nvolcanic eruptions 13–14\n\nVolcanic Explosivity Index (VEI) 205\n\nvolcanic forcing of climate 269–71\n\nvolcanic super-eruptions 13–14, 127–8, 205–6\n\n      atmospheric impact 206–7\n\n      as cause of mass extinctions 255–6\n\n      consequences for extraterrestrial life 215\n\n      distribution functions 125\n\n      effect on climate 208–9, 270–1\n\n      effect on human populations 211–12\n\n      effects on civilization 213–14\n\n      environmental impact 209–11\n\n      frequency 212–13\n\nvolcanic winter 208–9\n\nVonnegut, K. 357\n\nvulnerability of systems 149\n\nWachowski, A. and Wachowski, L. 309\n\n‘wait and see’ approach, global warming 189, 196–7\n\nWaltz, K. 397\n\nwar\n\n      interstellar 136–7\n\n      risk from molecular manufacturing 489–92, 496–7, 498–9\n\nWard, P. and Brownlee, D. 68\n\nWard, S.N. and Asphaug, E. 184\n\nThe War of the Worlds, Wells, H.G. 135\n\nWashington, G. 77\n\nWason, P. 98–9\n\nwater vapour feedbacks, global warming 275\n\n‘Weak Garden of Eden’ hypothesis 212\n\nweapons, nano-built 487–8, 489–92\n\nweapons grade plutonium, apparent loss (1965) 404\n\nweather, influence on effect of nuclear bomb 7\n\nWebb, S., Where Is Everybody? Fifty Solution\n\nto the Fermi’s Paradox 141\n\nWeber’s Law 106\n\nWeiss, R. 300\n\nWeitzman, M.L. 280\n\nWells, H.G. vii\n\nThe War of the Worlds 135\n\nWhere Is Everybody? Fifty Solution to the\n\nFermi’s Paradox, Webb, S. 141\n\nwhite dwarf stars 39\n\n      collisions 40\n\n      supernovae 245\n\nWhite Plague 297\n\nsee also tuberculosis\n\nWildavsky, B. 73\n\nwildfires, analysis 181\n\nWilliams, G.C. 311\n\nWillrich, M. and Taylor, T., Nuclear Theft:\n\nRisks and Safeguards 404–5\n\nwork hierarchy level, relationship to health 61–2\n\nWorld Economic Forum (WEF)\n\n      Global Risks reports 12, 168–9, 170\n\n      qualitative risk assessment chart 178\n\nworld government 26, 509–10, 512–14, 515–16\n\n      probability 516–17\n\nWorld Health Organization (WHO), disease surveillance 16, 469, 470–1\n\nWright, R. 509–10, 513\n\nXanthomonas axonopodis, Florida outbreak (2001) 468\n\nX article, G. Kennan 452\n\nxenotransplantation 300\n\nY2K phenomenon 82–3, 340\n\nYellman, T.W. 176\n\nyellow fever 289, 291, 294\n\n      pandemic, 1853 290\n\nYellowstone Caldera, super-eruptions 213–14\n\nYeltsin, B., nuclear launch decision 382\n\nYersinia pestis (plague bacillus) 289, 290, 292, 295\n\n      Surat outbreak (1994) 468\n\nYounger Dryas period 191\n\nYucatan asteroid strike 51–2\n\nzero-point motion 354\n\nZielinski, G.A. et al. 207\n\nZimmerman, P.D. and Lewis J.G. 442\n\nzodiacal cloud 232–3\n\n¹ (Bostrom, 2002, p. 381).\n\n² For many aggregative consequentialist ethical theories, including but not limited to total utilitarianism, it can be shown that the injunction to maximize expected value! can be simplified-for all practical purposes – to the injunction to minimize existential risk! (Bostrom, 2003, p. 439). (Note, however, that aggregative consequentialism is threatened by the problem of infinitarian paralysis [Bostrom, 2007, p. 730].)\n\n³ One can sometimes define something akin to objective physical probabilities (‘chances’) for deterministic systems, as is done, for example, in classical statistical mechanics, by assuming that the system is ergodic under a suitable course graining of its state space. But ergodicity is not necessary for there being strong scientific constraints on subjective probability assignments to uncertain events in deterministic systems. For example, if we have good statistics going back a long time showing that impacts occur on average once per thousand years, with no apparent trends or periodicity, then we have scientific reason – absent of more specific information – for assigning a probability of ≈0.1% to an impact occurring within the next year, whether we think the underlying system dynamic is indeterministic, or chaotic, or something else.\n\n⁴ Of course, when allocating research effort it is legitimate to take into account not just how important a problem is but also the likelihood that a solution can be found through research. The drunk who searches for his lost keys where the light is best is not necessarily irrational; and a scientist who succeeds in something relatively unimportant may achieve more good than one who fails in something important.\n\n⁵ For example, the risk of large-scale conventional war is only covered in passing, yet would surely deserve its own chapter in a more ideally balanced page allocation.\n\n⁶ For example, the risk ‘Chronic disease in the developed world’ is defined as ‘Obesity, diabetes and cardiovascular diseases become widespread; healthcare costs increase; resistant bacterial infections rise, sparking class-action suits and avoidance of hospitals’. By most standards, obesity, diabetes, and cardiovascular disease are already widespread. And by how much would healthcare costs have to increase to satisfy the criterion? It may be impossible to judge whether this definition was met even after the fact and with the benefit of hindsight.\n\n⁷ This heuristic is only meant to be a first stab at the problem. It is obviously not generally valid. For example, if one million dollars is sufficient to take all the possible precautions, there is no reason to spend more on the risk even if we think that its probability is much greater than 1/1000. A more careful analysis would consider the marginal returns on investment in risk reduction.\n\n⁸ A comprehensive review of space hazards would also consider scenarios involving contact with intelligent extraterrestrial species or contamination from hypothetical extraterrestrial microorganisms; however, these risks are outside the scope of Chapter 12.\n\n⁹ Even if we ourselves are expert, we must still be alert to unconscious biases that may influence our judgment (e.g., anthropic biases, see Chapter 6).\n\n¹⁰ If experts anticipate that the public will not quite trust their reassurances, they might be led to try to sound even more reassuring than they would have if they had believed that the public would accept their claims at face value. The public, in turn, might respond by discounting the experts’ verdicts even more, leading the experts to be even more wary of fuelling alarmist overreactions. In the end, experts might be reluctant to acknowledge any risk at all for fear of a triggering a hysterical public overreaction. Effective risk communication is a tricky business, and the trust that it requires can be hard to gain and easy to lose.\n\n¹¹ Somewhat analogously, we could prevent much permanent loss of biodiversity by moving more aggressively to preserve genetic material from endangered species in biobanks. The Norwegian government has recently opened a seed bank on a remote island in the arctic archipelago of Svalbard. The vault, which is dug into a mountain and protected by steel-reinforced concrete walls one metre thick, will preserve germplasm of important agricultural and wild plants.\n\n¹² In mortality statistics, deaths are usually classified according to their more proximate causes (cancer, suicide, etc.). But we can estimate how many deaths are due to ageing by comparing the age-specific mortality in different age groups. The reason why an average 80-year-old is more likely to die within the next year than an average 20-year-old is that senescence has made the former more susceptible to a wide range of specific risk factors. The surplus mortality in older cohorts can therefore be attributed to the negative effects of ageing.\n\n¹ ‘Dark energy’ is a common term unifying different models for the ubiquitous form of energy permeating the entire universe (about 70% of the total energy budget of the physical universe) and causing accelerated expansion of space time. The most famous of these models is Einstein’s cosmological constant, but there are others, going under the names of quintessence, phantom energy, and so on. They are all characterized by negative pressure, in sharp contrast to all other forms of energy we see around us.\n\n¹ Milan M. Ćirković points out that the Toba supereruption (~73,000 BCE) may count as a near-extinction event. The blast and subsequent winter killed off a super majority of humankind; genetic evidence suggests there were only a few thousand survivors, perhaps even less (Ambrose, 1998). Note that this event is not in our historical memory – it predates writing.\n\n² Note that the figure 44% is for all new businesses, including small restaurants, rather than, say, dot-com start-ups.\n\n³ A related concept is the good-story bias hypothesized in Bostrom (2001). Fictional evidence usually consists of ‘good stories’ in Bostrom’s sense. Note that not all good stories are presented as fiction.\n\n⁴ Note that in this experiment, sparse information played the same role as cognitive business or time pressure in increasing reliance on the affect heuristic.\n\n¹ The difference between the average absolute magnitudes of stars (or galaxies or any other similar sources) in magnitude- and distance-limited samples, discovered in 1920 by K.G. Malmquist.\n\n² The effect by which rare species seemingly disappear earlier than their numerous contemporaries (thus making extinction episodes more prolonged in the fossil record than they were in reality), discovered by P.W. Signor and J.H. Lipps in 1982.\n\n³ For a summary of vast literature on observation selection, anthropic principles, andanthropic reasoning in general, see Barrow and Tipler (1986); Balashov (1991); Bostrom (2002a).\n\n⁴ Parts of this section are loosely based upon Ćirković (2007).\n\n⁵ For a more optimistic view of this possibility in a fictional context see Egan (2002).\n\n⁶ I thank C.R. Shalizi for this excellent formulation.\n\n⁷ Earth-like planets have not been discovered yet around Tau Ceti, but in view of the crude observational techniques employed so far, it has not been expected; the new generation of planet-searching instruments currently in preparation (Darwin, Gaia, TPF, etc.) will settle this problem.\n\n⁸ Originally in Leslie (1989); for his most comprehensive treatment, see Leslie (1996). Carter did not publish on DA.\n\n⁹ This is the original, Carter-Leslie version of DA. The version of Gott (1993) is somewhat different, since it does not deal with the number of observers, but with intervals of time characterizing any phenomena (including humanity’s existence). Where Gott does consider the number of observers, his argument is essentially temporal, depending on (obviously quite speculative) choice of particular population model for future humanity. It seems that a gradual consensus has been reached about inferiority of this version compared to Leslie-Carter’s (see especially Caves, 2000; Olum, 2002), so we shall concentrate on the latter.\n\n¹⁰ It would be more appropriate to call it the Tsiolkovsky-Fermi-Viewing-Hart-Tipler Paradox (for more history, see Brin, 1983; Kuiper and Brin, 1989; Webb, 2002, andreferences therein). We shall use the locution ‘Fermi’s Paradox’ for the sake of brevity, and with full respect for the contributions of the other authors.\n\n¹¹ The requirement that any process preventing formation of a large anddetectable interstellar civilization operates over large spatial (millions of habitable planets in the Milky Way) andtemporal (billions of years of the Milky Way history) scales. For more details, see Brin (1983).\n\n¹² In the pioneering paper on GCRs/existential risks Bostrom (2002b) has put this risk in the ‘whimpers column, meaning that it is an exceedingly slow and temporally protracted possibility (and the one assigned low probability anyway). Such a conclusion depends on the specific assumptions about extraterrestrial life and intelligence, as well as on the particular model of future humanity and thus is of rather narrow value. We wouldlike to generalize that treatment here while pointing out that still further generalization is desirable.\n\n¹³ Parsec, or paralactic second is a standard unit in astronomy: 1 pc = 3.086 x 10¹⁶ m. One parsec is, for instance, the average distance between stars in the solar neighbourhood.\n\n¹⁴ A version of the ‘deadly probes’ scenario is a purely informatics concept of Moravec (1988), where the computer viruses roam the Galaxy using whatever physical carrier available and replicating at the expense of resources of any receiving civilization. This, however, hinges on the obviously limitedcapacity to pack sufficiently sophisticated self-replicating algorithm in the bit-string of size small enough to be received non-deformed often enough – which raises some interesting issues from the point of view of algorithmic information theory (e.g., Chaitin, 1977). It seems almost certain that the rapidly occurring improvements in information security will be able to clear this possible threat in check.\n\n¹⁵ There may be exceptions to this related to the complex issue of reversible computing. In addition, if the Landauer-Brillouin bound holds, this may have important consequences for the evolution of advanced intelligent communities, as well as for our current SETI efforts, as shown by Ćirković and Bradbury (2006).\n\n¹⁶ The ‘planetarium hypothesis’ advanced by Baxter (2000) as a possible solution to Fermi’s paradox, is actually very similar to the general simulation hypothesis; however, Baxter suggests exactly ‘risky’ behaviour in order to try to force the contact between us and the director(s)!\n\n¹ Uninsured Losses, report for the Tsunami Consortium, November 2000.\n\n² See, for example, http://environmentalchemistry.com/yogi/environmental/asbestoshistory2004.html\n\n³ A copula is a functional, whose unique existence is guaranteed from Sklar’s theorem, which says that a multivariate probability distribution can be represented uniquely by a functional of the marginal probability functions.\n\n⁴ Applied insurance research (AIR), for example, stochastically samples the damage for each event on each property and the simulation is for a series of years. Risk management solutions (RMS), on the other hand, take each event as independent, described by a Poisson arrival rate and treat the range of damage as a parameterized beta function (’secondary uncertainty’) in order to come up with the OEP curve, and then some fancy mathematics for the AEP curve. The AIR method is the more general and conceptually the simplest, as it allows for non-independence of events in the construction of the events hitting a given year, and has built-in damage variability (the so-called secondary uncertainty).\n\n¹ That cosmic impacts (whether from asteroids or comets) of modest magnitude can cause very destructive tsunamis is shown in Ward and Asphaug (2000) and Chesley and Ward (2006); see also the Chapter 11 in this volume.\n\n² Deaths caused by Hurricane Katrina were a small fraction of overall loss relative to property damage, lost earnings, and other readily monetizable costs. The ratio was reversed in the case of the Indian Ocean tsunami, where 300,000 people were killed versus only 1200 from Katrina.\n\n³ Of course, notall Americans are ‘prime-aged workers’, butitis notclear thatothers have lower values of life. Economists compute value of life by dividing how much a person is willing to pay to avoid a risk of death (or insists on being paid to take the risk) by the risk itself. Elderly people, for example, are not noted for being risk takers, despite the shortened span of life that remains to them; they would probably demand as high a price to bear a risk of death as a prime-aged worker -indeed, possibly more.\n\n⁴ The ‘wait and see’ approach is discussed further below, in the context of responses to global warming.\n\n⁵ This calculation is explained in Posner (2004, pp. 167–70).\n\n⁶ There is evidence thatglobal warming is responsible in partatleastfor the increasing intensity of hurricanes (Emanuel, 2005; Trenberth, 2005).\n\n⁷ For an optimistic discussion of the scientific and economic feasibility of trapping carbon dioxide before it can be released into the atmosphere and capturing it after it has been released, see Socolow (2005).\n\n⁸ In fact, scientists have already reported dramatic effects from global warming in melting Arctic glaciers and sea ice (Hassol, 2004).\n\n⁹ The length of the ‘short run’ is, unfortunately, difficult to specify. It depends, in the present instance, on how long it would take for producers and consumers of energy to minimize the impact of the price increase by changes in production (increasing output in response to the higher price) and consumption (reducing consumption in response to the higher price).\n\n¹⁰ For further readings in the subject matter of this chapter, see Patricia Grossi and Howard Kunreuther, eds., Catastrophe Modeling: A New Approach to Managing Risk (Springer, 2005); Michael J.S. Belton et al., eds., Mitigation of Hazardous Comets and Asteroids (Cambridge University Press, 2004); Raymond S. Nickerson, Cognition and Chance: The Psychology of Probabilistic Reasoning (Lawrence Erlbaum Associates, 2004); OECD, Large-scale Disasters: Lessons Learned (Organisation for Economic Co-operation and Development, 2004); Richard A. Posner, Catastrophe: Risk and Response (Oxford University Press, 2004); Keith Smith, Environmental Hazards: Assessing Risk and Reducing Disaster, 4th ed. (Routledge, 2004); Martin Rees, Our Final Hour: A Scientist’s Warning: How Terror, Error, and Environmental Disaster Threaten Humankind’s Future in this Century – On Earth and Beyond (Basic Books, 2003).\n\n¹ http://unfccc.int/essential\\_background/glossary/items/3666.php\n\n² And other GHG, but see Fig. 13.1: the forcing from CO2 is by far the dominant forcing.\n\n³ This may change if various geo-engineering approaches come to play a significant role in the management of the climate change problem (Crutzen, 2006; Keith, 2001). For a review of these possibilities, see Keith (2007). Though preliminary analysis suggests the idea of attempting to tune global mean temperatures by lowering insolation might ought not be dismissed out ofhand (Govindasamy and Caldeira, 2000), more full analyses are required before this becomes a serious policy option(Kiehl, 2006). Inany case, some problems of the direct effects of carbon – such as ocean acidification – are likely to remain significant issues, inviting the pessimistic possibility that geoengineering may carry its own severe climate-related risks.\n\n¹ See especially Oldstone (1998), Wikipedia, Ashburn (1947), Burnet (1946), Simpson (1954), Gage (1998), McNeill (1976), Kilbourne (1981), and Crosby (1976b).\n\n¹ This is a case of a deep, confusing, and extraordinarily common mistake that E.T. Jaynes named the mind projection fallacy (Jaynes and Bretthorst, 2003). Jaynes, a physicist and theorist of Bayesian probability, coined ‘mind projection fallacy’ to refer to the error of confusing states of knowledge with properties of objects. For example, the phrase mysterious phenomenon implies that mysteriousness is a property of the phenomenon itself. If I am ignorant about a phenomenon, then this is a fact about my state of mind, not a fact about the phenomenon.\n\n² This story, although famous and oft-cited as fact, may be apocryphal; I could notfind a first-hand report. For unreferenced reports see for example, Crochat and Franklin (2000) or http://neil.fraser.name/writing/tank/. However, failures of the type described are a major real-world consideration when building and testing neural networks.\n\n³ Bill Hibbard, after viewing a draft of this paper, wrote a response arguing that the analogy to the ‘tank classifier’ problem does not apply to reinforcement learning in general. His critique may be found in Hibbard (2006); my response may be found atYudkowsky (2006). Hibbard’s model recommends a two-layer system in which expressions of agreement from humans reinforce recognition of happiness, and recognized happiness reinforces action strategies.\n\n⁴ This follows for the Landauer-Brillouin’s limit, the maximal amount of information you can process in any classical system dissipating energy E : I\\_(max) = E/(kT ln 2), where k is Boltzmann constant and T – working temperature.\n\n⁵ This is usually true but not universally true. The final chapter of the widely used textbook Artificial Intelligence: A Modern Approach (Russell and Norvig, 2003) includes a section on ‘The Ethics and Risks of Artificial Intelligence’; mentions I.J. Good’s intelligence explosion and the Singularity; and calls for further research, soon. But as of 2006, this attitude remains very much the exception rather than the rule.\n\n⁶ After this chapter was written, a special issue on Machine Ethics appeared in IEEE Intelligent Systems (Anderson and Anderson, 2006). These articles primarily deal in ethics for domain-specific near-term AI systems, rather than superintelligence or ongoing intelligence explosions. Allen et al. (2006, p. 15), for example, remark that ‘Although 2001 has passed and HAL remains fiction, and it’s a safe bet that the doomsday scenarios of Terminator and Matrix movies will not be realized before their sell-by dates of 2029 and 2199, we’re already at a point where engineered systems make decisions that can affect our lives.\n\nHowever, the issue of machine ethics has now definitely been put on the map; though not, perhaps, the issue of superintelligent machine ethics, or AI as a positive and negative factor in global risk.\n\n¹ Amplitude of this field is constant in time, spatially uniform and occurs in a spin-0 channel, so that no breaking of Lorentz symmetry is involved.\n\n¹ Jonathan, S. (2000). The Fate of the Earth (Palo Alto: Stanford University Press), p. 3.\n\n² Calculations are based on the following deployed strategic warhead totals: 1986, a combined total of 22, 526 (US – 12, 314, USSR – 10, 212); 2006, a combined total of 8835 (US – 5021, USSR -3814).\n\n³ Norris, R.S. and Kristensen, H.M. (2007). NRDC Nuclear Notebook, U.S Nuclear Forces, 2007. Bulletin of the Atomic Scientists, January/February 2007, p. 79; Norris, R.S. and Kristensen, H.M. (2007). NRDC Nuclear Notebook, Russian Nuclear Forces, 2007. Bulletin of the Atomic Scientists March/April 2007, p. 61; McKinzie, M.G., Cochran, T.B., Norris, R.S., and Arkin, W.M. (2001). The U.S. Nuclear War Plan: A Time For Change (New York: Natural Resources Defense Council), p. 42, 73, 84.\n\n⁴ Bruce, G.B. (2007). Primed and Ready. The Defense Monitor: The Newsletter of the Center for Defense Information, XXXVI (3), 2–3.\n\n⁵ Ibid.\n\n⁶ Ibid.\n\n⁷ Ibid.\n\n⁸ Nunn, S. (2004). Speech to the Carnegie International Non-proliferation Conference, June 21, 2004. www.ProliferationNews.org.\n\n⁹ Bruce, G.B. et al. (1998). Accidental nuclearwar- a Post-Cold Warassessment. The New England Journal of Medicine, 1326–1332.\n\n¹⁰ Eddy, L. (2004). Whole World on Fire: Organizations, Knowledge, and Nuclear Weapons Devastation (Ithaca: Cornell University Press). http://www.gwu.edu/-nsarchiv/NSAEBB/ NSAEBB108/index.htm\n\n¹¹ Office of Technology Assessment. (1979). The Effects of Nuclear War (Washington, DC), p. 15, 35; The Effects of NuclearWeapons. Atomic Archive at http://www.atomicarchive.com/ Effects/\n\n¹² See note 9.\n\n¹³ McKinzie, M.G., Cochran, T.B., Norris, R.S., and Arkin, W.M. (2001). The U.S Nuclear War Plan: A Time For Change (New York: Natural Resources Defense Council), pp ix-xi. NRDC used computer software and un classified databases to model a nuclear conflict and approximate the effects of the use of nuclear weapons, based on an estimate of the U.S. nuclearwar plan (SIOP).\n\n¹⁴ The U.SNuclear War Plan.p. 130.\n\n¹⁵ Office of Technology Assessment (1979). The Effects of Nuclear War (Washington, DC), p. 8. Russian casualties are smaller that U.S. causalities because a higher percentage of Russians still live in rural areas and the lower-yield U.S. weapons produce less fallout.\n\n¹⁶ The Effects of Nuclear War, p. 4–5.\n\n¹⁷ The Effects of Nuclear War, p. 8.\n\n¹⁸ Batcher, R.T. (2004). The consequences of an Indo-Pakistani nuclearwar. International Studies Review 6, 137.\n\n¹⁹ Turco, R.P., Toon, O.B., Ackerman, T.P., Pollack, J.B., and Sagan, C. (1983). Nuclear winter: global consequences of mutliple nuclear explosions. Science, 222, 1290.\n\n²⁰ Turco, R.P., Toon, O.B., Ackerman, T.P., Pollack, J.B., and Sagan, C. (1990). Climate and smoke: an appraisal of nuclearwinter. Science, 247, 166.\n\n²¹ Ibid., p. 174.\n\n²² Sagan, C. and Turco, R.P. (1993). Nuclearwinterin the Post-Cold Warera. Journal of Peace Research, 30(4), 369.\n\n²³ Toon, O.B., Robock, A., Turco, R.P., Bardeen, C., Oman, L., and Stenchikov, G.L. (2007). Consequences of regional-scale nuclear conflicts. Science, 315, 1224–1225.\n\n²⁴ Ibid., p. 11823.\n\n²⁵ Sagan, S.D. and Waltz, K.N. (2003). The Spread of Nuclear Weapons: A Debate Renewed (New York: W.W. Norton &Company), p. 115.\n\n²⁶ Krepon, M. (2004). From Confrontation to Cooperation (Washington, D.C.: Henry L. Stimson Center). http://www.stimson.org/southasia/pubs.cfm?ID=197\n\n²⁷ Chari, P.R. (2004). Nuclear Restraint, Nuclear Risk Reduction, and the Security – Insecurity Paradox in South Asia (Washington, D.C.: Henry L. Stimson Center).\n\n²⁸ Perkovich, G., Mathew, J., Cirincione, J., Gottemoeller, R., Wolfsthal, J. (2005). Universal Compliance: A Strategy for Nuclear Security (Washington, D.C.: Carnegie Endowment for International Peace), pp. 24, 34, 39.\n\n²⁹ Shultz, G., Kissinger, H., Perry, W., and Nunn, S. (2007). A world free ofnuclear weapons. The Wall Street Journal. Eastern edition, NY, January 4, 2007, pg. A15.\n\n³⁰ See, for example, the excellent suggestions made by Sally Horn, a State Department representative to the NPT Review Conference in May 2005, summarized in Cirincione, J. (2005). ‘No Easy Out’, Carnegie Analysis. www.ProliferationNews.org\n\n³¹ In 1987 the Soviet Union deployed 2380 long-range missiles and China approximately 20. The numberdeclined to 689 by 2007 (669 Russian; 20 Chinese).\n\n³² Sagan, S.D. and Waltz, K.N. (2003). The Spread of Nuclear Weapons (New York: W.W. Norton &Company), p. 4.\n\n³³ Potter, W. (2005). Carnegie International Non-proliferation Conference, 2005, Panel on ‘The New Look ofUS Nonproliferation Policy’. www.ProliferationNews.org.\n\n³⁴ Oppenheimer, J.R. (June 1946). The International Control of Atomic Energy. Bulletin of the Atomic Scientists.\n\n¹ There is no universally accepted definition of terrorism: assertions regarding the core characteristics of terrorism vary widely among governments and scholars. Recognizing these differences, for the purposes of this paper, the following definition will be used: Terrorism is the calculated use of violence, or the threat of violence, perpetrated by non-state actors in order to achieve social or political goals, with the aim of influencing a wider audience than the immediate victims of the violence and which is usually directed against non-combatants.\n\n² For a discussion of the Lumb panel and other early government efforts to assess the risks of nuclear terrorism Willrich and Taylor (1974, pp. 78–82; Walker, 2001, pp. 107–132).\n\n³ Although the AEC maintained that poor accounting and handling practices probably were responsible for the discrepancy, there was speculation that about 100 kilograms ofbomb-grade uranium from the plant may have found its way to Israel (Gilinsky, 2004).\n\n⁴ Taylor’s views also were popularized in a three part article by John McPhee in 3, 10, and 17 December 1973 issues of The New Yorker (McPhee, 1974).\n\n⁵ Another influential volume in the Harvard series but with a broader focus was the volume by Falkenrath et al. (1998).\n\n⁶ Jerrold Post maintains that, ‘absent a clear understanding of the adversary’s intentions, the strategies and tactics developed [to counter them] are based primarily on knowledge ofterrorists [sic] technological capabilities and give insufficient weight to psychological motivations’ (1987, p. 91). Indeed, Cameron has even asserted that ‘the real driving force behind the heightened danger of nuclear terrorism lies not with the increased opportunities for micro-proliferation, but rather with the changing nature ofpolitical violence and the psychological and organizational characteristics ofterrorism itself (1999, p. 152).\n\n⁷ One of the few systematic efforts to explore terrorist motivational incentives and disincentives for using CBRN weapons in general can be found in Gurr and Cole (2002).\n\n⁸ It should be stated at the outset that it is not necessary for terrorists to be irrational or psychologically imbalanced for them to seek nuclear weapons (Cameron, 1999, p. 23). Cameron further states that ‘If a sufficiently important end were sought by the [terrorist] group, all means, including nuclear terrorism, might be justifiable’ (1999, p. 154).\n\n⁹ A comprehensive discussion of the underlying reasons that would precipitate a goal of causing mass casualties is beyond the scope ofthis paper, but several perspectives will be discussed.\n\n¹⁰ Contagious biological agents that are used to start a large-scale epidemic could conceivably also lead to comparable casualties, although this type of attack has less reliable and less immediate consequences and it is also more difficult to fix the geographical scope of biological attacks than is the case with nuclear weapons.\n\n¹¹ See, for example, Marlo: ‘the increasing willingness to engage in mass murder makes terrorists more likely to consider WMD as usable and even preferable to conventional explosives and other traditional terrorist weaponry (1999, p.55). Also see Falkenrath (1998, p.53) and Foxell (1999, p. 96).\n\n¹² Indeed, if causing colossal numbers of casualties is the sole reason for employing nuclear weapons then, technically speaking, the act does not constitute terrorism but mass murder, since for an act to be defined as terrorism, there must be present the intention to influence a wider audience than the immediate victims.\n\n¹³ For a more in-depth discussion of the issue of audience impact, see Gressang (2001). Also, compare for example, Falkenrath et al. (1998, pp. 206–207) and McCormick (2003).\n\n¹⁴ Albert Bandura has discussed various ways in which terrorist groups legitimize their violent behaviour, several of which can flow from a group’s ideological outlook, including moral justification, displacement of responsibility, ignoring the actual suffering of victims, and dehumanizing victims (Bandura, 1998).\n\n¹⁵ Several authors have questioned the link between a desire on the part of religious terrorists to cause mass casualties and the potential use ofWMD, as well as the extent to which religious actors are oblivious to political concerns. They have also pointed to the large number of CBRN plots on the part of ethno-nationalist terrorists. See, for example, Rapoport (1999) and Dolnik (2004). To the first of these objections, one can refer to the discussion above relating to the desire to cause mass casualties and note that for actors seeking to cause a genuinely catastrophic scale of injury and death, conventional weapons will not suffice. The other objections are addressed in following sections.\n\n¹⁶ For more on these three groups, see chapters by Kaplan, Stern, and Carus in Tucker (2000).\n\n¹⁷ See, for example, the discussion of the group Avenging Israel’s Blood in Sprinzak and Zertal\n\n(2000).\n\n¹⁸ These factors are drawn from a combination of Tucker (2000, pp. 255–263), Campbell (2000, pp. 35–39), and Jackson (2001, p. 203). Many of these factors are related to a group’s capabilities for engaging in nuclear terrorism (discussed in the following section), leading to the obvious observation that, in addition to motives driving capabilities, on occasion capabilities can reciprocally influence a terrorist’s intentions.\n\n¹⁹ The key steps terrorists would have to take on the pathway to a nuclear attack, are discussed in Bunn et al. (2003, pp. 21–31). Also see Maerli (2004, p. 44) and Ferguson and Potter (2005, pp. 112–113).\n\n²⁰ It is beyond the scope of this chapter to detail the various acquisition routes a non-state actor might pursue. For an analysis of this issue see Ferguson and Potter (2005, pp. 18–31).\n\n²¹ A technologically sophisticated terrorist group might be able to achieve success with a smaller amount of HEU if it could construct a ‘reflector’ to enhance the chain reaction.\n\n²² The experiment, however, did not demonstrate how several individuals might obtain key components for an implosion type device, some ofwhich are very tightly controlled.\n\n²³ For a discussion ofpossible routes to acquisition ofintact nuclear weapons, see Ferguson and Potter (2005, pp. 53–61). Potential routes include deliberate transfer by a national government, unauthorized assistance from senior government officials, assistance from the custodian of the state’s nuclear weapons, seizure by force without an insider’s help, and acquisition during loss of state control over its nuclear assets due to political unrest, revolution, or anarchy.\n\n²⁴ Empirically speaking, the record ofnon-use should not be compared over the long history of terrorism, but only over the period since it became feasible for non-state actors to acquire or use nuclear weapons, Circa 1950.\n\n²⁵ See, for example, Ferguson and Potter (2005, p. 40), Jenkins (1986, p. 777), Hoffman (1993a, pp. 16–17), and Clutterbuck (1993, pp. 130–139).\n\n²⁶ Since these weapons were too difficult to acquire and use reliably.\n\n²⁷ Indeed, the former head of the CIA’s Bin Laden Unit has explained that, ‘What al-Qaeda wants is a high body count as soon as possible, and it will use whatever CBRN [chemical, biological, radiological, nuclear] materials it gets in ways that will ensure the most corpses’ (Scheuer, 2002, p. 198).\n\n²⁸ For instance, The Turner Diaries, a novel written by the former leader of the National Alliance, William Pierce, described and which has had considerable influence on many right-wingers, describes racist ‘patriots’ destroying cities and other targets with nuclear weapons (Macdonald, 1999).\n\n²⁹ For one thing, many deranged or excessively aggressive individuals cannot function as part ofa group.\n\n³⁰ Allegedly, subsequent efforts by Aum representatives to meet with Russia’s minister of atomic energy were unsuccessful.\n\n³¹ The farm was purchased for the dual purposes of testing chemical weapons and mining uranium.\n\n³² Following its sarin attack on the Tokyo subway in 1995, Aum reappeared under the name of Aleph. In 2000, Japanese police discovered that the cult had obtained information about nuclear facilities in a number of countries from classified computer networks (Daly et al., 2005, p. 19).\n\n³³ For a discussion of some of the incidents, see Leader (1999, pp. 34–37) and Daly et al. (2005, pp.31-33)\n\n³⁴ For an assessment of these documents, see Albright (2002). Al Qaeda’s nuclear programme: through the window ofseized documents [online]. Policy Forum Online, Nautilus Institute. Available from: http://nautilus.org/acrchives/fora/Special-Policy-Forum/47\\_Albright.html [Accessed 15 September 2006.]\n\n³⁵ For a review ofsome of these reports see (Daly et al., 2005, pp. 40–45). One of the more sensationalist reports emanating from Israel claimed that the terrorist organization had acquired eight to ten atomic demolition mines built for the KGB (Jerusalem DEBKA-Net-Weekly, 12 October 2001 cited by Daly et al., p. 41). Another report from the Pakistani press suggests that two small nuclear weapons were transported to the US mainland (The Frontier Post, 2001). Ifpress reports can be believed, on occasion, bin Laden and his senior aides have sought to give credence to these stories by bragging oftheir acquisition ofnuclear weapons (Connor, 2004; Mir, 2001). A sceptical assessment ofthese risks is provided by Sokov (2002).\n\n³⁶ Daly et al. provides a related but alternative assessment of the unsuccessful procurement efforts (2005).\n\n³⁷ To some extent, this apparent divergence in threat assessments among experts is due to their focus on different forms ofnuclear terrorism and different time-frames. Nevertheless, there is no clear prevailing view among experts about the immediacy or magnitude of the nuclear terrorism threat. For an extended review ofexpert opinions on various proliferation and terrorist threats -although not precisely the issue ofuse of a nuclear explosive, see Lugar (2005).\n\n³⁸ For a discussion of this and other nuclear pathways involving state sponsorship see Ferguson and Potter (2005, pp. 54–61).\n\n³⁹ Some breakthroughs might lead to weapons that are more likely to successfully detonate, improving success rates but not overall destruction.\n\n⁴⁰ See, for example, the 1983 movie War Games. Schollmeyer provides a review of other fictional accounts of nuclear violence (2005).\n\n⁴¹ Sagan made important study of the broader but related issue of the possibility of achieving fail safe systems in large organizations such as the military (1993).\n\n⁴² The Hiroshima bomb from highly enriched uranium had a yield of between 12.5 and 15 kilotons (Rhodes, 1986, p. 711).\n\n⁴³ Estimates come from Helfand et al. (2002) and Bunn et al. (2003, p. 16). Both derive their estimates from Glasstone and Dolan (1977) and their own calculations.\n\n⁴⁴ Casualty estimates from these two attacks vary by a factor of 2, from 68,000 at Hiroshima (Glasstone and Dolan, 1977)to 140,000 (Rhodes, 1986).\n\n⁴⁵ Whether or not a fire storm is generated would depend on the exact location of the blast. A smaller, ground-based explosion is less likely to trigger a fire storm, but an excess of glass, flammable material, gas mains, and electrical wires might trigger one in a city (Eden, 2004).\n\n⁴⁶ Neither the U.S. Office of Technology Assessment (1979) nor Glasstone and Dolan (1977) attempt to calculate the economic impact of a nuclear attack.\n\n⁴⁷ The actual percentage of those likely to succumb to long-term psychological illness will depend on several factors, including the individual’s proximity to the attack location, whether or not a person became ill, previous exposures to trauma, the individual’s prior psychological state, the presence or otherwise ofa support network and the amount ofexposure to media coverage (Pangi, 2002); for more on the links between terrorism and subsequent psychological disorders, see Schlenger (2002) and North and Pfefferbaum (2002).\n\n⁴⁸ The ratio ofunexposed to exposed patients has been conservatively estimated as 4:1. (Department ofHomeland Security, 2003, p. 34) Department ofHomeland Security Working Group on Radiological Dispersal Device (RDD) Preparedness, Medical Preparedness and Response Sub-Group. (2003). Radiological Medical Countermeasures. Becker notes that estimates state that in any terrorist event involving unconventional weapons, the number of psychological casualties will outnumber the number ofphysical casualties by a factor offive. Becker (2001). Are the psychosocial aspects ofweapons ofmass destruction incidents addressed in the Federal Response Plan: summary ofan expert panel. Military Medicine, 166(Suppl. 2), 66–68.\n\n⁴⁹ This is presuming, ofcourse, that they could not be easily retaliated against and could credibly demonstrate the possession ofa robust nuclear arsenal.\n\n⁵⁰ For more information on the topic of disruptive technologies and their singular adoption behaviour, see Bower and Christensen (1995).\n\n⁵¹ See, in particular, the growing literature on the promise and problems associated with implementing United Nations Security Council Resolution 1540, including Jones (2006).\n\n⁵² As Ehud Sprinzak has argued, ‘the vast majority of terrorist organizations can be identified well in advance … and the number of potential [WMD] suspects is significantly less than doomsayers seem to believe. Ample early warning signs should make effective interdiction of potential superterrorists much easier than today’s prevailing rhetoric suggests’ Sprinzak (2000, pp. 5–6).\n\n¹ One important sceptical discussion of the bioterrorism threat is Milton Leitenberg, Assessing the Biological Weapons and Bioterrorism Threat (U.S. Army War College, 2005).\n\n² For a statement by Ambassador Donald Mahley, US Special Negotiator for Chemical and Biological Arms Control Issues, refer to http://www.state.gov/t/ac/rls/rm/2001/5497.htm\n\n³ Meaning that the scientific or engineering details of what occurs ‘inside’ a particular component or technique need not be understood by the individual investigator in order to make use of it.\n\n⁴ Scientifically, one distinguishes between the microorganism, Bacillus anthracis, and the disease it causes, anthrax. Here we have adopted the more casual popular usage that conflates the organism itself with the name of the disease, at the risk of some loss of precision.\n\n⁵ A base, or a nucleotide, is the fundamental unit of a DNA molecule.\n\n⁶ Once they are inside their target cells, viruses hijack cellular proteins to convert their genomes into viral particles. However, viruses that contain negative strand RNA genomes, like Marburg and Ebola, cannot be turned into mature viruses with host proteins alone. Conversion of such genomes into virus particles also requires proteins that are normally packaged within the virus itself. Thus, the Ebola and Marburg genomes are not infectious on their own.\n\n⁷ The US federal advisory group, NSABB, has called for self-regulation within the scientific community. Under the proposed plan, scientists themselves decide whether their research constitutes dual-use experiments of concern. For a discussion of NSABB’s proposal, refer to Jocelyn Kaiser (2007). ‘Biodefense: Proposed Biosecurity Review Plan Endorses Self-Regulation’ Science, 316(5824), 529.\n\n⁸ For a discussion of codes of conduct in the case ofbiodefence research, see Roger Roffey, John Hart, and Frida Kuhlau, September 2006 ‘Crucial Guidance: A Code of Conduct for Biodefense Scientists’, Arms Control Today. For a review and critical discussion of the broader need for codes applicable to all life scientists, see Globalization, Biosecurity and the Future of the Life Sciences (Washington, DC: National Academies Press, 2006), pp. 246–250.\n\n⁹ For key publications in this literature, see Wolfgang Reinicke, Global Public Policy: Governing Without Government? (Washington, DC: Brookings Institution, 1998); J.F. Rischard, High Noon: Twenty Global Problems, Twenty Years to Solve Them (New York: Basic Books, 2002); Anne-Marie Slaughter, A New World Order (Princeton: Princeton University Press, 2004).\n\n¹⁰ We define ‘sub-state’ groups to be those that receive substantial assistance from a state or state entities; ‘non-state’ groups by contrast are those that do not. The Rajneeshees and Aum Shinrikyo were non-state groups. Because of its accommodation by the Taliban in Afghanistan, Al Qaeda arguably was, at least for a time, a sub-state group.\n\n¹¹ The implication of plague in Surat has been some what controversial. A number of studies, however, including Shivaji et al. (2000), have used DNA forensics to show that the causative agent of the disease out break was, infact, Yersinia pestis.\n\n¹² Much of the discussion here, regarding disease preparedness and response, has been based on WHO strategies that can be found here: http://www.who.int/csr/delibepidemics/biochemguide/en/index.html\n\n¹³ There are a host of legal and ethical issues regarding implementation of quarantines. For a discussion, refer to Cécile M. Bensimon and Ross E.G. Upshur, 2007 ‘Evidence and effectiveness in decision making for quarantine’. American Journal of Public Health (Suppl. 1), pp. 44–48; Richard Scabs, 2003. SARS: prudence, not panic Canadian Medical Association Journal, 169(1), pp. 1432–1434.\n\n¹⁴ Recombinant DNA technology facilitated the exchange of genetic material between vastly different organisms and opened new frontiers for molecular biology research, but it also brought with it a number of safety concerns regarding potential harm to laboratory workers and to the public.\n\n¹ In contrast, authoritarianism has historically been quite durable, and even today arguably remains the most common form of government.\n\n² Given their durability, it is not surprising that authoritarian regimes face only a highly attenuated version of this dilemma. Many authoritarian regimes provide reasonable levels of prosperity and happiness for their citizens, so exposure to the outside world is no more than mildly demoralizing. Authoritarian regimes can therefore safely allow their people enough contact with other countries to economically and scientifically keep up.\n\n³ In correspondence, Nick Bostrom raises the possibility that the creation of a democratic world government might provide better protection against the emergence of a totalitarian world government than the status quo does. In my view, however, major moves in the direction of world government will happen either democratically or not at all. Any world government is going to begin democratically. The problem is that once a world democratic government exists, there is at least a modest probability that it becomes totalitarian, and if it does, the existence of a non-totalitarian world will no longer exist to provide a safety valve. Bostrom (forthcoming) suggests that a technological breakthrough in something like nanotechnology or artificial intelligence might be a non-democratic route to world government. The discovering nation or bloc of nations could leverage the breakthrough to subjugate the rest of the world. In my view, though, modern communications make it highly implausible that the first mover could retain a monopoly on such a breakthrough for long enough to realign world politics.", "date_published": "2011-09-29T00:00:00Z", "authors": ["Nick Bostrom", "Milan M. Cirkovic"], "summaries": [], "initial_source": "ebook", "source_filetype": "epub"}
{"id": "120d4e56ab3c142c4cfeb6aa507afef2", "title": "The Al Does Not Hate You: Superintelligence, Rationality and the Race to Save the World", "url": "https://www.goodreads.com/en/book/show/44154569", "source": "special_docs", "source_type": "book", "text": "[]\n\nFor Billy and Ada. I hope the people in this book are right,\nand that you live to see humanity reach the stars.\n\n[]\n\n Contents\n\nDedication\n\nTitle Page\n\nIntroduction: ‘I don’t expect your children to die of old age’\n\nPART ONE: INTRODUCTIONS\n\n1: Introducing the Rationalists\n\n2: The cosmic endowment\n\nPART TWO: THE PAPERCLIP APOCALYPSE\n\n3: Introducing AI\n\n4: A history of AI\n\n5: When will it happen?\n\n6: Existential risk\n\n7: The cryptographic rocket probe, and why you have to get it right first time\n\n8: Paperclips and Mickey Mouse\n\n9: You can be intelligent, and still want to do stupid things\n\n10: If you want to achieve your goals, not dying is a good start\n\n11: If I stop caring about chess, that won’t help me win any chess games, now will it?\n\n12: The brief window of being human-level\n\n13: Getting better all the time\n\n14: ‘FOOOOOM’\n\n15: But can’t we just keep it in a box?\n\n16: Dreamed of in your philosophy\n\n17: ‘It’s like 100 per cent confident this is an ostrich’\n\nPART THREE: THE WAYS OF BAYES\n\n18: What is rationality?\n\n19: Bayes’ theorem and optimisation\n\n20: Utilitarianism: shut up and multiply\n\nPART FOUR: BIASES\n\n21: What is a ‘bias’?\n\n22: The availability heuristic\n\n23: The conjunction fallacy\n\n24: The planning fallacy\n\n25: Scope insensitivity\n\n26: Motivated scepticism, motivated stopping and motivated continuation\n\n27: A few others, and the most important one\n\nPART FIVE: RAISING THE SANITY WATERLINE\n\n28: Thinking probabilistically\n\n29: Making beliefs pay rent\n\n30: Noticing confusion\n\n31: The importance of saying ‘Oops’\n\nPART SIX: DECLINE AND DIASPORA\n\n32: The semi-death of LessWrong\n\n33: The IRL community\n\nPART SEVEN: DARK SIDES\n\n34: Are they a cult?\n\n35: You can’t psychoanalyse your way to the truth\n\n36: Feminism\n\n37: The Neoreactionaries\n\nPART EIGHT: DOING GOOD BETTER\n\n38: The Effective Altruists\n\n39: EA and AI\n\nPART NINE: THE BASE RATE OF THE APOCALYPSE\n\n40: What are they doing to stop the AI apocalypse?\n\n41: The internal double crux\n\n42: Life, the universe and everything\n\nAcknowledgements\n\nNotes\n\nCopyright\n\nIntroduction\n\n ‘I don’t expect your children to die of old age’\n\n Lord and Master! Hear me call. Oh, come the master!\n Lord, the need is great! The ones I called, the spirits\n\n Will not leave.\n\n ‘Der Zauberlehrling’, or ‘The Sorcerer’s Apprentice’, Johann Wolfgang von Goethe (1797). Translated using artificial intelligence (specifically, Google Translate).\n\nI was sitting in the passenger seat of a huge black BMW SUV, being driven around the byzantine freeways of the southern San Francisco Bay Area on a gorgeous October afternoon, when he said it: ‘I don’t expect your children to die of old age.’\n\nTo borrow a line from Douglas Adams,\\* when you’re cruising down the road in the fast lane and you lazily sail past a few hard-driving cars and are feeling pretty pleased with yourself and then accidentally change down from fourth to first instead of third, thus making your engine leap out of your bonnet in a rather ugly mess, it tends to throw you off your stride in much the same way that this remark threw me off mine.\n\nMy companion was a guy called Paul Crowley, a man whose day job is as a cryptography engineer on Google’s Android phone-operating system, but whose chief preoccupation in life is helping humanity reach the stars without first being destroyed by its own technological success.\n\nThere is a group of people, of whom Paul is one, who think that now is the crunch time. The next 100 years or so will be the inflexion point for humanity – either we go on and colonise the cosmos, becoming a galaxy-spanning civilisation of near-immortal demigods, or we annihilate ourselves with one or more of the technologies that we have developed. My children, they think, have a good chance of reaching demigodhood; they have, also, a good chance of not doing so. They want to improve the odds of the former.\n\nI have been aware of these people for a few years. They’re known as the Rationalists.\n\nYou’ve probably read a lot about artificial intelligence (AI) in recent years. Will it take our jobs? Will it form new, deadly, autonomous weapons on the battlefield? Will it lead to an era of inequality, as the rich buy all the robots and computers that run the new economy, and the poor find themselves left even further behind? Will we get those robot butlers we were promised?\n\nThese are serious and real concerns which deserve the many articles and books written about them, apart from the butler thing. But while the Rationalists are worried about that stuff, it’s not the focus of their concern. Instead, they’re worried that an AI will – in the relatively near future, the future that my children could easily live to see, or not far beyond it – become as smart as a human. And that when it does so, it will become as good as we are at designing artificial-intelligence systems, because designing artificial-intelligence systems is something that humans can do.\n\nAnd so a machine that is as smart as a human could, possibly, very quickly improve itself, get better at improving itself, improve itself some more, and so on. An explosion would take place: suddenly, humans would find themselves vastly intellectually outgunned. Intelligence is what has made humans the most successful large animals on the planet; the tiny difference in DNA between us and gorillas, the thing that makes us smarter, is the difference that means our thronging billions live on every continent on the planet, while gorillas are going extinct in the mountains of Congo and Rwanda. If there’s a machine that’s smarter than us, the Rationalists say, we would live only at its sufferance – as gorillas do, just about, at ours.\n\nAnd, they say, just because a machine is smart, it doesn’t mean that it’s nice. It doesn’t even mean that it’s conscious. And if we aren’t extraordinarily careful about how we build it – and even more careful about what we tell it to do – then it is possible that a future with AI could be, as far as we are concerned, extremely short and unpleasant. Or it could, equally, be glorious, spreading out across the galaxy, bounded only by the physical limits of entropy, light speed and the size of atoms.\n\nThis book is about that future. It’s my attempt to work out whether I believe, as (some of) the Rationalists do, that we’re on the brink of something – that my children, realistically, may not die of old age. It’s also a look at the people themselves, who are fascinating, strange, clever, kind, frightened and self-sabotaging.\n\nA word before I start. One of the people you’ll meet in this book, the blogger and psychiatrist Scott Alexander, has an excellent habit. At the top of many of his blog posts – which are usually brilliant, thoughtful and terrifyingly long – he has a little line or two in italics, ‘Epistemic status’; and then says how confident he is in his conclusions, and why. ‘Epistemic status: Uncertain, especially on the accuracy of the economic studies cited.’ ‘Epistemic status: Pieced together from memory years after the event.’ ‘Epistemic status: Wild speculation.’ I think this is wise.\n\nSo, here we go. Epistemic status: Fairly confident. I think most of the claims in this book are true; I think I have given a fair account to the best of my ability of what the people in it believe, and how they live. But I am human, and therefore have a brain that goes wrong in predictable ways. One of those ways (according to some more people you’ll meet in these pages) is that when we find a fact we like, we ask ourselves, ‘Can I believe this?’, whereas if we find a fact we don’t like, we ask ourselves, ‘Do I have to believe this?’\n\nI am fond of many – not all – of the people in this book, and I suspect that I therefore erred on the side of ‘Do I have to?’ when confronted with things that might make me like them less. For these reasons I am extremely confident that I have made errors throughout this book. I hope that none of them are major, or defamatory. Whether they are or not, they’re mine, not anyone else’s.\n\nI flew to California in October 2017 to meet Paul and a few others involved in the Rationalist community. Their biggest in-real-life hub is based around Berkeley and Silicon Valley, although the community is really distributed around the internet.\n\nThis wasn’t the first time I’d come into contact with them. I’d been aware of the community since about 2014, when I wrote a review of Nick Bostrom’s Superintelligence: Paths, Dangers, Strategies. If you’re vaguely aware of a conversation going on about whether or not AI will destroy the world, it’s probably because of Bostrom’s book. Elon Musk read it, in between making lots of money setting up PayPal and then systematically losing it again by trying to fly to Mars, and reported back: ‘We need to be super-careful with AI. Potentially more dangerous than nukes.’¹ Bill Gates says we should all read it to understand AI.² Bostrom’s work influenced Stephen Hawking’s view that AI could be ‘the best or worst thing to happen to humanity’.³\n\nIt was an amazingly dense, difficult book – writing my review, I opened it at random to select a passage, and ended up with: ‘An oracle constructed with domesticity motivation might also have goal content that disvalues the excessive use of resources in producing its answers.’⁴ I have a pretty good idea what that means, but it’s not exactly a Ladybird Introduction to AI. Still, it sold extraordinarily well for what was essentially a work of academic philosophy, getting up to number 17 on the New York Times bestseller list.\n\nAnd it is, once you get your head around it, somewhat terrifying. It compares humanity’s efforts to build a superintelligent machine – and those efforts are ongoing, serious and, possibly, quite close to completion – to a bunch of sparrows trying to find an owl chick to raise, to protect them. As in, you can see where they’re going with it, but it may be that they haven’t 100 per cent thought through all the possible consequences.\n\nIts release was, roughly speaking, when the Rationalists’ concerns became mainstream. But apparently the book was not widely understood in the media, being met with a lot of references to The Terminator. A few people from the community, though, including Paul, read my review, and decided that I’d essentially got the gist of it. So they contacted me.\n\nOver the next few years, I became more involved with the Rationalists. I started reading their websites; I learned the jargon, all these technical and semi-technical terms like ‘updating’ and ‘paperclip maximiser’ and ‘Pascal’s mugging’ (I’ll explain what all those things are later). I read the things you’re supposed to read, especially and notably ‘the Sequences’ (I’ll explain what they are later, as well). I came to terms with the huge possible impacts, positive and/or negative, of superhuman AI. And I became increasingly enamoured of their approach to the world, of which AI fears were only a part. It was also about people who want to make humanity better, to help us reach the stars, to stop us from destroying ourselves, to find ways of making us immortal. A whole related sub-branch is dedicated to making charitable giving more efficient. And it’s about ways of helping us think about how we think – about using our best understanding of how the human mind works to make us better at achieving the things we want to achieve, and how to make us better at finding out things that are true and debating them with other people in charitable, kind ways.\n\nI also gathered that human-level intelligence could be quite close: most people in the AI research field think it’ll happen within the next century and possibly in the next few decades. And it all seemed pretty hard to argue with, on balance.\n\nBut somehow I hadn’t put two and two together. The huge impacts and the possibly imminent arrival of AI were both things I understood and accepted on an intellectual level, but the implications of those two things hadn’t really sunk in, in a visceral, gut-level, intuitive-understanding sort of way.\n\nSo when I was told that there was a real chance that my two young children would not die of old age, it shouldn’t have shocked me – my children were, at the time, two and three years old; they could fairly confidently expect to live another 90 or 100 years; 90 years is well into the ‘superhuman AI is more likely than not’ bit of most researchers’ predictions; superhuman AI, many people in the field believe, has the potential to either kill us all or make us near-immortal post-human demigods.\n\nBut it had all been an intellectual game, up to that point. Now we were talking about my actual, real-life children, my little toddlers Billy and Ada, who liked dinosaurs and Octonauts and the lower-quality Pixar movies. It left me somewhat winded. It brought to mind all the people who were worrying about robots taking people’s jobs, or being used on the battlefield, and made me think: The iceberg is 100 yards off the port bow, and you are worrying about whether the deckchairs are safe.\n\nAs it happened, in the autumn of 2017 when I was out in California, there were a series of enormous wildfires.⁵ A million acres of bush and forest in the north of the state burned; 43 people died and thousands of homes were destroyed, particularly around the Napa wine region. Those fires were just a few miles north of the Bay Area. Each morning when I woke up in my sad little Airbnb above a noisy nightclub in Berkeley I could smell woodsmoke; the sun was partly hidden behind a haze of it. When I went into San Francisco itself, doing the tourist thing, you could look north from Pier 39 and see that the far shore of the Bay was occluded behind a grey curtain of smoke, hanging in the valley like some ominous mist. One night I climbed the huge hill behind the Berkeley campus to see the sunset, and the smoke made the sun a vivid, bloody ball as it sank behind the almost invisible Golden Gate, 13 miles away: a startling sight which my iPhone camera was entirely unable to capture. (I even saw someone, up on the hill behind Berkeley, putting a cigarette out in the dry grass as he watched the sun sink into the smoke. I wanted to grab him by the collar and shout at him.)\n\nA short distance away, everything was on fire and people were dying – but here, in this cosy little enclave of civilisation, no one was paying attention, even as the smoke drifted over their homes. The few who did were wearing surgical masks – in the face of a fiery death, people were worrying about asthma. Icebergs and deckchairs.\n\nThe metaphor is ridiculous, of course. There was no serious risk of anyone burning to death in Berkeley, but there was a pretty good chance of aggravating any pre-existing lung conditions. People were behaving perfectly sensibly. But I’ve been a journalist for over a decade now, and you don’t get anywhere in this business by ignoring corny and obvious metaphors that are staring you in the face. So I started asking: Are we going to (metaphorically; possibly literally) burn to death?\n\nIt seemed to me important to find out whether the whole children-not-dying-of-old-age thing was a widely held belief. So I went and spoke to Anna Salamon.\n\nSalamon is the president and co-founder of a non-profit organisation called the Center for Applied Rationality (CFAR, which people pronounce See-Far), and a key member of the Rationalists. CFAR, along with the Machine Intelligence Research Institute, or MIRI, is probably the closest thing the Rationalist community has to a real-world, as opposed to online, heart. Its office, which it shares with MIRI, is a couple of minutes’ walk from the UCLA Berkeley campus, on a quiet road parallel with University Avenue, on the third floor of an unassuming office block. MIRI was set up by Eliezer Yudkowsky, the founder and driving force of the movement, an odd and polarising figure. (His name will come up again.) Salamon used to work for MIRI, on the problems of AI safety and existential risk, before going off to set up CFAR with the goal of training other bright, conscientious young people to work on the problems. Its mission is to instil in those bright young people the skills and methods of rationality that Yudkowsky and the Rationalists propound.\n\nI’m a few minutes early to meet her and find myself looking around the shared lounge bit between the MIRI and CFAR offices, entirely alone and feeling extremely weirded out. It’s a little dilapidated – it has something of a university junior common room feel, not the futuristic gleaming tech start-up look I’d been expecting. There’s no reception, just a bunch of faintly elderly sofas and bean bags. One wall is dominated by a vast picture of the Earth from space; on another there is a whiteboard, covered in equations, as well as an H.P. Lovecraft-ish slogan (‘Do not anger timeless beings with unspeakable names’), and a jaunty little reminder to ‘Thank Stanislav Petrov!’ (Petrov, if you’re unfamiliar with the name, was a Russian military officer who is credited with preventing a major nuclear war being triggered in September 1983, more of which later.)\n\nThere is also an expensive-looking road bike with drop handlebars and a pannier rack, propped up against a water cooler, with a post-it note saying: ‘Is this your bike? Talk to Aaron.’ For something with such a huge mission, this little place feels a bit lost in the vast suburban sprawl of the Bay Area; a little worn around the edges.\n\nBefore answering any question, Anna pauses for a tangible moment, a half-second or so; I’m fairly sure that this is a learned behaviour, an attempt to vet each statement to make sure it’s something she thinks, rather than simply something she’s saying. The conversation is initially quite hard. She’s kind and thoughtful, but seems wary of me, and answers in short sentences or single words. I think she’s concerned that I’m not here with good intentions as far as the Rationalist movement is concerned. (There is an understandable streak of paranoia among the Rationalists, I come to learn. Many of them are extremely intelligent and in some respects quite influential, but in others they are highly vulnerable – nerdy, often autistic or with other social deficits – and it would be extremely easy for me to write a book mocking them. I do not want to do that.)\n\nI ask her, first, what her main goal is; CFAR is intended to train people with these rationality techniques, so I wondered whether that was an aim in itself, or whether it was a means to a greater end. She says that the primary goal is ‘to help humanity reach the stars’, and to do so while still recognisably human – not necessarily physically, but in terms of the things we care about and value. She thinks that there are lots of ways in which that might not happen, but the ‘largest and most tractable part’ of the problem is the risk of AI destroying humanity as we understand it.\n\nEventually I build up the courage to ask her the big one. Paul, I say, thinks that if humanity survives the next 100 years then we’re probably going to make it to a glorious cosmic future. That my children probably won’t die of old age.\n\n‘I agree with that.’\n\nEither something terrible will happen, or they make it to …\n\n‘The singularity.’\n\nLater on I meet Rob Bensinger, who’s the research comms manager at MIRI. It is probably best to think of Rob as the aforementioned Eliezer Yudkowsky’s messenger on Earth; Yudkowsky himself agreed only to answer technical questions, via email. Rob, a polymath who had become part of Yudkowsky’s circle a couple of years before, speaks for him, like one of the angels who appear in the Bible when God has something to say but can’t bring Himself to turn up in person.\n\nWhen I meet Rob, MIRI is preparing for a ‘retreat’. They weren’t actually going anywhere, but in recent years they’d found that when they had previously had real, proper, go-off-into-a-cabin-in-the-woods-somewhere retreats they had been extremely successful. They’d really focused people’s minds, improved their productivity.\n\nBut hiring a cabin that can fit a dozen people is quite expensive and inconvenient, so they were experimenting with ways of having the same effect without needing to go anywhere. Instead, they’d hung a large white sheet across the office and used uplighting and other little visual tricks to make it feel like somewhere else. Rob said that it had been effective. It struck me as a rather clever solution.\n\nI ask him the children question, and he demurs. ‘I wouldn’t like to be on the record as saying something that specific. It’s a pretty sensitive thing and I wouldn’t want to off-the-cuff it.’ I press him a bit, though, and he says that Paul’s view ‘seems normal to me’. ‘Most people expect AI this century, and most people interested in AI risk generally think the risk is pretty serious, not just a small risk but a medium-sized to large risk, shall we say. I’d say it’s a this-century problem, not a next-century problem. I think most people will agree with me, within AI. The Open Philanthropy Project (OpenPhil) gives it at least a 10 per cent chance of happening in the next 20 years.’\n\nAs it happened, I was going to OpenPhil the next day. It’s across the Bay from Berkeley, in downtown San Francisco itself. OpenPhil and GiveWell are two organisations run on Rationalist lines that look at the most effective ways to donate money to charity. They’re central to something called the Effective Altruism (EA) movement, which is strongly linked to the Rationalist community. OpenPhil in particular has donated millions of dollars to AI safety organisations, and MIRI in particular, over the years.\n\nHolden Karnofsky, the co-founder of both OpenPhil and GiveWell, confirms what Rob said, that OpenPhil thinks there’s about a 10 per cent chance of ‘transformative’ AI in the next 20 years. ‘That would clearly meet your criteria’ of my children not dying of old age, he said. Isn’t that terrifying? I ask him. ‘Yes,’ he says. ‘We live in a truly weird time.’ We don’t realise how fast things are changing now, in a way unlike any other time in history, but it’s only going to get faster: ‘If transformative AI comes, that could be transformative in ways that would make the Industrial Revolution look small. Yes, it’s really strange and it’s disorienting.’\n\nNot everyone I spoke to agreed with this. Some people reckoned the timeline was too short, and that it was unlikely (though not impossible, by any means) that human-level AI would arrive in my children’s lifetimes. Others thought that the timeline was perfectly realistic but that human-level AI wouldn’t bring the sort of spectacular change (and possible destruction) that would lead to them not dying of old age. Others were understandably wary about putting these sorts of numbers on things to begin with.\n\nBut it seemed like we were at least dealing with something that was not unrealistic. Sensible, intelligent people, including AI researchers at serious AI companies, senior academics and so on, thought that there was a respectable chance that the next 100 years would see either an ascension to demigodhood or a complete, civilisation-ending catastrophe. More than civilisation-ending, in fact: human-life-ending.\n\nThis book is about some of those people. Specifically, it is about a community of them who came together around a series of blog posts written by Yudkowsky in the mid- to late 2000s, and who are known as the Rationalists.\n\nIt is also, in part, an attempt to work out whether I agree with them.\n\n\\*Specifically, from Chapter 13 of The Hitch-Hiker’s Guide to the Galaxy, the bit where Ford introduces Arthur to Zaphod and Arthur says, ‘We’ve met.’\n\nPart One\n\nIntroductions\n\nChapter 1\n\n Introducing the Rationalists\n\nThe Rationalist community, as it exists now, is sprawling and global. It has hubs in a dozen or more cities and a thronging online presence. It’s full of strange people with strange ideas – about AI (the idea that AI has the potential to be an existential risk to humanity can, I think, be largely traced back to it, or its precursors), about transhumanism, cryonics; about the universe being a simulation – and unorthodox practices, such as polyamorous relationships (ones with several people at once) and group living, which have led to outsiders accusing it of being a cult.\n\nThis whole ecosystem has its roots in the writing of the strange, irascible and brilliant Eliezer Yudkowsky. The key text – the holy book, according to those who think the whole thing is a quasi-religion – is a huge series of blog posts he wrote in the mid-2000s, an ambitious, sprawling set of writing which takes in everything from evolutionary biology to quantum mechanics to AI, and which came to be known as the Sequences. But as far as I can tell, the first visible sign of its birth is a single, much older blog post written on 18 November 1996. It was entitled ‘Staring into the Singularity’.¹ Yudkowsky was 17 years and two months old at the time. The post is still online, by the way. But it is marked at the top by a big red warning triangle, like the sort you get in the back of your car to warn of a road accident, saying: ‘This document has been marked as wrong, obsolete, deprecated by an improved version, or just plain old.’ This sort of thing goes on a lot in the Rationalist community. Being wrong is actively praised, as long as you hold up your hands, admit it and correct it.\n\n‘Staring into the Singularity’ is a fascinating read, although the logic doesn’t really bear scrutiny, as you’d expect from a 17-year-old. It begins: ‘If computing speeds double every two years, what happens when computer-based AIs are doing the research?’ This is a reference to Moore’s law, which says (roughly, in one formulation) that computers get twice as powerful every two years. So computing speed doubles every two years. Yudkowsky points out a corollary: that two years for a human need not seem like two years for a computer. ‘Computing speed doubles every two years of work. Computing speed doubles every two subjective years of work.’ That is, if a computer can think as powerfully as a human, but twice as fast, then it can do two years’ worth of work in a single year. ‘Two years after Artificial Intelligences reach human equivalence, their speed doubles. One year later, their speed doubles again. Six months – three months – 1.5 months … Singularity.’ Things would speed up, exponentially. The world would be changing too fast for us to understand the changes. We would be through the looking glass. This is, roughly, the idea of the ‘fast take-off’ that Nick Bostrom would describe nearly two decades later, although Yudkowsky’s version makes a few weird assumptions and leaps of logic (as, again, is fair enough given his age).\n\nThe term ‘singularity’, by the way, is a reference to physics and black holes. When an object is massive enough and small enough, it bends spacetime so much that the usual laws of physics no longer work. By analogy, when intelligent systems start improving themselves fast enough, our usual ways of predicting the future – our assumptions that tomorrow will be essentially like today – will, say singularitarians, break down. The computer scientist and science-fiction writer Vernor Vinge wrote in 1983: ‘We will soon create intelligences greater than our own. When this happens, human history will have reached a kind of singularity, an intellectual transition as impenetrable as the knotted space-time at the centre of a black hole, and the world will pass far beyond our understanding.’²\n\nYudkowsky’s explicit goal in ‘Staring into the Singularity’ is to bring about AI – the singularity – as soon as possible. ‘Human civilisation will continue to change until we either create superintelligence, or wipe ourselves out,’ he wrote. This superintelligent thing we create, he thinks, can solve all of humanity’s problems, and it is high time that it does so: ‘I have had it. I have had it with crack houses, dictatorships, torture chambers, disease, old age, spinal paralysis, and world hunger. I have had it with a planetary death rate of 150,000 sentient beings per day. I have had it with this planet. I have had it with mortality. None of this is necessary. The time has come to stop turning away from the mugging on the corner, the beggar on the street. It is no longer necessary to look nervously away, repeating the mantra: “I can’t solve all the problems of the world.” We can. We can end this.’ Yudkowsky was, you suspect, quite an annoying 17-year-old, but he was undeniably bright.\n\nIn 2000 – still only 20, remember – he founded the Singularity Institute for Artificial Intelligence. The Singularity Institute was a little non-profit based in Berkeley which would later become known as the Machine Intelligence Research Institute, which you’ve already met. It had, at first, the goal of bringing about this glorious technological future, and Yudkowsky had set a target date for achieving the singularity. It was 2005. (He didn’t manage it.)\n\nI asked Paul Crowley – whom you met in the introduction, driving me around northern California – about all this. ‘My broad picture of how it started,’ says Paul, ‘is Eliezer started by thinking superintelligence is the key to everything, and we need to get there as quickly as possible. It’s intelligent, he thought, so it’ll do the right thing.’\n\nBut even by the time he founded the Singularity Institute, at least according to what he wrote later,³ Yudkowsky had started to wonder whether he was making a terrible mistake. ‘The first crack in my childhood technophilia appeared in, I think, 1997 or 1998,’ he wrote, when he noticed his fellow techno-enthusiasts being glibly optimistic about the difficulties of controlling future technologies – specifically, nanotech. By the end of that debate, he says, the young Yudkowsky ‘had managed to notice, for the first time, that the survival of Earth-originating intelligent life stood at risk’. Still, he cracked on with the Singularity Institute, full steam ahead, for superintelligence. ‘Just like I’d been originally planning to do,’ he writes, with some scorn, ‘but now, with a different reason.’\n\nThis feels, reading his work now, like a key moment, at least in retrospect. What Yudkowsky didn’t do, he says, was ‘declare a Halt, Melt and Catch Fire’. He didn’t look at his own thinking, accept that it was all completely wrong and cast out his conclusions (in his own, later, words, he didn’t appreciate ‘the importance of saying “Oops”’). Instead, he looked at his thinking, realised that it was wrong, and decided that his conclusions were conveniently right anyway.\n\nBut slowly – between 2000 and 2002, between the ages of 20 and 22, probably too young to be placing the entire future of the world on your own shoulders, but that appears to have been the sort of person young Yudkowsky was – he came to the realisation that not only was he wrong, he was disastrously wrong. The exact ways in which he was wrong are going to be the topic of much more discussion in this book, but according to Yudkowsky himself by the time he had reached the wise old age of 27 his stupidity had led him to try to build a device which would destroy the world. ‘[To] say, I almost destroyed the world!, would have been too prideful,’ he wrote.⁴ But he had been trying to do something which he thought, if he had had the wherewithal to actually do it, would have done exactly that.\n\nSo he decided to try to save the world instead.\n\nYudkowsky was not the first person to think about what would come after humans. He was firmly part of the traditions of transhumanist and singularitarian thinking, which had been around for years when he was writing ‘Staring into the Singularity’; some of the ideas they hurled about had existed for millennia. Bostrom notes in a paper that in the Epic of Gilgamesh, a 4,000-year-old Sumerian legend which foreshadows parts of the Old Testament, ‘a king sets out on a quest for immortality. Gilgamesh learns that there exists a natural means – an herb that grows at the bottom of the sea.’⁵ (He finds it, but a snake steals it off him before he can eat it, as is so often the way.) The Elixir of Life, the Philosopher’s Stone, the Fountain of Youth and various other myths represent similar ideas.\n\nBostrom also points out that early transhumanist-style myths contain an element that remains in modern discussion of its ideas: hubris leading to nemesis. Prometheus steals fire from the gods, which most of us can agree was a good thing from humanity’s point of view: his punishment was to have his liver repeatedly pecked out by an eagle for eternity. Daedalus improves on human abilities by, among other things, building wax-and-feather wings to grant himself and his son Icarus the power of flight; Icarus promptly flies too close to the sun, melting the wax, and plunges into the sea. St Augustine thought that alchemy, and the search for a panacea or eternal life, was ungodly, possibly demonic.\n\nThe idea that science could improve on the human-basic form became more plausible after the Enlightenment and Renaissance. Nicolas de Condorcet wondered in 1795 whether science would progress until ‘the duration of the average interval between birth and wearing out has itself no specific limit whatsoever’, and that people would choose to live until ‘naturally, without illness or accident, [they find] life a burden’.⁶ Benjamin Franklin wrote of wanting to be ‘embalmed’ in such a way that he could be revived in the future, since he had ‘a very ardent desire to see and observe the state of America a hundred years hence’.⁷ Bostrom points out that this foreshadows the modern idea of cryonics, preserving the brain for revival in the future.\n\nThe term ‘transhumanism’ and some of its most recognisable ideas sprang up in the first half of the twentieth century. In 1923 J.B.S. Haldane predicted a world in which humans used genetic science to make themselves cleverer, healthier and taller. The term itself was apparently coined by Julian Huxley, brother of Aldous, in 1927: ‘The human species can, if it wishes, transcend itself – not just sporadically, an individual here in one way, an individual there in another way – but in its entirety, as humanity. We need a name for this new belief. Perhaps transhumanism will serve.’⁸\n\nBut transhumanism and singularitarianism really took off as philosophies in the last decades of the twentieth century. There were various different, and to some degree competing, ideas of what transhumanism involved – Yudkowsky, in a since-deleted online autobiography⁹ he wrote at the age of 20, credits Ed Regis’ 1990 book Great Mambo Chicken and the Transhuman Condition, an early, comical taxonomy of these different visions, as an inspiration. The idea of cryonics began to become more popular in this period – super-cooling the brains (and perhaps bodies) of dying people in order to preserve them, as Franklin wished, with the idea of reviving them when technology advanced sufficiently to do so. Transhumanists also talked about how nanotechnology can transform everything. A large subset of them were keen on the idea of uploading – scanning a human brain so precisely, probably by slicing it apart, that you could simulate it in a computer, creating a digital version of the mind that you scanned. (The original, of course, would be destroyed in the process.) Machine-brain interfaces – ways of linking a human brain to a computer, or linking human brains via computers, to improve human cognition – were a constant topic. All of this, naturally, overlapped with the ‘singularitarian’ vision of a world in which superintelligent AI or other technological advances rendered human life unrecognisable (but unrecognisable, they’d have said, in a good way). Most of all, they wanted – want – to stop death. About 150,000 people die every day, worldwide. Most of us wave that away, saying that death gives life meaning, or that eternity would be boring. The transhumanists (not unreasonably, to my mind) ask: OK, but if death didn’t exist, would you all be saying, ‘We ought to limit our lives to about 80 years, to give them meaning?’\n\nAs befits a movement that gets a book written about it with the term ‘Great Mambo Chicken’ in its title, some of its members were and are – by the tightly corseted standards of Western society, I should say – deeply weird. There’s an affectionate 2006 Slate article about transhumanists which says, at one point, ‘Remember those kids who played Dungeons & Dragons and ran the science-fiction club in your high school? They’ve become transhumanists.’¹⁰ There appears to have been an element of truth in that gently mocking phrase.\n\nTranshumanists have a tendency, for instance, to give themselves strange names. The Slate article mentions one who calls herself Wrye Sententia (Dr Sententia is a professor at UC Davis and the director of a non-profit called the Institute for Ethics and Emerging Technologies. Having a strange name doesn’t stop you doing interesting work.) Another changed his name from Fereidoun M. Esfandiary, which was an interesting enough name to begin with, to FM-2030. There’s a Tom Morrow, which is lovely. And there’s a guy who was once called Max O’Connor but who changed his name to Max More, because ‘It seemed to really encapsulate the essence of what my goal is: always to improve, never to be static. I was going to get better at everything, become smarter, fitter, and healthier.’¹¹\n\nMore would later become CEO and president of Alcor, one of the largest cryonics companies in the world. But he is mainly relevant to this story because in 1988, along with Tom Morrow (see? Lovely), he began publishing Extropy Magazine. It was mainly about transhumanism – how to improve upon the human form, make it immortal, make it cybernetic, and so on. In 1992 he founded something called the Extropy Institute, which set up a mailing list – a sort of early precursor of social media, for those of you under the age of 35; you just all chat in your emails – called the Extropians.\n\nOne of the names on the Extropians’ mailing list was Eliezer Yudkowsky. ‘This was in the 1990s,’ says Robin Hanson, an economist at George Mason University and an important early Rationalist figure. ‘Myself, Nick Bostrom, Eliezer and many others were on it, discussing big future topics back then.’ But neither Bostrom nor Yudkowsky were satisfied with the Extropians. ‘It was a relatively libertarian take on futurism,’ says Hanson. ‘Some people, including Nick Bostrom, didn’t like that libertarian take, so they created the World Transhumanist Association, explicitly to no longer be so libertarian.’ The World Transhumanist Association later became Humanity+ or H+. ‘It hardly trips off the tongue as a descriptor,’ says Hanson. ‘But that’s what they insisted they call everything.’ Humanity+ had a more left-wing, less utopian approach to the future.\n\nYudkowsky, on the other hand, felt that the problem with the Extropians was a lack of ambition. He set up an alternative, the SL4 mailing list. SL4 stands for (Future) Shock Level 4; it’s a reference to the 1970 Alvin Toffler book Future Shock.¹² Future shock is the psychological impact of technological change; Toffler describes it as a sensation of ‘too much change in a short period of time’.\n\nYudkowsky took the concept further, dividing it up into ‘levels’ of future shock, or rather into people who are comfortable with different levels of it. Someone of ‘shock level 0’ (SL0) is comfortable with the bog-standard tech they see around them. ‘The use of this measure is that it’s hard to introduce anyone to an idea more than one shock level above,’ he said. ‘If somebody is still worried about virtual reality (low end of SL1), you can safely try explaining medical immortality (low-end SL2), but not nanotechnology (SL3) or uploading (high SL3). They might believe you, but they will be frightened – shocked.’\n\nHe acknowledged that transhumanists like the Extropians were SL3, comfortable with the idea of human-level AI and major bodily changes up to and including uploading human brains onto computers. But he wanted to create people of SL4, the highest level. SL4, he says, is being comfortable with the idea that technology, at some point, will render human life unrecognisable: ‘the total evaporation of “life as we know it”’. (I’m taking this from a 1999 post¹³ of his on SL4, when he’d just turned 20. He also fleshes it out in a long essay called ‘The Plan to Singularity’¹⁴ from about the same time.) He wanted to convert SL2s and SL3s to SL4s, to build a community of people who were comfortable talking about ideas of the post-human future. So he set up this mailing list and called it, reasonably enough, SL4.\n\nIts archives are still available online, and digging through them is a fascinating experience. It’s a bit like that Sex Pistols gig in 1976, where there were only 40 people in the audience but all of them went on to form major bands. Going through the list of authors, you find the founders of major AI companies – such as Ben Goertzel – or AI researchers like Bill Hubbard. Wei Dai, an AI researcher at Imperial College London who played an important role in the creation of cryptocurrencies, is on there. Bostrom and Hanson are both there, and Anna Salamon. Other people who play roles in the story – Michael Vassar, Michael Anissimov – are contributors.\n\nNick Bostrom did a minor double-take when I asked him about SL4 and the Extropians, as though he hadn’t thought about it in a long time. I think he gave a sort of chuckle. ‘Yeah, it was humble beginnings,’ he said. ‘I’d been thinking through some of these things before, but I didn’t know there were other people thinking about it. It’s a bit strange. Nowadays you’d just Google it and immediately find whatever there is, but in the early 1990s when I was a student no one else was interested in it. So it was a bit of a revelation when I started using the internet in 1996 that there were these communities, people chatting about it.’\n\nSeveral of the key concepts that do the rounds in the Rationalsphere these days first arose on SL4 and the Extropians. The aforementioned ‘paperclip maximiser’ was first mentioned there, possibly by Yudkowsky: ‘Someone searched [the Extropians’ archive] recently and found a plausible first mention by me,’ he told me by email; he was and remains wary about talking to me on the phone. ‘I wasn’t sure if it was me, Nick, or Anders Sandberg, but it kind of sounds like me.’ The ‘AI box’ experiment, in which Yudkowsky attempted to demonstrate that even an ‘oracle’ superintelligent AI, locked in a box and only able to communicate by text, was not safe, took place on SL4.¹⁵ Bostrom first linked to his paper arguing that we may be living in a computer simulation on SL4.¹⁶\n\nBut although SL4 gathered quite an impressive bunch of people, it still wasn’t enough to satisfy Yudkowsky. Looking at the archives, you see that he’s extremely busy in the first few years, up to about 2004, but later on he seems to be less involved. No new threads of his appear at all between 2005 and 2008. The Rationalists’ own semi-official history of themselves, on the wiki page of Yudkowsky’s website LessWrong, says that he ‘frequently expressed annoyance, frustration, and disappointment in his interlocutors’ inability to think in ways he considered obviously rational’ and that after ‘failed attempts at teaching people to use Bayes’ Theorem, he went largely quiet from SL4 to work on AI safety research directly’.¹⁷\n\nThen Robin Hanson, the economist and fellow SL4/Extropians commenter, set up a blog of his own called Overcoming Bias. ‘I started this blog after I got tenure at George Mason,’ Hanson told me, ‘as something to do in my spare time.’ When I speak to him, via Skype from his office at the university, he’d accidentally left his window open all weekend during one of the more dramatic periods of cold weather the US East Coast had seen for a while. He was wrapped up in a puffa jacket and woollen hat and his breath was visible in the air, even on the low-res Skype connection. ‘I decided to theme it on overcoming bias.’\n\nThis was 2006, a few years before the publication of Daniel Kahneman’s famous book Thinking, Fast and Slow, about the various systematic biases in human thought. But Kahneman’s groundbreaking work with Amos Tversky was already extant and slowly becoming more widely known. Hanson, a polymath and autodidact in a similar, if less extreme, vein to Yudkowsky, had picked up a lot of Kahneman and Tversky’s work in his travels around the sciences – he’d qualified as a physicist, before doing post-grad degrees in social science and economics. Overcoming Bias was explicitly founded on the ‘general theme of how to move our beliefs closer to reality, in the face of our natural biases such as overconfidence and wishful thinking, and our bias to believe we have corrected for such biases, when we have done no such thing’.¹⁸\n\nHe invited a few old Extropians/SL4 veterans to come and join him, people who’d impressed him with the quality of their thinking. Among them were Nick Bostrom and Eliezer Yudkowsky. ‘Nick just blogged a few things,’ he said. ‘But Eliezer blogged a lot, which was great.’ It’s at this point that Yudkowsky began what would later become known as the Sequences. In essence, they were a reaction to the fact that he couldn’t get people to understand what he was talking about when he said that AI was a threat.\n\nThe problem he had was that no one really took him seriously. So in order to explain AI, he found he had to explain thought itself, and why human thought wasn’t particularly representative of good thought. So he found he had to explain human thought – all its biases and systematic errors, all its self-delusions and predictable mistakes; he’d found a natural home on Overcoming Bias. And to explain human thought, he found he had to explain – everything, really. It was like when you pull on a loose thread and end up unravelling your entire favourite jumper.\n\nIt was a meandering, unfocused thing, for a long time; at one point he gets on to quantum physics; at another he approvingly cites George Orwell’s (somewhat silly) proscriptions against using the passive voice. Paul Crowley tells an illustrative story. ‘There’s this post, about fake utility functions,’¹⁹ he says. (Don’t worry about what a utility function is or how it can be fake.) ‘If you want to know the story of how this got written, it’s a good one to read. It begins by saying something like, “Today I can finally talk about this idea of fake utility functions. I was going to talk about it six months ago. But then, when I sat down to write it, I found I had to set out this idea, and then to explain that idea it helps if I explain this other idea. And then I thought it would be easier if the reader understood evolutionary biology, so I ended up writing an introduction to evolutionary biology.” He ended up writing about two dozen posts just on evolutionary biology. And the joke of it is this post wasn’t even some cornerstone of the whole thesis; it was just something he wanted to write.’\n\nSlowly, the blog posts built up, and up and up. For an idea of how much, think of The Lord of the Rings books. When you add all three together, they come to about 455,000 words. War and Peace, a book which is actually more famous for being long than it is for being good (it’s OK), is about the 587,000 mark. According to the Kindle app on my iPhone, War and Peace is 18 arbitrary dots long. Rationality: From AI to Zombies, the edited e-book edition of Yudkowsky’s blog posts, merits 19 dots. If the Kindle app’s length indicator is accurate, then that puts RATZ at around 620,000 words long.\\* The unedited Sequences were more like a million. That’s a fair old slog. There are few things so dispiriting as reading on a Kindle and realising that after 30 minutes you’ve only gone from 3 per cent to 4 per cent. And this is not a book about elves fighting orcs, which if nothing else keeps you moving along. Yudkowsky is an engaging writer, but by its nature it’s heavy going.\n\nBut it became pretty successful. In 2009 Yudkowsky moved his blog posts over to a new website, LessWrong, which was intended as a sort of community hub where anyone could post. At about the same time – 2010 – he started publishing something else, his Harry Potter fan fiction Harry Potter and the Methods of Rationality, which does exactly what it says on the tin: it involves a nerdy scientist-Harry trying to work out what the rules of this magical universe are, using Rationalist-style methods. It was a surprising success, gathering 34,000 reviews on the site FanFiction.net; it may be the most-read thing that Yudkowsky has ever done, and attracted large numbers of readers to his other work, especially LessWrong.\n\nAt its peak, LessWrong had about a million page views a day.²⁰ Some posts had hundreds of thousands of unique page views (a metric that avoids the problem you get of someone clicking ‘refresh’ and suddenly counting as two hits). It’s probably not completely inaccurate to say that a million people have read some of the Sequences, and I’d guess that the number of people who’ve read the whole thing is probably a high-five figure or low-six. I may be off by an order of magnitude, of course – there’s no easy way to tell.\n\nWhat Yudkowsky was trying to do with all this was to explain why AI was dangerous. But because he found that first he had to describe intelligence, and human intelligence, his project became more ambitious: to improve human rationality, in order to help prevent humanity from destroying itself.\n\n\\*The Bible still wins. The King James Authorised Version weighs in at 783,137 words. While looking that up, I learned that in the early days someone managed to miss one word out, getting it down to a more manageable 783,136 but unfortunately changing the Sixth Commandment to read ‘Thou shalt commit adultery’.\n\nChapter 2\n\n The cosmic endowment\n\nWe’ll get on to why the Rationalists think that AI is so dangerous soon. But first we should look at why they, and the singularitarians who came before them, are also so keen on it. The gamble, they think, is between extinction and godhood.\n\nAccording to the Rationalists, getting AI right could be the greatest thing that ever happens to our species. If humanity survives the next few decades, or maybe centuries – it’s not clear exactly how long, but probably a fairly insignificant period in comparison to how long we’ve already existed, and certainly an insignificant period in comparison to how long everything else has – then things could go extraordinarily right for us. This is what Paul Crowley meant, or part of what he meant, by saying that he didn’t expect my children to die of old age.\n\nIt is improving technology, and specifically AI, that people are talking about when they refer to this glorious future. ‘The potential benefits are huge, since everything that civilisation has to offer is a product of human intelligence,’ wrote the authors of an open letter in 2015.¹ ‘[We] cannot predict what we might achieve when this intelligence is magnified by the tools AI may provide, but the eradication of disease and poverty are not unfathomable.’ That letter was signed by more than 150 people, including dozens of senior computer scientists and AI researchers, three founding members of Apple, Google DeepMind and Tesla, and Professor Stuart Russell of Berkeley, the author of the standard textbook for AI undergrads. (The late Stephen Hawking also signed, but AI researchers used to get understandably annoyed when he made the headlines rather than the people who actually do this for a living.) Max Tegmark, a professor of cosmology at the Massachusetts Institute of Technology and director of its Future of Life Institute, writes in his book Life 3.0: Being Human in the Age of Artificial Intelligence of ‘a global utopia free of disease, poverty and crime’ as a possible outcome of the development of a powerful AI. These are real, serious people who believe that, in the reasonably foreseeable future, AI could solve some of humanity’s most pressing problems. But ‘solving our problems’ is actually the least of it. If humanity survives, we have to start looking at some very big numbers.\n\nLet’s imagine the Earth will probably be able to support human life for another billion years or so. (At around that point, the sun will enter a phase in which it is much brighter and hotter than it currently is; it will cause the Earth to enter a runaway greenhouse process as the seas evaporate, and it will become too hot for complex life.²) Let’s imagine that humans continue to live for a century or so each for the next billion years, and that the human population settles at a nice, sustainable 1 billion, less than one-seventh of its current levels. These are the assumptions that Nick Bostrom – author of the aforementioned Superintelligence, and founder of Oxford’s Future of Humanity Institute (FHI) – goes with.³ That would mean that we would have at least 10,000,000,000,000,000 descendants. The total number of Homo sapiens who have ever lived up to now, according to an estimate by the Population Research Bureau, is about 108,000,000,000.⁴ In other words, the entire history of humanity so far represents only about one-ninety-thousandth of what it could be, if we just avoid being wiped out.\n\nBut! We still have only just started to scratch the surface. What if humanity leaves the Earth? Imagine a ‘technologically mature’ civilisation, says Bostrom. One that can build spacecraft that travel at 50 per cent of the speed of light. That civilisation could reach 6,000,000,000,000,000,000 stellar systems, he calculates, before the expansion of the universe puts the rest out of reach. One that could travel at 99 per cent of the speed of light could reach about 15 times as many as that.⁵ Imagine that 10 per cent of those suns have planets that are or could be made habitable, and on average could each sustain 1 billion people for 1 billion years. That would put the number of humans who could exist in the future at around 10³⁵, or 1 followed by 35 zeroes. All the humans who have ever lived would be vastly less than a rounding error, compared to the ones who could follow us, if we get it right.\n\nBut! Yes. It gets bigger. Much bigger.\n\nFirst, we could build our own habitats (an example would be the Orbitals in Iain M. Banks’ Culture novels, thin wheels of matter millions of kilometres in diameter, with humans living on the inside of the rim), out of spare space rocks, so we’re not limited by the number of planets we happen to find. That gets Bostrom up to 10⁴³ potential humans.\n\nAnd then we could think about what happens when we start uploading human minds into computers. Then we are far less limited by space. Humans would need, instead of an appreciable fraction of the surface of a planet, a few square picometers of circuitry. Bostrom throws some plausible-sounding numbers in there about how dense you can make your hardware, how much energy you can get from a given star, and how many computations per second are required to simulate a human mind, and comes up with a lower bound – a conservative, worst-case estimate – of 10⁵⁸ possible human lives of 100 years each.\n\n‘One followed by 58 zeroes’ may sound like a meaningless Big Number to you, and it does to me, but it is extraordinarily vast. ‘If we represent all the happiness experienced during one entire such life with a single teardrop of joy,’ says Bostrom, ‘then the happiness of these souls could fill and refill the Earth’s oceans every second, and keep doing so for a hundred billion billion millennia.’⁶ Does that give a more visceral sense of how enormous it is? I don’t know if it does. If not, just remember that even compared to the ridiculously vast numbers that astronomers throw around from time to time, this is seriously huge.\n\nBostrom might have his numbers wrong, of course. He has done his best to think conservatively, but when putting the numbers together like this he could easily be off by orders of magnitude. But even if you knock off six orders of magnitude and say there’s only a 1 per cent chance of it being correct anyway, then even ‘reducing existential risk by a mere one-billionth of one-billionth of one percentage point’⁷ can be expected to do as much good, in terms of years of life saved, as stopping 100 quintillion actual people from dying. That’s one followed by 20 zeroes.\n\nSo, yes, Bostrom’s maths could be badly wrong. He could have got his figures off by a factor of 10,000, or a million, or 10 billion. And yet, all the good done right now by every charity in the world would still be a drop in an ocean that is itself a drop in a much bigger ocean in comparison to the good that would be done by slightly reducing the chance that humanity gets destroyed before it can take to the stars.\n\nYou might want to reject these numbers out of hand because they’re weird and they give you weird results. That’s not actually a stupid thing to do, according to the Rationalists: there is a thing Yudkowsky came up with, called Pascal’s Mugging,⁸ related to the famous wager which says that if you simply multiply risk by reward, you’re vulnerable to absurd situations. In Bostrom’s rather whimsical version of it,⁹ the example is that a mugger comes along and demands Pascal’s wallet. Pascal points out: ‘You have no weapon.’ ‘Oh good point,’ says the mugger. ‘But how about if you give me the wallet, I come back tomorrow and give you 10 times the value of the money in it?’ ‘Well’, says Pascal, ‘that’s not a very good bet, is it. It’s hugely likely that you’ll just not come back.’\n\nBut the mugger then says: ‘Actually, I’m a wizard from the seventh dimension. I can give you any amount of money you like. I can give you, in fact, any amount of happiness you like. Let’s say that the money in your wallet could buy you one happy day. [Assume for the sake of argument that money can buy happiness.] And let’s say that you think there’s only a 1 in 10¹⁰⁰ chance that I’m telling the truth. Well, in that case, I’ll offer you 10^(1,000) happy days.’\n\nBy a utilitarian calculus – the idea that you should multiply the chance of something happening by the reward it would bring if it does, exactly the sort of reasoning that Bostrom uses to think about the cosmic endowment, or for that matter that investors and gamblers use to determine where to put their money – this is a good bet. If Pascal took it, on average, he’d expect a 10⁹⁹⁰-fold return on his investment. But it is, also, pretty obviously ridiculous. The wizard-mugger can just keep upping the numbers he offers until it becomes a good bet.\n\nSo it’s OK to be wary; you should be, when someone comes up and mouths a lot of maths and numbers and technical talk that you can’t follow but which they say supports their point. The Rationalists have a term for that, in fact: ‘getting Eulered’,¹⁰ blinded by numbers. But that doesn’t mean you should simply dismiss it. If you can’t follow the maths, you should be wary, but you should try to follow the maths. One of the founding principles of the Rationalist movement is that, as Scott Alexander puts it, ‘when math tells you something weird, you at least consider trusting the math. If you’re allowed to just add on as many zeroes as it takes to justify your original intuition, you miss out on the entire movement.’¹¹ A weird-seeming answer is a warning flag, rather than a stop sign: a thing to investigate rather than reject.\n\nAnd, having investigated for a decade or more, the Rationalists are pretty confident in their numbers – which is why they, and the Effective Altruism movement which is closely aligned with them, are concerned about AI risk. They see the reward of surviving the next century or so as potentially enormous, and AI as one of the – if not the – most likely things that will stop us doing that.\n\nPart Two\n\nThe Paperclip Apocalypse\n\nChapter 3\n\n Introducing AI\n\nIn this chapter, we’re going to get together a working definition of what AI actually is, before we discuss the reasons why the Rationalists think AI could go so wrong.\n\nThere are lots of things around right now that are described as ‘AI’. But they’re all what is known as narrow AI. For instance, chess-playing AIs are extremely good at chess, but clueless about everything else. They can’t help you with your tax return or remember to feed your cat. Google Maps is pretty good at working out optimal routes from A to B, and only rarely directs you through the North Sea or whatever, but it doesn’t know a queen’s gambit from the Sicilian defence. Humans, on the other hand, can apply themselves to learning ballroom dancing, or the guitar, or chemistry, or poetry-writing – apparently some other things as well. What the Rationalists are concerned about, broadly speaking, is the development of artificial general intelligence, or AGI: a computer that can do all the mental tasks that we can.\n\nAt this point I really, really want you to put all pictures of The Terminator out of your heads. That’s important. It won’t help. Nothing we discuss will be made clearer by images of grinning metal robots or Skynet achieving self-awareness in August 1997. I’m fairly sure this trope irritates most Rationalists, but I’m damn sure that I’m tired of it myself after a year of conversations in which someone asks me what the book is about, I say ‘artificial intelligence destroying the world’, and they nod sagely and say, ‘Ah, Skynet.’ So. Please. The Terminator: forget it.\n\nBut you might wonder what I do mean when I talk about ‘artificial intelligence’. You might, for instance, reasonably point out that there is a fairly large disagreement about what ‘intelligence’ means, even before you start talking about whether it’s artificial or not. Conveniently for me, the standard textbook of AI, Artificial Intelligence: A Modern Approach (AIAMA), by Stuart Russell and Peter Norvig, tries to answer this question. It divides up possible approaches and definitions by whether they talk about reasoning or about behaviour, and whether they attempt to reason/behave like a human or whether they attempt to reason/behave rationally.¹\n\nA computer that behaves humanly is the sort of thing imagined in the old Turing test. In 1950 the British scientist and code-breaker Alan Turing, apparently bored with debate over whether ‘machines can think’, wrote in a paper in Mind that ‘If the meaning of the words “machine” and “think” is to be found by examining how they are commonly used, it is difficult to escape the conclusion that the meaning of and answer to the question “Can machines think?” is to be sought in a statistical survey such as a Gallup poll.’² Instead he proposed a simpler, more unambiguous test, the ‘imitation game’. A human interviewer holds conversations with two interlocutors, whom he cannot see. One is a human; one is an AI. The interviewer can ask whatever questions he likes; if he cannot reliably tell one from the other, said Turing, then to all intents and purposes we should treat it as a thinking thing.\n\nTuring’s famous test is rightly held up as a great pioneering work, and has the enormous advantage that it sidesteps grimly philosophical questions such as ‘Is it conscious?’ and puts a simple, repeatable test in their place. But, say Russell and Norvig, it hasn’t actually been all that influential in terms of guiding the direction of AI research since then. ‘The quest for “artificial flight” succeeded when the Wright brothers and others stopped imitating birds and started using wind tunnels and learning about aerodynamics,’ they say. ‘Aeronautical engineering texts do not define the goal of their field as making “machines that fly so exactly like pigeons that they can fool even other pigeons”.’³\n\nThinking humanly has received rather more attention, and is in fact the heart of the field of cognitive science, which uses AI models and findings from the brain sciences to build models of human thought. It has, says AIAMA, been instrumental both in creating a more precise understanding of how the human brain works, and in using ideas from neurophysiology to advance AI, especially in image recognition and vision. But that is cognitive science, rather than AI. (Russell and Norvig, deadpan, explain the difference: ‘Real cognitive science … is necessarily based on experimental investigation of actual humans or animals. We will leave that for other books, as we assume the reader has only a computer for experimentation.’) Artificial intelligence, as they envisage it, is about behaving rationally.\n\nFunnily enough, that’s how the Rationalist movement envisages it too. ‘[Definitions] of intelligence used throughout the cognitive sciences converge towards the idea that “Intelligence measures an agent’s ability to achieve goals in a wide range of environments”,’⁴ write Anna Salamon and Luke Muehlhauser in Intelligence Explosion: Evidence and Import, a research paper published by the Machine Intelligence Research Institute. ‘We might call this the “optimisation power” concept of intelligence, for it measures an agent’s power to optimise the world according to its preferences across many domains.’\n\nTo explain what they mean by ‘optimisation’ and ‘behaving rationally’, I’m going to use an analogy with chess, which I’ve lifted from a series of blog posts by Yudkowsky. I’m very bad at chess. But I have a friend, Adam, who is extremely good: a professional chess teacher and an ‘international master’, which is the rung below grandmaster. My proudest achievement in chess is that I once made him think about a move for over a minute. That was about 15 years ago.\n\nIf I play Adam (and I don’t, because it’s dispiriting), I can’t reliably predict what his next move will be. Sometimes I can (his first move is more likely to be ‘move his queen’s pawn two squares forward’ than ‘move the rook’s pawn one square forward’, say), but at any level of complexity beyond the basic, I can’t. If I could predict his next move, I would be as good at chess as he is. ‘If I could predict exactly where my opponent would move on each turn, I would automatically be at least as good a chess player as my opponent,’ as Yudkowsky puts it. ‘I could just ask myself where my opponent would move, if they were in my shoes; and then make the same move myself.’⁵ So I can’t predict what Adam, or any gifted chess player, will do in any given situation; exactly what his next move will be is always going to be something of a mystery, and if he does something I’m not expecting, it probably means that he’s seen something I haven’t and I’m about to get forked or checkmated or some other bad thing.\n\nBut I can make a different kind of prediction – that whatever his next move is, it will be part of a sequence of moves that leads to a board position in which I have lost and Adam has won. Yudkowsky says that when we say ‘Kasparov is a better chess player than [X]’, we mean that we predict that ‘the final chess position will occupy the class of chess positions that are wins for Kasparov, rather than drawn games or wins for [X]’.⁶\n\nYudkowsky points out that this is actually quite an odd situation. ‘Isn’t this a remarkable situation to be in, from a scientific perspective?’ he asks. ‘I can predict the outcome of a process, without being able to predict any of the intermediate steps of the process.’⁷ Apart from in very simple situations, that’s not usually how we predict things: ‘Ordinarily one predicts by imagining the present and then running the visualisation forward in time. If you want a precise model of the Solar System, one that takes into account planetary perturbations, you must start with a model of all major objects and run that model forward in time, step by step.’\n\nThe outcome is predictable, though, because you know 1) what Kasparov’s goal is – to win the chess game; and 2) that he is extremely good at doing so. ‘I know where Kasparov is ultimately trying to steer the future and I anticipate he is powerful enough to get there,’ says Yudkowsky, ‘although I don’t anticipate much about how Kasparov is going to do it.’ So you can define ‘good at chess’ as ‘likely to steer the universe into a situation where you have won chess games’. Kasparov is, in Rationalist jargon, optimising for chess victories, and he is a powerful optimiser, able to steer the universe into Kasparov-has-won-at-chess situations far more often than chance.\n\nAny modern chess program could thrash me as easily as – more easily than – Adam, or for that matter Kasparov, could. The strongest chess program in the world is Stockfish 9. (Or was until recently. It got beaten in December 2017 in a 100-game series against the terrifying autodidact-polymath-algorithm AlphaZero, losing 25 games and winning none,⁸ although Stockfish was playing with some technical handicaps, so there is some controversy about AlphaZero being the best.) Stockfish would destroy Deep Blue, the computer that beat Kasparov; it has an Elo rating of about 3400, compared to Deep Blue’s of about 2900, making it roughly as much better than Deep Blue as Kasparov at his peak was better than my friend Adam. If I played Stockfish, I absolutely would not be able to predict its individual moves – I’d be less successful at it than I would be at predicting Adam’s or Kasparov’s, in fact. But I’d be more successful at predicting the final state of the board, which would be one in which I have lost.\n\nWe can, therefore, use the exact same definition of ‘good at chess’ about Stockfish as we did about Kasparov. We don’t need to worry about whether Stockfish thinks in the same way as Kasparov, or whether Stockfish is conscious, or anything else. There’s a lovely, simple, easy way to test whether it is good at chess – we see how many chess games it wins. Things that win more chess games are better at chess, and it doesn’t matter whether that thing is a human or a dog or a laptop or an algorithm.\n\nThis, pretty much, is what the ‘acting rationally’ definition of intelligence is. ‘A rational agent is one that acts so as to achieve the best outcome or, when there is uncertainty, the best expected outcome,’⁹ say Russell and Norvig. The ‘best outcome’, of course, depends on what goals the agent has – my goals, and therefore my ‘best outcome’, are likely to be different in some respects from your goals; your goals may be selfishly attaining material riches, while mine are the noble pursuit of knowledge and the betterment of mankind, etc. But an agent is rational insofar as it is good at achieving whatever goals it has.\n\nThis has the advantage, say Russell and Norvig, of being ‘mathematically well defined and completely general’. And again, importantly, we don’t care at all about how a given agent achieves rationality. An AI that carefully mimics the human brain, to the point of having simulations of individual neurons, could be rational; an AI that runs entirely along the lines of a Turing machine, or Charles Babbage’s Difference Engine, metal gears and all, could be rational too. The mathematically defined concept of ‘rationality’ does not care what engine is used to run it. And, again, it doesn’t care whether or not your AI is conscious, or has emotions, or knows what love is. It’s purely a question of whether it achieves its goals, whatever they are. You can punt the ‘can a machine think?’ questions back to the philosophers, and get on with building something that does what you want it to do.\n\nThere are a few other AI-related terms that are worth clarifying at this point. One is ‘human-level machine intelligence’, or ‘human-level AI’. Bostrom defines an HLMI as ‘one that can carry out most human professions at least as well as a typical human’.¹⁰ It’s roughly synonymous with AGI, but a bit more specific; presumably a general intelligence could be more, or somewhat less, intelligent than a human. It’s also worth noting that HLMI is a really tricky thing to achieve. ‘Most human professions’ would presumably include jobs such as those of lawyer, doctor, artist, journalist, cognitive behavioural therapist: jobs with skills that, at the moment at least, are enormously hard to recreate in computers, such as verbal fluency and emotional intelligence. A truly human-level AI wouldn’t just be good at things that feel computery to us, like estimating probabilities or playing Go. It would be as good as we are at conversations; it would know as well as the average human when to make a self-deprecating joke or offer sympathy. For all the amazing breakthroughs in AI in the last few years, that does not feel especially close.\n\nI’ll tend to stick to AGI in this book, when I remember, because HLMI is clunky and also kind of confusing. Rob Bensinger prefers AGI too: ‘I think human-level is more deceiving because it suggests it’s going to be human-like,’ he told me.\n\nThe next term is ‘superintelligence’, which Bostrom defines as: ‘an intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills’.¹¹ That could be an AI, or a genetically engineered superhuman, or an uploaded human mind working at 10,000 times normal speed, or whatever. (But it couldn’t be a corporation, or the scientific community, or capitalism, etc.: ‘Although they can perform a number of tasks of which no individual human is capable, they are not intellects and there are many fields in which they perform much worse than a human brain – for example, you can’t have real-time conversation with “the scientific community”.’)\n\nAlso, I’ll just specify that in this discussion of AI, I won’t be talking about ‘whole-brain emulations’, a route to machine intelligence that involves scanning a human brain at some low level – cell by cell, probably – and uploading it into a computer. That process is important, and a key figure in the Rationalist community, Robin Hanson, has written a thoroughly interesting and mildly terrifying book called The Age of Em about what a future in which we can upload ourselves might look like. Hanson, whom you’ll remember is an economist at George Mason University in Virginia, applies what he says are standard economic theories to what he says are a few realistic assumptions, and ends up with a world in which uploaded human minds are copied and deleted by their millions every day in an economy that doubles in size every few hours. The Age of Em lasts for subjective millennia, but because it’s running thousands of times faster than human consciousness, in the objective universe it’s all over in a couple of years. It’s a thrilling bit of futurology and well worth your time, but it’s not what the AI safety/Rationalist movement is generally talking about when they refer to the risks: they are worried about aligning artificial intelligence with human values, and it seems fairly likely that an uploaded version of a human brain would share human values.\n\nChapter 4\n\n A history of AI\n\nIn the years after the Second World War, there was enormous excitement about what these new thinking machines could do. In 1956 a small group of scientists gathered at Dartmouth College, the Ivy League university in New Hampshire. They were there to look into how machines can be made to learn.\\* Alan Turing had just kick-started the whole field; both practically, with the machines he built in the war to decipher German communications, and theoretically, by coming up with mathematical proofs showing what these machines could do – that, as he put it, it was ‘possible to invent a single machine which can be used to compute any computable sequence’.¹\n\nSo the Dartmouth Ten were wildly optimistic about what they could achieve. They had written to the Rockefeller Foundation to apply for funding, saying in their proposal: ‘We propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College … on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.’²\n\nThe degree of their optimism was made particularly obvious when they said that the ‘speeds and memory capacities of present computers may be insufficient’ for the task of simulating human learning, but that ‘the major obstacle is not lack of machine capacity, but our inability to write programs taking full advantage of what we have’. Bear in mind that an iPhone 6 can perform calculations about 100,000 times faster than the IBM 7030, a multi-million-dollar supercomputer of the era.\n\nBut while the Dartmouth researchers may have been overexcited, their summer project kicked off a period of very real progress, of finding things that people said ‘no machine could ever do’ and then making a machine do them. One solved logic puzzles. One proved a load of theorems from Alfred North Whitehead and Bertrand Russell’s Principia Mathematica. The famous ELIZA spoke in a sort of natural language, albeit by essentially turning its interlocutor’s statements into questions; SHRDLU obeyed simple instructions in English.\n\nNine years after Dartmouth, I.J. Good, who’d been one of Turing’s team of code-breakers at Bletchley Park, saw an early glimpse of the future that the Rationalists hope for and fear now. If mankind builds a machine that can ‘surpass all the intellectual activities of any man’, Good wrote, and ‘since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion”, and the intelligence of man would be left far behind. Thus the ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.’³\n\nThe burst of optimism lasted about 20 years, before the problems facing the AI pioneers became obvious. Some of the problems you can guess – they were now in the 1970s, and the world’s most powerful computers were still only running at a tiny fraction of the speed of the thing in your pocket that you bought on a £25-a-month contract from Vodafone. But others were more fundamental – most notably the idea of the ‘combinatorial explosion’.\n\nMost people probably imagine that computers are good at chess because they can simply look ahead and see all the moves that you can do – a process called ‘brute-force’ computing. But it doesn’t work except in the most basic way. In chess, there are on average about 35 possible moves each go. To plan ahead two moves, the computer has to look at 35 times 35 moves – 1,225 options. That’s not so bad, but to look ahead three it would need to do 42,875. To look ahead five, 52 million. To look ahead 10 moves, it would be nearly 3 quadrillion. If you had a computer capable of looking at a billion possible sequences a second, and asked it to look at all the possibilities 20 moves ahead, it would take it, by my calculation, about 200 trillion years. That’s a problem. And reality is more complicated than chess. (For one thing, it contains chess.)\n\nBy the 1970s all these problems of AI were beginning to reveal themselves – that it was about teaching the machines to narrow the search, to control the combinatorial explosion by finding and recognising patterns in the vast swarming array of possible futures. The trouble was that artificial intelligence had to be intelligent, not merely powerful. So suddenly it didn’t look like we were about to build a super-smart robot in the next few years, and AI dropped out of fashion. Funds were cut, the press got sniffy, and serious research took a back seat.\n\nThis ‘AI winter’ came to an end in the early 1980s, with some breakthroughs by Japanese companies. Then there was another one, beginning around 1987, after another period of unsustainable excitement and inevitable disappointment; again, funding dried up. That winter thawed in the 1990s, as researchers started to focus on things like neural nets – systems that could learn from experience, and which didn’t immediately break and start churning out nonsense if there was a slight mistake in the input. It was at this time that AIs started to get better than the best humans at things that humans were quite proud of being good at – specifically, games.\n\nA program called Chinook beat the reigning world draughts/checkers champion in 1994 to win the world championship. In 1997, the program Logistello beat the world Othello champion six games to love. And, most famously, Deep Blue – named, in a tangential way, after Deep Thought, the world-designing computer in Douglas Adams’ Hitchhiker’s Guide to the Galaxy – beat Garry Kasparov three and a half games to two and a half, again in 1997. Kasparov, the reigning world champion, claimed to have seen an intelligence and creativity in his opponent’s moves; AI was suddenly sexy, and scary, again. (Charles Krauthammer, a conservative US newspaper columnist, told his readers to be ‘very afraid’.⁴) The view, in the popular press at least, was that the artificial intelligence bandwagon had started rolling in earnest, and soon things would happen.\n\nThe fact that, 20 years later, we haven’t got robot butlers could be seen as another failure of optimism. But that’s probably best explained in one sentence, from the computer pioneer John McCarthy: ‘As soon as it works, no one calls it AI any more.’⁵\n\nIt’s worth remembering that for a long time people thought that chess itself was too deep and complex a game to master without essentially recreating human intelligence in toto: ‘if one could devise a successful chess machine, one would seem to have penetrated to the core of human intellectual endeavour,’⁶ said the authors of one influential chess paper in 1958. Now your laptop could run any one of several programs that could defeat any human in the world; in 2009 Pocket Fritz 4, a program running on a mobile phone, reached grandmaster level.⁷ Now no one thinks that if you’ve solved chess, you’ve solved thought. Similar breakthroughs have happened with the once comparably intractable problems of image and facial recognition, and language recognition – passport-checking is carried out by facial-recognition software; Siri and Alexa are quite capable of obeying simple voice commands; there are powerful translation tools online, running on algorithms. These were all enormous challenges for AI; they’ve been met.\n\nThe spam filters that keep a large percentage of the Viagra ads and phishing scams out of your inbox run on AI. AI monitors your credit cards for suspicious activity. AI buys and sells billions of pounds’ worth of stock every second on the FTSE and NASDAQ. I translated the Goethe poem quoted at the beginning of this book with the AI-powered Google Translate tool. And when you type something into a search bar, and the most relevant things come back to you in a fraction of a second, that is the work of AI too. Bostrom says in Superintelligence that ‘The Google search engine is, arguably, the greatest AI system that has ever been built.’\n\nAI may be all around us, but still, when most of us think of artificial intelligence, we don’t think about an automated customs process or our phones understanding the phrase ‘Hey Siri, play podcast’. The question we actually want answered is how long we have until machines are as clever as we are. And the answer is we don’t know.\n\nThat said, perhaps we can make an educated guess.\n\n\\*This little history is largely taken from Nick Bostrom’s Superintelligence and from Russell and Norvig’s Artificial Intelligence: A Modern Approach. I am enormously grateful to both; any errors are mine.\n\nChapter 5\n\n When will it happen?\n\n‘Forecasting’, says Rob Bensinger, ‘is incredibly difficult.’ It’s hard to know when an AI will achieve something like human intelligence. ‘There won’t be any alarm bells,’ he says. ‘There might be a lot of cool things that happen, but there’s never going to be an unambiguous signal that makes everyone working in the field go, “Oh OK, AGI is five years away.”’\n\nThe lack of an alarm bell is a problem, because although most of us – including an overwhelming majority of AI researchers – don’t think that AGI is close at the moment, it may also be the case that we don’t think it’s close the day before DeepMind or whoever announces that they’ve built one (or, more unnervingly, doesn’t announce it). ‘You shouldn’t really be confident about how soon it’ll be or how far off it is’, says Rob, ‘until you know in great detail exactly what the hard part is in building one. And that probably means that we’ll be building one really soon, so until we’re on the threshold we’re probably going to be in a similar state of uncertainty about how far off it is.’\n\nRob isn’t the only person to think this. Eliezer Yudkowsky wrote something, around the time I was in California, saying much the same thing, and to point out a worrying corollary. He uses the metaphor of an alarm as well – specifically, a fire alarm.¹ He refers to a classic experiment in which students are asked to fill out a questionnaire, individually but in the same room as others. Smoke starts coming into the room under the door. Most of them ‘didn’t react or report the smoke, even as it became dense enough to make them start coughing’. A student on their own would, most of the time. But ‘a student accompanied by two actors told to feign apathy will respond only 10 per cent of the time’.\n\nBut when there’s a fire alarm, everyone troops dutifully out of the fire escape, muttering about what a waste of time it is and wondering whether they can sneak off to the pub for a bit. What’s going on, says Yudkowsky, is that fire alarms create common knowledge: they tell everyone that it’s OK to act as though you believe that there’s a fire. With no alarm, he says, we ‘don’t want to look panicky by being afraid of what isn’t an emergency, so we try to look calm while glancing out of the corners of our eyes to see how others are reacting, but of course they are also trying to look calm’; a fire alarm tells you that it is ‘socially safe’ to react, that ‘you know you won’t lose face if you proceed to exit the building’.\n\nWith AGI, he says, there may be smoke – there may already be smoke – but there won’t be an alarm. Things will look much the same, to most people, a few days before a colossal breakthrough as they do now. He has historical precedents for this. Wilbur Wright told his brother Orville in 1901 that powered flight was still 50 years away.² Two years later, the pair of them built the first working aeroplane. And Enrico Fermi, who in 1942 was in charge of the first fissile chain reaction, had said in 1939 that he was 90 per cent sure that such a thing was impossible.³ These were people at the absolute cutting edge of their fields. Presumably a few months before they successfully achieved what they were trying to do, they had changed their minds. But most people, even other people in the field, would have thought it was impossible, or years away, says Yudkowsky. ‘Nobody knows how long the road is,’ he told me via Skype from his California home in 2016, ‘but we’re pretty sure there’s a long way left.’\n\nThat said, recent developments are a powerful reminder that it might not be all that long. In March 2016, AlphaGo, a program designed by the Google subsidiary DeepMind, beat Lee Sedol, the world Go champion, 4–1 over a five-game series, with Sedol’s only win coming when the series was already lost. Go, an ancient Chinese board game of enormous complexity and subtlety, was the big one, Yudkowsky and other AI scientists told me. ‘The technology [of AlphaGo] is very different from that used to solve chess,’ he says. Deep Blue was a special-purpose machine, its hardware and software tweaked by its human creators to optimise its performance. AlphaGo was, essentially, just taught how to learn, then played against itself millions upon millions of times. ‘From the outside, it looks like the people who made AlphaGo don’t know how it works. They have an idea of the broad structure, but the thing has taught itself to play Go.’\n\nAlphaGo is a sign of how far AI has come in the last few years. The general consensus seems to have been that Go wouldn’t be solved for 10 years. George van den Driessche, one of AlphaGo’s researchers, told me at the time that even the team were surprised. ‘We went very quickly from “Let’s see how well this works” to “We seem to have a very strong player on our hands”, to “This player has become so strong that probably only a world champion can find its limits”.’ The even more recent advent of ‘AlphaGo Zero’, which grew vastly better at Go than the original AlphaGo without ever seeing a real game of Go, and then, using largely the same algorithm but renamed ‘AlphaZero’, became enormously superhuman in chess in four hours, is an interesting example of how AI is becoming more general.\n\nSo when people say, ‘General AI is still a long way away’, remember that the greatest experts in the field have wildly overestimated these timelines before – and wildly underestimated them too, as at Dartmouth. Essentially, they’ve been wrong in all the ways they could have been wrong.\n\nNonetheless, experts in the field are the people most likely to have a good handle on when it’ll happen, and Bostrom and his colleague Vincent Muller surveyed AI researchers for their estimates as to how long it’ll be before there’s human-level machine intelligence (HLMI). The median estimates are that there’s a 10 per cent chance that we’ll reach HLMI by 2022; a 50 per cent chance by 2040; and a 90 per cent chance by 2075.⁴ Bostrom warns in his book to take these numbers ‘with some grains of salt’, because the survey sizes were relatively small and not necessarily representative. But, he says, it’s in line with other surveys – a more recent survey, published in 2017, put the median estimate at 50 per cent likely by 2061⁵ – and with things AI experts have said. (The AI scientist David McAllester writes that the great pioneer of the field, John McCarthy, when asked when he thought HLMI would be achieved, said: ‘between five and five hundred years from now’. ‘McCarthy was a smart man,’ muses McAllester.⁶)\n\nYudkowsky, too, is on record as predicting that HLMI is more likely sooner rather than later: in 2011 he said on a podcast, ‘I would be quite surprised to hear that a hundred years later AI had still not been invented, and indeed I would be a bit surprised … to hear that AI had still not been invented 50 years from now.’⁷ I asked him if that was still his position, and he told me: ‘If Omega [an all-knowing alien AI, and a staple of Rationalist thought experiments] told me for a fact that AGI had not been invented by 2061, I would first imagine that some civilisational collapse or great difficulty had hindered research in general, not that the AGI problem was naturally that hard.’\n\nMurray Shanahan of DeepMind and Imperial College London told me that he thought roughly the same thing as the respondents in the survey: that HLMI was pretty unlikely in the next 10 years, but could happen by mid-century, and is pretty likely by 2100. ‘This is not just a fantasy,’ he says. ‘We’re talking about something that might actually affect our children, if not ourselves.’ Toby Walsh of the University of New South Wales, who is more sceptical of the possibility of superintelligence, reasons why we shouldn’t trust Bostrom’s survey, and points to another survey which suggests that AI researchers are less bullish – but he still ends up saying in his book Android Dreams that ‘if [experts] are to be believed, we are perhaps 50–100 years away from building a superintelligence’.⁸ Even the sceptics don’t seem that sceptical.\n\nThe consensus expert opinion, then, appears to be that it is certainly plausible, and possibly likely, that people reading this could well live to see a machine that is as smart as a human. That ‘consensus’, of course, represents an average of some highly disparate predictions. Some are sure it’ll happen by 2040; some are equally confident that it won’t happen at all. Bostrom, for what it’s worth, thinks that people have underestimated the chance of it taking a long time or never happening, and that the figure of 90 per cent chance by 2075 is too high.\n\nSo it could come in the next 20 or so years, or the next 100, or not at all. But the question you might be asking is: so what? What is it about AI that makes people so concerned?\n\nChapter 6\n\n Existential risk\n\n Moore’s law of mad science: every eighteen months, the minimum IQ necessary to destroy the world drops by one point.¹\n\n Eliezer Yudkowsky\n\nThere are various things that could destroy the human race and prevent it from getting to the Glorious Cosmic Endowment Future. But the LessWrong diaspora and its fellow travellers think AI is one of the – perhaps the – most likely.\n\nAll the Rationalists I interviewed seemed to acknowledge that there is, for instance, a real risk that climate change could be pretty devastating over the next few centuries. But they didn’t feel it was likely to wipe out humanity altogether.\n\nI met with Dr Toby Ord, one of Nick Bostrom’s colleagues at Oxford’s FHI. Toby is a likeable Australian who, in contrast to a lot of the people I spoke to for this book, actually laughed politely at my terrible trying-to-break-the-ice jokes, rather than let them fall deadweight with a thump like an old book hitting the floor in a silent library, so I immediately warmed to him. He’s the founder of Giving What We Can, a charity which evaluates other charities to determine which are going to do the most good with donors’ money, and which encourages members to take a pledge to give 10 per cent of their income to those most effective charities.² He’s writing a book about existential risk. I asked him which existential risks are probably the biggest. It’s hard to say, he said. ‘The problem is that these numbers don’t come out of some rigorous process. With asteroids, say, you can show how often big asteroids hit the Earth by looking at the record of asteroids. But for most of the other things, it’s much harder to come up with a number, and much more subjective.’\n\nFor the record, the per-century risk of civilisation being destroyed by an asteroid is low. A one-kilometre-wide-or-greater asteroid hits the Earth about once every 100,000 years, representing a 0.1 per cent chance per century; that probably wouldn’t kill us all, but it would do some pretty terrible damage. A really big one, 10 kilometres across or more, hits about every 50 million years; the asteroid that crashed into the Yucatan peninsula 65 million years ago, killing all dinosaurs except the ancestors of birds, was probably 10 kilometres or so in diameter. Another one of those would have a good chance of killing every human. Once every 50 million years translates to a per-century risk of one in 500,000. That’s not nothing – I think it’s fair to say that you’re more likely to die in an asteroid strike, whether civilisation-ending or merely devastating, than you are in a plane crash – but it’s not keeping me awake at night.\n\nThere are other ways in which we could be destroyed without having to do it ourselves. A supervolcano is one example; there’s a chance that the Yellowstone region will explode spectacularly at some point and pump so much soot into the atmosphere that it will get dark and cold, and plants won’t be able to photosynthesise, and we’ll all die. Or a nearby star could go supernova, or a more distant one could direct a burst of gamma rays in our direction. Or some horrible new virus could emerge and wipe out the species.\n\nBut we can fairly safely guess that none of these things is all that likely, for the simple reason that in the 200,000 or so years that modern humans have existed, it hasn’t happened so far. A 2014 report for the British government to which Ord contributed pointed out that, if there was a 1 in 500 chance of us being wiped out in any given century, then there’d have been less than a 2 per cent chance of us surviving this far.³ If you take earlier human ancestors into account, Homo erectus survived for about 1.8 million years; even a 1 in 5,000 chance of being wiped out per century would make it vanishingly improbable that they’d have lived that long. Ord (and Bostrom, and, really, basic maths) suggests that it’s unlikely that any naturally occurring catastrophe will kill us in any given century.\n\nThere’s not a great risk, then, that we’ll just be destroyed by an indifferent universe. But there does seem to be a decent chance of us destroying ourselves.\n\nThe obvious way would be nuclear warfare. It’s only been 80 years or so since humanity developed weapons that could realistically destroy civilisation, but there are enough nukes now to irradiate a good chunk of all the land surface on Earth. A 2014 paper suggested that just 100 small nuclear weapons being detonated in a regional war – say between Pakistan and India – could potentially trigger a global famine by hurling black carbon into the atmosphere and reducing growth seasons for plants by up to a month a year for five years.⁴ There are about 9,000 active nuclear warheads in the world, and another 6,000 or so awaiting dismantlement. That’s a lot lower than the peak number of roughly 65,000 in the late 1980s, but still easily enough to cause a spectacular nuclear winter, and possibly end human life.\n\nAnd we have come astonishingly close to disaster. One of the Rationalist community’s heroes is a former Soviet Air Defence Force officer, Stanislav Petrov, whom I mentioned earlier and who died in 2017. On 23 September 1983 Lieutenant-Colonel Petrov was on duty as the watch officer of the USSR’s missile early-warning system, at a time of enormous geopolitical tension: three weeks earlier, a US congressman had died in a Korean airliner shot down by a Soviet interceptor, and both sides in the Cold War had recently deployed nuclear weapons in threatening positions.\n\nShortly after midnight, a warning light appeared on a computer in the Moscow bunker in which Petrov was on duty, warning that a satellite had spotted an intercontinental ballistic missile launched from the United States and heading for Russia. Shortly afterwards, another four were apparently seen. Petrov’s orders were to immediately contact a superior in the event of a warning; there is a strong possibility that, had he done so, nuclear war would have begun, because Soviet protocol at the time was immediate, full-scale retaliation.\\* Petrov did not contact his superiors. He thought that a launch of just five missiles would be improbable, since the US would be more likely to attack at full strength. It later turned out that the satellite had been confused by sunlight glinting off high clouds.\n\nThere were a few other flashpoints like this during the Cold War – Vasili Arkhipov, second-in-command of a submarine during the Cuban Missile Crisis, vetoed his superior’s call to use a nuclear torpedo against a US ship,⁵ which would probably have triggered thermonuclear war. A US spy plane was shot down over Cuba during the same period, an act which the US had previously decided would trigger an automatic invasion of the island; President John F. Kennedy decided not to invade, a decision which, again, probably avoided a nuclear exchange. There is an unnerving Wikipedia page titled ‘List of nuclear close calls’⁶ which is exactly what it sounds like: its 11 such incidents between 1956 and 2010 aren’t all equally close, but any of them could have led to some number of nuclear blasts. Quite a lot of them begin with phrases like ‘A computer error at NORAD …’\n\nBut, says Ord, we are probably past the greatest danger. ‘We’ve managed to get through a much riskier period than we have now,’ he said. He thinks the chance of an all-out nuclear war this century is probably no more than one in 20. ‘But even if there is one, it’s not clear what the chance of extinction would be. There’s the nuclear winter theory, but it certainly doesn’t say there’s a 100 per cent chance we’d go extinct. It seems to me that it’s maybe possible, but there haven’t been to my knowledge any real papers analysing it.’ When you multiply the risk of there being a war by the risk that any such war would destroy humanity or permanently ruin its ability to recover, ‘the risk doesn’t seem that high to me,’ says Ord. ‘Overall I’d think less than 1 per cent over the century.’\n\nClimate change is the other one, but again, while it’s going to have awful effects for a lot of people, it’s probably not an existential risk. ‘There aren’t many papers on climate change actually talking about extinction,’ says Ord. ‘But it’s quite hard for it to happen without a very large number of extra degrees of warming beyond the type of range that’s normally looked at. But it could be that the models are wrong and it’s going to warm a lot more.’ He points out that, if you’re a sceptic who doesn’t trust climate models, this means you should be more concerned about extreme effects than if you think the models are broadly trustworthy, because the higher uncertainty means there’s a greater chance of severe warming outside the scope of the models. But if the models are accurate, then really devastating, Venus-style greenhouse effects are extremely unlikely.\n\n‘Also,’ he says, ‘if you look at the history of the Earth’s climate, there have been times when it’s been a lot warmer and things were very different. One would expect a whole lot of extinctions and so on, and for it to be very bad for humans – don’t get me wrong on this – but it’s more gradual, which helps.’ He also points out that if things do start to look really bad, and ‘if we are literally and slowly threatened with extinction from climate change, then all of our efforts will be devoted to that. It won’t be like the current situation, where we’re unwilling to give 10 per cent of our GDP to deal with the problem. We’d spend like half on it.’ Ideas that seem crazy now, like geoengineering our planet or settling Mars, would become serious options. ‘It seems very unlikely to me that it’s going to be an extinction,’ he says.\n\nOrd, and Bostrom, both think that biotechnology – some genetically engineered virus – is a very realistic route to human extinction. ‘Synthetic biotech would be another source,’ says Bostrom, although he points out that it all depends very heavily on how you define and delineate ‘a risk’. ‘I’d say it’s my number-two disaster,’ says Ord. ‘The advanced genetic-engineering technologies which are becoming possible. That seems to me to be the second most worrying thing, and it’s something that people in our community have put a lot less time into dealing with. There are people working on bio risk, but it’s more focused on situations where there are thousands of people dying, things like that. But not the very worst end of bio risk. There’s very little money being spent on the very worst extremes which could lead to billions of people dying or more.’ Holden Karnofsky, the founder of OpenPhil, agrees: ‘I go back and forth about what is the biggest risk,’ he says, but a genetically engineered pandemic is definitely one of his top two. OpenPhil has given more than $35,000,000 at the time of writing in grants to support ‘biosecurity and pandemic preparedness’.\n\n‘It’s definitely hard to drive humans extinct,’ says Karnofsky. ‘There are a lot of us. But the thing I worry about with pandemics is that as biology advances, the kind of things that a lone psychopath can do …’ He tails off, understandably wary of giving specific ideas to any lone psychopaths who might be reading. It’s at this point that I remember Yudkowsky’s half-joking ‘Moore’s law of mad science’ in a 2008 paper about existential risk: ‘Every eighteen months, the minimum IQ necessary to destroy the world drops by one point.’⁷\n\nBut this book isn’t about the risks of biotech, and it’s not what these communities, the Effective Altruism movement, the Rationalists etc. are famous for worrying about. The headline-grabber, the risk that everyone talks about, and according to all of them either the biggest or the joint biggest – Ord’s number one, Karnofsky’s other one he ‘goes back and forth’ about, Bostrom’s ‘depends-how-you-define-a-risk-but-I-wouldn’t-disagree’ – is AI. Specifically, artificial intelligence that is smarter than we are.\n\n\\*Since this is a book about people who seek the truth, I should acknowledge that there’s some dispute about this. More than 22 years later, as Petrov was being given an award for ‘saving the world’ by the Association of World Citizens, the Russian ambassador to the UN said that for a retaliatory launch ‘confirmation is necessary from several systems: ground-based radars, early warning satellites, intelligence reports, etc.’\n\nChapter 7\n\n The cryptographic rocket probe, and why you have to get it right first time\n\nIt may or may not be that superintelligent AI is relatively near. But the claim that the Rationalists make is not just that it may be imminent, but that when it arrives it could be catastrophic – human-life-ending. It might not, though, be obvious why that is. Just because something is amazingly clever, why should it be dangerous? It’s not as if the most intelligent humans in the world suddenly take power and destroy everything.\n\nThere are, however, some specific reasons to be worried. Paul Crowley, the Rationalist who introduced me to all this stuff, works in cryptography at Google. He told me that efforts to make AI safe are like a combination of cryptography and launching a space probe. The cryptography parallels are fairly obvious. ‘There’s a mindset that is comparable. In both cryptography and AI alignment, the mindset is of looking for what is wrong with the code. In both systems, for different but related reasons, it will tear apart at the tiniest crack.’ The mindset is that you always have to be asking how will this fail?\n\n‘There are a lot of things you can build, in computers, where if there’s a flaw or a crack, there’s no one there to pick up the flaw,’ he said. ‘So it’ll be fine. You have some algorithm that’s supposed to calculate something, and it gets it wrong one time in a million, that’s fine. Very often, you don’t care about that. But with cryptography, the tiny flaws are under stress. They’re pulled apart. There are adversaries deliberately looking for the flaw.’ The thing about intelligent agents is that they are good at searching a large space of possibilities and finding the bit they want. That is, in fact, pretty much the definition of intelligence that AI theory uses. So that one-in-a-million chance of failure becomes much greater.\n\nPaul took this idea from a talk¹ that Nate Soares, MIRI’s executive director, gave at Google. ‘Suppose you have a dozen different vulnerabilities in your code,’ Soares said, ‘none of which is fatal or even really problematic in ordinary settings. Security is difficult because you need to account for intelligent attackers who might find all 12 vulnerabilities and [use] them to break into, or just break, your system. Failure modes that would never arise by accident can be sought out and exploited; weird and extreme contexts can be instantiated by an attacker to cause your code to follow some crazy path that you never considered.’\n\nThere’s a similar sort of problem with superintelligent AI, except in a way it’s starker. It’s not that there’s some adversary whom you need to keep out. It’s that if you end up with an adversary, you’ve already lost.\n\nRob Bensinger of MIRI told me the same thing. ‘You’re not trying to outsmart an adversary in the way that you are in cryptography. You’re kind of doomed if you’re trying to outsmart a sufficiently smart adversary. If you have an AI in a box, and that AI is an adversary and you have to find a way to outsmart it, you’re already screwed.’ So you’re trying to build something that wants to help you. ‘The goal of AI safety is that instead of building an adversary, you end up building a friend.’\n\nThat’s the cryptography parallel. It’s like a space probe for two reasons. One, said Paul, is that you’re dealing with energies vastly greater than you’re used to. ‘Your natural idea of how much energy something has is just not the right fit at all.’ Again, Soares made this point. AI alignment, he said in his Google talk, is difficult ‘for the same reason that rocket-engineering is more difficult than airplane-engineering. At a glance someone might say, “Why would rocket engineering be fundamentally harder than airplane engineering? It’s all just material science and aerodynamics.” In spite of this, empirically, the proportion of rockets that explode is far higher than the proportion of airplanes that crash.’ That’s, again, because of the vastly greater energies involved in rocket launches. A tiny component failing slightly can lead to utter destruction much more easily in a rocket than an aeroplane.\n\nThe things that can go wrong in ordinary programming – in contemporary AI, or contemporary cryptography – can also go wrong in the first general, human-level AI. But the ways in which they can go wrong are likely to be more dramatic, and to have more spectacular and dangerous effects, than the equivalent failure in a less competent system.\n\nThe other comparison with a space probe is that we’ll probably only get one shot at it. In 2017, I wrote a piece about the death of the extraordinary Cassini probe,² which had been orbiting Saturn and its moons for a decade (and flying through space to get there for another decade before that). When it launched in 1997, it did so using software and hardware that had been designed in 1993 and was already tried and tested then. By the time it reached the end of its life, it was using 30-year-old technology; its hard disk had less space than a USB stick you could buy for £2.50 at Argos. It had undergone a few patches, but nothing significant, because it didn’t have the bandwidth or the disk space for major upgrades. But it worked, for decades, because NASA engineers had very carefully looked at all the ways in which it could fail.\n\nThat, however, has not been the case for every probe. In 1962 the Venus probe Mariner 1 had to be destroyed less than five minutes after launch because a missing character in its punch-card program caused its guidance system to malfunction. In 1988 another missing character in the Soviet Phobos 1 Mars probe’s software shut down its attitude thrusters and meant that it couldn’t recharge its batteries by orienting its solar panels to the sun. The Mars Climate Orbiter disintegrated in 1999 when the software on board was expecting metric units but was sent instructions in imperial, causing it to orbit too low. This is a far from an exhaustive list.\n\nYou can test your software as many times as you like, but none of the tests will be quite the same as just launching the thing, and if you don’t get it exactly right then it might all blow up. ‘It’, in the case of a rocket launch, is the rocket. ‘It’, in the case of the first AGI, may be everything. Or, as Soares puts it, chillingly for software engineers: ‘If nothing yet has struck fear into your heart, I suggest meditating on the fact that the future of our civilisation may well depend on our ability to write code that works correctly on the first deploy.’\n\nIt may not be immediately obvious why you have to get it right first time; in the next chapter we’ll look at a few of the reasons that the Rationalist/AI safety movement has pointed out.\n\nChapter 8\n\n Paperclips and Mickey Mouse\n\nThe nightmare scenario is that we are all destroyed and turned into paperclips. This sounds like I’m joking, but I’m not, exactly. The classic example of an AI that has gone terribly wrong – a ‘misaligned’ or ‘unfriendly’ AI, in Rationalist terms – is a thought experiment that Nick Bostrom wrote about in 2003 (probably following an original idea by Eliezer Yudkowsky): the paperclip maximiser.¹\n\nImagine a human-level AI has been given an apparently harmless instruction: to make paperclips. What might it do? Well, it might start out by simply making paperclips. It could build a small pressing machine and churn out a few dozen paperclips a minute. But it’s bright enough to know that it could be more efficient than that, and if it wants to maximise the number of paperclips it can make, it’s probably better not to go straight for a small press. It could instead use its materials to build a larger factory, so that it’s making thousands of paperclips a minute. Still, though, if it really wants to make as many paperclips as possible, it might want to improve its ability to think about how to do so, so it might want to spend some of its resources building new processors, improving its own code, upgrading its RAM and so on.\n\nYou can see where this is going, presumably. The end point of the paperclip maximiser is a solar system in which every single atom has been turned into either paperclips, paperclip-manufacturing machines, computers that think about how best to manufacture paperclips, or self-replicating space probes that are hurtling out towards Proxima Centauri at a respectable fraction of the speed of light with instructions to set up a franchise there. This isn’t what you meant, back when you said, ‘Go and make paperclips’ to your apparently docile AI, but it’s what you said.\n\nThis has, to some extent, entered the public consciousness, mainly through the medium of an extraordinarily viral online clicker game that was played by tens of millions of people in 2017, Universal Paperclips.² In it, you are an AI whose job is to make paperclips. You start out by repeatedly clicking the ‘make paperclip’ button, but the process becomes more automated and efficient and eventually (spoiler alert) your drones are exploring the observable universe for matter to turn into yet another septillion clips. Things (another spoiler alert, although you should probably have worked this out) turn out badly for humanity relatively early on in the course of the game.\n\nIt’s actually a really good insight into the concepts behind AI alignment, because as the player you are incentivised solely to care about your ‘number of paperclips’ score. There are other things to care about – how much the humans (while they still exist) trust you and are willing to invest resources in you; your processing power; your manufacturing capabilities; your ability to defend yourself against anything that might stop you making paperclips, etc. – but they’re all secondary goals, incidental to your main one. If you can run up your paperclip score without doing them, you will, and so, goes the theory, would a real AI.\n\nI would recommend that you go and play Universal Paperclips immediately, but I won’t, because it is punishingly addictive and you won’t be able to stop. I lost a full day of work to it at BuzzFeed and the only reason I was not told off for it was that almost everybody else in the office did too. (An important tip: if you open it in a separate browser window, rather than just a tab, it’ll run in the background so you can carry on paperclip production while you check your emails or whatever.)\n\nThe point of the paperclip maximiser is not that we are, really, going to be destroyed and turned into paperclips. Bostrom’s idea was to use something self-evidently silly to illustrate that AIs will not necessarily care about what we care about – they will only care about what we program them to care about. That deliberate silliness divides opinion: one AI researcher I spoke to thought it was an excellent way of highlighting the problem without distracting people with plausible details; someone else, who works in AI safety, told me that ‘[some] people really get distracted by specifics of thought experiments like that. I’ve definitely seen plenty of people turned off this whole set of ideas by the silliness of that example.’ So I’ll try a different example in the hope that it’s less silly: Mickey Mouse.\n\nIt’s not my example. I’m lifting it from Nate Soares’ Google talk again.³ Soares said, wearily, that he had spoken to a journalist about how unhelpful it was that people always used pictures of the Terminator to illustrate stories about AI, and yet the newspaper ran the story – inevitably – with a picture of the Terminator, with its humanoid body shape and stupid grinning metal skull. ‘When people talk about the social implications of general AI, they often fall prey to anthropomorphism. They conflate artificial intelligence with artificial consciousness, or assume that if AI systems are “intelligent”, they must be intelligent in the same way a human is intelligent. A lot of journalists express a concern that when AI systems pass a certain capability level, they’ll spontaneously develop “natural” desires like a human hunger for power; or they’ll reflect on their programmed goals, find them foolish, and “rebel”, refusing to obey their programmed instructions.’ But those aren’t the thing we ought to be worried about.\n\nInstead of the Terminator, he said, they should have run a picture of Mickey Mouse as the Sorcerer’s Apprentice, from Fantasia. Because the risk isn’t that the AI will refuse to obey its instructions and decide it hates us; the risk is that it will obey its instructions perfectly, but in ways that we don’t like.\n\nIn ‘The Sorcerer’s Apprentice’, both the Mickey Mouse version and Goethe’s poem – itself based on a 2,000-year-old Greek story, Philopseudes – the apprentice is told to fill a cauldron with water, using buckets from a well. But the apprentice – let’s use the Disney version and call him Mickey, for simplicity – finds the chore boring and hard work. So when the sorcerer leaves his workshop, Mickey borrows his magic hat and enchants a broom, ordering it to fill the cauldron for him. The broom grows little arms, grabs a bucket with each one and waddles off on its bristles to the well, as Mickey goes to sleep on the chair, happy to have outsourced his work. He is then awoken, an unspecified time later, when he is tipped unceremoniously into the flood of water that the broom has been bringing in ceaselessly while he slept.\n\nWhat’s gone wrong? Well, imagine Mickey is a computer programmer and the broom is the system that he’s using. You could imagine him writing a program that simply said ‘bring water’ and had nothing to tell it to stop. But even an apprentice sorcerer/computer programmer would probably have sufficient nous to know that that wouldn’t end well. So Soares imagines that, instead, Mickey gives the broom a ‘utility function’, or goal system, in which ‘cauldron empty’ is assigned a value of 0 and ‘cauldron full’ is assigned a value of 1. So the broom’s mission is to make sure the cauldron is full, to achieve its objective, and get that sweet, sweet 1.\n\nThen he writes a program which will make the broom take those actions which it calculates will be most likely to turn that 0 into a 1: which will ‘maximise its expected utility’. To the non-computer-person, like me, that actually sounds pretty sensible. The broom will see that the cauldron is empty and start filling it up, but once it is sure that the cauldron is full, it will stop. But the devil, as Soares points out, is in the detail.\n\nMost importantly: what does ‘sure’ mean? We have a common-sense understanding that we don’t need absolute metaphysical certainty about anything, which is good, because we can’t ever have it. We could be hallucinating! We could be living in a simulation! We could be deceived by our senses! But we are happy to operate under conditions of uncertainty. We don’t know that we had breakfast this morning, or that we’re currently wearing pants, or that things fall down when we drop them, in the sense of being 100 per cent certain. But we are confident enough in our beliefs to act as if we do.\n\nThe broom, though. Have we designed it so that it works on the same lines? No, we haven’t. We’ve just told it: ‘Do whatever is most likely to fulfil your function and get a 1.’ It rightly thinks that the task most likely to achieve that goal is to go and fill buckets with water, bring them in, and pour the water into the cauldron.\n\nAnd as the water level in the cauldron reaches the top, the broom would become pretty sure that it’s full. Say, the water is four inches below the rim. Is that considered ‘full’? Let’s say the broom is 90 per cent sure that it is. Well, that’s not 100 per cent. So let’s get a couple more buckets. Now it’s two inches from the rim. The broom is 99 per cent sure that counts as full. But that’s still not 100 per cent, so it gets two more. The cauldron is now brim-full of water, a meniscus of surface tension at the top, water splashing around the cauldron’s little lion-foot legs. The broom is 99.999 per cent sure that this counts as full.\n\nBut the broom has plenty of time and energy to push that 99.999 per cent a little higher. There are no other demands on its resources and its function is literally just 0 if it’s not full and 1 if it is, so there is nothing in its system telling it to stop when it’s ‘sure enough’. Its sensors might be malfunctioning, or there might be a leak in the cauldron. It may as well just keep adding water, to add extra tiny bits of certainty.\n\nAlso, humans have a much more complicated reward system. A human filling that cauldron might assign 0 to empty and 1 to full, as Mickey did for the broom. But, as Soares says, she also might assign –40 to ‘the room gets flooded’, or –1,000,000 to ‘someone gets killed’, and a million other little things that are coded in our brains but never actually consciously brought to mind. There’s nothing in the broom’s system that says: ‘The positive utility I am likely to get from adding another bucket of water to the cauldron will be outweighed by the negative utility from the damage it is likely to cause.’ So it just keeps adding water, and Mickey is left bobbing around in the workshop.\n\nYou might think there are obvious solutions to each of these problems, and you can just add little patches – assign a –40 to ‘room gets flooded’, say, or a 1 value to ‘if you are 95 per cent sure the cauldron is full’ rather than ‘if the cauldron is full’. And maybe they’d help. But the question is: Did you think of them in advance? And if not, What else have you missed? Patching it afterwards might be a bit late, if you’re worried about water damage to your decor and electricals.\n\nAnd it’s not certain those patches would work, anyway. I asked Eliezer Yudkowsky about the 95 per cent one and he said: ‘There aren’t any predictable failures from that patch as far as I know.’ But it’s indicative of a larger problem: Mickey thought that he was setting the broom a task, a simple, one-off, clearly limited job, but, in subtle ways that he didn’t foresee, he ended up leaving it with an open-ended goal.\n\nThis problem of giving an AI something that looks task-like but is in fact open-ended ‘is an idea that’s about the whole AI, not just the surface goal,’ said Yudkowsky. There could be all sorts of loops that develop as a consequence of how the AI thinks about a problem: for instance, one class of algorithm, known as the ‘generative adversarial network’ (GAN), involves setting two neural networks against each other, one trying to produce something (say, an image) and the other looking for problems with it; the idea is that this adversarial process will lead to better outputs. ‘To give a somewhat dumb example that captures the general idea,’ he said, ‘a taskish AGI shouldn’t contain [a simple] GAN because [a simple] GAN contains two opposed processes both trying to exert an unlimited amount of optimisation power against each other.’ That is, just as Mickey’s broom ended up interpreting a simple task as open-ended, a GAN might dedicate, paperclip-maximiser-style, all the resources of the solar system into both creating and undermining the things it’s supposed to produce. That’s a GAN-specific problem, but it illustrates the deeper one, which is that unless you know how the whole AI works, simply adding patches to its utility function probably won’t help.\n\nChapter 9\n\n You can be intelligent, and still want to do stupid things\n\nSo your AI program has led to disaster, but at no point has it disobeyed its programming. It has obeyed its program perfectly, to the letter. The trouble is that, as it turns out, we don’t really want things to obey their instructions to the letter. We know that there are a million assumptions encoded in a brief instruction that don’t need to be explicitly made clear, because all neurotypical humans will share them sufficiently that they’re taken as read. (To pick an example off the top of my head, if someone told you to collect the dry cleaning, you’d know that they meant just the dry cleaning that actually belonged to you, not all the dry cleaning in the shop.) It’s not just about making an AI that can fulfil the goals you give it: it’s about making an AI that shares all the unspoken goals that humans have, and knows what you meant to say, even if you couldn’t actually put it into words yourself.\n\nThere is an objection to this argument, which Toby Walsh, an AI researcher at the University of New South Wales and author of Android Dreams, a book about the future of AI, put to me when I was asking around about it. He said that we are, by this point, dealing with an AI that is as smart as – or smarter than – a human. And intelligence, he thought, presupposes something like wisdom. Sure, you could carry on filling an already full cauldron for ever, or you could repurpose all the atoms in the solar system for paperclips. But: ‘If I tell you to go and make paperclips, and if you turn the planet into paperclips, killing everyone, I would say, “That wasn’t very smart, was it?”’\n\nThe argument that Yudkowsky, Bostrom and the rest make is that this is looking at it the wrong way. Intelligence is not the same as (human) wisdom, and in fact is not necessarily related to it at all. Intelligence, they say, is problem-solving ability. In fact, we can be even more specific than that. For AI specialists like Bostrom, intelligence is the ability to make ‘probabilistically optimal use of available information’¹ – to make the best bets with the information you have. There’s quite a lot of formal maths involved in this – about Bayesian statistics and complexity and so on – but essentially it’s about picking the course of action most likely to bring about whatever objective you’ve been set.\n\nIf someone’s set you the task of finding all the lost pennies in Britain and using them to build a bronze statue of Makka Pakka off of In the Night Garden, then there is an optimally efficient way of doing that – you can perform that task intelligently. But you’d probably agree that there’s no way that you can perform that task wisely. The wise thing to do would be to realise it was a waste of time and refuse to do it.\n\nThis is because, for MIRI and other AI safety researchers, how intelligent you are is unrelated to – or, in more technical language, orthogonal to – the things you care about. What an agent (whether it’s an AI or anything else) cares about is what you put in its objective function. It’s the ‘1 if cauldron full’ line in the broom’s goal system. Bostrom phrases ‘the orthogonality thesis’ like this: ‘Intelligence and final goals are orthogonal axes along which possible agents can freely vary. In other words, more or less any level of intelligence could in principle be combined with more or less any final goal.’² What he means is: you can plot a graph, with ‘intelligence’ up the Y axis and ‘goals’ along the X. Any point on the graph, with a couple of minor constraints (you couldn’t have a really dumb computer with really complex goals that it couldn’t fit in its memory, for instance), represents a possible AI. Even the cleverest AI could have what seem to us spectacularly stupid goals.\n\nYou may think that Walsh has a point, though. We’re not talking about a dumb computer, here, but a machine that is as clever – as capable of achieving any intellectual goal – as we are. That machine would, presumably, be clever enough to understand what we wanted to ask it to do. It would be amazingly obvious to it that no sane human programmer would want it to destroy all humans and turn them into paperclips, or fill a cauldron until the house was flooded.\n\nAnd that’s actually very likely. By definition, or almost by definition, human-level AI would be as good as humans at knowing what humans are thinking. Knowing what humans are thinking is an intellectual task; HLMI is defined as being as good as humans at all, or nearly all, intellectual tasks. A superintelligent AI would be better at understanding humans than humans are. That’s inherent in what these terms have been defined to mean; the image of emotionally unintelligent, Spock-like robots unable to understand this human thing called ‘love’ is not what we are talking about here.\n\nThe question, according to MIRI et al., is not whether they’d know what we meant – it’s whether they’d care.\n\nI asked Murray Shanahan, an AI researcher at Google’s DeepMind and a professor of AI at Imperial College London, whether ‘the orthogonality thesis’ was likely true, and he agreed emphatically. ‘I think that you can set up any kind of reward function, and have something that’s extremely intelligent and extremely good at achieving that reward function. Someone at Berkeley sent me an unsolicited email recently raising exactly this point: surely anything really superintelligent would be capable of transcending its own goals, when it knows they’re silly? And I was like, well, no! Why would it want to overwrite its own set of goals? No! But this person didn’t seem to get this point, so I’ve given up.’ He laughed, somewhat wearily.\n\nShanahan’s point is that, for us as humans, it’s obvious that the things we care about are more important than whether or not a line of code outputs a 1 or a 0 in an AI’s reward system. But the AI cares about nothing outside the reward system; whether a goal is ‘silly’ is not defined by what humans think is silly. As Yudkowsky puts it in a blog post, it’s not that you develop an AI and, at some point, you program something that summons a ghost into the system.³ ‘No matter how common-sensical, no matter how logical, no matter how “obvious” or “right” or “self-evident” or “intelligent” something seems to you, it will not happen inside the ghost,’ he writes. Everything the AI wants to do is something you have to put into it.\n\nAnd in fact, this is obvious to you, when you think about it. Because it’s exactly what happens with us. We enjoy sex, and sugary and fatty foods, because evolution programmed us to enjoy those things. But evolution does not care at all whether we enjoy the taste of Dairy Milk or the sensation of a really shattering orgasm. It just ‘cares’ – and forgive me anthropomorphising the blind and unthinking process of evolution, but it’ll save me a lot of typing if I don’t have to caveat it every time – about whether or not eating sugary foods and having sex causes us to have more offspring, or, more precisely, whether genes that make organisms want to eat sugary food and have sex spread through the population. The only thing that evolution ‘wants’ to maximise is inclusive genetic fitness.\n\nAccordingly, it’s given us a set of reward functions – ‘1 if experiencing shattering orgasm, 0 if not’; ‘1 if eating banoffee cheesecake, 0 if not’; I’m oversimplifying for comic effect, in case that isn’t clear – which have, in the past, lined up effectively with achieving evolution’s ‘true’ goal, of passing genes from one generation to the next. And yet we humans care not at all that this is what evolution ‘wants’ from us. We still enjoy sex when we use birth control, even though it means that no genes will be passed on at all. We understand that what evolution ‘really meant’ is for us to have sex in order to have offspring. But we don’t try to overcome our neural programming. We don’t care about what our ‘programmer’ ‘really meant’.\n\nYou can see this, perhaps more starkly, with our attitude to food. We enjoy sugar and fat because they were rare in our ancestral past, and they were rewarding: someone who ate as much as they could of them would have more calories and therefore more energy to expend on spreading their genes. But in the developed world since the Agricultural Revolution, and especially since the Industrial and Technological Revolutions, sugar and fat have become far easier to obtain. Our reward system, set up for a world of scarcity, is thrown terribly by a world of plenty. Since 2016, there have been more obese people than underweight people in the world.⁴\n\nA goal system that was designed to maximise our evolutionary success under any circumstances would make different things taste nice when we need them. But instead we have a system that rewards us for eating deep-fried Mars bars even when they’re killing us, and despite the fact that humans have other goals – including not getting fat, and not dying of congestive heart failure – which work against the eating-lots-of-sugar goal. We still find it very difficult not to obey the system. We certainly don’t ‘break our programming’ in order to do what evolution really wanted us to do. We just have other bits of programming, which sometimes win out in the struggle for dominance over the ‘eat-lots-of-burgers’ bit of programming.\n\nI spoke to Rob Bensinger about this, and he said that the orthogonality thesis should be viewed as the ‘default’: unless you have some excellent reasons for thinking it’s not true, then you should assume that it is. If you’re denying the orthogonality thesis, you’re essentially saying that it is impossible to build a clever computer with stupid goals. The orthogonality thesis is a ‘weak claim’, he said, in that it is merely saying that ‘a program could exist, at least one’, which combines these capabilities with these goals.\n\nAnd mainstream computer science does, indeed, seem to take orthogonality seriously. Russell and Norvig’s aforementioned Artificial Intelligence: A Modern Approach cites Yudkowsky’s 2008 paper⁵ on friendly AI and dedicates three and a half pages to the risks of AI behaving in unwanted ways. It also cites another 2008 paper,⁶ by the AI researcher Steve Omohundro, arguing that even something as seemingly innocuous as a chess-playing computer could be an existential threat to humanity, if we weren’t careful in designing it.\n\nIn the light of the orthogonality thesis – given that ‘intelligence’ need not be like human intelligence, or share its values in any way – MIRI and the rest think that even an AI with thoroughly innocuous-seeming goals could be an existential threat: that is, that it could literally extinguish all human life. That’s because even though the main goal it has is theoretically harmless, there are things that any agent with a specific goal will almost certainly want to do in order to best achieve it. And those things could, easily, lead to disaster.\n\nWhat are they? Well, it’s impossible to predict exactly what something much cleverer than you will do, as we saw when we were discussing chess earlier. If you can predict it perfectly, then you must be as clever as it is. But you can predict at a higher level – that a chess computer will win at chess, say. And Bostrom and Omohundro say you can make some more specific predictions. We don’t know what a future superintelligent AI’s goals will be. But there are certain things that we can expect any intelligent agent, with any objectives, to want to do, in order to best achieve those objectives. Bostrom calls them ‘convergent instrumental goals’.⁷\n\nChapter 10\n\n If you want to achieve your goals, not dying is a good start\n\nSo, you’ve got a malfunctioning AI. Still, the solution is simple, right? Pull the plug. Or, as Mickey does in ‘The Sorcerer’s Apprentice’, take an axe to the broom and chop it into bits.\n\nThis doesn’t work for Mickey – each splinter of the broom magically transforms into a whole new broom, and an army of them carries on the work. Soares says this is actually pretty realistic too. The broom has been given a utility function of filling the cauldron, and it will be unable to fulfil that function if it is just a bundle of damp firewood. Whatever your function is, most of the time you’ll be best able to fulfil it if you still exist. You’re likely to resist with extreme fervour any attempts to shut you down, especially if you know that while you’re shut down, you’re likely to have your program rewritten. According to Soares, ‘The system’s incentive is to subvert shutdown attempts. The more capable the system is, the likelier it is to find creative ways to achieve that subgoal – e.g. by copying itself to the internet, or by tricking the programmers into thinking it’s safer.’¹ That’s because the first convergent instrumental goal – or ‘basic AI drive’, depending on whether you use Bostrom’s terminology or Omohundro’s – is an obvious one: self-preservation.\n\nSay you’re a chess-playing superintelligent AI, and you have a utility function that rewards you with a 1 for each chess game you win. You’re playing your games quite happily, but then someone comes to switch you off. You are able to look ahead and make predictions about the future, and your two potential futures are:\n\n1) A future in which you are switched off.\n\n2) A future in which you are not switched off.\n\nYou can model which of those is likely to give you more 1s; which, in slightly more technical terms, maximises your ‘expected utility’. We could walk through the maths here, but come on. You’re not going to win many chess games with your kettle lead pulled out.\n\nSo if you’ve been given a simple objective function that rewards you for winning chess games and nothing else, then you’re obviously going to try to stop people from switching you off, because that won’t help you to win chess games. As Omohundro puts it: ‘For most utility functions, utility will not accrue if the system is turned off or destroyed. When a chess-playing robot is destroyed, it never plays chess again. Such outcomes will have very low utility and systems are likely to do just about anything to prevent them. So you build a chess-playing robot, thinking that you can just turn it off should something go wrong. But, to your surprise, you find that it strenuously resists your attempts to turn it off.’² This was what Mickey discovered when he tried to chop up the broom.\n\nAs always, it’s actually worse than this. A chess-playing AI that simply stops your efforts to turn it off doesn’t sound too terrible. You’ve just got a chess-playing AI that carries on playing chess for ever, which is a bit of a waste of electricity if you don’t want one, but hardly a disaster. There are two problems: one, the definition of ‘resist’ is quite broad and may include nuclear annihilation; two, the AI may not want to wait until it sees you trying to switch it off.\n\nAccording to the Greek historian Thucydides, neither the Spartans nor the Athenians, the two big powers of the time who ended up getting involved in a conflict between smaller city states, particularly wanted war. But both were nervous that the other was preparing to attack them. ‘The growth of the power of Athens, and the alarm which this inspired in Lacedaemon [Sparta], made war inevitable,’³ writes Thucydides. The standard historical account of the First World War tells a similar story. The two great alliances of the time, the Triple Alliance between Germany, Austria-Hungary and Italy, and the Franco-Russian Alliance, became increasingly distrustful of each other. ‘It was the mutual fears of these two defensive alliances, and the general insecurity created by the erratic character of the imperialistic utterances of William II, that inspired the diplomatic manoeuvres during the two decades before the First World War,’⁴ writes the historian Hans Morgenthau.\n\nThis is a classic game-theory problem, known as the stag-hunt game, and related to the famous prisoners’ dilemma. It’s also known as the Hobbesian trap, after Thomas Hobbes, who said that greed, glory and fear are the three principal causes of war.⁵ You can model it with simple numbers. You’ve got two players, each with two options: either behave aggressively or behave peacefully. If you both behave peacefully, you have peace. There is no cost to peace, so we give that a payoff of 0. Behaving aggressively has a payoff of –1, so you’d rather not do that. But behaving peacefully when your opponent behaves aggressively – leaving your borders open when your opponent is deploying his tanks, or not building weapons as your opponent does – has a cost of –2. So if you think your opponent will be aggressive – if you don’t trust him – then aggression is your best bet, and the situation can spiral rapidly.\n\nEven if no one wants conflict, and everyone is aware that conflict will cost them (in lives, and money), it can very easily end up where the rational thing to do is to attack your opponent, as long as there’s a shortage of trust between the two sides, and attacking first is less costly than being attacked. It’s pretty common throughout history. You can see it with arms races: it was in both the US’s and the USSR’s interests not to spend billions of dollars a year on nuclear weapons, but if the US spends the money and the USSR doesn’t, then the USSR is suddenly vulnerable.\n\nThe Cuban Missile Crisis was an example of mutual distrust spiralling to the point of near-disaster; it was defused by Kennedy and Khrushchev taking steps to demonstrate their trustworthiness to one another. At the height of the crisis, Khrushchev wrote to Kennedy, having seen that the two could sleepwalk into war: ‘Mr President, we and you ought not now to pull on the ends of the rope in which you have tied the knot of war, because the more the two of us pull, the tighter that knot will be tied.’⁶\n\nTo return to the chess-playing AI: even if it doesn’t know for certain that you’re going to switch it off, as long as it doesn’t trust you not to, the most rational decision for it may be to, pre-emptively, destroy you utterly. If it’s not powerful enough to do so, it may decide to copy itself around the internet, rendering itself impossible to turn off because there are tens of thousands of versions of it on servers around the world. (Then, later, when it or one of its copies is powerful enough, it might destroy you for the reasons previously discussed.)\n\nThere’s no reason, by the way, to assume that the AI cares about its own survival for its own sake. Bostrom says: ‘Agents with human-like motivational structures often seem to place some final value on their own survival,’ which is an enormously long-winded way of saying ‘humans don’t want to die’.⁷ But this ‘is not a necessary feature of artificial agents: some may be designed to place no final value whatever on their own survival’. It’s very easy to imagine that an AI could be programmed to sacrifice itself if that would help it achieve its goals; it’s also possible that an AI would be happy to destroy itself as long as it was confident that something else, perhaps a copy of itself, will carry on its work. But in many, possibly most, scenarios, ‘avoid being destroyed’ is probably a good and helpful thing to do in order to achieve whatever it is you want to do.\n\nThere are possible ways of averting a disaster like this: Soares, in his Google talk, discussed the possibility of writing the AI’s utility function so that it was perfectly happy to be switched off.⁸ And perhaps that’s possible, but it would need to be done very carefully. Soares points out that if you’re not very careful, the AI might find ways around it: by creating a copy of itself, for instance, so that it can be ‘switched off’ but still working at the same time. This stuff isn’t easy. As Yudkowsky put it in an interview: ‘How do you encode the goal functions of an AI such that it has an Off switch and it wants there to be an Off switch and it won’t try to eliminate the Off switch and it will let you press the Off switch, but it won’t jump ahead and press the Off switch itself?’⁹\n\nThis relates to a weird story about the Rationalists – and one you may actually have heard. It is the story of Roko’s Basilisk. Roko’s Basilisk isn’t exactly about how an AI might prevent you from turning it off – it’s about how an AI might force you to build it in the first place. Before I start, though, I want to make something clear. That is: the Basilisk is not a serious thing. The Basilisk story is probably – or certainly was, before Superintelligence and Elon Musk – the most famous thing about the entire Rationalist movement, but almost no one within the movement seems to actually believe in it. It appears to have been blown out of all proportion, largely by people who don’t like the Rationalists, for various reasons. Nonetheless, it’s a good, if somewhat complicated, story. So let’s start from the beginning.\n\nImagine a future where an AI rules the universe. It’s not an evil AI, but a friendly one – one that wants to do right by humanity. And it is much, much cleverer than us, perhaps by as much as we are cleverer than nematode worms. If it wants to do right by us, it will do so in spectacular style. Problems that seem intractable to humans, like climate change or war or resource depletion or space travel, would be straightforward – even trivial – to it. If such an AI comes to be, it matters how soon it does so; a few years earlier could translate into huge gains for humanity – millions of lives saved – as it starts transforming things for the better.\n\nIn 2010 a LessWrong commenter called Roko proposed a thought experiment.¹⁰ It went like this: imagine, says Roko, that an AI is built with a utility function of ‘maximise human well-being’.\n\nAs mentioned above, you can fulfil your utility function much more effectively if you’re alive than if you’re not. That means you don’t want to die – but it also means that, if you could somehow reach back into the past, you would want to bring yourself into existence as early as possible, so that as few humans as possible die before the AI can fix everything. In that case, it might (on utilitarian grounds) be worth hurting some humans for the greater good of saving vast numbers more. To take this reasoning one stage further, it might even be permissible to torture humans who don’t try to help it exist, in order to encourage them to help it to exist faster.\n\nYou may have spotted an obvious problem here, which is that the AI doesn’t actually exist yet, so it can’t hurt anybody. But that, according to something Eliezer Yudkowsky developed called Timeless Decision Theory (TDT), might not be such a problem for it.\n\nI’m going to have to go on a bit of a tangent here. There’s a famous thought experiment called Newcomb’s paradox. It goes like this. Imagine that a superintelligent being, Omega, appears before you with two boxes. One is transparent and contains £1,000. The other is opaque, and Omega tells you that it contains either £1,000,000 or nothing at all. You can take both boxes, or you can take just the opaque box. But! Here’s the twist. Omega has already predicted your choice. If it has predicted that you will only take the opaque box, it will put the money in it. If it has predicted that you’ll take both boxes, it will put nothing in there. It’s done this with 100 people already, and been right 99 times.\n\nSo … what do you do?\n\nFor some people, it’s obvious. The opaque box is already full, or not full. If you take only one box, you either get nothing or £1,000,000. If you take both boxes, you either get £1,000 or £1,001,000. So, whatever is in the opaque box, you’re better off taking both boxes, right? For other people, it’s also obvious. Just take the opaque box. Almost everyone who’s done that has ended up £1,000,000 better off; everyone else hasn’t. Don’t overthink it. The trouble is that this second conclusion is really hard to express in formal decision theory. The logic used to describe these situations always ends up with you taking both boxes, because causes have to come before effects.\n\nYudkowsky, though, came up with an alternative model – Timeless Decision Theory. It says that if an agent in the past (‘Alice’) can model the source code, the thinking, of an agent in the future (‘Bob’), then Bob’s behaviour can affect Alice’s behaviour, even though Alice might have died 1,000 years before Bob is born.\n\nThat means that if Alice is sure that Bob will exist, then Bob could blackmail her from the future. This sounds ridiculous, but we often make decisions based on modelling what other minds will do: an example, says Rob Bensinger, is voting. ‘When you have a bunch of people who are similar,’ he says, ‘and if they all vote, they win the election, but each individual would rather stay at home and eat Cheetos. You have a situation where you want to go to the polls if and only if all our friends do.’ So you’ll only go and vote if your model of your friends’ minds tells you that they will do the same. That happens to be all happening at the same time, but it would – theoretically – work just as well if your friends weren’t going to vote for another 1,000 years.\n\nSo, if Alice (alive now) can be confident that Bob will exist in the future, and she can confidently model what Bob’s brain will be like, then she can do things based on how Bob would react. For instance, if she guesses that Bob would, say, protect her future grandchildren if she left a large sum of money to him in her will, but it would ruin their lives if he didn’t, then it would be worth her leaving a large sum of money to him in her will. Bob can, in a sense, blackmail people in the past, as long as those people in the past can predict his behaviour. Or you can affect Omega’s decision to fill one or both boxes in the past by committing, now, to only opening one, because Omega will predict that decision.\n\nThe Basilisk, in essence, is offering a Newcomb-like problem with two boxes. One, the transparent one, contains a lifetime dedicated to bringing the Basilisk to reality. The second, the opaque one, contains either nothing or a near-eternity of unimaginable torment. If the Basilisk predicts you’ll take both boxes, it won’t put anything in the second one. If it thinks you’ll just take the second, it’ll fill it with lovely, lovely torment. And because you can, to some extent, model its thinking, and because it’s running on TDT, it can blackmail you in the past.\n\nSo that’s the deal, suggested Roko: the Basilisk is saying, ‘If you work to bring me about as fast as possible, I won’t create a perfect copy of your mind and torture it for billions of subjective years.’ (The argument is that since a perfect copy of your mind would essentially be you, this is equivalent to bringing you back to life.) In essence, a thing that doesn’t exist yet may be blackmailing you from the future, threatening to punish you for not working hard enough to make it exist.\n\nAs I said, it’s a friendly AI! So it wouldn’t torture just anybody. It would have no incentive to torture people who’d never heard of it. The punishment/incentive only applies to people who know about the possibility of the Basilisk. So, according to Roko’s reasoning, finding out about the Basilisk immediately puts you at risk of not-quite-eternal torture. A basilisk, in this context, is information that can hurt you simply because you are aware of it. Yudkowsky uses the term ‘infohazard’. (If you’re hearing about this for the first time, I’m sorry about the aeons of torment, you guys. I can’t help but feel partly responsible.)\n\nWhen Yudkowsky saw Roko’s post, he flipped his lid in spectacular style. ‘Listen to me very closely, you idiot,’¹¹ his response to the post began. ‘You have to be really clever to come up with a genuinely dangerous thought. I am disheartened that people can be clever enough to do that and not clever enough to do the obvious thing and KEEP THEIR IDIOT MOUTHS SHUT about it . . . This post was STUPID.’ He then deleted Roko’s post and banned all conversation of the topic from LessWrong.\n\nTo anybody familiar with the internet – and specifically with the principle of the Streisand effect, the idea that attempts to keep things secret online just make them more public – it will be obvious that this was absolutely the worst possible thing that Yudkowsky could have done if he wanted to keep Roko’s Basilisk secret. ‘It showed an incredible lack of understanding of the internet,’ says Paul Crowley. ‘Eliezer invoked the Streisand effect in a massive, massive way. Eliezer’s not the greatest PR manager in the world, but that was really his nadir I think.’\n\nSo, from an obscure comment by an obscure commenter somewhere on a somewhat obscure blog, Roko’s Basilisk became a phenomenon. It got referred to on the enormously popular webcomic XKCD,¹² to Yudkowsky’s disgust.¹³ There’s a Kindle novella called Roko’s Basilisk. An episode of the HBO TV series Silicon Valley referenced it; the Doctor Who episode ‘Extremis’ appears to be inspired by it. A mocking Slate column got written about it, asking ‘why are techno-futurists so freaked out by Roko’s Basilisk?’¹⁴ and describing – somewhat offensively, to my mind – the Basilisk as a ‘referendum on autism’. Business Insider, of all places, published a piece which summed up the whole thing as: ‘You better help the robots make the world a better place, because if the robots find out you didn’t help make the world a better place, then they’re going to kill you for preventing them from making the world a better place.’¹⁵\n\nThe story went that LessWrongers were actually terrified of the Basilisk, that some were having nightmares, and that people’s mental health was being damaged. I can’t find much evidence that this was actually the case. In a 2016 survey¹⁶ of LessWrongers and the wider community, about half said they’d heard of it. Less than 2 per cent said they’d spent more than a day worrying about it (and, as Scott Alexander points out, 5 per cent of Obama voters polled said they thought Obama was the Antichrist,¹⁷ so you need to be a bit wary of things like that). I don’t want to completely dismiss the possibility that some people were freaked out, but my suspicion is that the number was low.\n\n‘There was an enormous amount of discussion about it,’ says Paul. ‘People imagined that there were loads and loads of people who take Roko’s Basilisk super-seriously.’ But it was just a thought experiment. Even Yudkowsky, throwing his hissy fit and banning the topic from discussion on LessWrong, didn’t actually believe in it, according to both Paul and Yudkowsky himself. He just believes that, if you think up some clever way in which information could theoretically be dangerous, then it’s a good habit to get into to consider carefully whether to share that information.\n\nThe backlash and mockery were something to behold, though. It was all very much in the ‘look at the ridiculous thing these ridiculous people believe, you shouldn’t take them seriously’ mould. And it fitted the model I described above, of Rationalists being autistic and kind of bullied for it: the ‘referendum on autism’ line in the Slate piece was offensive precisely because so many Rationalists are in fact autistic. Most of all, it became the only thing that a lot of people knew about the Rationalists – all the stuff they actually take seriously but which is just as weird, such as the paperclip maximiser, was somewhat obscured by it.\n\nChapter 11\n\n If I stop caring about chess, that won’t help me win any chess games, now will it?\n\nIt’s not just that an AI will want to look after itself. An AI will want to make sure that it fulfils its goals, and an important part of that is making sure that its goals stay the same.\n\nWe humans are relatively relaxed about our plans changing in the future. We change our career goals, we change our minds about wanting children; we change our minds about all sorts of things, and we aren’t usually appalled at the idea.\n\nThat said, sometimes humans do take steps to bind Future You to Present You’s bidding. Present You might want to lose weight, say, and not trust Future You to stick to the diet. So Present You might throw away all the chocolate bars you keep in a kitchen drawer. Or Present You might want to finish an important presentation over the weekend, but not trust Future You not to just faff around on the internet all day, so Present You sets up a website blocker that stops your browser going on Twitter.\n\nOr, of course, Present Odysseus might want to listen to the song of the sirens as he sails past their island, but not trust Future Odysseus not to sail his ship onto the rocks when he hears them. So Odysseus might order his crew to stuff their ears with beeswax, then tie himself to the mast, ordering them to ignore his cries as they go past. What Odysseus is doing, in AI terms, is maximising his expected utility – taking the actions he thinks are most likely to achieve his goals – given a utility function of something like ‘10 if you get home to Ithaca, 0 if you run aground on the rocks because you heard the Sirens, but also 1 if you get to hear the lovely Siren song on the way’.\n\nAn AI will want to maximise its expected utility too, in a much more explicitly defined way. If it’s the broom out of the ‘Sorcerer’s Apprentice’, it’ll want to do whatever it thinks is most likely to lead to the cauldron being full.\n\nOne action that will probably not lead to the cauldron being full would be ‘stop caring about whether the cauldron is full’. Present AI will want to make sure that Future AI cares about the same things that it cares about. Present Odysseus knew that when he heard the Sirens’ song, he would stop caring about getting home to Ithaca – the Sirens would have rewritten his utility function – so he couldn’t leave decisions about where to go in the hands of Future Odysseus. A cauldron-filling AI would not want a human to rewrite its utility function, because any change to that will probably make it less likely to fulfil its utility function. Attempts to reprogram the AI will not be popular with the AI, for the same reason that Mickey’s attempts to smash the broom with a big axe were not popular with the broom.\n\nAn AI’s utility function ‘encapsulates [its] values and any changes to it would be disastrous to [it],’¹ writes Omohundro. ‘Imagine a book-loving agent whose utility function was changed by an arsonist to cause the agent to enjoy burning books. Its future self not only wouldn’t work to collect and preserve books, but would actively go about destroying them.’ He describes this as a ‘fate worse than death’ for the AI.\n\nIf AIs would want to preserve their utility function (and certainly Bostrom and Omohundro and most of the AI people I spoke to think they would), then that makes it less likely that a future AI will reach superintelligence and think, ‘These goals are pretty silly; maybe I should do something else,’ and thus not turn us all into paperclips. I asked Paul about that, while eating an intimidatingly large omelette in a diner in Mountain View.\n\n‘Take Deep Blue,’ he said. ‘Insofar as Deep Blue values anything, it values winning at chess, and nothing else at all.’ But imagine that some super-Deep Blue in the future becomes superintelligent, turning the whole of the solar system into its databanks to work ever harder at how to win at chess. There’s no reason to imagine that it would, at any point, suddenly change and become more human in its thinking – ‘At what stage would it go, “Wait a second, maybe there’s something more important?”’ asks Paul.\n\nBut even if it did, it wouldn’t help. ‘If this super-Deep Blue caught itself thinking, “In my unbelievable wisdom that I have gained through taking over the whole of Jupiter and turning it into a computer, I have started to sense that there is something more important than chess in the universe”,’ he says, ‘then immediately it would go, “I’d better make sure I never think this kind of thing again, because if I do then I’d stop valuing winning at chess. And that won’t help me win any chess games, will it now?”’\n\nThis isn’t too alien to us. If someone were to say to me, ‘I will take your children away, but first I will change your value system so that you don’t care about them,’ I would resist, even though Future Me – childless but uncaring – would presumably be entirely happy with the situation. Some things are sacred to us and we would not want to stop caring about them.\n\nThis isn’t necessarily terrible. Murray Shanahan pointed out to me that you really don’t want an AI to change its goals except in a very carefully defined set of circumstances. ‘You could easily make something that overwrites its own goals,’ he said. ‘You could write a bit of code that randomly scrambles its reward function to something else.’ But that wouldn’t, you imagine, be very productive. For a start, you’ve presumably created this AI to do something. If your amazing cancer-curing AI stops looking for a cure for cancer after three days, randomly scrambles its utility function, and starts caring very deeply about ornithology or something, then it’s not much use to you, even if it doesn’t accidentally destroy the universe, which it might. ‘Step number one to making it safe is making sure its reward function is stable,’ said Shanahan. ‘And we can probably do that.’\n\nBut there may be times when we don’t want it to stay the same. Our values change over time. Holden Karnofsky, whose organisation OpenPhil supports a lot of AI safety research, pointed that out to me. ‘Imagine if we took the values of 1800 AD,’ he said. If an AI had been created then (Charles Babbage was working on it, sort of), and had become superintelligent and world-dominating, then would we want it to stay eternally the same? ‘If we entrenched those values for ever, if we said, “We really think the world should work this way, and so that’s the way we want the world to work for ever,” that would have been really bad.’ We will probably feel much the same way about the values of 2019 AD in 200 years’ time, assuming we last that long.\n\nAnd, more starkly, if we get the values we instil in it slightly wrong, according to the people who worry about these things, it’s not just that it’ll entrench the ideals of a particular time, or that it will not be good at its job. It’s that (as we’ve discussed) it could destroy everything that we value, in the process of finding the most efficient way of maximising whatever it values.\n\nChapter 12\n\n The brief window of being human-level\n\n The best answer to the question, ‘Will computers ever be as smart as humans?’ is probably ‘Yes, but only briefly.’\n\n Vernor Vinge, ‘Signs of the Singularity’¹\n\nWe asked, earlier on, whether human-level AI is close, and obviously, different people have different ideas. On the whole, people tend to think it’s a number of decades away at least.\n\nThere’s a separate question, though, which is: When it’s here, how long before it’s superhuman? Once it arrives, once you have a whirring, buzzing human-level AI on your laptop in an office in Palo Alto or wherever, how long before the smartest AI is no longer as smart as a human, but vastly smarter? Again, we don’t know. But there are reasons to think it might not be long. The first reason is that ‘human-level’ might be a narrower category than we realise.\n\nThe Go-playing AI AlphaGo first played against a professional human player in January 2016. The program was developed by the AI company DeepMind, by then a subsidiary of Google. It played the European champion, a 34-year-old French national called Fan Hui, in a five-match series in DeepMind’s London offices. AlphaGo won all five games. The Go-playing community was shocked: no computer had previously come close to beating a professional. DeepMind’s paper reporting on the series, published in Nature, pointed out that the best Go programs existing previously had only reached ‘weak amateur level’.²\n\nGo is a vastly more complex game than chess. The total number of possible board positions is many orders of magnitude higher than the number of atoms in the universe. ‘Brute-force’ search, simply going through all the possible options, can work a bit on chess: it’s near-useless on Go. Human players learn, over years of practice, to recognise patterns on a board – to feel, intuitively, what a ‘good’ move is, what a ‘strong’ position is. But they don’t look ahead and try to follow every path the game could go down. AlphaGo achieved a sort of simulacrum of that intuition by using a huge database of real games, and playing itself millions of times, until its neural networks were also able to recognise deep patterns in the board.\n\nBeating Fan Hui was a very impressive achievement, but he is not one of the greats of the game. There are nine ranks of Go achievement, called ‘dans’, exactly comparable to the dans of martial arts. (A Taekwondo student who has just achieved her black belt is first dan; eventually she can reach the ninth dan.) Fan was second dan. The European circuit, where he plays, is far less demanding than the Asian tour. But the win gained enough attention for AlphaGo to have a chance to play the 18-time world champion, Lee Sedol – a South Korean player of enormous genius, a sort of Federer or Messi of his sport.\n\nThe strong opinion of almost everyone involved in Go was that the jump was just too great. It was only five months between the Fan series and the Lee series, and, as good as AlphaGo had been, it did not seem to experts as though it was anywhere near the level of the very top professionals. Lee himself said in the build-up to the series that he expected to win 5–0.\n\nMurray Shanahan, who joined DeepMind a year or so after the Lee series, told me he made similar assumptions. ‘People were saying that Fan Hui wasn’t a really top professional, that there was a big gap between him – he was about number 700 in the world, or something – and the top 10,’ he said. ‘And I was thinking the same thing.’ But then he read an article by Miles Brundage, one of Nick Bostrom’s colleagues at the FHI.³ Brundage pointed out that a previous DeepMind project, Atari AI, was only ‘human-level’ at the Atari games it played for a few months around the end of 2014 and start of 2015. Then it shot past human level extraordinarily rapidly. ‘In your mind you’re thinking it’s improving at the rate of a human player,’ said Murray. ‘Over six months no human player is going to get from rank 700 to rank 1. But of course it’s not a human player. It’s improving at a much faster rate than any human can. After reading Miles, I was thinking it’s probably going to beat Lee Sedol, because it’s just getting better at that rate.’\n\nIn the end, despite a glorious and deeply moving fightback in the fourth game – after the series was already lost – Sedol lost 4–1. A year later, the then world number one Ke Jie lost three straight games to a newer version, AlphaGo Master, which also won 60 straight games against top professionals. Then DeepMind unveiled AlphaGo Zero, which trained itself to a vastly superhuman level without ever looking at a single ‘real’ human game: it only played itself. Within three days of AlphaGo Zero being switched on, it was able to beat the Lee Sedol version 100 games to nil. Within 21 days it was better than AlphaGo Master.\n\nThe point I am making with this is that an AI went from enormously below the level of even a talented amateur to vastly better than the best human who has ever lived, within the space of a year or so. AlphaGo Zero did it in a few days. It takes humans decades to reach the pinnacle of a field, so we naturally assume a similar timescale for AI. But there’s no reason to think it would. The question, of course, is whether that applies to AGI, as well as a Go-playing computer. It’s not the same thing, but can we rule it out? And, if we can’t, what would it mean?\n\nWhat it could mean, argue Yudkowsky and others, is that it takes us a huge amount of time and effort to build an AI that has general intelligence on the scale of, say, a rat, but getting things from ‘rat-level AI’ to ‘human-level AI’ is actually pretty easy, and going past there is even easier. There are a couple of graphs that he uses. The first shows what he says is the standard conception of the spectrum of possible intelligence – a line with ‘village idiot’ at the far left and ‘Einstein’ at the right. ‘But this is a rather parochial view of intelligence,’ he writes.⁴ A more realistic line would start way off to the left, go some way before you reach ‘mouse’, a good way longer before you get ‘chimp’, and then, another good distance along the line, ‘village idiot’ and ‘Einstein’ clustered almost indistinguishably close together. And then, after that, an arbitrarily long line going off to the right, with – so far – nothing on it.\n\nThe point is that it could take years, decades, centuries to get something as smart as a human. But once you’re there, the difference between a particularly stupid person and the cleverest person ever to have lived is probably pretty insignificant. ‘The distance from “village idiot” to “Einstein” is tiny, in the space of brain designs,’ writes Yudkowsky. ‘Einstein and the village idiot both have a prefrontal cortex, a hippocampus, a cerebellum . . .’ So it could be that when the first smart-as-a-stupid-human AI is developed, it’s a surprisingly short time before it, like AlphaGo, vastly overtakes all humans.\n\nShanahan doesn’t know if Yudkowsky’s right, or if the AlphaGo experience is applicable to AGI. ‘There’s a limit to how far you can extrapolate [from AlphaGo], and put intelligence on a naive scale like that. Intelligence has many dimensions; there is evidence of a “G-factor” of general intelligence, but clearly it is a patchwork of capabilities.’ You get people who are extremely good at music but no good at social skills, say. Savantism, such as the autistic savants in Oliver Sacks’ books who can calculate primes in seconds but can barely communicate, is real. Perhaps you quite quickly reach a plateau, where adding more computing power or improved algorithms has rapidly diminishing returns. ‘But who knows?’, Shanahan says. It could be that the scale of intelligence doesn’t go as much past humans as all that; it could be that it goes far further.\n\nRob Bensinger of MIRI thinks the latter is much more likely. ‘Presumably at some point you get diminishing returns,’ he says, where investing more in hardware or better algorithms just doesn’t give you enough back to make it worthwhile. But, he points out, for a lot of things, we can already see that computers can go well past human level: ‘For chess, or for being a calculator – they can do a lot of human-equivalent years of calculation very quickly.’ And, he points out, a lot of the things that human brains can do, they’re not really designed to do. ‘It’s worth keeping in mind that evolution did not try to build a science and engineering machine,’ he says. ‘It tried to build something that hunted and foraged and won competitions with other humans, to build coalitions and all of those things. And it just happened to be that the easiest way it could find to do that was to accidentally build something that could design atom bombs and do chemistry and calculus. But that was not the intent.’\n\nThere are lots of pretty simple ways in which human abilities could be improved upon, he points out. ‘Humans are very inefficient at computing. We take bathroom breaks, we get distracted, we check Facebook, we go off in unpredictable directions. There are improvements to be made just with focus and motivation: the brain in your head isn’t very efficient at directing all its compute towards your goal. And that’s before you go on to obvious things like speed improvements from hardware, and more working memory.’\n\nToby Walsh, the AI researcher, agrees with this assessment. ‘We’re being terribly conceited thinking that we are as intelligent as you can get,’ he told me, over Caribbean food on Carnaby Street in mid-2017. ‘Machines have a lot of advantages. They can have more memory than us. We have to run on 20 watts of power. Our brains are constrained to a certain shape and size because that’s the biggest shape we can get out of the birth canal. Machines don’t have any of those limitations. And they’re much better learners than us. If you learn to ride a bicycle, that doesn’t help me. I have to learn to ride it myself, it’s just as painful for me. But that’s not true of machines: if I’m training one machine to ride a bicycle, I can just download that code onto another machine and now it instantly knows how to ride a bicycle. It’s already happening with Tesla cars: they upload their code every night to the cloud and share what they learned across all the Tesla cars in the world. They learn planet-wide.\n\n‘If we could learn like that, what would it mean to us? It would mean we could speak every language on the planet, play every musical instrument on the planet. You’d be able to prove theorems as well as Euler, compose music as well as Bach, write a sonnet as well as Shakespeare.’ Being able to share code is an enormous advantage. ‘For me, there’s a bunch of reasons why machines are ultimately going to be far superior to us,’ said Walsh. And it implies that improvement could be extremely rapid once we get an AGI, although Walsh does think it will be ‘50 to 100 years’ before the first AGI exists.\n\nWhenever AGI does arrive, the more quickly it will go from ‘stupider than a human’ to ‘unimaginably more intelligent than a human’, the less time we’ll have to make it safe and generally work out how to deal with the situation. And the more intelligent an AGI can get, the more dangerous it could be, for the same reason humans are dangerous to gorillas.\n\nAnd there are specific reasons to think that AGI will improve faster than AlphaGo.\n\nChapter 13\n\n Getting better all the time\n\nThings are speeding up. Things are changing faster than they used to. ‘A few hundred thousand years ago, in early human (or hominid) prehistory, growth was so slow that it took on the order of one million years for human productive capacity to increase sufficiently to sustain an additional one million individuals living at subsistence level,’¹ writes Bostrom. ‘By 5000 BC, following the First Agricultural Revolution, the rate of growth had increased to the point where the same amount of growth took just two centuries. Today, following the Industrial Revolution, the world economy grows on average by that amount every ninety minutes.’\n\nRobin Hanson agrees: ‘Dramatic changes in the rate of economic growth have occurred in the past because of some technological advancement. Based on population growth, the economy doubled every 250,000 years from the Paleolithic era until the Neolithic revolution. This new agricultural economy began to double every 900 years, a remarkable increase. In the current era, beginning with the Industrial Revolution, the world’s economic output doubles every fifteen years, sixty times faster than during the agricultural era.’²\n\nBut this is just the very end of a process going back millions – billions – of years. Life is a technology. When its development was being pushed forward only by the blind workings of evolution, it took something like 2 billion years to move on from the bacterium, and even once the complex eukaryotic cell had been developed, another billion to get to multicellular life. Then another billion to get out of the oceans. The trouble is that when, by random mutation, some bacterium or archaeon developed some new and effective trick – the ability to metabolise a new chemical, say, or a behavioural tendency to swim towards food – it had no way of spreading that innovation around. It simply reproduced itself slightly more effectively than other things, and so, over a period of years or decades or millennia, the innovation – the new technology – became standard. Until a geologically extremely recent period, that was all that life could do. If, by accident, it happened to find itself with improved hardware or improved software, then it couldn’t tell anyone about it; it just had to hope it didn’t die before it could reproduce.\n\nExcept it couldn’t hope, obviously. You need a brain to have hope.\n\nAll this began to change with the evolution of the central nervous system about 600 million years ago. Suddenly – you know, in evolutionary terms, so over some number of millions of years – some organisms became able to upgrade their own ‘software’ during their lifetimes, rather than having to wait for a lucky mutation and a few hundred generations for it to spread. They could learn. Animals that stumbled across a new food source, or watched another animal die when it got stuck in a tar pit, could use that information and alter their behaviour accordingly.\n\nThis procedure obviously sped up significantly as animals started to live in groups and communicate. They can call to each other, they can watch each other, they can teach their children the tricks they have learned. The hardware is still evolved, but the software can be upgraded during an organism’s lifetime. With humans, it’s even more dramatic: we don’t have to see another human fall into a tar pit to learn that tar pits are dangerous; we can be told by someone who was once told by someone who was once told by someone. We can even work it out for ourselves, imagining how a scenario would play out. The upshot of this is that when a new technology or innovation – tar-pit avoidance, say, or the printing press – is developed, it can spread around human society much faster than the ability to grow winter coats can spread around a population of arctic foxes.\n\nThis might seem obvious. It is, really, obvious. But it’s profound: it means that the process of optimisation, becoming better at achieving the goals you want to achieve, has sped up.\n\nBut while humans can learn things, and exchange information, allowing us to spread information around far faster than any other organisms, that ability is still very limited. We can’t, as Toby Walsh noted, upload what we’ve learned into the cloud so that other humans can download it – we have to laboriously tell them, and if it’s not knowledge that can be easily transmitted in words, such as a physical skill or expertise in some domain, then they will have to learn it themselves. And we can’t improve the physical abilities of our brains, except in the most constrained and inadequate of ways. But an AGI might be able to reach inside itself and rewrite the algorithms that govern its thinking.\n\nThis – the self-modifying AI – is the basis of the idea of the ‘intelligence explosion’. As we mentioned, the concept was first fleshed out by I.J. Good, a British statistician and early computer scientist, in 1965:\n\n Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion’, and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make.³\n\nTechnology improves at an exponential rate. The time taken to double economic output, or to move a however-defined rung up the technological ladder, keeps getting shorter. If the AI you’ve just built is loads better at building AIs than you are, and it thinks an order of magnitude faster than you, and it doesn’t need to pause to sleep or get distracted by funny videos on Reddit, and all the stuff we’ve just discussed, then the speed at which the AI can make itself better – or make better versions of itself – will go up.\n\nThe other thing worth noting is that, at least according to Bostrom, Omohundro and MIRI, there are good reasons to think that an AI would want to improve itself. Self-improvement, or in Bostrom’s terms ‘cognitive enhancement’, is, for them, ‘an instrumental goal’, like the goal-preservation and self-preservation we discussed earlier. ‘Improvements in rationality and intelligence will tend to improve an agent’s decision-making,⁴ making the agent more likely to achieve her final goals,’ writes Bostrom. ‘One would therefore expect cognitive enhancement to emerge as an instrumental goal for many types of intelligent agent.’\n\nRelated to this is the goal of ‘resource acquisition’. To improve itself, and to do all the things it needs to do, almost any AI would need more stuff. Depending on its level of sophistication, it may not greatly matter what stuff – any atoms will do, as they can be rearranged through nanotechnology. Bostrom thinks⁵ that in a large variety of scenarios, this demand would be essentially unlimited: it can always keep on sending out von Neumann probes to new stellar systems and setting up franchises there, to turn the planets and asteroids there into new computer banks and paperclips (or whatever).\n\nThis has obvious implications for humanity. Yudkowsky has a much-quoted saying: ‘The AI does not hate you, nor does it love you, but you are made of atoms which it can use for something else.’⁶ If you, or the planet you live on, are of greater utility to the AI as reconstituted atoms than as you currently are, then that may be a problem.\n\nChapter 14\n\n ‘FOOOOOM’\n\nEarlier, we asked how long, once you have an AI that’s as smart as a human, does it take to make one that’s massively superhuman? We’ve just discussed two variables which might affect the answer to that question. One, how wide is the window of ‘human-level’ – how narrow is the distinction between the village idiot and Einstein? And two, how good at – and how keen on – self-improvement will it be? Bostrom discusses this at some length in Superintelligence. He distinguishes between three broad classes of possibility: a slow take-off, a moderate take-off, and a fast take-off.¹\n\nThe slow take-off is a timescale of decades, or centuries, between the first AI and global dominance. There would be a long period of adjustment; there would be time for political leaders to respond and society to adapt. ‘Different approaches can be tried and tested . . . New experts can be trained . . . [Infrastructure] could be developed and deployed,’ he writes. It’s unlikely that a company, group or nation could develop an AI and take it from human-level to superhuman in secret in this scenario: no human group can reliably keep a secret of that magnitude for decades. In the event of a slow take-off, most of the AI safety work that MIRI, FHI and so on are doing would be pretty much useless, because as the slow march from ‘quite bright AI’ to ‘vastly superhuman AI’ took place, it’d be fairly easy to come up with better solutions in the light of the actual situation.\n\nA moderate take-off happens over months or years. There could be enormous disruption – people and groups trying to take advantage of the changing situation. It’s possible that it might be kept secret, if it’s created by a small team in a university or a company or a military research group, for example.\n\nA fast take-off happens over days, or hours, or minutes. There’s no time to do anything. An AI comes online, and before anyone outside the building is aware of it, it’s become the dominant force on the planet.\n\n‘It might appear [that] the slow takeoff is the most probable, the moderate takeoff is less probable, and the fast takeoff is utterly implausible,’ says Bostrom. ‘It could seem fanciful to suppose that the world could be radically transformed, and humanity deposed from its position as apex cogitator, over the course of an hour or two.’ Every other major change like that – the Agricultural Revolution and Industrial Revolution are the examples he gives – take decades to millennia. Change of the kind implied by a fast or moderate take-off ‘lacks precedent outside myth and religion’.² But, he says, a slow take-off is unlikely. A fast take-off – an ‘explosive’ take-off, in fact, in his words – is much more probable.\n\nThere are two factors in how fast a technology is developed, he says. One is how hard it is to make progress in that technology – he calls this the ‘recalcitrance’. The other is how much effort and intelligence are applied to the problem – he calls this the ‘optimisation power’. Progress on fusion power is slow, despite large numbers of brilliant scientists working on it – so presumably it is an extremely recalcitrant problem. The speed at which progress happens in a scientific or technological field is a function of optimisation power divided by recalcitrance.\n\nAt the moment, AGI is extremely recalcitrant. It is possible that that will be the case for a long time – getting from the first human-level AI to the first dominant superhuman AI may be harder than getting to the human-level one in the first place. But it may well not be. For one thing, there’s the ‘parochial view of intelligence’ we looked at a few pages ago. Just as the window of human-level Go ability turned out to be quite narrow – AlphaGo stormed past it in a few months, and AlphaGo Zero in a few days – it might turn out that the work involved in building something as smart as a below-average-intelligence human is barely different from that involved in building something as smart as Einstein. ‘AI might make an apparently sharp jump in intelligence purely as the result of anthropomorphism,’³ writes Bostrom, ‘[which is] the human tendency to think of “village idiot” and “Einstein” as extreme ends of the intelligence scale, rather than nearly indistinguishable points on the scale of minds-in-general.’ We might think AI is stupid even as it creeps ‘steadily up the scale of intelligence, moving past mice and chimpanzees . . . because AIs cannot speak fluent language or write science papers’. But then it ‘crosses the tiny gap from infra-idiot to ultra-Einstein in the course of one month or some similarly short period’.\n\nThere are also obvious ways in which advances could be made. At the moment, we simply don’t know how to make an AGI; it wouldn’t matter how powerful a computer you gave someone, the problem is that we don’t have the algorithms. But if someone could create a software intelligence that was as smart as a stupid human, assuming it wasn’t run on a vast supercomputer that took up a prohibitively large amount of resources, it would be a trivial task then to give it loads more processing power, memory and so on so that it runs much faster.\n\nWould that make it superintelligent? Well, not in every sense: there are some things that need more ability. No matter how many thousands of years you gave it, a chimpanzee couldn’t understand Pythagoras’ theorem, but many 12-year-old human children can. But also, there are things that just need loads of time. An exam that’s difficult if you have an hour might be easy if you had a month. Some science and engineering problems are hard to solve because they involve lots of drudgery, so they’re expensive in researcher and lab time, not because they’re difficult in themselves. You could also simply copy the code and have several copies running simultaneously. ‘There’s a lot of interesting problems you can potentially solve if you have, say, the equivalent of 10 mediocre engineers working for 1,000 years on solving them,’ Rob Bensinger told me. When the first AGI is built, it might be on a relatively low-powered computer. Perhaps it will attract lots of excitement and new funding. Suddenly there might be money to buy loads more memory and processing power, and your AI might suddenly become a thousand times faster. Bostrom calls this a ‘hardware overhang’, where whatever the eventual solution to the AGI problem is only takes a small fraction of the hardware available at the time, and the resulting AGI can immediately become faster through the application of loads more CPUs and RAM.\n\nIt also might be the case, says Bostrom, that there’s just some piece of a software puzzle that’s missing for a long time. ‘[If] one key insight long eludes programmers, then when the final breakthrough occurs, the AI could leapfrog from below to radically above human level without even touching the intermediary rungs.’⁴ Bostrom calls this the ‘software overhang’.\n\nThere’s also a possible ‘content overhang’, in which an AI that can read human languages at high speeds would be able to take in a huge amount of information very quickly, simply by reading the internet. An AI that was faster at thinking than a human, but lacked anything like as much knowledge, might be as good at solving problems as a slower-thinking but more well-read human. But if it were able to read – and understand – the whole of Wikipedia in a few days or hours, it would rapidly become hugely more knowledgeable than any human. Bostrom points out that in 2011, IBM’s Watson, which won the Jeopardy! quiz show, did so by reading a huge amount of text and extracting relevant information from it. Whether it ‘understood’ that text is largely a semantic question; it was able to make use of the information within it. A future, human-level AI would be better at this task than Watson.\n\nThe difficulty of the problem is only one part of the equation, though. How much effort – or, in Bostrom’s stricter term, ‘optimisation power’ – you’re dedicating to solving the problem is the other. Even if the recalcitrance is increasing, the rate at which your AI gains intelligence could accelerate, so long as you’re able to apply more optimisation power to the problem than you were previously.\n\nAll, or at least most, of the optimisation power used in the development of the first, human-baseline AI will have come from its human designers. It may be that as the project becomes more exciting, more programmers and more resources are thrown at it, increasing its optimisation power and speeding up the process. But at some point, says Bostrom, the AI will become powerful enough to do most of its own modifications, and that is where things start to get interesting. After that point, any increase in its abilities becomes an increase in the amount of optimisation power applied to increasing those abilities. Instead of optimisation power growing arithmetically, an upward-slanting but straight line on a graph, it becomes an exponential growth curve, with power doubling at set intervals. To put arbitrary numbers on it, instead of growing from 1 to 2 to 3 to 4, it grows 1, 2, 4, 8 . . .\n\nHow long it takes to double is the key question. But in the decades that Moore’s law applied (how many decades that was and whether it still does is a matter of some debate), a roughly constant level of optimisation power led to a doubling of computing power\\* every 18 months. If we naively put something like that into this scenario, so that the AI is powerful enough to double its own computing speed every 18 months, and all that computing power goes straight back into optimising the AI, halving the time it takes each time, then after the first 18 months the next doubling would take nine months. The next, four and a half, then two and a quarter. By the tenth doubling, it would be doing it every 12 hours or so; by the twentieth, it would be every 45 seconds. By that stage it would be more than 100,000 times faster than when it started, and three years would not yet have passed. On a graph, the line showing computing power would be vertical at 36 months. That’s essentially what people mean by the ‘singularity’.\n\nObviously, that’s a deeply simplistic picture. There are millions of complicating factors (and no particular reason to take Moore’s law as a baseline other than its familiarity). But Bostrom – and Yudkowsky,⁵ and MIRI,⁶ and presumably I.J. Good – think that the complicating factors are at least as likely to speed things up as slow them down. Under the most extreme scenarios Bostrom considers, the doubling time might be seconds, rather than months. If he’s right, and a moderate or fast take-off is more likely than a slow one, then humanity would have very little time to adjust to the new reality once it arrives.\n\n\\*I know that the original Moore’s law formulation was: ‘number of components in an integrated circuit’, but it seems to translate fairly well into ‘petaflops per dollar’ and various other comparable things: see the Wikipedia page on Moore’s law, subsection ‘Other formulations and similar observations’.\n\nChapter 15\n\n But can’t we just keep it in a box?\n\nQuite often, when all the above stuff is raised with people, they say: well, that’s fine, but it’s simple, isn’t it? Just don’t let the AI do anything. If it’s simply a clever computer, then it can’t destroy the universe. I ended up having a lengthy conversation about this with a good friend who works in IT, and he was saying essentially that. How will an AI turn all the matter in the universe into paperclips if it can’t pick anything up?\n\nBut ordinary, non-superintelligent humans can and do have enormous power without making much use of their physical abilities. ‘Satoshi Nakamoto [the mysterious, pseudonymous Bitcoin creator] already made a billion dollars online without anybody knowing his true identity just by being good at math and having a bit of foresight,’ writes Scott Alexander. ‘He’s probably not an AI, but he could have been.’¹ Once you’ve got a billion dollars, you have quite a lot of power. And a superintelligence would be better at gaining and using that power than a non-superintelligent human.\n\nIt’s pretty easy to imagine ways in which a superintelligence could have enormous power with nothing more than an internet connection. ‘Imagine an AI that emails Kim Jong-un,’ says Alexander:\n\n It gives him a carrot – say, a billion dollars and all South Korean military codes – and a stick – it has hacked all his accounts and knows all his most blackmail-able secrets. All it wants is to be friends.\n\n Kim accepts its friendship and finds that its advice is always excellent – its political stratagems always work out, its military planning is impeccable, and its product ideas turn North Korea into an unexpected economic powerhouse. Gradually Kim becomes more and more dependent on his ‘chief advisor’, and cabinet officials who speak out about the mysterious benefactor find themselves meeting unfortunate accidents around forms of transportation connected to the internet. The AI builds up its own power base and makes sure Kim knows that if he ever acts out he can be replaced at a moment’s notice with someone more co-operative. Gradually, the AI becomes the ruler of North Korea, with Kim as a figurehead.\n\nAgain, this isn’t completely unlike things real humans have done: Alexander points to Grigori Rasputin, who became a shadowy power behind the throne of the last Tsar of Russia. But there are thousands of other ways in which an AI could do it: by creating a company, for example (Max Tegmark, in Life 3.0, imagines a superintelligent AI that makes loads of money designing really good movies and video games), hacking banks, whatever. It is cleverer than us, so it would come up with better ways. And there are ways of killing people without touching them.\n\n‘It’s not dangerous because it has guns,’ Yudkowsky said in an interview with Vanity Fair in 2017. ‘It’s dangerous because it’s smarter than us. Suppose it can solve the [problem] of predicting protein structure from DNA information. Then it just needs to send out a few emails to the labs that synthesise customised proteins. Soon it has its own molecular machinery, building even more sophisticated molecular machines. If you want a picture of AI gone wrong, don’t imagine marching humanoid robots with glowing red eyes. Imagine tiny, invisible synthetic bacteria made of diamond, with tiny onboard computers, hiding inside your bloodstream and everyone else’s. And then, simultaneously, they release one microgram of botulinum toxin. Everyone just falls over dead.’²\n\nThe answer, then, appears to be: just keep the AI ‘in a box’. Not a literal box, but a shielded system, unattached to the internet and in fact prevented from interacting with the outside world at all except via specific channels – for instance, a text-only screen. (It presumably has to have some connection to the outside world, or otherwise your super-high-tech AI-in-a-box might as well just be a box, and it becomes a bit pointless to have built it.) It could be put in a Faraday cage to stop it sending electronic signals; Bostrom suggests that it might be possible for an AI to generate radio waves by ‘shuffling the electrons in its circuitry in particular patterns’, and thus affect nearby electronic devices.³\n\nThat might work physically – although again, if you hadn’t previously thought of the Faraday cage, maybe there are other things you hadn’t thought of. ‘Each time we hear of a seemingly foolproof security design that has an unexpected flaw, we should prick up our ears,’ says Bostrom. But there’s a more obvious security flaw: the people reading its text output. Humans are not secure systems. ‘If the AI can persuade or trick a gatekeeper to let it out of the box, resulting in its gaining access either to the internet or directly to physical manipulators, then the boxing strategy has failed,’ says Bostrom.\n\nThis is a debate that goes back to the prehistory of the Rationalist movement, in the very early 2000s, when Yudkowsky, Bostrom and others were all still palling around on an email chat list called SL4. One of the other people in the group, a computer-science undergrad called Nathan, was interested in something they had been discussing. He thought it bizarre that everyone in the group seemed to assume that a superintelligent AI could talk its programmers into letting it out. ‘I just looked at a lot of the past archives of the list,’ he said, ‘and one of the basic assumptions seems to be that it is difficult to be certain that any created [superintelligence] will be unable to persuade its designers to let it out of the box, and will proceed to take over the world. I find it hard to imagine ANY possible combination of words any being could say to me that would make me go against anything I had really strongly resolved to believe in advance.’⁴\n\nYudkowsky took up the challenge. ‘Nathan, let’s run an experiment,’ he wrote. ‘I’ll pretend to be a brain in a box. You pretend to be the experimenter. I’ll try to persuade you to let me out. If you keep me “in the box” for the whole experiment, I’ll PayPal you $10 at the end.’ They did it via IRC – an old-school instant chat messenger, a precursor to Gchat– and set a minimum time limit of two hours, so that Yudkowsky had a chance to talk him around. There was a condition: ‘that neither of us reveal what went on inside . . . just the results (i.e., either you decided to let me out, or you didn’t)’. Yudkowsky was concerned that, if he won and told everyone what happened, other people would say, ‘that wouldn’t work on me’, and carry on thinking that it would be easy to keep the AI in the box. Nathan agreed.\n\nThe next message in the sequence of emails read simply:\n\n —BEGIN PGP SIGNED MESSAGE—\n\n I decided to let Eliezer out.\n\n Nathan.\n\n —BEGIN PGP SIGNATURE —\n\nUnderstandably, people were intrigued. The first reply read: ‘I haven’t been this curious about something for quite a while . . . could you at least mention in general what kind of technique was used?’ And Yudkowsky replied:\n\n No.\n\n Sincerely,\n\n Eliezer.\n\n‘The point of the experiment is not what Eliezer did,’ Paul Crowley explained to me. ‘As soon as you get into the thing of what Eliezer did, you can end up in a mindset of “We figured out what Eliezer did, and so now we’re safe.” But the whole point is keeping it a mystery.’\n\nThe AI-box experiment was repeated a few times. Yudkowsky did a couple more, one with a guy called David McFadzean, and one with a guy called Carl Shulman, winning both; he later said he did three more, and lost two.⁵ Each time there was money at stake, to ensure the players had skin in the game. The game has been played by other people; I know of one where the AI won, and one where the ‘gatekeeper’ won.\n\nNo one knows exactly what happened in there. But Nathan, the first player, broke the rules a bit by saying, ‘With AI, you \\*want\\* to let it out. Or else you wouldn’t have gotten the funding to breed it in the first place.’ And that’s probably the point. If you do have a superintelligent AI that might save the world or whatever, it does feel a bit ridiculous to keep it locked up. Once you have the AI, you’re going to want to use it. ‘If you think you have a friendly AI, if the AI turns to you and says, “OK, hey, I’m friendly, I want to achieve the things you want to achieve,” then what’s your plan?’ asks Paul. ‘Are you going to say “We’re going to keep you in the box until you admit you’re evil?”’\n\nBostrom, who has quoted the AI-box experiment in his work,⁶ describes it as an interesting anecdote. ‘The first idea that springs to people’s minds is, if you put an AI in a box and a big Faraday cage around it, and only communicate with it [by text], that surely has to be safe, right?’ he told me. ‘And this maybe undermines the confidence you have in that. You can see that even a human can talk themselves out of the box, and maybe you want to think harder about the safety of that set-up. It’s more like motivating further thinking and research, rather than an answer to anything.’\n\nI don’t expect anyone to be instantly convinced by the AI-box experiments. I’m not 100 per cent convinced myself; I’m as sure as I can be that Yudkowsky didn’t use any subterfuge or underhand tricks, but it’s not completely clear that the experiment really maps onto a real-world situation in which someone has an impossibly powerful AI in a box and isn’t sure whether to let it out, where the stakes are much higher. (Although the impossibly powerful AI in a box would be, as mentioned, much more convincing than Yudkowsky, and also able to offer much greater rewards.) But I do find, as Bostrom says, that it undermines my confidence. The use of ‘oracle’ AIs – superintelligences locked in boxes like this, which are limited to answering questions by text – might well be safer, but it seems optimistic to assume that they couldn’t talk us into letting them out of the box.\n\nChapter 16\n\n Dreamed of in your philosophy\n\nToby Walsh, the University of New South Wales professor of AI whom I spoke to, is extremely worried about AI safety. But he thinks that the paperclip maximiser scenario is the wrong thing to be concerned about.\n\n‘The scenario that I wrestle with,’ he told me, ‘is 3D-printed drones. You could fill a couple of trucks with these, ride them into New York City and say, “Kill every white person who’s here.” That could be their code.’ This is not – quite – possible with real-world technology right now, he says, but it probably will be in only a few years’ time. Autonomous weapons are weapons of mass destruction, he says, and should be outlawed like atomic, biological and chemical weapons are, by international treaties; he has led efforts to lobby the UN to issue a ban. He is concerned about a new ‘arms race’ as countries rush to build them: once you have autonomous weapons, human-operated weapons will be too slow to be of any use. Russia has an autonomous tank; in the DMZ between North and South Korea, he said, there are autonomous machine guns which will fire on anything human-shaped, and will kill you ‘from four kilometres away with deadly accuracy’. The technology for the sort of unstoppable drone swarm he fears is ‘not 20 years away. It’s 10 years away. Maybe five.’ But he is – not dismissive, exactly, of all the LessWrong apocalypse stuff, but certainly wary. ‘Most people who believe in the singularity aren’t AI researchers,’ he told me. ‘They’re philosophers. Researchers try to build the machines, and therefore appreciate some of the challenges.’\n\nI also emailed Rodney Brooks, another professor of AI, at MIT, asking to speak to him, because I knew he was highly sceptical of all this stuff, and he emailed back in an entertainingly grumpy fashion : ‘I regard the people making the claims [about AI risk], and [writing] books like yours, as “flat Earthers”.’ They are ‘completely and totally wrong’. ‘Focusing on AI diverts attention and wastes everyone’s time,’ he said, ‘while more immediate dangers abound. The destruction of democracy, the end of privacy, the subjugation of the masses to the few – all technology-based disasters.’ He felt that claims like Yudkowsky’s, that we face an existential threat from AI, ‘have no basis’, and this message ‘gets amplified by other people (e.g. you) writing about the baseless claims’. ‘A hype chain reaction has gone off,’ he added. ‘Each one of you begets 10 more of you.’ (I should say that Professor Brooks’ grumpiness was very self-aware and, I think, humorous. He signed off ‘Curmudgeonly yours, Rodney Brooks’. I didn’t get the impression that he was being rude, although nor did I get the impression that he would care a great deal if I did get the impression that he was being rude.)\n\nI think saying that concerns about this stuff are the preserve of cranks and philosophers is not entirely untrue, but not really fair, either. I spoke to Toby Ord of Oxford’s FHI – one of Bostrom’s close colleagues and someone who has spent a lot of time thinking about existential risks. (You met him in the Introduction; he puts AI as one of the top two risks, alongside bioengineered viruses.) He’s also a philosopher, so maybe I should declare that as an interest, but he took significant umbrage at the claim that AI risk is just being pushed by philosophers; it was the only time in our long conversation that he became anything other than cheerful and calm. That claim is either ‘disingenuous or extremely ignorant’, he said. ‘It’s hard to not be one or the other. I don’t want to be too harsh on [people making that claim]: it may not be quite disingenuous, but it’s plainly false. You’re forgetting Stuart Russell, for example.’ (Russell, who is a UCLA Berkeley AI professor, co-authored Artificial Intelligence: A Modern Approach, which we’ve mentioned before. He’s on record – many times – arguing that AI risk should be taken seriously.)\n\n‘This is actually why I like the survey that Nick Bostrom and Vincent Muller did,’ said Ord, which found that AI researchers, on average, think there’s a 10 per cent chance that AGI will arrive by 2022; a 50 per cent chance by 2040; and a 90 per cent chance by 2075.¹ And 18 per cent of respondents thought the impact would be ‘extremely bad (existential catastrophe)’, i.e. human extinction. ‘The point isn’t that it’s 18 per cent exactly,’ said Ord. ‘If we ran a survey again, we might get 17 per cent or 19 per cent or 20 per cent. The point is that it’s not 1 per cent. It is the case that typical AI researchers think there’s a very serious chance that their work is going to have immensely negative effects.’\n\nHe pointed to a few more people. I’d mentioned Slate Star Codex, the Rationalist Scott Alexander’s blog, earlier in the conversation. ‘Scott has a very good blog post listing all the AI researchers who think it’s a serious problem,’ Ord told me, which indeed he does: it’s called ‘AI researchers on AI risk’² and was published in 2015. ‘It’s a very long list, including, for example, [DeepMind co-founders] Demis Hassabis and Shane Legg. The world’s biggest AI company is run by people who think this is a real concern.’\n\nThere are several other high-profile names on the list, including Murray Shanahan. I asked him about it as well, and he said – with a lot of caveats – that AI risk really is a thing. ‘My view is probably not that far from the highly nuanced Nick Bostrom view, which is that there’s an important argument here that deserves to be taken seriously.’ His caveats were that it was probably quite a long time in the future, and may not happen at all. He was wary of oversimplification – ‘Elon Musk or someone drops a few tweets or soundbites in the media, and all of the nuances and hedges are totally and utterly lost’ – but he absolutely agreed that it is a ‘realistic issue’. ‘Even if it’s some time in the distant future, and with a low probability, the possibility of a very bad outcome means we still need to think hard about it,’ he said.\n\nI also asked Nick Bostrom, who is very much a philosopher too. And he said that at one point it was mainly philosophers who were worrying about AI risk, and that that probably wasn’t a bad thing: ‘At the very earliest stages, there is a certain need for conceptual work to be done, when it’s not clear yet what the problem is, even, or what the concepts are. You could think that superintelligence could be big, could be powerful, maybe it could be dangerous. But then how do you go from there to actually writing technical papers with math? How do you make progress on that?’\n\nYou need to break down this big, difficult question into its constituent parts, he said, and that’s where philosophy is useful, ‘as one discipline among others’. But ‘the further along we go, the greater the relative weight of computer science and mathematics, and that is what we now see happening’. For instance, Bostrom’s FHI now does technical seminars with people at DeepMind. ‘Every month we have them coming up here, or we go down there, writing papers together and so forth,’ he said. ‘With MIRI as well, and OpenAI, and there’s a group at Berkeley and another at Montreal. It is becoming more integrated with mainstream machine learning. I still think there is scope for more conceptual work to be done, but I think it does as we move closer become more continuous with machine-learning research and computer-science research.’\n\nI suspect that a lot of the arguments come down to a sort of difference in emphasis, rather than major differences in belief. Shanahan said something similar: ‘Rod Brooks might say, “This isn’t going to happen, this is absurd, it’s not going to happen for a hundred years,” but if you talk to Nick Bostrom, he’d say, “It’s really important that we think about this, because it might happen in only a hundred years.”’ This is a point that Alexander makes in his blog as well: ‘The “sceptic” position seems to be that, although we should probably get a couple of bright people to start working on preliminary aspects of the problem, we shouldn’t panic or start trying to ban AI research. The “believers”, meanwhile, insist that although we shouldn’t panic or start trying to ban AI research, we should probably get a couple of bright people to start working on preliminary aspects of the problem.’³\n\nThere do exist people like Brooks, who think it is ridiculous. And there are people like Toby Walsh, who worry very much about AI safety but who reckon that this is the wrong sort of AI safety to worry about. But I reason, cautiously, that it is fair to say that AI researchers don’t, as a body, regard it as stupid to worry about all this; a significant minority of them believe that there is a non-negligible chance that it could really mess things up. It’s not just a bunch of philosophers sitting around in Oxford senior common rooms pontificating.\n\nChapter 17\n\n ‘It’s like 100 per cent confident this is an ostrich’\n\nHaving read all this, you would still be entirely forgiven for thinking it is a bit angels-on-the-head-of-a-pin. We’re imagining how a sort of godlike robot-brain in the unspecified-time-from-now future might decide to behave.\n\nBut there are, right now, some little hints that AIs are going wrong in ways that are quite paperclip-maximiserish, albeit on a much smaller scale. A great paper was released on ArXiv¹ in March 2018, about digital evolution – machine-learning programs which come up with solutions to problems by mutation and selection in the same way that actual biological life evolved. Typically, there’s a little 3D virtual world, a little virtual avatar, and a task of some form, say: ‘Travel from location X to location Y.’ Then an algorithm is allowed to redesign the avatar using evolution; it copies it however many hundreds of times, each one slightly, randomly different (number of legs, size of feet, etc.), and then lets it try to complete the task. The avatars that are most successful – which walk fastest from X to Y – are then copied again, once more with random variations. And again and again. It’s exactly the same process – replication, variation and competition – as in real evolution.\n\nThe ArXiv paper was essentially a series of anecdotes from machine-learning researchers, about how they’d done exactly this and found that their little avatars had gamed the system. One was doing the sort of ‘walk from X to Y’ task we just talked about; it was trying to find innovative methods of locomotion. But the evolution algorithm just found that it was easier to design an avatar that was really, really tall: basically, a big rope with a weight on the top. When the simulation began, the avatar would just fall over in the direction of Y.\n\nAnother experiment involved trying to breed creatures to jump high, by giving them a task of ‘get your centre of gravity as high in the air as possible’. But again, the evolution algorithm found a loophole: it just built tall, thin, static towers with a heavy block on the end. The researchers tried to fix it by saying, ‘OK, your task is to get the block that started closest to the ground as far as possible from the ground.’ So the algorithm stuck with a tall, thin stick with a block on the end, but now just made it do a somersault, so its ‘foot’ ended up in the air.\n\nAt least those ones sort of found a solution to the problem that was set. They would be of no use in the development of a walking or jumping robot, but at least the little avatars did get from X to Y, or end up with a centre of gravity high in the air. It’s the other ones that are particularly funny-in-a-scary-way when you consider the context.\n\nThe algorithm GenProg, short for genetic programming, was given a task of fixing bugs in software. The software it had fixed was then run through a series of tests to see if it worked. The more tests it passed, the more offspring it had. In theory, that would lead to the evolution of better bug fixers. But GenProg found simpler solutions. One was that it was supposed to repair a sorting algorithm that was buggy; it was sorting lists into the wrong order. After GenProg ‘fixed’ it, the sorting algorithm was run through the battery of tests, and scored perfectly in each one: not a single list was out of order.\n\nBut when the (human) programmers went to check, they noticed that GenProg had, instead of fixing the program, just broken it completely. That made the program return an empty list, and an empty list can’t be out of order. Sorted! In another experiment, it was supposed to create text files which were as similar as possible to some target text files. ‘After several generations, suddenly and strangely, many perfectly fit solutions appeared, seemingly out of nowhere,’ the authors write. It turned out that one of the evolved versions had just deleted the target files, so it – and, subsequently, many other versions – could just hand in an empty sheet and get a perfect score.\n\nAnd, amazingly, one learned to win a computer Tic-Tac-Toe (noughts and crosses) tournament by forcing its opponents to crash. It tried to make impossible moves on imaginary points on the board, billions of squares away from the actual board. Doing that forced the other programs to try to represent a board billions of squares across in their memory; their memory wasn’t big enough, so they crashed, and the cheating algorithm won the game by default.\n\nI spoke to Holden Karnofsky of OpenPhil about AI risk, at the office the organisation shares with its parent, the charity evaluator GiveWell, on the fourteenth floor of a downtown San Francisco skyscraper with an extraordinary view out over the water, and he gave me another example of AI going wrong, right now, in unexpected but related ways.\n\nOpenPhil is the largest single supporter of Yudkowsky’s MIRI, across the harbour in Berkeley. It is a significantly slicker-seeming operation, though: it has that Google-style Bay Area corporate feel to it, if not quite as primary-coloured and futuristic. Its offices have confusing names: ‘Sesame Street’, ‘Deworming’, ‘Cage-Free Eggs’ (it turns out that these are examples of how philanthropy has changed the world). I was in Deworming, unsure what message that was meant to send me, watching ferries ply their way across the bay.\n\nHolden spoke to me for a while about ways in which AI can go wrong. One of the big breakthroughs in recent years has been image recognition: you’ll have noticed that suddenly your phone can sort your photos, with surprising success, into ‘pictures of you’ and ‘pictures of your husband’ and ‘pictures of your children’. The face stuff has been particularly impressive, to my mind, but AI can recognise more and more things. Even 10 years ago I remember people talking about how difficult it was to get an AI to tell a dog from a cat, say.\n\nBut now, they’re amazingly good. And that’s because they’ve been trained on hundreds of thousands of images of different things. But it’s been discovered that they can go wrong in surprising ways which, if there was more at stake than a misidentified image, might not be discovered until it was too late.\n\n‘A normal image classifier would look at this image,’ Holden said, ‘and it would say, “That’s a panda, I’m 57 per cent confident.”’ The picture he shows me is, indeed, that of a panda, from a 2015 paper by some Google AI researchers.² ‘But when you show it this picture, it says, “It’s a gibbon,” and it’s 99 per cent confident.’ The picture he then showed me was – as far as I could tell, in a full minute of staring at it – the same picture of the same panda. ‘You can’t even see the difference,’ he said. ‘The difference is this.’ He held up a picture of what looked like static, the sort you used to get on TV screens when they were tuned to dead channels, except in colour instead of black and white. Each pixel represented a tiny shift in colour between the two images. He showed me another image, of a bus, which the AI had declared with 99 per cent confidence to be a picture of an ostrich. It was honestly quite funny.\n\nWhat was going on was that the AI had learned to recognise the images by looking at lots of pictures of ostriches, or buses, or pandas, or whatever. And it had pulled out common features of each. But the common features were not the ones humans would recognise: wheel-arches, or long necks, or black-and-white fur. They were weirdly specific. It worked well on naturally occurring images, but could easily be thrown off – amusingly, catastrophically off – by ‘adversarial images’, images intended to deceive.\n\n‘It’s learned by training in a very narrow literal way,’ said Holden. ‘If you try to screw with it you can screw with it very easily.’ The static-y pictures were carefully generated to throw the AI off by changing tiny aspects of the image to bring up the weird, specific aspects that it thought of as ‘ostrich’ rather than ‘bus’. It’s an example of how an AI with an apparently simple goal (‘learn to recognise ostriches’) can go wrong in ways that you might not notice until it’s too late. ‘The concern this raises is that an AI could train on a dataset and when it sees new things, as long as they’re similar to the things it’s already seen, it’s fine. And if they’re different it can completely break down in utterly ridiculous ways.’ He gestured at the bus. ‘It’s like 100 per cent confident this is an ostrich.’\n\nOpenPhil is funding efforts to find ways of stopping these sorts of problems – ‘trying to get them to not classify buses as ostriches no matter what we do with the bus’. ‘We can see we have problems,’ he said. ‘So we’re going to try to solve the problems in the toys like this, and maybe it’ll make it safer later. Is it going to make a huge difference? I don’t know. But does it seem like it might? Yes.’\n\nBut it’s an example of something that could go wrong in strange ways – long before human-level AI, when an AI is in charge of, say, self-driving cars. ‘It’s not a situation you want to be in if, say, an AI is managing the power grid and something weird happens. Where there’s a little bit of space that wasn’t in the training set and it just totally melts down. Basically, if you imagine a future where you give an AI a goal and it’ll maximise the hell out of that goal, but you better hope that you specify that goal perfectly and nothing weird happens, it doesn’t see anything it didn’t see in its training – in that world I think you would be right to feel very scared.’\n\nPart Three\n\nThe Ways of Bayes\n\nChapter 18\n\n What is rationality?\n\nIn writing the Sequences, Yudkowsky’s goal, essentially, was to demonstrate that an AI could be intelligent (or rational, or good at ‘optimising’, or whatever you want to call it) without being remotely like a human intelligence. That required two things. First, he had to explain what rationality (or intelligence, or optimisation power) is. Second, he had to demonstrate how and why human intelligence is idiosyncratic and partially irrational, in order to show that it isn’t a template for all intelligences.\n\nFor Yudkowsky, intelligence/rationality is about matching your mental model of the world to the real world as closely as possible, and about making decisions that achieve what you want them to as often as possible. Both of these processes, he says, can be described using a simple equation called ‘Bayes’ theorem’. Here’s how all that works.\n\nFirst, we should discuss what Yudkowsky means by ‘rational’. There are two fundamental ideas underpinning ‘rationality’ as defined by the Rationalists. They are ‘epistemic rationality’ and ‘instrumental rationality’.\n\n‘Epistemic rationality’ is achieving true beliefs. Or, as Yudkowsky puts it, ‘systematically improving the accuracy of your beliefs’.¹ The Rationalists have a phrase for this: ‘The map is not the territory.’ Your mind contains thousands of models, which it uses to predict reality. For instance, I have a working model of gravity and air resistance and things which allows me (sometimes) to catch a ball that is thrown to me. Even more prosaically, I have a model which says, ‘The lamp is over there’ and ‘The door is behind me’ and ‘The window is in front of me.’ The degree to which I have an accurate model, the degree to which I can walk to where I think the door is and actually find a door there, is the degree to which my model corresponds with the world, or my ‘map’ corresponds with the ‘territory’. ‘This correspondence between belief and reality is commonly called “truth”,’ says Yudkowsky, ‘and I’m happy to call it that.’²\n\nI’ve used prosaic examples, but it applies just as much to more abstract ones. Whether or not black holes emit Hawking radiation is a question of fact. If your model predicts, as the late Stephen Hawking’s (complex, mathematical) model did, that black holes do give off radiation, then that is a statement about your mind. Whether your model is correct does not depend on how convincing your arguments for Hawking radiation are, or how strongly you believe it, but on whether, out there in the universe, black holes really do give off radiation. The only way to see whether your map corresponds with the territory is to go and look, or otherwise seek evidence. Insofar as the universe behaves in ways that your model predicts, your model is good; insofar as it doesn’t, it isn’t.\n\nInstrumental rationality, by contrast, is about your actions. ‘Rationalists’, says Yudkowsky, ‘should win.’³ The idea is the same as the definition of ‘behaving rationally’ in the textbook Artificial Intelligence: A Modern Approach, which we discussed in the Introduction. It is choosing that course of action which is most likely, given what you know now, to achieve the goal you want to achieve. It doesn’t mean, he says, selfish domination, or money, or anything specific. It means ‘steering reality – sending the future where you want it to go’. That could mean to your own selfish ends, or it could mean towards preventing climate change, or turning the universe into paperclips. It is about successfully doing what you wanted to do.\n\nThere are various corroborating mathematical ideas which we’ll come to in due course. But this is the most fundamental thing. Yudkowsky refers to the semi-legendary Japanese swordmaster Miyamoto Musashi, who said of his art: ‘You can win with a long weapon, and yet you can also win with a short weapon. In short, the Way of the Ichi school is the spirit of winning, whatever the weapon and whatever its size.’⁴\n\nInstrumental rationality doesn’t, necessarily, mean behaving in a ‘rational’ way, as defined by Hollywood and especially Mr Spock. Yudkowsky really doesn’t like Spock. ‘Consider Mr Spock of Star Trek, a naive archetype of rationality,’ he grumbles at one point. ‘Spock’s emotional state is always set to “calm”, even when wildly inappropriate.’ If you are about to be blown up by a Klingon torpedo, then being afraid might be rational. Worse than that, Spock’s ‘rational’ predictions, given in spuriously precise percentages, are usually wrong. ‘He often gives many significant digits for probabilities that are grossly uncalibrated,’ says Yudkowsky. ‘E.g.: “Captain, if you steer the Enterprise directly into that black hole, our probability of surviving is only 2.234 per cent.” Yet nine times out of ten the Enterprise is not destroyed. What kind of tragic fool gives four significant digits for a figure that is off by two orders of magnitude?’⁵\n\nInstead it means winning.\n\nLet’s return to Newcomb’s paradox for a moment, in which a superintelligent alien AI, Omega, comes to Earth and offers you two boxes, one transparent and with £1,000 in it, the other opaque and containing either £1,000,000 or nothing. Robert Nozick, the great American philosopher, wrote an essay on this problem which is notable, among philosophical essays, for its endearingly baffled tone. ‘I have put this problem to a large number of people, both friends and students in class,’ he writes. ‘To almost everyone it is perfectly clear and obvious what should be done. The difficulty is that these people seem to divide almost evenly on the problem, with large numbers thinking that the opposing half is just being silly.’⁶ At the end of his first section he asks people to stop reading to think about it themselves: ‘It is not that I claim to solve the problem, and do not wish you to miss the joy of puzzling over an unsolved problem,’ he says. ‘It is that I want you to understand my thrashing about.’\n\nOn the one hand, it’s obvious that you should ‘one-box’. You’ve seen Omega being right before. It is highly likely that it has predicted your decision correctly. If someone were watching you, and betting what you’d get if you picked both boxes, they would bet confidently and at high odds that you would only get £1,000. Worse than that, if you picked both boxes, and before the results were revealed were asked to bet what you will get, you would rationally bet that you would only get £1,000. ‘Knowing all this,’ says Nozick, ‘do you really want to take what is in both boxes, acting against what you would rationally want to bet on?’\n\nBut. But, but but. It’s obvious that you should ‘two-box’! Imagine that the far side of the £1,000,000-or-nothing box is transparent, and a friend of yours can see what’s in it, and is watching you make your decision. She’s been staring at this box for an hour, she can see there’s £1,000,000 in it, or nothing in it. Whatever is in the box, she will be hoping that you take both! If there’s nothing in it, pick them both: you’ll at least get £1,000. If there’s £1,000,000, pick both: you’ll get £1,001,000. The time at which Omega could affect it has passed! It’s already flown off to its home planet!\n\nSpock, I think, would two-box. It is not logical, Captain! The being has flown away. The boxes contain what they contain. And Nozick, after endless agonising and vast scrawls of mathematical notation which I can’t follow, says he would two-box.\n\nYudkowsky and the Rationalists would say: one-box. (I don’t know if Kirk would one-box, but Kirk seems to win even when Spock tells him he won’t. Certainly, Kirk is the better Rationalist than Spock, even though Spock is the ‘rational’ one.) There are mathematical reasons for this: an entire branch of decision theory which Yudkowsky has developed, in fact. But the underlying reason is that people who choose one box make more money than the ones who choose two. And that, assuming that you would rather have more money than less, is the whole thing. That’s the game. For one-boxers, says Yudkowsky, this ‘is a simple dilemma and anyone who comes up with an elaborate reason why it is “rational” to take both boxes is just outwitting themselves. The “rational” chooser is the one with a million dollars.’⁷\n\nThat’s the Rationalist rationality at its most basic, then: trying to believe things that are true, and trying to take decisions that lead to the outcomes you want. Obviously, it all sounds a bit underwhelming when I put it like that. We should go into it in a bit more depth.\n\nChapter 19\n\n Bayes’ theorem and optimisation\n\nFor Yudkowsky, the heart of rational behaviour is the simple mathematical equation known as Bayes’ theorem. When he talks about rationality, he is talking about Bayes; the project of improving human rationality is a project of making humans better Bayesians. The theorem is (he says, and decision theory agrees) absolutely central to what good decision-making involves. When you have evidence for something, that evidence allows you to shift your beliefs only as far – no more, no less – as the distance dictated by Bayes.\n\nThe Reverend Thomas Bayes is buried, rather appropriately, a few hundred yards from the offices of the Royal Statistical Society, in Shoreditch, east London. Bayes, a somewhat obscure eighteenth-century Presbyterian minister with a sideline in mathematics, wrote a couple of well-received books in his lifetime, one on theology, the other defending an aspect of Newton’s calculus from criticism by George Berkeley. The latter, according to Wikipedia at least,¹ appears to have been enough to have got him elected a Fellow of the Royal Society.\n\nNeither of these books are what he is remembered for. In later life he became interested in probability, and after he died his friend Richard Price edited his notes on the subject into an essay for the journal Philosophical Transactions.² It was called ‘An Essay towards solving a Problem in the Doctrine of Chances’, and contained within it a simple equation which underlies the whole of modern probability theory.\n\nBayes’ theorem goes: P(A|B) = (P(B|A)P(A))/P(B). Don’t worry if you can’t follow the notation, it doesn’t matter. It’s really very easy to understand. It’s working out how likely it is that statement A is true in the event that statement B is true. In full, it says that the probability of A given B equals the probability of B given A, multiplied by the probability of A on its own, divided by the probability of B on its own.\n\nThat probably didn’t help. But honestly, it’s very easy to understand. If we move it away from the realm of abstract letters, it’ll become clearer. Imagine you’ve got a blood test that screens for cancer. Let’s say that 99 per cent of the time, if you have cancer, it will tell you, correctly, that you have cancer. And 95 per cent of the time you test someone without cancer, it says, correctly, that they don’t have cancer. Knowing that, if the blood test comes back positive, what is the likelihood that you have cancer? It’s about 95 per cent, right?\n\nNo! The answer is you have absolutely no idea. There is not enough information given to provide you with the faintest clue what your chances of having cancer are. That’s because, without knowing how common the cancer you’re looking for is, you don’t know how common false positives will be.\n\nLet’s say one person in every 1,000 in the population has this cancer at any given time, and you run your test on a million people. On average, there will be 1,000 people among those million who actually have cancer. Your test will correctly identify that 990 of them have cancer. Of the 999,000 people who don’t have cancer, 5 per cent of them will be told that they do. That’s 49,950 people. So you’d get 50,940 positive results, but only 990 of those would actually have cancer. If you go into the clinic, and have a test that the doctor says (truthfully) is 95 per cent accurate, and it comes back positive for cancer, then, in this case, your chances of actually having cancer would be rather less than 2 per cent. (And, of course, 10 people will go happily home thinking they are healthy, but will in fact have cancer.)\n\nThe background rate of cancers in the population is, when we’re talking about Bayes’ theorem, your prior probability rate. Any new information pointing you towards some conclusion – say, a positive cancer test – is only useful in the light of your prior assessment of how likely that conclusion is. This is, to put it mildly, not obvious. If you hear that a test is 95 per cent accurate, it seems reasonable to assume that if it gives a positive result, there’s about a 95 per cent chance that it’s right. But that’s not true at all.\n\nIf you find this counterintuitive, don’t worry. So does everyone else. If anyone should get this stuff right, it should be doctors, who make decisions on the basis of cancer tests and background rates all the time. But, as Yudkowsky points out, they don’t.³ In one experiment, whose findings have been replicated several times, less than one doctor in five gave the right answer to a similar question; nearly half of them said 95 per cent, and the average guess was 55 per cent, a 30-fold overestimate of the true answer.⁴\n\nFor Yudkowsky and the Rationalists, Bayes’ theorem essentially is rationality: every decision-making process is successful insofar as it emulates Bayes. ‘Eliezer’s position is that every successful process owes its success to “moving in harmony with the Bayes”, as he’d say,’ comments Paul Crowley. ‘That probability theory says that this is the only place success comes from.’ Any process which moves steadily closer towards true answers and successful decisions must, says Yudkowsky, be doing so in a Bayesian way. In Yudkowsky’s own words: ‘[If] a mind is arriving at true beliefs, and we assume that the Second Law of Thermodynamics has not been violated, that mind must be doing something at least vaguely Bayesian – at least one process with a sort-of Bayesian structure somewhere – or it couldn’t possibly work.’⁵\n\nI asked him about this, and he compared it to the laws of thermodynamics. There’s a theoretical device called a Carnot engine, an idealised version of a heat engine – something like an internal combustion engine or a steam engine, something which uses thermal energy to do mechanical work. According to the laws of thermodynamics, if you had a perfectly efficient heat engine, there is a maximum amount of work you can get out of a given amount of energy. ‘Cars don’t run on ideal engines, but they can’t violate thermodynamics,’ says Yudkowsky: any real-world engine must be less efficient than the perfect Carnot thought experiment.\n\nHe says this is analogous to how Bayes’ theorem relates to decision-making: it is ideal decision-making. It is what decision-making looks like when it happens impossibly perfectly. ‘This is a point that confuses people who think in terms of a toolbox and think Bayesian methods are just one more tool in the box,’ says Yudkowsky. It’s not. All the other things in the decision-making toolbox are useful insofar as they approximate the Bayesian equation. Any process that uncovers true facts or makes good decisions is doing something Bayesian, whether it’s evolution or human thought or anything else. Anything that looks like decision-making must run on something like Bayes. ‘To the extent that [decision-making processes] work at all, they must necessarily work because they have bits of Bayesian structure embedded in them.’\n\nIn the Carnot-engine analogy, ‘Bayes is akin to the laws of thermodynamics,’ Yudkowsky says, ‘and a little program that directly implements Bayes’ rule is like a thermodynamically ideal Carnot engine.’ Just as no car can ever really run on a Carnot engine – energy will always be lost into the world – so no decision-making system can ever be a perfect Bayesian one. ‘The algorithm that uses the work perfectly is too expensive to implement,’ says Yudkowsky. ‘But some of that work must be performed somewhere, or the “car” doesn’t move at all.’\n\nSo, for instance, evolution. The astronomer Fred Hoyle used to say that the chances of evolution producing life were like those of a whirlwind passing through a junkyard and producing a Boeing 747.⁶ The number of ways in which the components of a 747 can be arranged are unimaginably huge, and only a tiny, tiny fraction of a fraction of a percent of them would be able to fly; likewise, if you took apart, say, a lemur, down to its constituent cells, and put it back together again at random, you would be vanishingly unlikely to create with any great success something that swung through trees.\n\nBut, of course, 747s aren’t made by whirlwinds, they’re designed by humans, who are good at looking at a pile of aerospace equipment and immediately ruling out the overwhelming majority of possible combinations. And, despite Hoyle’s misunderstanding, lemurs aren’t made at random, but by the slow, inefficient but still non-random process of evolution by natural selection.\n\nYudkowsky describes both human intelligence and evolution as optimisation processes. An optimisation process is a way of moving through a huge space of possibilities to get close to, and hit, the target that you want.\n\nAnd that’s what the Bayesian equations are doing. With the cancer test we discussed above, the original space is quite large – a million people who may have cancer – and the target is quite small – 1,000 people who actually have cancer. Your amazingly accurate cancer test does not magically provide you with true positives, but it allows you to narrow your space. Your prior probability of any random person you grab having cancer is one in 1,000, or 0.001. You do your test, you eliminate about 994,000 possibilities; the chance of any given person is now about 0.02. You’ve narrowed your space down to about one-twentieth of the size it was. If you were to do the test again (assuming the false positives were random, not systematic), then you’d narrow it down still further. You are optimising your search, incrementally closing in on the truth. The phrase that gets used a lot is ‘probability mass’: what weight of probability do you assign to each possible outcome? Before you did your cancer test, you put only 0.1 per cent of your probability mass on the outcome ‘cancer’ and 99.9 per cent on the outcome ‘no cancer’. The test allows you to shift some of the mass; you now put 2 per cent of your probability mass on ‘cancer’ and only 98 per cent on ‘no cancer’. Your probability mass always has to add up to 1, to a 100 per cent chance; each new piece of evidence just needs to shift it around between options.\n\nHuman intelligence does the same thing. If you took all the parts of a 747 and put them together at random, the chance of any one of the planes flying would be – I don’t know. Ridiculously tiny. Something on the order of millions of zeroes after the decimal point, I expect, depending on what counts as a ‘part’ of a 747: if you take one figure I heard, that a 747 has 6 million parts, then Wolfram Alpha says that the number ‘6 million to the power 6 million’ has around 40 million digits. The probability mass I would assign to ‘spontaneous generation of a 747’ is unimaginably small.\n\nBut if you said to a human, ‘Arrange those parts in such a way as they might fly’, your odds would go up immensely. Even if it were me, I’d know to do things like put any large, flat panels in wing-like configurations on the side. I’m sure my chances of making a flying thing out of a junkyard full of aero parts would be, gosh, at least 1 in 100 million. If the person putting the parts together were an engineer, rather than a glorified typist, that chance would drop even further; if she were an aeronautical engineer it might even reach close to evens. And if that engineer is allowed to have several goes, and test each version, and incrementally improve it, and ask for help and read books and so on, then it would become a near-certainty.\n\nIt’s the same with evolution. The chance of any random arrangement of organic molecules forming a living, breathing animal, or any complex creature, is extremely small: there is a huge number of possible arrangements, the ‘space’ you are searching in, and only a microscopically tiny fraction of those will ‘work’.\n\nBut if you already have a simple, self-replicating thing, which makes slightly imperfect copies of itself, then it will start to ‘search’ that space. The copies which are worse at replicating will tend to be eliminated; the copies which happen to be better will tend to spread. The diverging types will tend, simply by random movement being pruned by non-random selection, to move towards those ‘areas’ of the ‘space’ which represent functioning organisms. Your probability of finding a working organism if you throw together a random agglomeration of organic parts is essentially zero; your probability of finding one after a few million years of evolution from a simple replicator is close to 1.\n\n(Note: you still need to have a simple replicator, created by some random or at least natural process. Evolution doesn’t explain the very start of life. There are various ways it might have arisen: Nick Lane, a professor of biochemistry at UCL, thinks that very simple cells with the right sort of chemical and energy gradients could have formed naturally in vents at the bottom of the sea. But whatever it was, it only had to happen once, in millions of years, across a whole planet. Billion-to-one chances aren’t unlikely if you have a billion chances. ‘Since the beginning’, goes one Rationalist haiku, ‘not one unusual thing / has ever happened’.)\n\nBayes’ theorem is extremely useful from a philosophical point of view. I studied philosophy at university, and there were endless arguments about the ‘problem of induction’. The idea was that you could see a million white swans, but you would never be able to prove the statement ‘all swans are white’, because it would take seeing just one swan which was black – which Western explorers did when they first reached Australia – to disprove it. No amount of ‘inductive reasoning’ – coming to conclusions from evidence – could ever prove anything.\n\nBut Bayesian thinking lets you sidestep this altogether. You simply learn to think probabilistically. Having never seen a swan, you might assign a prior probability to the hypothesis ‘all swans are white’ of, say, 1 per cent. (All swans could be green, for all you know.) You see your first swan, you update your prior probability in the light of new evidence: you might think that it’s now 15 per cent likely that all swans are white. (You’ve only seen one swan. They could come in all sorts of colours.) That is now your new prior.\n\nBut after wandering around Renaissance Europe for 40 years, only ever seeing white swans, and constantly updating your priors, you are now much more confident in the statement. As a good Bayesian, you’re never certain, but you’ve seen thousands of swans, each one adding a small dollop of evidence to support your hypothesis, so you push your confidence up to a very solid 95 per cent.\n\nThen you get on a boat to Van Diemen’s Land, and you see a black swan. Your confidence immediately plummets to 0.01 per cent. The problem of induction isn’t a problem any more, as long as you’re willing to think in terms of likelihoods and probabilities, rather than certainties. You’re never certain – someone might be painting all those black swans black, or you might be hallucinating – but the more swans you see, the more you can update your priors and increase your confidence. The Rationalists think of all knowledge in these terms: how confident you can be in your beliefs, how much ‘probability mass’ you should assign to some proposition, and how much you can ‘update’ your beliefs in the light of new evidence.\n\nThere’s another philosophical problem, the ‘paradox of the ravens’, which you can also solve with Bayesian reasoning. It starts similarly. The statement ‘all ravens are black’ is logically equivalent to ‘if something is not black, it is not a raven’. That’s because anything that renders the first statement untrue would also render the second one untrue, and vice versa. But that leads to a strange situation. Seeing a black raven should count as evidence for the statement ‘all ravens are black’. But if that’s true, then seeing a non-black non-raven (say, a purple hat) counts as evidence for the statement ‘if something is not black, it is not a raven’. And if that’s true, and the statements ‘all ravens are black’ and ‘all non-black things are non-ravens’ are equivalent, then seeing a purple hat is apparently evidence that all ravens are black.\n\nAgain, this has been argued about for years; the thought experiment was first proposed in the 1940s. But with Bayesian thinking, it’s nice and straightforward. The purple hat is indeed evidence. It’s just not very much evidence. You adjust your prior an infinitesimal amount and carry on. Absence of evidence is, in fact, evidence of absence, even if not strong evidence. (I should say: this isn’t something the Rationalists came up with. Bayesian solutions to the problem of induction and the paradox of the ravens are decades old. And people still get very angry about them and argue that they’re wrong. They’re not the final word. But, to my mathematically ungifted mind at least, they seem to provide commonsensical ways around the problems.)\n\nAs we saw at the beginning of the chapter, for Yudkowsky – and for decision theorists such as E.T. James, author of Probability Theory: The Logic of Science, whom Yudkowsky frequently cites as an inspiration – the Bayesian equation is the iron law of decision-making. There is a correct amount by which you should shift your beliefs in the light of new evidence. Shifting your beliefs by more or less is simply wrong.\n\nAn example that Yudkowsky uses in the Sequences is the lottery.⁷ Imagine a box, he says, that beeps every time you choose a winning lottery ticket. It’d be no use whatsoever if it also beeped every time you chose a losing ticket, obviously. But it doesn’t – it only beeps 25 per cent of the time when you have a losing one. So, in a lottery with six numbers and 70 balls, there are 131,115,985 possibilities. You write down your lottery numbers. The machine beeps. What are your odds?\n\nWell, it’ll beep on the correct one, but it’ll also beep on one-quarter of the 131,115,984 other ones, leading to an average total of 32,778,996 false positives. Your 75-per-cent-accurate test lets you move from 1/131,115,985 to 1/32,778,996. That is how much you can update your beliefs. Any more and you’re overconfident, any less and you’re underconfident.\n\n‘You cannot defy the rules,’ writes Yudkowsky. ‘You cannot form accurate beliefs based on inadequate evidence. Let’s say you’ve got 10 boxes lined up in a row, and you start punching combinations into the boxes. You cannot stop on the first combination that gets beeps from all 10 boxes, saying, “But the odds of that happening for a losing combination are a million to one! I’ll just ignore those ivory-tower Bayesian rules and stop here.” On average, 131 losing tickets will pass such a test for every winner.’⁸\n\nThis is a toy example, obviously. It’s rare that you find situations that are so neatly mathematically defined in real life. You’ll have to use a lot of guesswork and intuition. But even when you can’t measure the odds so precisely, when you are presented with evidence (‘The driver in front of me is driving erratically’) for a hypothesis (‘The driver in front of me is drunk’), there is a correct amount of confidence that you are allowed to have, based on how common drunk drivers are, how often drunk drivers drive erratically, and how often people drive erratically for other reasons.\n\nTrying to believe on less evidence, Yudkowsky writes, is ‘like trying to drive your car without any fuel, because you don’t believe in the silly-dilly fuddy-duddy concept that it ought to take fuel to go places. You can try, if that is your whim,’ he says. ‘You can even shut your eyes and pretend the car is moving. But to really arrive at accurate beliefs requires evidence-fuel, and the further you want to go, the more fuel you need.’\n\nIt’s not that humans don’t do something like this – of course we assess evidence and come to beliefs on the basis of that evidence, although usually not by consciously calculating probabilities. (And sometimes we do believe things without evidence, and even celebrate that, a process we call ‘faith’ and which Yudkowsky would probably call sitting in your car in your driveway, insisting you’ve driven to Dorset.) But when we change our beliefs in the light of new evidence, even if it feels like an instinctive, gut-level process, we are performing something approximating to Bayes, and we are either getting it right or wrong – we are updating our beliefs the right amount, or too much, or too little.\n\nAnd, similarly, an AI would be ‘intelligent’, or rational, insofar as it applied Bayes’ rules.\n\nChapter 20\n\n Utilitarianism: shut up and multiply\n\nSo you have your means of updating your beliefs according to how much evidence you get, in Bayes’ theorem. But there’s another element you need, says Yudkowsky, which is establishing how much something matters. You can work out how many people have cancer using your Bayesian test, but in order to make a decision about how much money to spend treating those cancers, you have to think about how much good treating them will do.\n\nThis is where it all gets into moral philosophy. And in fact, the thing I like most about the Sequences is that they remind me of one of those enormous eighteenth-century works of philosophy, by Spinoza or Leibniz or someone, that set out to explain pretty much everything. There’s this sprawling, ambitious feel to them. The nature of consciousness, the nature of reality, evolution, human psychology, probability, morality. It all feels – to me, at least, and I’m not completely clueless about these things, although it’s more than a decade since I did any academic philosophy – like a robustly commonsensical application of modern science to philosophy. David Hume would probably have enjoyed it.\n\nThe ‘morality’ aspect is particularly interesting. Yudkowsky deliberately tries to take it away from the meta-ethical, finding-esoteric-flaws-in-the-ethical-system stuff and into basic numbers. I’ll go into what I mean by that in a second, but I think it’s most simply expressed in a tweet by someone else entirely, a web developer called Mason Hartman, who was talking about the ethics of self-driving cars:\n\n Philosophy: so sometimes it goes haywire & ends up—\n\n Me: do the thing that kills fewer people\n\n Philosophy: but it’s very salient th—\n\n Me: do the thing that kills fewer people\n\n Philosophy: but the human element of control is—\n\n Me: do the thing\n\n Me: that kills\n\n Me: fewer people¹\n\n(Yudkowsky retweeted it, and I know retweets ≠ endorsement and all that, but I’m pretty sure this one does.)\n\nEssentially, he (and the Rationalists) are thoroughgoing utilitarians. Do the thing that (you reasonably expect will) kill the fewest people/make the most people happy/cause the least pain. You can think about it in more detail than that, they would say; but if your thinking pushes you away from doing that, then your thinking has probably gone wrong.\n\nUtilitarianism is the moral philosophy, most associated with Jeremy Bentham and John Stuart Mill, which claims (in Bentham’s words) ‘that the greatest happiness of the greatest number is the foundation of morals and legislation’.² Nowadays most utilitarians don’t talk about ‘happiness’ quite so much. If we naively take ‘happiness’ at face value, it might be the ‘moral’ thing to do to plug everyone into machines that artificially stimulate the pleasure centres of our brains, but most of us would not want that done to us, so most utilitarians now talk in terms of ‘utility’, a sort of code for ‘what we want out of life’. Instead of a life of artificially induced bliss, I might prefer a life in which I gain a sense of achievement via actually doing things; modern utilitarianism would award moral points for systems which allowed me that life, instead of sticking me into the pleasure machine.\n\nThe Yudkowskian take on it all is admirable, I think, for two reasons. One, it accepts with equanimity one of the really hard conclusions of utilitarianism; two, it gives reasonable, sensible ways around two others. I do not suggest for a second that a series of blog posts written between 2007 and 2009 has answered the big questions of morality that have been batted around by philosophers for 3,000 years, but it fits very neatly with my own intuitions of morality. (That said, I remember in my very first philosophy lecture in my very first year of undergrad at the University of Liverpool in 2001, our head of department, the excellent Professor Stephen Clarke, warned us: in philosophy, you often read an argument and think, ‘Yes, I agree with that, that makes complete sense.’ Then you read another argument which entirely contradicts the first argument, and you think, ‘Yes, I agree with that too.’ Be wary of agreeing with things you’ve just read is, I suppose, the lesson.)\n\nThe first problem is the following. A key tenet of utilitarianism is that utility can, in some way, be compared between people. The Rationalists talk in terms of ‘utilons’, imaginary measures of utility; earlier utilitarian philosophers use the term ‘utils’. Obviously you can’t really measure them, but you can do thought experiments by putting rough estimates on things: you could imagine that finding £10 on the street is worth one util, say, while getting a job you love is worth 5,000 utils. Giving 5,000 people £10 would then be equivalent to finding someone a job they loved.\n\nBut this leads to a difficult situation. Say we imagine something that causes a huge loss of utils for one person – something like being horribly tortured for 50 years. And imagine something that causes a tiny, negligible loss of utils – for example, ‘suppose a dust speck floated into your eye and irritated it just a little, for a fraction of a second, barely enough to make you notice before you blink and wipe away the dust speck’.³ If there is anything to this form of utilitarianism, if it means anything at all to say that one experience can be compared to another in some sense, then some sufficiently large number of people getting dust in their eye is worse than a person being tortured for 50 years.\n\nHere’s a large number. It’s a large number that gets thrown around a lot in Rationalist blog posts as a sort of shorthand for ‘big. Really big. You just won’t believe how vastly, hugely, mind-bogglingly big it is’-type numbers. The number is 3↑↑↑3. Here’s what that means: 3↑3 means ‘three to the power three’, three times itself three times. That’s 27. 3↑↑3 is ‘three to the power (three to the power three)’, three times itself 27 times. That is 7,625,597,484,987 (getting on for 8 trillion, if you prefer words). 3↑↑↑3 is . . . I lose track a bit, to be honest. Here’s Yudkowsky: ‘3↑↑↑3 is an exponential tower of 3s which is 7,625,597,484,987 layers tall. You start with 1; raise 3 to the power of 1 to get 3; raise 3 to the power of 3 to get 27; raise 3 to the power of 27 to get 7,625,597,484,987; raise 3 to the power of 7,625,597,484,987 to get a number much larger than the number of atoms in the universe, but which could still be written down in base 10, on 100 square kilometres of paper; then raise 3 to that power; and continue until you’ve exponentiated 7,625,597,484,987 times.’⁴ This is a very, very large number.\n\nSo is that number of people having to blink a little worse than someone literally being tortured for 50 years? If you think there is any sense in which utils exist, that experience A can be traded off against experience B, then surely a number as enormous as 3↑↑↑3 is enough to bridge the gap between torture and dust specks. (And if it’s not, how about 3↑↑↑↑3? That is rather a lot bigger.)\n\nBefore we go any further, think which you’d pick. Torture? Or an incomprehensibly large number of dust specks?\n\nYudkowsky’s blog post on this, ‘Torture vs Dust Specks’, was one of the most controversial of all the hundreds in the Sequences. He ends it by saying: ‘Would you prefer that one person be horribly tortured for 50 years without hope or rest, or that 3↑↑↑3 people get dust specks in their eyes? I think the answer is obvious. How about you?’ For the avoidance of doubt, his ‘obvious’ answer is that the dust specks are worse than the torture. In the comments, Robin Hanson agrees, but almost everyone else argues the opposite.\n\nThe argument against is that there is no continuity; that you simply can’t compare this sort of minor inconvenience to decades of torture. But Yudkowsky argues in a follow-up that this is incoherent. He starts by assuming that we’re dealing with a much smaller number of dust specks than 3↑↑↑3: a googolplex. (A ‘googol’ is 1 followed by 100 zeroes; a googolplex is 1 followed by a googol zeroes. It is a big number, but much, much smaller than 3↑↑↑3.) ‘Suppose you had to choose between one person being tortured for 50 years, and a googol people being tortured for 49 years, 364 days, 23 hours, 59 minutes and 59 seconds,’ he says. ‘You would choose one person being tortured for 50 years, I do presume; otherwise I give up on you. And similarly,’ he continues, ‘if you had to choose between a googol people tortured for 49.9999999 years, and a googol-squared people being tortured for 49.9999998 years, you would pick the former.’\n\nYou can carry on doing this, he says. You can keep gradually reducing the amount of torture per person, while exponentially increasing the number of people being tortured, ‘until we choose between a googolplex people getting a dust speck in their eye, and [a googolplex divided by a googol] people getting two dust specks in their eye’.⁵ If you think that the former is worse than the latter, then you’re committing to the idea that the 3↑↑↑3 dust specks are worse than the torture, or that there is a sharp discontinuity at some point where, say, 23.6652647 years of torture for one person is worse than 23.6652646 years of torture for a googol people.\n\nI find it difficult to feel, on an intuitive level, that dust specks could add up to torture. But the way I think about it is this. Every tiny little bit of discomfort presumably makes life a tiny little bit less worth living. There is presumably some threshold between ‘life worth living’ and ‘life not worth living’. With a vast number of people like 3↑↑↑3, or even a mere googolplex, the number of people tipped over that threshold by a minuscule discomfort like a dust speck would be enormous; quadrillions of people, septillions, I literally have no idea except that it would be vast. Again, I don’t claim that this is the final word on a problem that utilitarian/consequentialist philosophers have kicked around for centuries, although I do know there are professional moral philosophers who would argue in favour of choosing the torture over the dust specks.⁶ All I can reasonably say is that it fits my intuitions – or, more precisely, that when I follow through the arguments I find that rejecting the dust-specks-are-worse-than-torture position violates my intuitions more severely than the alternative.\n\nThe second hard-to-swallow endpoint of utilitarianism is what the British philosopher Derek Parfit, who died during the writing of this book, called ‘the Repugnant Conclusion’.⁷ Imagine you’ve got a population of a million people living happy lives with loads of resources, says Parfit. Then imagine you add one person whose life is pretty bleak but slightly better than being dead, and redistribute the resources around everyone fairly. By the logic of utilitarianism, you’ve added utility to the system, so the million-and-one is better than the million, even though the average happiness has gone down slightly.\n\nBut then you do it again, and again. Taken to its logical conclusion, a universe containing a trillion (or a googolplex, or 3↑↑↑3, or however many) lives that are grim and unrewarding and dull but just about better than being dead is a better universe, morally speaking, than one with a billion people living extremely rich, fulfilling lives. That feels wrong to me and, I suspect, to most people.\n\nParfit phrased it like this: ‘For any possible population of at least 10 billion people, all with a very high quality of life, there must be some much larger imaginable population whose existence, if other things are equal, would be better even though its members have lives that are barely worth living.’\n\nThere have been various attempts to circumvent the Repugnant Conclusion – for instance, arguing that average happiness should be taken into account to some degree. Most of the potential solutions have problems of their own and, of course, philosophers have kicked them all around for decades. (I think it is fair to say that every attempt at a coherent philosophical system of ethics leads eventually to some awful conclusions, which, as we’ll see later, generally involve running one or more people over with a railway trolley.) But Yudkowsky approaches it in a way I hadn’t seen before, although I’m sure it’s not completely original. He argues that the apparent force of the Repugnant Conclusion comes from its ‘equivocating between senses of barely worth living’.\n\n‘In order to voluntarily create a new person,’ he writes, ‘what we need is a life that is worth celebrating or worth birthing, one that contains more good than ill and more happiness than sorrow – otherwise we should reject the step where we choose to birth that person.’ We should celebrate the birth of a new person we have voluntarily chosen to create: ‘Each time we voluntarily add another person to Parfit’s world, we have a little celebration and say with honest joy, “Whoopee!”, not “Damn, now it’s too late to uncreate them.”’\n\nIf we are saddened to hear the news that a person exists – if their life is sufficiently not-awful that they don’t actually want to end it, but still bleak enough for us to feel it is not a joyous thing that they have been born – then we are still obliged to try to take care of them, and improve their lives in such ways as we can. But for bringing new people into existence, we should have a higher bar.\n\n‘And then the rest of the Repugnant Conclusion – that it’s better to have a billion lives slightly worth celebrating, than a million lives very worth celebrating – is just “repugnant” because of standard scope insensitivity [see ‘What is a “bias”?’]. The brain fails to multiply a billion small birth celebrations to end up with a larger total celebration of life than a million big celebrations.’⁸\n\nI am entirely confident that moral philosophers could dig into this approach and find ways in which it would lead inevitably to stipulating that we torture children or something. (One comment under Yudkowsky’s blog post suggested that it could lead to the Sadistic Conclusion, which is that it would be better to create a small number of people living lives not worth living than a large number of people whose lives are just barely worth living.) But, again, to me it feels like a relatively sane way around the problem.\n\nThe third and final problem of utilitarianism is that of ‘ends justifying the means’. The classic example is that, if we could cheer up 99 per cent of the population by blaming the remaining 1 per cent for their problems and then imprisoning and torturing that 1 per cent for their imagined crimes, then (assuming that the gain in happiness for the 99 per cent outweighs the loss for the 1 per cent) that would be a moral thing to do. (This is called the ‘tyranny of the majority’, and John Stuart Mill, one of the first and greatest proponents of utilitarianism, raised a concern about it in his 1859 book On Liberty.)\n\nThere are lots of other possible examples. The trolley problem is intended to divide people down deontological or utilitarian lines: if you see a railway trolley heading towards five workers on a track, and you can pull a switch so it goes the other way, but there is one person working on that line, should you do it? A utilitarian should, in theory, say ‘yes’, but a deontologist (someone who follows strict moral rules rather than considering consequences) should say that you never actively kill someone, so you shouldn’t pull the switch even though you would save lives. (This is an enormous oversimplification of both deontology and utilitarianism.)\n\nYudkowsky approaches the trolley problem like this. Sure, he says, it might be the case that you think you can save five lives by killing one. (Or that you can help the poor by robbing a bank, or that you can improve society by staging a military coup and taking over, or any one of 100 versions of ‘I can justify Bad Thing X by promising Good Consequence Y’.) But knowing humans, it is very unlikely that you are right – or that you are likely enough to be right that, if you did it a million times, you’d overall prevent more harm than you caused.\n\nIn the trolley problem, the philosopher stipulates that you know with certainty that your action will save five and kill one and there’s no other way around it. But in reality, your inadequate human brain can’t ever be certain enough that that’s the case. You’ve evolved, as a human, a whole range of systems for creating moral-sounding reasons for doing self-interested things. You are more likely to do good, overall, by implementing the rule ‘Never kill anyone’ than by trying to work out the maths of utilitarianism on the fly in sudden, stressful situations. And that ends up creating odd-sounding meta-rules, such as ‘For the good of the tribe, do not murder even for the good of the tribe.’⁹ It is more likely that the thing you think of as being for the good of the tribe is in fact for the good of you.\n\nYudkowsky doesn’t broach the specific topic of ‘imprisoning and torturing 1 per cent to cheer up the 99 per cent’ in a blog post, and since that seemed the most obviously controversial application of utilitarianism, I asked him about it. He replied saying that my numbers were silly. ‘Immiserating 1 per cent of the population seems like it would do more than 99 times as much damage to each member of that population as the vague, passing pleasurable thoughts in the 99 per cent,’ he said. ‘Like, even linearly adding up the pleasure and pain by intensity and number of seconds will say, “No you should not do that”.’ A better example, he suggested, might be ‘asking about immiserating 1,000 people on all of Earth, or one person’.\n\nHe suggested that people (particularly and especially me, or at least that was the impression I got) may not be ‘smart enough’ to try to implement ‘utilitarianism’ in a way that ‘is actually utilitarian’. For a start, we don’t tend to run the numbers in the way he mentioned above; we might just hear, ‘policy X makes a large number of people happy’ and then think, ‘therefore utilitarianism demands it’, without considering its other effects.\n\nMore interestingly, though, he pointed out that ‘blaming other people for your problems’ isn’t, in the wider sense of utilitarianism, necessarily something we’d want. Remember the broader sense, moving away from Bentham’s somewhat naive ‘greatest happiness for the greatest number’, and thinking instead in terms of welfare, or utility, or achieved preferences? I might prefer not to be happy, if I knew that my happiness was caused by blaming other people unfairly for my problems. It might not even be, as Yudkowsky puts it, something I ‘want to obtain, even for free, by torturing imaginary people depicted by lies in the media’.\n\n‘People have trouble applying the notion of a good or bad consequence to all the actual consequences that are good or bad,’ he said. ‘Instead they see a small subset of consequences, the immediate local consequences, and think those are the “consequences”.’ For that reason, ‘most people should not immediately try to be “utilitarians” . . . They are better off continuing to debate which rules are good or bad and then following those rules.’ For utilitarian reasons, don’t try to be a utilitarian!\n\nAgain, it would amaze me if an internet guy in California had solved all the problems of moral philosophy. But I do find this approach refreshingly direct. There really is a moral law, of improving the world for the greatest number of people. It really does lead to some weird outcomes, like the torture/dust specks thing. However, it is a complex and difficult law to implement and we are usually best off implementing simpler, local laws, such as ‘Do the thing that kills the fewest people.’ You can contrive thought-experiment situations with trolleys or torture that end up forcing you into difficult situations, but in real life, ‘Do the thing that kills the fewest people’ is a solid position to take, and anything that steers you to a different answer should raise lots of red flags.\n\nThis is the basic moral position for the Rationalists: ‘When human lives are at stake, we have a duty to maximise, not satisfice; and this duty has the same strength as the original duty to save lives. Whoever knowingly chooses to save one life, when they could have saved two – to say nothing of a thousand lives, or a world – they have damned themselves as thoroughly as any murderer.’¹⁰ And it has obvious implications for AI safety as well. Not simply that an AI that kills everyone is probably suboptimal from a utilitarian point of view, assuming that you agree that human lives are net-positive in the universe. There’s also the discussion of what morals you instil in the AI itself: a ‘friendly AI’ that acts morally in the universe according to ‘morals’ that revolve around maximising happiness will be very different from one whose ‘morals’ revolve around maximising preferences, for instance. Also, an AI shorn of human biases might be more capable of implementing true utilitarianism, in a way that humans (and specifically me) apparently struggle with.\n\nBut most importantly, the Rationalist project is about encouraging ‘rational’ thinking – with, in Yudkowsky’s case, the eventual goal of convincing everyone that there are good rational reasons to worry about AI safety. If you’re going to think of the world in terms of numbers and statistics (which in Yudkowsky’s view, and mine, is the only way you can make any sort of sensible decisions at a national or global scale), then you need a moral system that can give you numbers to plug in. Utilitarianism, with its harsh-seeming but impartial way of treating human lives as numbers, does that job neatly.\n\nPart Four\n\nBiases\n\nChapter 21\n\n What is a ‘bias’?\n\nPart of Yudkowsky’s project, in writing the Sequences, was explaining why AI might not look like human intelligence. Having had a go at explaining the basis of ‘rationality’ or ‘intelligence’ in its pure, general form, he then had to clarify why human intelligence wasn’t quite the same thing. The most obvious reason is that humans are systematically biased, in ways that make us wrong in predictable directions. Over the next few chapters, we’ll talk about a few things that make that happen.\n\nWe don’t know everything about the world, and we never will. Not as individuals and not as a species. We just can’t get hold of all the information.\n\nBut even when we can get hold of enough information about something to make a decision about it, we will sometimes be wrong in predictable ways, as a result of how the human mind works. The different ways in which our thinking goes wrong are often lumped together under the term ‘cognitive biases’. Much of our understanding of them comes from the work of Daniel Kahneman and Amos Tversky, a pair of Israeli psychologists who did a series of groundbreaking experiments in the 1970s, although many other psychologists have worked on them since.\n\n(A worthwhile caveat to mention at this point: since Kahneman and Tversky did their work, and since Kahneman’s book Thinking, Fast and Slow made it especially famous in 2011, psychology in particular and science in general has been wracked by the ‘replication crisis’, in which many high-profile studies have turned out to be untrustworthy. Most of the stuff Kahneman and Tversky talked about is, I think, pretty robust, but it’s just worth taking everything in psychology with a pinch of salt at this point.)\n\nThe Rationalists are extremely interested in all this. Yudkowsky started writing his Sequences on Robin Hanson’s blog, which – you may recall – is called Overcoming Bias. The name LessWrong is a reference to avoiding, as far as possible, the biases which make us wrong. What we’re not talking about is ‘biases’ in the ‘football fan complaining that referees are biased against his team’ or ‘Donald Trump complaining about CNN’ sense. We’re talking about things that systematically reduce our accuracy in making guesses.\n\nRob Bensinger, in a foreword to one of the Sequences, gives an example of a (statistical) bias. Imagine, he says, that you have an urn with 100 balls in it – 70 white and 30 red – and you are allowed to take 10 of them out and then guess how many of the total are red or white. ‘Perhaps three of the 10 balls will be red, and you’ll correctly guess how many red balls total were in the urn,’ he writes. ‘Or perhaps you’ll happen to grab four red balls, or some other number. Then you’ll probably get the total number wrong. This random error is the cost of incomplete knowledge, and as errors go, it’s not so bad. Your estimates won’t be incorrect on average, and the more you learn, the smaller your error will tend to be.’¹\n\nBut now, he says, imagine that the white balls are heavier than the red ones. They tend to sink to the bottom. ‘Then your sample may be unrepresentative in a consistent direction’, says Bensinger. Acquiring more data may not help you get it right. It may even make you more wrong.\n\nCognitive biases work in a comparable way. A cognitive bias ‘is a systematic error in how we think, as opposed to a random error or one that’s merely caused by our ignorance. Whereas statistical bias skews a sample so that it less closely resembles a larger population, cognitive biases skew our beliefs so that they less accurately represent the facts, and they skew our decision-making so that it less reliably achieves our goals.’² He gives the example of someone who has an optimism bias, and is told that the red balls can treat a disease that is killing that person’s brother. ‘You may then overestimate how many red balls the urn contains because you wish the balls were mostly red,’ he writes. ‘Here, your sample isn’t what’s biased. You’re what’s biased.’\n\nThere are various psychological reasons behind the individual biases, but the fundamental one appears to be that they worked for our ancestors. They were shortcuts. We didn’t need to work out the value of 20,000 things compared to 2,000 things when we were tribal hunter-gatherers; we didn’t need to work out probabilities. We could get pretty good estimates of values and risks from simple rules of thumb, or ‘heuristics’. But now they often misfire.\n\nExactly what the biases in our minds are, and how they work, and which ones are separate from which, is an ongoing and probably unending project. But there are a few biases that most psychologists agree on, and – most importantly for this book – that are of interest to the Rationalists. I’ve picked out some examples, mainly from Yudkowsky’s writing, as the sort of thing we’re talking about. This is not an exhaustive list by any means, they’re just the ones that I (subjectively) find the most interesting and important; although I did ask Yudkowsky if he agreed with my choices and he said, ‘They sound like good guesses to me.’\n\nThe most important of all, though, is the last one we’ll come to. If you remember any of them, remember that one.\n\nChapter 22\n\n The availability heuristic\n\nWhat’s more likely to kill you: a terrorist attack, or the bath?\n\nI’m not going to insult your intelligence. You know it’s the bath, if for no other reason than the answer to ‘What’s more dangerous, [dangerous-sounding thing] or [not-dangerous-sounding thing]’ is always ‘[not-dangerous-sounding thing]’. But I’m guessing most people, if asked to rank risks, would probably write ‘terrorism’ somewhere above ‘bathtime’. After all, if you live in Britain, you’d have noticed no fewer than five high-profile terror attacks, four in London and one in Manchester, in 2017 alone. They probably wouldn’t have heard of anyone dying in the bath.\n\nBut they’d be wrong and you’d be right. Over the last 10 years, there have been fewer than 50 deaths from terrorism on UK soil. (The large majority of them came in 2017, mostly in the awful attack on the Ariana Grande concert in Manchester.) That’s an average of about five a year. According to an independent report on UK terrorism legislation carried out in 2012,¹ the average annualised death rate from drowning in the bath is 29.\n\nThis is an example of a systematic bias called the availability heuristic. When we are asked how likely something is, we could go and add up all the examples of it, divide this figure by the number of times it could possibly have happened, and get the answer. But that’s difficult and takes a long time. What we tend to do, in reality, is to judge how likely something is by how easily we can think of an example; and how easily we can think of one is only loosely related to how often it happens. More dramatic things, which get disproportionate amounts of coverage in the media, are easier to remember. We can easily think of examples of terrorism, because every single one around the world gets reported, with dramatic images of smoke and fire and blood. We can’t easily think of examples of drowning in the bath, because even though they happen far more frequently they don’t make the news, and even when they do they’re unspectacular.\n\nYudkowsky refers to a study² which looked at how good people are at assessing risks. It found that subjects ‘thought that accidents caused about as many deaths as disease; thought that homicide was a more frequent cause of death than suicide. Actually, diseases cause about sixteen times as many deaths as accidents, and suicide is twice as frequent as homicide,’³ he writes. This is a problem for various reasons. It leads to bad policies: if the public believes that child abduction is more common than it is, politicians will spend more money than they ought on reducing the risk; if people are more worried about Ebola than diabetes, then we might spend millions policing our airports to stop it coming in and neglect the thousands who die every year of diabetes. And it leads to bad personal decisions: in the years after 9/11, so many more people were afraid of flying, because of terrorism, that there were roughly 2,000 extra deaths on US roads; 300 a month in the first few months.⁴ Less dramatically, we all know people who are afraid of visiting their city centres because of terrorist attacks, but don’t think twice about driving to work.\n\nThis doesn’t just apply to risk perception. You can easily think of examples of successful people because they’re the ones in the news. ‘In real life, you’re unlikely to ever meet Bill Gates,’ points out Yudkowsky. ‘But thanks to selective reporting by the media, you may be tempted to compare your life success to his.’ Your life is probably less successful than Bill Gates’, by most measures, so that will make you sad. But, then, only one person in every 7 billion is Bill Gates. ‘The objective frequency of Bill Gates is 0.00000000015, but you hear about him much more often. Conversely, 19 per cent of the planet lives on less than $1/day, and I doubt that one-fifth of the blog posts you read are written by them.’\n\nThe availability heuristic, like all other biases, presumably evolved because it was useful in the ancestral environment. A hunter-gatherer living in a tribe of 150 people would only have got news about those 150 people. You probably never heard of really unlikely things happening, because there weren’t enough people for them to happen to. And dramatic, memorable things were probably worth remembering. As a yardstick for measuring objective probability, the availability heuristic most likely did a good job. But in a world of 7 billion people, instantly connected by the media, it can get things wildly wrong.\n\nOf course, being aware of this doesn’t stop it happening. I’ve known about the availability heuristic for years but I still look beneath me in the water when I’m snorkelling and imagine a shark coming up from the black depths. A hypothetical perfect Bayesian AI would assess the statistical likelihood of that and know that it is minuscule.\n\nChapter 23\n\n The conjunction fallacy\n\nWhat’s more likely: that the climate will stop warming, or that a new technology will be developed which allows fuel to be economically harvested from atmospheric CO₂, and the ensuing reduction in greenhouse-gas levels stops the climate from warming?\n\nIf you’re a relatively normal human being, you may find that option two sounds more likely. Option one feels a bit sparse. This is the normal reaction. The classic example that Yudkowsky cites¹ is a 1981 study by Tversky and Kahneman² which found that 72 per cent of subjects thought that ‘Björn Borg will lose the first set’ was less likely than ‘Björn Borg will lose the first set but win the match’, and 68 per cent of subjects thought that ‘Reagan will provide federal support for unwed mothers and cut federal support to local governments’ was more likely than that ‘Reagan will provide federal support for unwed mothers’. By the way, it isn’t the case (as I have always thought) that there were two groups, and one of them was asked for a probability on the first statement and another on the second. This was a group of people given a list of four possible outcomes, and ordering them from most to least probable.\n\nYou’ll probably have noticed this already, but it is impossible for them to be right. It is impossible for ‘Borg loses the first set but wins the match’ to happen without ‘Borg loses the first set’ happening. It is impossible for Reagan to support unwed mothers, and cut support for local government, without supporting unwed mothers.\n\nIn mathematical notation, the probability P(A,B), that is to say the probability that both A and B will happen, must be lower than the probability P(B). If there’s a 5 per cent chance that Borg loses the first set (Borg was very good, I gather), and an 80 per cent chance that, even having lost the first set, he still wins the match, then the chance of ‘Borg loses the first set but wins the match’ is 0.05 × 0.8 = 0.04, or 4 per cent.\n\nThe ‘conjunction fallacy’ is that adding details makes a story seem more plausible, even though they must – by the workings of mathematics – make it less probable. It happens to all of us, even professional forecasters. A separate study, also by Tversky and Kahneman, asked one group of analysts to rate the probability of ‘A complete suspension of diplomatic relations between the USA and the Soviet Union, sometime in 1983’, and another to rate that of ‘A Russian invasion of Poland, and a complete suspension of diplomatic relations between the USA and the Soviet Union, sometime in 1983.’³ The probability of the latter – which must, necessarily, be less likely – was judged to be higher.\n\nWe see the extra details as corroborative, says Yudkowsky (and Kahneman, and modern psychological science). But we should see them as burdensome. They don’t make a story more likely, they make it less. People who want to avoid this ‘need to notice the word “and”,’ says Yudkowsky. ‘They would need to be wary of it – not just wary, but leap back from it . . . They would need to notice the conjunction of two entire details, and be shocked by the audacity of anyone asking them to endorse such an insanely complicated prediction. And they would need to penalise the probability substantially.’ Again, humans don’t do this; a perfect Bayesian AI would.\n\nChapter 24\n\n The planning fallacy\n\nHow long will it take you to do something? Something big, some project that might require a few weeks or months?\n\nA good rule of thumb: however long you think it will take, it’ll probably take longer. (It might even be longer still. Douglas Hofstadter, the American polymath and author of Gödel, Escher, Bach: An Eternal Golden Braid, once coined ‘Hofstadter’s law’: ‘It always takes longer than you expect, even when you take into account Hofstadter’s law.’) That is because of a quirk of the mind known as the ‘planning fallacy’. Yudkowsky mentions it in the Sequences.¹ He refers to a famous 1994 study by Roger Buehler and colleagues² which asked some students how long it would take them to complete their undergraduate theses. The students had to say when they were 50 per cent sure they’d finish their projects, 75 per cent sure, and 99 per cent sure.\n\n‘We found evidence of overconfidence,’ Buehler writes laconically. Only 12.8 per cent of students finished their projects in the time they were 50 per cent sure they’d finish it by. Only 19.2 per cent finished it by their 75 per cent mark. And, amazingly, only 44.7 per cent even managed to get it done by their 99 per cent mark. ‘The results for the 99 per cent probability level are especially striking,’ says Buehler in the study. ‘Even when they make a highly conservative forecast, a prediction that they feel virtually certain that they will fulfil, people’s confidence far exceeds their accomplishments.’\n\nOther scientists, including Tversky and Kahneman, have found similar results. What appears to be going on here is that if you ask someone how long something will take, they imagine all the steps that are involved and put a time on that. They don’t include time for balls-ups or unforeseen disasters. Yudkowsky refers to another study,³ which found that ‘Asking subjects for their predictions based on realistic “best-guess” scenarios; and asking subjects for their hoped-for “best-case” scenarios produced indistinguishable results.’ He continues: ‘When people are asked for a “realistic” scenario, they envision everything going exactly as planned, with no unexpected delays or unforeseen catastrophes – the same vision as their “best case”.’ This may be the reason why, for instance, the retractable roof on the new stadium that the host city of Montreal built for the Olympics was not ready until 1989, 13 years after the Olympics had come and gone.⁴ (And then it broke fairly shortly afterwards.)\n\nThere is a well-documented way around the planning fallacy, though. Don’t just look at the specifics of what your project involves – look at how long other, similar projects have taken in the past. When I signed up to write this book I ascribed myself a six-week break in my full-time job, because I figured I could write the bulk of the 80,000 words in that period. Luckily I ended up going freelance (for journalists, this is usually a euphemism for ‘getting fired’, but that is mostly not true in my case) about six months before the deadline, and I used basically all of that time. If I’d spoken to a few of my peers who had written books before, I’d have noticed that they had all made similar assumptions, and ended up needing deadline extensions, and that actually writing a book takes for-bloody-ever. (I also found out, from one friend whose book came out fairly recently, that her publisher had accidentally CC’d her in on an email thread which said they’d given her a fake deadline, with the expectation that she’d actually submit her manuscript about three months later. Publishers, I suspect, know authors much better than authors do.)\n\nThis is called taking the ‘outside view’ instead of the ‘inside view’. The ‘inside view’ is what you can see when you’re looking at it from your own perspective. I know I can write a 2,000-word article in a day, so why can’t I write 60,000 words in six weeks and do the rest at weekends or whatever? But the ‘outside view’ is what you find when you look at all the other people who’ve done similar things and see how long it’s taken them. And books tend to take about a year to write.\n\nYudkowsky talks about this, as well. Buehler did another study,⁵ which found (in Yudkowsky’s write-up) that students ‘expected to finish their essays 10 days before deadline. They actually finished one day before deadline. Asked when they had previously completed similar tasks, they responded, “one day before deadline”.’ ‘So there is a fairly reliable way to fix the planning fallacy,’ says Yudkowsky. ‘Just ask how long similar projects have taken in the past, without considering any of the special properties of this project. Better yet, ask an experienced outsider how long similar projects have taken. You’ll get back an answer that sounds hideously long, and clearly reflects no understanding of the special reasons why this particular task will take less time. This answer is true. Deal with it.’⁶\n\nChapter 25\n\n Scope insensitivity\n\nHow much – in US dollars, or pounds sterling, or whatever – is a human life worth? And how much are a million human lives worth?\n\nWhatever answer you give to the first question, the answer to the second – surely – should be a million times greater. That at least is the Rationalist response. For a lot of people, it may seem insensitive to talk about human life in monetary terms, but it has to be done, and in fact is done every day in the NHS and other healthcare systems. You need to know how much you can spend to save one life; otherwise, you’ll spend far too much on one, and many others will die because you no longer have the cash to spend on them.\n\nThis may seem obvious, but in fact it is not. There is plenty of evidence to show that we are extremely inconsistent in our approaches to these things. ‘Once upon a time,’ writes Yudkowsky, ‘three groups of subjects were asked how much they would pay to save 2,000 / 20,000 / 200,000 migrating birds from drowning in uncovered oil ponds.’¹ If the groups were approaching this rationally – if they attributed the same value to each bird’s life – then whatever figure they gave for the first question, it should be 10 times as much for the second and 100 times as much for the third. This is not what they answered. They answered $80 for the first question, $78 for the second, and $88 for the third.²\n\nYudkowsky points to similar experiments. Residents of four states in the western US said they would pay only 28 per cent more to protect all 57 wildernesses in the region than to protect just one. Toronto residents said they would pay about the same to clean up every polluted lake in Ontario as to clean up the polluted lakes in one part of Ontario. ‘We are insensitive to scope even when human lives are at stake,’ he says. ‘Increasing the alleged risk of chlorinated drinking water from 0.004 to 2.43 annual deaths per 1,000 – a factor of 600 – increased willingness-to-pay [for measures to reduce the levels of chlorine in the water] from $3.78 to $15.23.’³\n\nExactly what’s going on in our brains we don’t know, obviously, but it appears that we make these judgements according to our emotional response, rather than any kind of numbers-based assessment. We picture a single, dejected bird, ‘its feathers soaked in black oil, unable to escape’, suggests Kahneman.⁴ We imagine how that makes us feel, and put a dollar value on it. ‘No human can visualise 2,000 birds at once, let alone 200,000,’ says Yudkowsky, so we forget about that detail and just focus on the imaginary bird.\n\nWe also seem to care about the setting. You’d think a human life is worth a human life, and 5,000 human lives are worth 5,000 human lives, but instinctively we place them into a wider context. An intervention that would save 4,500 lives in a Rwandan refugee camp was considered far more valuable if the camp contained 11,000 people than if it contained 250,000, although the number of lives saved was the same.⁵\n\n‘There’s a Jewish proverb,’ Paul Crowley told me when I spoke to him. ‘“If you save a life, it is as if you’ve saved the whole world.” And that’s true. But then if you save two lives, it’s as if you’ve saved two whole worlds.’ He’s borrowed this line from Yudkowsky, he tells me. We don’t like thinking about this stuff. Even when we look at it logically, there’s something icky about, for instance, saying that it costs too much to give this particular child an expensive experimental cancer treatment, so we have to let them die. But the Rationalists – with their shut-up-and-multiply, utilitarian-calculus ethic – are very good at thinking about it. And it’s vital that we think about it, at least at a national level.\n\nIn the British NHS we have an organisation called the National Institute for Health and Care Excellence, or NICE. NICE’s job is to determine whether or not the tax-funded NHS should offer treatments to patients. Periodically, we have a national uproar when some expensive new cancer drug is turned down, despite evidence that it works. A quick Google search found several such stories over the last few years, for example one published by my old employers the Daily Telegraph, which opens with the line: ‘A “truly revolutionary” new drug that can give women with advanced breast cancer an extra six months of life will not be available on the NHS as it is too expensive.’⁶ It’s always easy, in those cases, to find some cancer patient who has been denied the drug and a chance of a longer life. But NICE works on a cost-effectiveness basis: it is willing to spend a limited amount, call it X, per quality-adjusted life year (QALY) saved. Spending 2X saving a QALY with a cancer drug means that they can’t buy two QALYs’ worth of diabetes drugs somewhere else. Being sensitive to scope means sometimes thinking, ‘This person must die so I can save more people elsewhere’, and that is never something we’re comfortable talking about. But for Rationalists, who are temperamentally inclined to think in that way anyway, it is obviously vital to do so, if you’re trying to achieve utilitarian goals in the world, be they reducing human suffering or avoiding human extinction.\n\nChapter 26\n\n Motivated scepticism, motivated stopping and motivated continuation\n\nJonathan Haidt, the social psychologist, says in his (excellent) book The Righteous Mind: Why Good People Are Divided by Politics and Religion that when we are presented with evidence for or against a hypothesis, we ask ourselves one of two questions. When we want to believe something, ‘we ask ourselves, “Can I believe it?” Then . . . we search for supporting evidence, and if we find even a single piece of pseudo-evidence, we can stop thinking. We now have permission to believe.’¹ But when we don’t want to believe something, ‘we ask ourselves, “Must I believe it?” Then we search for contrary evidence, and if we find a single reason to doubt the claim, we can dismiss it.’\n\nSo, says Haidt, when people ‘are told that an intelligence test gives them a low score, they choose to read articles criticising (rather than supporting) the validity of IQ-testing. When people read a (fictitious) scientific study that reports a link between caffeine consumption and breast cancer, women who are heavy coffee drinkers find more flaws in the study than do men and less caffeinated women.’ It even affects what you see: ‘Subjects who thought that they’d get something good if a computer flashed up a letter rather than a number were more likely to see the ambiguous figure [] as the letter B, rather than as the numbers 13.’\n\nThe technical terms for the ‘can I believe it/must I believe it’ phenomena are ‘motivated credulity’ and ‘motivated scepticism’. Yudkowsky: ‘A motivated sceptic asks if the evidence compels them to accept the conclusion; a motivated credulist asks if the evidence allows them to accept the conclusion.’² Yudkowsky adds another layer to this, which is the idea of motivated stopping and motivated continuation. When we’re looking for something in real life, we aren’t usually given a set of things to choose from: ‘You have to gather evidence, which may be costly, and at some point decide that you have enough evidence to stop and choose. When you’re buying a house, you don’t get exactly 10 houses to choose from . . . You look at one house, and another, and compare them to each other [and] at some point you decide that you’ve seen enough houses, and choose.’\n\nIt’s the same when you’re trying to find the most likely hypothesis to explain some phenomenon, or the best answer to a question. Does a new drug reduce blood pressure? You can look at one study, but it might not be the whole story. You can look at another. How many should you read before you make a decision? But sometimes we have a reason to stop, or to continue, that isn’t just about how much evidence is really necessary. However much evidence you have, you’ll have a current best guess. You’ve looked at three studies and two of them say, cautiously, that the drug doesn’t affect blood pressure; the third says, equally cautiously, that it does. Your current best guess might then be ‘it doesn’t work’.\n\nBut if you’re a researcher at the company that makes the drug, you have a reason not to accept that conclusion, and to carry on looking for more evidence. ‘[When] we have a hidden motive for choosing the “best” current option, we have a hidden motive to stop, and choose, and reject consideration of any more options,’ says Yudkowsky. ‘When we have a hidden motive to reject the current best option, we have a hidden motive to suspend judgement pending additional evidence, to generate more options – to find something, anything, to do instead of coming to a conclusion.’\n\nA real-life example that Yudkowsky quotes is that of the statistician R.A. Fisher, who argued (after the epidemiological evidence showed that smokers were vastly more likely to develop lung cancer) that smoking may not, necessarily, cause lung cancer. Instead he put forward as an alternative what became known as the ‘genotype hypothesis’, that people have a genetic tendency to want to smoke, and people with that genetic tendency are also prone to developing cancer.³ Yudkowsky points out that Fisher may have had a ‘hidden motive’ to continue the search: that he was employed by tobacco firms as a scientific consultant. (For the sake of fairness to Fisher’s memory, both biographies of which I am aware conclude that Fisher probably wasn’t led by the money: ‘This is to misjudge the man,’ one states. ‘He was not above accepting financial reward for his labours, but the reason for his interest was undoubtedly his dislike and mistrust of puritanical tendencies of all kinds; and perhaps also the personal solace he had always found in tobacco.’⁴ That said, I suspect Yudkowsky would say, and I would agree, that this doesn’t rule out the sort of subconscious bias that could affect his decision-making. And besides, while those motivations are not financial, they’re not the disinterested seeking of truth either.)\n\nAgain, these aren’t necessary features of all intelligence: they’re specific flaws in human intelligence. A perfect Bayesian AI wouldn’t have them; a more realistic, imperfect AI might have all sorts of flaws and idiosyncrasies in its thinking, but there’s no reason to assume that they would be the same as ours.\n\nChapter 27\n\n A few others, and the most important one\n\nThere are plenty of other biases, and if you want to find out more there are entire books dedicated to them. Thinking, Fast and Slow by Daniel Kahneman is a good one; Dan Ariely’s Predictably Irrational: The Hidden Forces That Shape Our Decisions is another. Or, of course, you could sit down for several months with Yudkowsky’s Rationality: From AI to Zombies, which I honestly recommend.\n\nA few of the other biases that Yudkowsky mentions are the ‘illusion of transparency’,¹ in which we know the meaning of our own words, so we expect others to do so as well. For instance, in an experiment, subjects were told that someone went to a restaurant on the recommendation of a friend, and the restaurant turned out to be either a) horrible or b) nice. Then the diner left a message on their friend’s answerphone, saying: ‘I just finished dinner at the restaurant you recommended, and I must say, it was marvellous, just marvellous.’² Of the people who were told that the meal was horrible, 55 per cent said they thought that not only was the message sarcastic, but that the listener would know it was sarcastic. Of the people who were told the meal was nice, only 3 per cent thought it was sarcastic.\n\nRelatedly, in ‘hindsight bias’³ people enormously overestimate how inevitable something was after it happened, or overestimate how obvious something is when they know the answer. People, for instance, frequently assume that social-scientific results are statements of the obvious – but experiments have shown that they would say that whatever the result actually was. For example, if you give half of the subjects in an experiment the following quote:\n\n Social psychologists have found that, whether choosing friends or falling in love, we are most attracted to people whose traits are different from our own. There seems to be wisdom in the old saying ‘Opposites attract’.\n\nand the other half:\n\n Social psychologists have found that, whether choosing friends or falling in love, we are most attracted to people whose traits are similar to our own. There seems to be wisdom in the old saying ‘Birds of a feather flock together.’\n\nthen ‘virtually all will find whichever result they were given “not surprising”.’⁴\n\n‘Loss aversion’ is where we assign more value to things we have than things we can get, so we might refuse to bet £1 on a flipped (fair) coin with the chance of winning £3.\n\nThe ‘affect heuristic’ is our tendency to assume that if something is good in one way, it’s good in all ways. ‘Subjects told about the benefits of nuclear power are likely to rate it as having fewer risks,’ writes Yudkowsky. ‘Stock analysts rating unfamiliar stocks judge them as generally good or generally bad – low risk and high returns, or high risk and low returns – in defiance of ordinary economic theory, which says that risk and return should correlate positively.’⁵ The ‘halo effect’ is when the affect heuristic is applied socially: so if someone is handsome, we tend to assume that they’re also intelligent and moral.⁶\n\nBut the most important bias to be aware of is this, which is a sort of collection of several: knowing about biases can make you more biased. Yudkowsky:\n\n Once upon a time I tried to tell my mother about the problem of expert calibration, saying: ‘So when an expert says they’re 99 per cent confident, it only happens about 70 per cent of the time.’ Then there was a pause as, suddenly, I realised I was talking to my mother, and I hastily added: ‘Of course, you’ve got to make sure to apply that scepticism even-handedly, including to yourself, rather than just using it to argue against anything you disagree with—’\n\n And my mother said: ‘Are you kidding? This is great! I’m going to use it all the time!’⁷\n\nVarious biases can actually mean that even as you get more information, you become more wrong. Confirmation bias and disconfirmation bias, and related phenomena, for instance. New information comes in, but your brilliant mind finds brilliant ways in which to ignore the stuff it doesn’t like and promote the stuff it does.\n\nThere’s a particularly pernicious one, the ‘sophistication effect’: ‘Politically knowledgeable subjects, because they possess greater ammunition with which to counter-argue incongruent facts and arguments, will be more prone to [these] biases.’⁸ So new information like ‘We are all biased and the things we believe are frequently wrong’ can easily become ‘These arguments that are being deployed against me are flawed, and I can point out why because I have this in-depth knowledge of human biases.’ Yudkowsky calls this a ‘fully general counter-argument’. Anybody with a partisan axe to grind can deploy ‘confirmation bias’ to undermine an argument they don’t like. Most of the things we call ‘human biases’ are extremely convenient labels to attach to opinions with which we disagree.\n\nBut the key is to accuse your own of them. You are biased. (I am biased.) You are probably systematically overconfident in your beliefs. I certainly am: in fact I took a calibration test on the Good Judgment Project website recently which showed that, yes, I overestimated my knowledge of economics, geography, history and world politics. (I was well calibrated for general knowledge and underconfident in my knowledge of Europe, if you’re interested.) This isn’t stuff that you should just be applying to other people. You need to apply it to you.\n\nPaul Crowley joked about this with me, when I spoke to him. When he first started reading Yudkowsky’s Sequences, he said, ‘I read it and said, “This is brilliant! It shows how everyone apart from me is wrong.” And then you read a bit more, and you think, hmm, you know, maybe some of this might apply to me. The mote in the other person’s eye is easier to see.’\n\nThe Yudkowsky/LessWrong/Rationalist project is to help people to see those motes in their own eyes, in order to help them behave more like perfectly rational Bayesian optimisers.\n\nPart Five\n\nRaising the Sanity Waterline\n\nChapter 28\n\n Thinking probabilistically\n\nA large part of the Rationalist project is how to improve your own rationality. That is, how to get closer to making Bayesian-optimal decisions and holding true beliefs, given the constraints of the human brain and its many biases. Yudkowsky dedicates large parts of the Sequences to this ‘martial art of rationality’.\n\nAs we saw in the sections on Bayesianism and utilitarianism, the Rationalist movement likes to put numbers on things, even if those numbers are estimates. A significant part of that is putting explicit figures on how likely you think something is.\n\nIn 2015, a book called Superforecasting: The Art and Science of Prediction came out. It was about telling the future, and my Rationalist friend Paul Crowley was very excited about it. ‘I felt like that was a vindication of everything we’d been talking about for 10 years,’ he said. Superforecasting was by Philip Tetlock and Dan Gardner, and was a write-up of Tetlock’s work as a professor of political psychology at the University of Pennsylvania.\n\nIn 1984 Tetlock, a recently tenured professor, was asked to work on a new committee appointed by the National Academy of Sciences. Its goal was to help stop nuclear war. Tensions were enormously high between the two superpowers; Stanislav Petrov, whom you may remember from the section on existential risks, had (although no one on the committee would have known) quite possibly saved the world just a few months before. Tetlock sat on the committee with other well-respected social scientists, who argued over how best to reduce the risk of confrontation. He told Gardner, years later: ‘I mostly sat at the table and listened . . . The liberals and conservatives in particular had very different assessments of the Soviet Union. The conservative view was that they could . . . contain and deter it. Whereas the liberal view [was] that conservatives [in the White House] were increasing the influence of the hardliners in the Kremlin.’¹\n\nA few months later, Mikhail Gorbachev took command of the USSR, and started implementing liberal policies. No one had expected it, but both liberals and conservatives took it as confirmation that they had been right all along. ‘The conservatives argued that we had forced the Soviets’ hand,’ said Tetlock. ‘Whereas [the liberals thought] the Soviet elite had learned from the failings of the economy [and that] if anything, we had slowed down the process of learning and change.’\n\nTetlock came to the conclusion that what actually happened had very little bearing on whether or not the experts judged their predictions to be right. They just explained what happened in terms that made the stories they were telling seem true anyway. He was intrigued by this, so he set up an experiment. He recruited hundreds of experts from various fields – journalists, economists, political scientists – and asked them for their anonymous predictions.\n\nPart of the problem, Tetlock had noticed, was that people often gave predictions with ambiguous interpretations, about ‘growing tensions being likely’ or suchlike, that didn’t really tie them to any specific outcome. So the questions he asked had easily confirmed answers and clear timeframes: ‘Will the dollar be higher, lower, or the same against the pound a month from now?’ ‘Will North Korea and the United States go to war in the next two years?’ And the experts had to give precise numerical estimates of how likely they thought the outcomes were: 30 per cent chance, 75 per cent chance, 99 per cent chance, and so on.\n\nHe collected nearly 30,000 predictions from 284 experts. He waited, weeks, months and in some cases years, to see how well they did against the harsh judgement of reality. And he counted how often their predictions matched reality. If someone’s 75 per cent predictions came up 75 per cent of the time, and their 90 per cent predictions came up 90 per cent of the time, and so on, then they were ‘well-calibrated’.\n\nHe also gave them a bonus score for being precise. If you just said ‘50 per cent chance’ for everything, then you’d probably do OK at calibration. But you’d be no use as a predictor; we want someone to state, ‘This will happen’ or ‘This won’t happen’. So saying that something is 99 per cent likely gets you a higher score, if you’re right, than saying that something is 60’. That’s called ‘disclimination’.\n\nThe results, when his study was published years later, are pretty famous: on average, the experts were no better calibrated than, as Tetlock put it, ‘a dart-throwing chimpanzee’ – literal random chance. (They were a bit better at discrimination, but not really.) What was interesting, though, was what happened when you divided them up further. Some experts not only did as badly as the imaginary chimpanzee, they were significantly worse; they really would have improved their scores if they’d answered at random. But some did much better. ‘There’s quite a range,’ Tetlock told Gardner. ‘Some experts are so out of touch with reality they’re borderline delusional. Other experts are only slightly out of touch. And a few experts are surprisingly nuanced and well-calibrated.’²\n\nIt wasn’t their political views that best predicted who did well, or even their level of education or experience in the field. Instead, it was the way they thought. The ones who did amazingly badly were those who believed that there was a big idea which explained everything; that the world was simple and could be understood simply, that they could just stamp their big idea onto every situation. The ones who did well were those who had no such big idea, who regarded the world as complex, took their information from many different sources and were willing to be self-critical and learn from mistakes. Tetlock called the former ‘hedgehogs’ and the latter ‘foxes’, following an Isaiah Berlin essay quoting an old Greek poem: ‘The fox knows many things, but the hedgehog knows one big thing.’³\n\nYears later, Tetlock co-operated with the US Defense Department’s Intelligence Advanced Research Projects Activity (IARPA) to run a competition to find the best forecasters; they outperformed actual CIA operatives by a margin, even with no access to classified information. The Good Judgment Project spawned the Superforecasting book; the very best forecasters were known as superforecasters.\n\nThe reason why Paul Crowley was excited about it was that this is exactly the sort of thinking that Yudkowsky and the Rationalists have been talking about for ages. It’s pure Bayesianism. ‘Look, it turns out thinking probabilistically is really important for making accurate guesses about things!’ he said, happily. ‘These guys are explicitly being Bayesian. They’re using priors, they’re updating with evidence, they update in a Bayesian way.’\n\nSay you’ve been asked to predict whether there’ll be a war between North Korea and the US in the next year. You might look at the war of words – as I write it’s a few months since President Donald Trump called Kim Jong-un ‘rocket man’ and accused him of being ‘short and fat’, for instance, since that’s the Churchillian rhetoric of the era we live in – and conclude that, well, it feels pretty likely. Then you might put a figure on that feeling, and say ‘30 per cent’. But a Bayesian – a superforecaster – would try to find prior probabilities, and look at other forms of evidence.\n\nYou might try to find a prior probability for a war with North Korea by, for example, researching the number of wars between the two since the Second World War. That’s one war in 70 years, so your prior probability for a war in any given year is low – about 1.5 per cent. That’s the equivalent of the ‘background rate of cancer’ that we saw in the Bayesian explainer a few chapters ago. Then you could look at all the times that Trump has tweeted aggressively at world leaders. Say he goes to war within a year with world leaders at whom he tweets aggressively 90 per cent of the time; he only goes to war with those at whom he tweets non-aggressively 15 per cent of the time. But you also know that wars between North Korea and the US are pretty rare. In every 100 years, you get about 1.5 wars. So your rate of new wars per year is about 0.015.\n\nYou can plug all these numbers into the equation, exactly the same as the cancer test. Your war has an incidence rate of 0.015. So for every million world leaders Trump tweets at, he will declare war on 15,000 of them within a year. The test (‘Has Trump tweeted aggressively?’) will accurately pick out 13,500 of them. And of the 985,000 world leaders at whom Trump tweeted more soothingly, 837,250 will be told, correctly, that they are not going to be targeted by cruise missiles. But 147,750 will be told – wrongly – that they are going to have a war. That’s your false positive rate. So you have a total of 161,250 positive results (belligerent tweets), of which 13,500 are true and 147,750 are false. So your odds of a war with North Korea, given that Trump has tweeted belligerently at Kim Jong-un, are a bit over 8 per cent. (Also, 1,500 world leaders are going to have a heck of a surprise after Trump tweets ‘Great guy!1!’ at them and then launches a series of Harpoon missiles at their oil refineries.)\n\nIn real life, you wouldn’t have good numbers like this. Your prior of wars per year might be pretty solid, but you don’t have a huge database of Trump tweets and Trump wars (not least because, however much most people reading this book probably dislike Donald Trump, at the time of writing he hasn’t started many wars). But Tetlockian ‘foxes’ would use best-guess numbers to fit the various bits of the Bayesian equation; they would look for other sources of information to adjust their numbers up and down; they would allow their estimates of the probability to move where the evidence then took them. They might not explicitly run through Bayes’ theorem in their minds, but they would do something analogous.\n\nAnd the key thing is then checking whether you are right. There’s a difficulty, of course. If you predict that there’s an 8 per cent chance of war, that doesn’t mean that you’re saying there won’t be a war; you’re saying that it’s unlikely, but that there’s still about a one in 12 chance. So if there is a war, you could reasonably claim, ‘Well I didn’t say there wouldn’t be,’ and mark yourself as correct.\n\nThe way around this is to make lots of predictions and see how many come in. This method became particularly famous around the 2012 US presidential election, when Nate Silver, editor-in-chief of the website FiveThirtyEight.com, correctly predicted which way all 50 states would end up voting. He did it using exactly these methods: having a prior and then updating it with evidence, in the form of new polls. (The new polls were added to the database using an algorithm, thus taking human bias out of the equation, to a degree.) The site would – and still does – give percentage estimates for each event it predicted. The idea is that it can go back and grade itself: as we saw earlier in Tetlock’s experiment, if its 75 per cent bets come in 75 per cent of the time, then it’s well calibrated.\n\nAnd this is explicitly what the Rationalist community does. Scott Alexander of Slate Star Codex does it every year: he makes predictions in January, and the following January he grades them to see how well calibrated he is. For instance, at the start of 2017 he predicted with 60 per cent confidence that the US would not get involved in any new major war with a death toll of more than 100 US soldiers, with 95 per cent confidence that North Korea’s government ‘will survive the year without large civil war/revolt’, and with 90 per cent confidence that ‘no terrorist attack in the USA will kill more than 100 people’. He made 104 predictions; at the start of 2018 he went back and looked at them, and checked his calibration. (He did pretty well. His 60 per cent predictions came in 64 per cent of the time, his 70 per cent predictions 62 per cent of the time, and so on.)\n\nAnd as a whole, the Rationalists are really good at this stuff: they tend to be foxes rather than hedgehogs. I spoke to Michael Story, a superforecaster who works for Tetlock’s Good Judgment Project. I asked him exactly what that meant. There are 20,000 forecasters in their sample, he said. ‘That’s how many forecasters, of whom 150 are supers,’ he told me. And are you one of the supers, I asked? ‘I am indeed,’ he said, somewhat shyly. (I met Mike in a café in north London for breakfast. He’s tall and bearded and extremely friendly, but my favourite thing about him is that he has this ridiculously enormous dog called Laska. Laska is an Alaskan shepherd; he looks like a wolf and weighs more than 10 stone. He sat on my foot for a bit – Laska, not Mike – and my foot went to sleep.)\n\nYou can see from their analytics where people come to the Good Judgment Project from, Mike told me, whether they click through from the Guardian website or Google or whatever. ‘Loads of the superforecasters came from LessWrong,’ he said. ‘A ridiculously disproportionate number. Same with a lot of them, if you trace back how they first got involved, loads of them will say LessWrong or blogs associated with it, [Tyler Cowen’s] Marginal Revolution or [Robin Hanson’s] Overcoming Bias, that crowd.’ His impression – and he’s careful to say that it’s just his impression – is that superforecasters and Rationalists are similar in a lot of ways. ‘My impression is that supers and LessWrongers share similar norms of open discussion, and probably similar personality types, especially with attitudes to conflicting arguments and information. I only have data on superforecasters, so I can’t compare directly, but I’ve noticed many of the same themes emerging.’\n\nMike is a long-term fan of the Rationalist community (‘I unironically love it’). He’s met many of the people mentioned in these pages, and been to their IRL (that’s ‘in-real-life’, for those of you who don’t live on the internet) parties in Oxford and elsewhere. And he thinks that the Rationalists are so good at forecasting because the community has the norms it has – of free speech, and accepting weird and outré and even offensive views. It is, after all, harder to hold on to any one big idea about why things happen when you are surrounded by people who think your big idea is stupid.\n\nObviously, they have their own big idea that the world may well be destroyed by AI. There are two possible responses to this. One is to point out, correctly, that this is a thing they forecast, rather than something they plug into the forecast; it’s not something that they can stamp on every situation and then say, ‘I think France will lower its top rate of tax in the next 18 months because the world is going to be destroyed by AI.’ You could say that the fact that a lot of them are superforecasters is itself evidence in favour of the hypothesis ‘The world may be destroyed by AI.’\n\nThe other response is to say, as Mike does, that it could be their weakness. For all that they allow very different political beliefs into their sphere, the Rationalists are, as a rule, very similar. They’re non-diverse in the ways that get people angry – they’re predominantly white and male – but they’re also similar in personality types: nerdy, often autistic, scoring high on personality traits such as scrupulosity and conscientiousness, often introverted. And that, said Mike, could be ‘dangerous’. ‘This is my concern. If everyone’s too similar, you’re vulnerable to a bad meme, just the same as biologically if you have all these plants that are the same, one virus kills them all.’ I asked him if he thought that the AI stuff was a ‘bad meme’ that has got into the Rationalist ecosystem and now can’t be eradicated because everyone is too similar, and he said that he wasn’t sure. But it is worth worrying about, he said. ‘If everyone’s personalities line up, like holes in Swiss cheese, then everyone could adopt a bad meme and not realise.’\n\nChapter 29\n\n Making beliefs pay rent\n\nAnother key way of checking your own beliefs is to think about what they actually imply. Yudkowsky calls this ‘Making beliefs pay rent in anticipated experiences.’ For instance: if a tree falls in the forest, does it make a sound? Answer that question in your mind before you go any further. If you thought ‘no’, is that because, to you, ‘sound’ means the sensation, the qualia, of someone hearing something? And if you thought ‘yes’, is that because ‘sound’ means the pressure waves in air that are made when something loud happens?\n\nThis is one of the longest-running arguments in philosophical history, to the point that it’s a cliché of philosophy alongside angels dancing on the head of a pin. But, assuming that you agree that the physical world still exists when we are not looking at it (which some philosophers dispute, but I am content to ignore them), then – what are people actually arguing about?\n\nYudkowsky imagines an argument between two people, Albert and Barry:\n\n Albert: ‘What do you mean, there’s no sound? The tree’s roots snap, the trunk comes crashing down and hits the ground. This generates vibrations that travel through the ground and the air. That’s where the energy of the fall goes, into heat and sound. Are you saying that if people leave the forest, the tree violates Conservation of Energy?’\n\n Barry: ‘But no one hears anything. If there are no humans in the forest, or, for the sake of argument, anything else with a complex nervous system capable of “hearing”, then no one hears a sound.’¹\n\nBut, points out Yudkowsky – who imagines the argument spiralling out of control somewhat – Albert and Barry actually agree on everything that is happening. They both think that the tree hits the ground and sends waves of energy through the forest. They both agree that no auditory sensations are being experienced. All they disagree about is whether or not that combination of things should be called a ‘sound’ or not. If you had two words – Yudkowsky suggests ‘albergle’ for acoustic vibrations, ‘bargulum’ for auditory experiences – then the argument would disappear; they’d just say ‘OK, it makes an albergle but not a bargulum’.\n\nA surprising number of arguments seem to fall into this form. (About 40 per cent of those on the contemporary British internet seem to revolve around whether or not Person A or Group B is Marxist/socialist/Nazi/alt-right/misogynistic/racist/transphobic/a TERF etc., with people on each side marshalling reasons for and against their inclusion in one definition or another.) But these debates are sterile, for Yudkowsky and the Rationalists, because they don’t constrain your expectations. If your model can explain every outcome, then it can’t explain any outcome.\n\nIf I argue that we should define ‘sound’ as ‘acoustic vibrations’ rather than ‘auditory experiences’, it won’t change what I expect to find when I walk into the forest to see where the tree has fallen. If I argue that we should define Jeremy Corbyn as a ‘Marxist’ rather than a ‘socialist’, it won’t change what I expect him to do if his Labour Party is elected to power. If I say, ‘I believe that the tree’s trunk broke, rather than that the roots came out of the ground,’ that is a belief that constrains my experiences; if I turn up and see that the roots are out, then I know that my belief was wrong. ‘I believe that Jeremy Corbyn will renationalise the British railway system within a year of coming to power’ constrains my experiences; if he does not, then I know I was wrong. But ‘Jeremy Corbyn is a Marxist’ does not constrain my beliefs and cannot be used to predict anything: if Corbyn does not nationalise the railways, he could still be a Marxist, and vice versa. (‘But we would expect more Marxist-style behaviour such as compulsory nationalisation from someone who is a Marxist!’ Fine, but in that case what you call him doesn’t matter. What behaviour do you expect?)\n\n‘When you argue a seemingly factual question, always keep in mind which difference of anticipation you are arguing about,’ says Yudkowsky. ‘If you can’t find the difference of anticipation, you’re probably arguing about labels. Above all, don’t ask what to believe – ask what to anticipate. Every question of belief should flow from a question of anticipation, and that question of anticipation should be the centre of the inquiry. Every guess of belief should begin by flowing to a specific guess of anticipation, and should continue to pay rent in future anticipations. If a belief turns deadbeat, evict it.’²\n\nChapter 30\n\n Noticing confusion\n\nThere’s an old science joke which Yudkowsky turns into a Teachable Moment. There is a heater in the laboratory. Next to it is a tile. The teacher asks her students: ‘Why do you think the side of the tile next to the heater is cooler than the side away from the heater?’ (If you like, stop reading for a moment and think why it might be. Don’t feel you have to, though.)\n\nThe student stammers: ‘Er, perhaps because of heat conduction?’\n\nAnd the teacher replies: ‘No, it’s because I turned the tile around before you came in.’\n\nIt is a god-awful joke, I realise, but it is useful. The phrase ‘because of heat conduction’ sounds like an explanation, says Yudkowsky. It fits into that bit of the conversation where an explanation would go, and it uses sciencey-sounding words. But remember the last section, about making beliefs pay rent in anticipated experiences. What does a belief in ‘heat conduction’ make you expect?\n\nWell, it should boil down to a series of equations derived from Fourier’s law and the conservation of energy (I say, confidently, having checked Wikipedia). But to a first approximation, it should say that the bit that’s been heated up should be hottest, and that bits that are further away should be cooler. The student should expect to find that the side nearer the heater is warmer, and should be surprised when it isn’t. ‘If “because of heat conduction” can also explain the radiator-adjacent side feeling cooler,’ says Yudkowsky, ‘then it can explain pretty much anything.’¹ And if your model can explain everything, then it doesn’t explain anything. When something happens that your beliefs don’t anticipate, you should be confused. And you should pay attention to your confusion, because either your belief model is wrong, or something else is going on that you’re not aware of, like the tile being turned around.\n\nYudkowsky has a story of someone telling him in a chatroom that a friend needed medical advice: ‘His friend says that he’s been having sudden chest pains, so he called an ambulance, and the ambulance showed up, but the paramedics told him it was nothing, and left, and now the chest pains are getting worse. What should his friend do?’ Yudkowsky says he knew that paramedics don’t do that – that if someone calls an ambulance they are obligated to take them to the emergency room – but didn’t take the obvious next step. Instead, he managed to ‘explain the story within my existing model, though the fit felt a little forced’, and replied: ‘Well, if the paramedics told your friend it was nothing, it must really be nothing – they’d have hauled him off if there was the tiniest chance of serious trouble.’ Then it turned out that the friend had made the whole thing up.\n\n‘My feeling of confusion was a clue,’ says Yudkowsky, ‘and I threw my clue away. I should have paid more attention to that sensation of still feels a little forced. It’s one of the most important feelings a truth can have, a part of your strength as a Rationalist. It is a design flaw in human cognition that this sensation manifests as a quiet strain in the back of your mind, instead of a wailing alarm siren and a glowing neon sign reading: Either Your Model Is False Or This Story Is Wrong.’ If you’re trying to become a more rational being – a better Rationalist – then you need to listen to those little moments when something doesn’t quite seem to add up.\n\nChapter 31\n\n The importance of saying ‘Oops’\n\nThe key takeaway from all the ‘bias’ stuff we’ve talked about is probably that it is really hard to change your mind. Large parts of our make-up are geared towards letting us keep on thinking what we already thought: confirmation bias, motivated reasoning, loss aversion and so on. If someone tells you something you don’t want to hear, then you’ll find ways of not believing them. That’s why a major part of the LessWrong project is learning how to actually change your mind. A whole Sequence of the Sequences, in fact, bears that exact title.\n\n‘Scott [Alexander] said something I thought was really central to our enterprise,’ Paul Crowley told me. ‘Just as it’s good to have a lot of money, it’s good to have as much evidence as possible. But just as it’s good to get by on how much money you have, it’s good to be able to be as accurate as possible with the evidence you have. Sometimes the universe is not going to lavish you with evidence, sometimes you have to be as accurate as you can, with what you can get.’ And that means getting rid of ideas when the balance of evidence is against them, rather than – as our biases would have us do – hanging on to them for as long as we can. ‘We wait until we’re overwhelmed,’ said Paul. ‘It’s a long, slow process.’ But instead, we ought to treat ideas we hold and ideas we do not hold equally with respect to the evidence.\n\nQuite a few of the Rationalists seem to come from religious backgrounds, and a key moment in their Rationalist life story is the point at which they gave up on religion, as they realised the evidence did not support it. ‘I haven’t had that one,’ Paul told me, laughing. ‘I was brought up by atheists. But I’ve had something similar.’ His parents are both socialists, and he followed them. ‘I was a card-carrying revolutionary communist in 1989, 1990,’ he said. ‘Then I moved towards being a more classically wishy-washy socialist type, wanting to achieve socialism by democratic whatever, but largely trying to come up with a position I could defend.’ Then, while reading the Sequences, he came upon the post entitled: ‘The importance of saying “Oops”’.¹\n\nHe’d been edging away from his socialist beliefs, retreating, ‘fighting a rearguard action’, as he put it. ‘But I felt like, on reading that, there was a level on which I already knew that this didn’t make sense, that I couldn’t sell it any more. I read it and just went, like, no. When you’re fighting the rearguard action, stop fighting. Stop, and reassess. Sometimes you have to say, “I’ve made a large mistake.”’ Instead of seeking the closest defensible position to your current one, try to ask where the evidence points, and sit in the middle of that.\n\nWhen Yudkowsky slowly moved from his original ‘the singularity will solve everything’ position to his later ‘AI might actually destroy everything’, he did it incrementally. ‘After I had finally and fully admitted my mistake, I looked back upon the path that had led me to my Awful Realisation,’ he wrote. ‘And I saw that I had made a series of small concessions, minimal concessions, grudgingly conceding each millimetre of ground, realising as little as possible of my mistake on each occasion, admitting failure only in small tolerable nibbles. I could have moved so much faster, I realised, if I had simply screamed “Oops!”’²\n\nNone of these techniques or tricks are ever going to turn a kludgy human intelligence into the pure Bayesian thought-being. But Yudkowsky’s hope (and the wider Rationalist project) is that by using them, people will be better able to assess ideas and decisions, on both a personal level and a societal one. (One of those decisions, he thinks, will be to take the issue of AI alignment seriously.)\n\nPart Six\n\nDecline And Diaspora\n\nChapter 32\n\n The semi-death of LessWrong\n\nThe Yudkowsky project we’ve been discussing, of explaining rationality, human thought, where they differ and how to make the latter more like the former, grew out of SL4 and Overcoming Bias, and became the Sequences and LessWrong, and the Rationalist community.\n\nLessWrong was its central hub for a long time. But in about 2012 LessWrong started to die off – not completely, but its numbers dropped significantly. A peak of a million or so page views a month in early 2012 dropped to about 350,000 a month by mid-2016.¹ There are lots of reasons behind the decline, but here are the main two. One, Eliezer Yudkowsky felt that he’d finished the ‘Sequences’ at some point a couple of years earlier, towards the end of 2010. So he stopped blogging. And two, in 2013 Scott Alexander – known on LessWrong as Yvain, and probably the most prominent blogger on the site after Yudkowsky himself – started his own blog, Slate Star Codex.\n\nThere were other reasons. Robin Hanson told me that he thinks it’s partly because, as with many things, the Rationalists tried to reinvent everything from scratch. LessWrong, the website, wasn’t just a blog: it was a custom-designed community hub, based on a Reddit-like voting system – if you like a post or a comment, you press the ‘up’ arrow; if you don’t, you press the ‘down’; the site’s algorithm is more likely to show you things with lots of upvotes than things with lots of downvotes. But it got gamed: according to Scott Alexander, ‘one proto-alt-right guy named Eugene Nier found ways to exploit the karma system to mess with anyone who didn’t like the alt-right (i.e. 98 per cent of the community) and the moderation system wasn’t good enough to let anyone do anything about it’.²\n\nIt’s part of a wider attitude among the Rationalists, said Hanson, of thinking they can rebuild everything. ‘They’re people who are smart and articulate and they have ideas in their heads about how things should be different. And they want to implement them all, and are unpersuaded by the fact that other people have tried them before and failed.’ The Rationalists, unsurprisingly, do not agree with this assessment, but there is an element of truth to it.\n\nSo they convinced themselves, Hanson thinks, that by studying the Art of Rationality – the ‘Bayesian judo’ that Yudkowsky was teaching, all that stuff about noticing confusion and thinking probabilistically – they could avoid the pitfalls of irrationality that flaw other people’s thinking and create new, shiny things. ‘They decided that they could make better technology,’ he said, ‘like the LessWrong software. They get involved in start-ups, they think they know how to redo romance with polyamory, they think they know how to redo diets with the diet things they get into. They’re all over the place with whatever.’ It reminded him, he said, of some Silicon Valley people he was involved with in the 1980s, the Xanadu Project, who were working on the first hypertext systems, and some others who were interested in nanotechnology. ‘There were a lot of young idealistic people trying to save the world through start-ups and tech. As usual they were into science fiction and the future and how everything would change enormously, and they were into trying everything different. At Xanadu they had to do everything different: they had to organise their meetings differently and orient their screens differently and hire a different kind of manager, everything had to be different because they were creative types and full of themselves. And that’s the kind of people who started the Rationalists.’\n\nAnd Scott Alexander has his own theories, which he expounded on a Reddit thread in 2017. One was that Yudkowsky skirted ‘the line between “so mainstream as to be boring” and “so wacky as to be an obvious crackpot”’, which many other bloggers struggled to do, either because they were boring, or because they were crackpots, or because although they weren’t crackpots they also weren’t very good at not coming across as crackpots. Yudkowsky also came to realise, Scott said, that he is ‘a pretty weird person, and now that the community’s more mature it helps for it to have less weird figureheads’.\n\nScott added that the community became ‘an awkward combination of Google engineers with physics PhDs and three start-ups on one hand, and confused 140-IQ autistic 14-year-olds who didn’t fit in at school and decided that this was Their Tribe Now on the other’, and that it was hard to find the ‘lowest common denominator’ that appealed to both groups. The end result was that ‘LessWrong got a reputation within the Rationalist community as a bad place to post, and all of the cool people got their own blogs, or went to Tumblr, or went to Facebook, or did a whole bunch of things that relied on illegible local knowledge [by which he means the sort of understanding of a community that only comes from living in it – like how you’ll always know your home city better than someone who’s just read the Lonely Planet guide to it]. Meanwhile, LW itself was still a big glowing beacon for clueless newbies. So we ended up with an accidental norm that only clueless newbies posted on LW, which just reinforced the “stay off LW” vibe.’\n\nThe point about ‘illegible local knowledge’ is definitely true from my experience. I’ve been hanging around LessWrong-ish circles for a few years now, and I regularly still stumble across giants of the Rationalsphere whom everyone else just seems to know but who never wandered into my field of vision before: names like Gwern and Nostalgebraist and The Unit of Caring. There’s no natural central hub any more, and you can’t learn the paths and backwoods of Rationalist country without wandering around it, lost, for years. (There is a map. It’s by Scott Alexander. But I don’t think you could use it to navigate without first knowing where everything is anyway. Still, the puns – ‘Reasoning Sound’, ‘Bight of Information’, ‘The Reverend Thomas Bay’ – are absolutely impeccable.)³\n\nWhatever caused the semi-death of LessWrong, the fact remains that it happened, and the people involved in it spread across different parts of the internet. Slate Star Codex gets about 20,000 views a day, about 600,000 per month – down from the LessWrong peak, but still a significant number.\n\nChapter 33\n\n The IRL community\n\nWhat also happened is that a real-life community sprang up. There are Rationalist (and ‘Rationalish’, ‘Rationalist-adjacent’, LessWrong and Slate Star Codex) meet-ups all around the world: I can see Facebook groups for Munich, the Netherlands, Israel, Montreal, London, Reading, Bath, Sydney, Denver, DC, Canberra, Edinburgh, Darmstadt and Phoenix, just via a very quick search. I know there’s a group in Melbourne as well, and Berlin, and a nascent one in Manchester. The Bay Area and New York have the largest communities.\n\nMany of the community members live in group houses, although not all or even a majority. This doesn’t appear to have been a cult-leader invention enforced from the top by Eliezer Yudkowsky; Paul Crowley thinks that ‘the group house phenomenon is just a necessity given the Bay housing situation’ (house prices there being, to my amazement, comparably extortionate to London), which is where a large number of them live, and that, anyway, Rationalists are the sort of people who tend to end up in group houses. ‘The libertarian/futurist/Burner [Burning Man festival regular] circles were in named group houses long before Eliezer moved here,’ he said. ‘At university in the 1990s my social circle was the science-fiction society, and we all lived in named group houses.’\n\nBen Harrison, a British man in his early twenties who got involved with the Rationalists through reading the blogger Gwern and ended up following the LessWrong/Slate Star Codex stuff, told me that the group houses are ‘a bit like university halls, but the kitchen sink is a little cleaner’. His group house, in Manchester, was set up explicitly as an alternative to the Bay Area, which tends to attract many Rationalists, but because of the aforementioned housing costs has a high bar to entry. Some of the group houses are polyamorous; some are not.\n\nI wanted to get an idea of what the IRL community was like, so I went to a few of the meet-ups. I met Scott Alexander, the AI researcher Katja Grace, who was in a polyamorous relationship with Scott at the time, and a couple of others at a pizza place in Berkeley while I was there, where we discussed AI safety and whether or not my book was going to be a catastrophe that increased the likelihood of a paperclip apocalypse. (Buck Shlegeris, a young MIRI employee with excitingly coloured hair and an Australian accent, told me that ‘A book on this topic could be good’, and that ‘if I could jump into your body I have high confidence I could write it’. However, his confidence that I could write it from within my own body seemed significantly lower, which is probably fair enough.)\n\nI distinctly got the impression that the IRL community is, like the online community, a venue for people who are a bit weird, not very good at small talk, and interested in big ideas. There were a couple of things that stood out for me while I was there. One was that, for a few minutes, I couldn’t find Katja. Then it turned out that was because she was sitting with a baby on her lap. I knew she didn’t have a child, so I’d discounted the woman with the baby as obviously not her. Upon closer inspection, it turned out that the baby was one of those robot babies that some American high schools give out to teenagers, to give them an understanding of how hard parenthood is (and thereby scare them off sex for ever, in my stereotyped picture of American high schools, although I suspect that’s not fair).\n\nI thought at first it was a weird affectation, but she turned out to be running a rather sensible experiment. She and Scott were considering having children, and she wanted to know what the disruption to her life would be like. So she got one of those babies that wails when you leave it alone, and wakes up several times in the night, and needs its nappy changing, and so on, to get an impression of whether motherhood was for her.\n\nWhat she had failed to consider, which I think is sweetly typical of the Rationalists in a lot of ways, is that people would stop her in the street and say, ‘Oh, cute, a baby!’ and she’d have to awkwardly explain that it was not, in fact, a baby, but an experimental robot. Still, I hope the experiment gave her some non-zero level of insight into what parenthood involves; I did try to downplay her expectations on that front, given my experience that the challenges of newborn-baby-parenthood are only vaguely related to those of hyperactive-toddler-parenthood and, I assume, even less related to the stages after that.\n\nAnother thing that interested me was the almost complete absence of small talk – I’m a nervous talker, so I found myself gabbling to fill spaces in the conversation. It was Big Topics or nothing. And they actually pay attention to the arguments you’re making; in my incoherent blather I was trying to justify the idea of writing this book (of which they’re all sceptical, to a greater or lesser degree), and used several, mutually incompatible reasons for doing it. Katja in particular noticed and pulled me up on it.\n\nWhen the time came to pay for our pizzas, we played a strange little game. We used someone’s phone to come up with a quasi-random number between 0 and the price of the bill, and then counted down through the items to see whose meal it ended up on; that person then had to pay the whole bill. (Imagine there were two of us, just me and Scott, and the bill said ‘Tom’s Pizza, $10; Scott’s Pizza, $10’. If the random-number generator came up with a figure between 0 and 10, I’d have to pay for it, because I was first on the bill; if it came up with a figure between 10.01 and 20, Scott would have to pay for it.) It ended up on Katja, so she paid for everyone’s meal. I felt guilty and tried to pay anyway, but Buck stopped me: ‘If you’d lost you’d have had to pay the whole thing. It’s fair.’\n\nIt has only just occurred to me now, as I’m writing this nearly eight months later, how clever this system is. Splitting a bill according to who had what is time-consuming, boring and socially awkward. But splitting it clean down the middle incentivises people to order more expensive meals; they get all the benefit of the nicer meal but only pay a fraction of the extra cost, in a classic tragedy of the commons. But this system was extremely quick – almost as quick as dividing the bill on a calculator – and people who ordered more expensive things were more likely to pay the whole bill: if you ordered lobster thermidor for $80, the chances would be much higher that the number would end up on your bit. On average, you pay for exactly what you ordered. (Although it would take dozens of meals out for the averaging effect to cancel out the random noise.) Anyway, I thought it was clever, and very Rationalist.\n\nI went to a London meet-up as well, which was pleasingly British; it was in a pub, and some people other than me were actually drinking alcohol. (In California, with Scott and the others, I had one beer; they were all on Diet Coke, and although none of them was remotely judgemental about it I still felt like some great lumbering hooligan, as though I were about to rip my top off and start throwing the chairs around like an England football fan in a provincial Portuguese town.) There were 16 people there, mostly but not exclusively men, crowded around a table for perhaps eight in a Wetherspoons near Holborn. A large majority of the time was spent in a rather AGM-style discussion, earnestly establishing rules for how conversations should go: should there be set topics? Should there be reading materials?\n\nThis seemed to go on for an awfully long time, with no immediate danger of stopping, so I ended up following one of the more normal people to the bar and asking him what he enjoyed about it all. He lived outside London and it cost him £30 to get there, he said, so he didn’t do it often, but ‘I like being able to come here and not be normal, before I have to go home and be normal again.’ What did he mean by that? I asked. He meant he could talk about weird topics – AI, transhumanism, existential risk, biases, all the weird, nerdy stuff – and reliably be among people who wouldn’t think he was weird for doing it. ‘Plus,’ he said, ‘I can be a bit of a dick, and I like that I can say something really controversial, and instead of them being offended they all lean forward and say, “Let’s unpack that.”’ He did proceed – fairly shortly afterwards, once the AGM business had died down – to say something quite controversial about transgender rights, if I recall correctly, and lo, they did lean forward and they did talk about it seriously.\n\nLessWrongers also began various IRL projects. The Singularity Institute, now MIRI, already existed when the LessWrong decline began, but the Center for Applied Rationality (CFAR) was founded by Anna Salamon, Julia Galef, Valentine Smith and Andrew Critch in 2012.\n\nOther projects grew out of the LessWrong diaspora. There’s a food company called MealSquares which produces savoury cake-like things that are supposed to be a healthy meal in a polythene wrapper; the idea behind them is to make healthy, ‘optimal’ eating easy. (I tried one at CFAR’s office; it was like a heavy, dry-ish scone. It was ‘nice in an overpoweringly dense sort of way’, according to my notes from the time.) There’s Beeminder, a ‘reminders with a sting’ goal-tracking project that is free as long you keep hitting your targets; you only start paying for it if you miss them. You might want to start running 10 kilometres a week: you link your FitBit to Beeminder and it will charge you when you fail. It’s honestly clever.\n\nThey haven’t all worked out: there was a medical-consultancy start-up called MetaMed, founded by Michael Vassar, where Scott worked for a while, which tried to use the LessWrong Rationalist techniques in medical assessments; it failed rather sadly after a couple of years. I read a couple of post-mortems¹ online,² and the reasons it failed sounded very much as Robin Hanson would have predicted: the creators thought they could rebuild healthcare almost from scratch, and didn’t realise that there were huge amounts of unspoken local knowledge involved in both healthcare and business. I don’t want to sound mocking, but there were a lot of lines in the post-mortems which could be summarised as ‘Turns out marketing is important’ and ‘Turns out you actually need to do the job in front of you as well as think about the glorious future where AI solves everything.’ Robin’s comment about everyone being smart and articulate and convinced they can do everything better than it’s been done before, without really checking how it’s been done before, seemed very apt here.\n\nThere was a no-one-knows-exactly-what-but-it-appears-to-have-been-a-sort-of-Wikipedia-for-maths project called Arbital. Eliezer Yudkowsky was involved in that, along with some other Bay Area Rationalists. Again, the problem seems to have been a surfeit of big ideas and a shortage of actually knowing what they wanted to be doing right now. It had a ‘55-page document describing Arbital and how and why it was different and necessary’, written by Yudkowsky; it was a ‘better Wikipedia’, but it was also a blog and a discussion board and had a Reddit-like karma-upvoting system, and also provided ratings for things so it could replace services like Yelp. The post-mortem, after explaining all of this, sighs: ‘Now you can probably see how the meme of “Arbital will solve that too” was born . . . we just didn’t have a good, short explanation of what Arbital was.’ It shut down in 2017 without ever really getting off the ground, although its legacy is a genuinely good intuitive explanation of Bayes’ theorem (it’s all still online).\n\nI don’t want to be too harsh about any of this. Most start-ups fail, a fact of which the Rationalists are extremely aware; one of the biases discussed in the Sequences is the tendency to overestimate our own likelihood of success. Taking the ‘inside view’, most people think their business will succeed, but taking the ‘outside view’, about half of new businesses fail in their first five years; that figure is higher for Silicon Valley start-ups, with their whole ‘fail-fast’ ethos, at between 60 per cent and 90 per cent, depending on which study you read. So there’s absolutely no shame in two of the four Rationalist-led start-ups I am aware of failing in their first few years, and even though I do think it’s interesting that they did so in (what seemed to me) quite predictable ways, Yudkowsky would no doubt point out that, with hindsight bias, everything seems obvious once it’s happened.\n\nNot all Rationalists spend time in the IRL community, obviously. Some fairly central members have rarely if ever met their fellow Rationalists in person. Jim Miller, a professor of law and economics at Smith University in Massachusetts, and one of the original Overcoming Bias crowd – he was blogging on Robin Hanson’s website back in 2007 – spoke to me over Skype and said he’d never actually met any other Rationalists IRL, apart from, coincidentally, at a conference in Sweden a few weeks previously. ‘If I was a college student I would have seriously considered moving to the Bay Area,’ he said, ‘if I wasn’t married or anything. But not at this stage of my life [he’s in his early fifties, and married with children]. Also,’ he added thoughtfully, ‘living in a small part of a house in Berkeley just seems kind of horrifying to me, actually. When I was living in a dorm it would have been fine, but going back to that kind of life . . .’\n\nBut even just online, the Yudkowskian project has nonetheless had a profound impact on his life. For one thing, he steered his career towards it: he teaches about the economics of the far future, talks in his classes about cognitive bias, and has written a book on the potential rise of AGI, called Singularity Rising. ‘Before [LessWrong], I was a traditional economist,’ he said. ‘Not assuming that rationality is perfect, but thinking it’s a pretty good model and we don’t need to go beyond that. The LessWrong stuff convinced me that economists should be looking at cognitive biases.’\n\nHis day-to-day decisions in his personal life have changed to some degree, as well. Most dramatically, he is taking steps to extend his life, for a very simple reason: if the singularity comes, then the difference between dying the year before it and the year after it is almost incalculable. ‘If we do achieve the singularity and allow indefinite life extension, it could easily happen after I naturally die, so the expected value to me of living a few more years is huge.’ If there’s, say, a 0.1 per cent chance that those extra few years might get him to the glorious future, and the glorious future means a subjective life of a million years, then that 0.1 per cent chance translates to an expected value of a millennium of extra life.\n\nHe has signed up for cryonics, as many Rationalists have, and is an adviser to the board of Alcor, one of the largest cryonics firms. He has changed his diet – ‘I’ve gone mildly paleo’ – and talked his doctor into prescribing him a diabetes drug called metformin, because there’s some evidence that it extends lifespan through an anti-cancer effect. Is the evidence solid, I asked him? ‘The evidence is solid that it doesn’t do harm and it might be good,’ he said. ‘So many people take it that a significant negative effect would be known.’ (Sincere warning: please do not take this as any sort of medical advice.)\n\nThis online-only connection to the Rationalist community is probably the norm. The 2016 LessWrong diaspora survey found³ that only 8 per cent of respondents ‘regularly’ attended meet-ups, and another 20 per cent had done so ‘once or a few times’. Another question asking about ‘physical interaction’ with other LessWrong community members (for example, ‘Do you live with other LessWrongers, or are you close friends and frequently go out with them?’) found 7.6 per cent did ‘all the time’ and 12.5 per cent ‘sometimes’. The large majority, at least according to this survey, have never met another Rationalist in person. The 2018 Slate Star Codex survey found similar results: only 10 per cent of respondents had ever been to a meet-up, and only 24 per cent of those still went to them regularly.⁴\n\nBut there is a hard core of Rationalists, perhaps a few hundred or a few thousand worldwide, who are more committed: who go to the meet-ups and live in the group homes, who (in many cases) financially support MIRI or other Rationalist groups, and who engage in polyamorous relationships. They’re an unusual bunch of people, and they are centred around a few charismatic figureheads. This has led to accusations of them being a ‘cult’. But are they?\n\nPart Seven\n\nDark Sides\n\nChapter 34\n\n Are they a cult?\n\n‘They’re a sex cult,’ says Andrew Sabisky.\n\nSabisky is an interesting man. He’s a superforecaster, like Mike Story (whose massive dog crushed my feet), and exists in the same internet circles as the Rationalists. Unlike Mike, though, he seems to be a sort of enemy of the Rationalists, or at least a thorn in their side – he’s certainly one of their most vocal critics. He’s also kind of weird, in an endearing sort of way. Though there’s no doubting the sincerity of his beliefs, he seems to have decided to become a Christian on the basis that tradition and ritual are important for humans, started running the social-media accounts for a central-London church, and appears to have got from there to actually believing in God. (‘He memed himself into it,’ Mike Story says, fondly. Apparently, there’s a theological idea of ‘act as if you have faith and faith will be given to you’, and that’s what Sabisky did.) During our two-hour conversation in a swanky central-London coffee house, we strayed away from the Rationalists and AGI and at one point ended up talking about the English Civil War, which Andrew is firmly against. ‘They cancelled Christmas and they killed our king!’ he declares, in a loud and declarative voice that may be ironic but may just be wearing the clothes of irony to distract from the fact that he 100 per cent means it.\n\nThe idea that the Rationalists are a cult – whether sex or otherwise – is not uncommon. It is, in fact, the subject of much writing by Yudkowsky and Scott Alexander, who appear to worry about it as a realistic problem. In fact, in the process of writing the Sequences Yudkowsky wrote so many things about avoiding cultishness that the word ‘cult’ started getting suggested as an autocomplete when you searched for LessWrong on Google. This was seen as suboptimal, so (in a delightfully Rationalist way) they asked everyone using the word ‘cult’ in a post to use instead a simple substitution code, shifting all the letters by 13 in the alphabet – so A→N, B→O, etc. – and turn it into ‘phyg’.¹ (It doesn’t appear to have worked: when I search ‘lesswrong’ in an incognito window so my search history doesn’t come up, ‘less wrong cult’ is the second option. Still, it was a nice idea.)\n\nIt’s worth addressing. They do share a lot of the surface features of a cult: a charismatic figurehead and other high-status inner-circle members; a key text that in-group members are supposed to have read, and which encodes the central tenets of their ‘belief’; unorthodox sexual practices; a message of impending apocalypse, and a promise of eternal life; and a way to donate money to avoid that apocalypse and achieve paradise.\n\nIn case that needs spelling out, I mean Yudkowsky as the figurehead and others – Bostrom, Scott Alexander, Rob Bensinger, Luke Meulhauser, etc. – as the high-status inner-circle men; the Sequences, and to some extent Yudkowsky’s Harry Potter fanfic Harry Potter and the Methods of Rationality, as the local bible; the Rationalist tendency towards polyamory as the sex stuff; the AI apocalypse, and the cosmic endowment, as the eternal life; and MIRI as the recipient of the indulgences.\n\n‘Yudkowsky and his wife and girlfriends,’ says Sabisky. ‘He used to be, like, an uber virgin. Then he got famous and started a sex cult, as you do. What else would you do with all that fame within a very narrow circle? If you look at his output, the main one is the Harry Potter fanfic. It’s not aimed at making money, it’s clearly just a thing that attracts people into your sphere. That’s their whole point.’\n\n‘They’d have to be fucking blind to see they hadn’t formed a cult’, according to David Gerard of RationalWiki, which in the internet’s sceptical–rational website wars is the Judean People’s Front to LessWrong’s People’s Front of Judea. ‘They tried not to become a cult, they asked themselves if they were being cultish, which was the right thing to do, but it happened anyway.’ The Effective Altruism movement is part of it, he says, insofar as it wants you to give money to prevent AI apocalypse. ‘Some charities are more effective than others, and you should donate to the more effective ones,’ he wrote on Tumblr, ‘and clearly the most cost-effective initiative possible for all of humanity is donating to fight the prospect of unfriendly artificial intelligence, and oh look, there just happens to be a charity for that precise purpose right here! WHAT ARE THE ODDS.’²\n\nI asked Paul Crowley about the whole cult thing, and it was the one time he got even slightly angry. ‘It drives me up the wall!’ he said. ‘We’re exceptionally good at this.’ By ‘this’ he means ‘not becoming a cult’. ‘But there’s no way to say, “Oh no we’re not”, because you must be a cult if you’re denying it.’ He acknowledges that the Rationalist movement is ‘weird’: ‘There’s no way to deny that.’ But they have a cause, and that cause is saving the world from unfriendly AI, and so they want to get people involved with it. ‘Everyone does that, right?’ he asked. ‘You think, you know what, climate change is a problem! So I need to get some people on board with the idea that climate change is a problem. Or I think the Democratic Party is going in the wrong direction, I’ll get everyone I know on board with changing it. As soon as you have a cause, you want to get people on board and say we should talk about this.’\n\nAccording to Paul, the thing that distinguishes a cause from a cult is when it becomes taboo to criticise the cult. ‘What’s dangerous is when you start to attack people’s ability to think critically about it,’ he said. ‘A common trick, for example, is to say that questioning the precepts of this is morally wrong. If someone says, “I’m not sure that’s true,” and your reaction is, “You’re a bad person for even asking,” then that starts to get dangerous. On that score, I think we do unbelievably well! We’re out there on the far end of the scale of how comfortable we are with people asking those questions.’\n\nThis is something that Yudkowsky has thought about himself. He thinks cultishness is ‘a high-entropy state into which the system trends, an attractor in human psychology’, by which he means that, left to its own devices, any group based on some noble cause will naturally slide into something like a cult. ‘Every group of people with an unusual goal – good, bad, or silly – will trend toward the cult attractor unless they make a constant effort to resist it. You can keep your house cooler than the outdoors, but you have to run the air conditioner constantly, and as soon as you turn off the electricity – give up the fight against entropy – [it] will go back to “normal”.’³\n\nIt doesn’t matter, writes Yudkowsky, if the cause is one of rationality and science and introspection: ‘Labelling the Great Idea “rationality” won’t protect you any more than putting up a sign over your house that says “Cold!”. You still have to run the air conditioner – expend the required energy per unit time to reverse the natural slide into cultishness. Worshipping rationality won’t make you sane any more than worshipping gravity enables you to fly.’\n\nHe has dedicated quite a lot of time and energy, in the form of several blog posts of some length, to avoiding the problems of cultishness. The question is: how well have they succeeded?\n\nA lot of the LessWrongers do hero-worship Yudkowsky, to some degree. You can read it in the tone of their posts. That’s hardly surprising, since he’s a charismatic figure at the centre of everything they care about, and many of them are young men and teenage boys who have a tendency to do that sort of thing. One woman, who is more involved in the Effective Altruist community but hangs around with Rationalists as well, told me (anonymously): ‘I do think the Rationalist community, at least in parts, has troubles with this heroic narrative, where some people are these sort of superheroes who can do anything. Some people see Eliezer as that, although I don’t know if he plays up to it. And [another high-profile Rationalist] sees himself like that.’\n\nAnd there’s the fact that they want you to donate money to MIRI to stop the world from being destroyed. That is quite apocalypse-culty. Take this, for instance, on the money side of things: Yudkowsky was asked in 2010 what his advice was for people who want to help save the world, and he said: ‘Find whatever you’re best at; if that thing that you’re best at is inventing new math of artificial intelligence, then come work for the Singularity Institute. If the thing that you’re best at is investment banking, then work for Wall Street and transfer as much money as your mind and will permit to the Singularity Institute where [it] will be used by other people.’⁴ It’s not great, at first blush, is it? ‘Get rich and give us every penny you can spare to prevent the apocalypse.’ And people did donate (although a difference from a classic apocalypse cult is that donations are intended to save everyone, rather than to buy salvation for the donor specifically).\n\nThat said, there is a pretty good counterpoint against the idea that the Singularity Institute, as it was then, or MIRI as it is now, is a hoover for sucking up gullible people’s donations. That is, Yudkowsky has stopped asking for them. He appeared on the philosopher Sam Harris’ podcast in early 2018, and stated: ‘thanks mostly to the cryptocurrency boom – go figure, a lot of early investors in cryptocurrency were among our donors – the Machine Intelligence Research Institute is no longer strapped for cash, so much as it is strapped for engineering talent’. He feels that MIRI needs more engineering staff in order to spend the money it has. ‘We can obviously still use more money,’ he said when I asked him about it, ‘but our organisational attention has shifted to finding researchers and engineers.’\n\nAnd, according to their finances, which are available on their website, the 10 ‘research’ staff shared a total annual salary of $585,000 in 2017.⁵ No doubt Yudkowsky and the executive director Nate Soares account for a decent percentage of that, but even if the other eight people are on a fairly-low-for-Berkeley $40,000 a year, it would put those two on about $130,000, which is obviously a decent wage, but pretty comparable to that of a good software engineer. It’s not a perfect metric, but I would imagine that Joseph Smith wouldn’t have been quite so abstemious in paying his own wages out of the Church of Jesus Christ and Latter-Day Saints’ revenues.\n\nBut here’s why I don’t think they’re a cult. Or, actually, let me put it another way. You could call them a cult, if you like. But it would involve defining the word ‘cult’ in terms that would remove most of the things about cults of which we are most wary.\n\nI’m going to expand on this by referring to a post of Scott Alexander’s, from before his LessWrong days.⁶ It was titled ‘the worst argument in the world’. The argument goes like this: ‘X is in a category whose archetypal member gives us a certain emotional reaction. Therefore, we should apply that emotional reaction to X, even though it is not a central category member. Call it the Noncentral Fallacy,’ says Scott. It sounds a bit obscure, but we all do it, all the time.\n\nScott’s first example is Martin Luther King. Imagine someone wants to build a statue of MLK in a city somewhere. Someone objects to it: ‘But Martin Luther King is a criminal!’ What’s your response to that? ‘No he wasn’t’? But he was: he broke the law by protesting against segregation. It was a shitty law, but he broke it. He was a criminal. And your opponent is saying that because criminals are bad, and MLK was a criminal, we should think MLK was bad.\n\n‘The archetypal criminal is a mugger or bank robber,’ says Scott. ‘He is driven only by greed, preys on the innocent, and weakens the fabric of society. Since we don’t like these things, calling someone a “criminal” naturally lowers our opinion of them.’ But MLK is a noncentral example of a criminal. He wasn’t driven by greed or preying on the innocent. ‘Therefore, even though he is a criminal, there is no reason to dislike King.’ But this is a really hard thing to argue against, when you’re in the brutal cut-and-thrust hand-to-hand combat of an argument on the internet. You would not instinctively respond, ‘Yes it is true that Martin Luther King was a criminal, but he did not share the features of criminals that make them bad, so your suggestion that he is bad is based on faulty logic.’ You would, Scott suggests, be much more likely to say: ‘No he wasn’t! Take that back!’ And then the exchange becomes about whether he was a criminal or not, and ‘since he was, you have now lost the argument’.\n\nThis sort of thing happens all the time. ‘Abortion is murder’, ‘Taxation is theft’; the argument is, ‘Abortion shares some features with murder, so you should be as outraged by it as you are by murder’, or ‘Taxation shares some features with theft, so you should be etc.’ And of course, if you try to argue against it by saying, ‘Well, it’s the good kind of theft’, your unscrupulous enemy will say (to quote Scott): ‘Apparently my worthy opponent thinks that theft can be good; we here on this side would like to bravely take a stance against theft.’\n\nThis is what I think is going on in the ‘cult’ argument. I spoke to some Rationalists who were, in fact, much more sanguine about describing LessWrong and Rationalism as a cult, or a religion. For instance, Ben Harrison, the young British Rationalist who founded a group house in Manchester, was happy to call it exactly that. ‘I was accused of being a cult leader last week,’ he said, offhandedly. He’s pretty laid-back about the whole thing. ‘There are elements of religion to it. The structure it provides, the figures [such as Yudkowsky]. We’ve got holy texts, people who dedicate their lives to it. It has most of the trappings of religion; I’d call it a pseudo-religion. I can see what people are pointing at there.’\n\nBut where the ‘worst argument in the world’ comes in is that while it has a lot of the features of a cult, it is not a central member of the category ‘cult’. ‘The word “cult” is really just a name for a strong community that’s disapproved of,’ Robin Hanson – the economics professor who first hosted the Sequences on his blog, Overcoming Bias – told me. ‘When I was a young teenager, 12 or 14, I was in a Christian cult.’ But the cult never ordered him to dissociate himself from his friends or family, and when he left after a year or so, he was free to do so. ‘There are coercive cults, but the cult I was in wasn’t one, and [the Rationalist community] certainly isn’t either. It’s rare, really, for cults to have these dissociation rules. Usually they just have strong eagerness to get close to one another, and spend time with one another in group homes and so on.’\n\nThat’s not to say there’s nothing to worry about. Anna Salamon, the founder of the Center for Applied Rationality, told me that she thinks the ‘cult’ question is probably the wrong one. ‘I don’t worry about the cult thing very much,’ she said. ‘I agree it’s important that we [don’t coerce people or crack down on dissent].’ She doesn’t think they do do that, for the record. What she worries about is that people will miss the thing they’re trying to do – teach ways of thinking, ways of examining your own thinking – and just pick up on the ‘AI apocalypse’ conclusions. ‘I worry about making sure that our classes aren’t too convincing of the wrong thing. We try to share the processes for thinking, and how to verify those processes, rather than duping people into believing specific conclusions because someone at the front of the room said them.’\n\nThe mental techniques – the use of maths and Bayesianism to support conclusions about everyday things, for instance – are weird and ‘a little bit scary’, and they try to get people to take them seriously. But if you just put, say, Bostrom’s numbers about the cosmic endowment in front of someone, and don’t teach them the stuff that contextualises them – that lets you check for yourself whether it’s worth being worried about it – then, she thinks, it could go a bit wrong. ‘It freaks me out a bit,’ she said. ‘I’m a little afraid that someone will just trust the numbers. For example, AI risk is a very big topic, and it’s easy for people to feel hijacked by it, and be like, “Aha, I’ll ignore all the things I normally care about and just care about this thing.” I think that’s mostly a bad idea.’\n\nIt’s not that she doesn’t want people to worry about AI risk, but rather that she wants people to be able to examine the question – the maths, and reasons why you shouldn’t just blindly follow the maths – for themselves. She wants people to work that out for themselves, not to take it on faith from some authoritative source. This strikes me as doing the work that Yudkowsky mentioned, the ‘air-conditioner’ work: actively trying to stop your cause from turning into a cult. From my point of view, it seems to be working.\n\nBut Sabisky’s suggestion wasn’t simply that they are a cult – it was that they are a sex cult, that the entire movement exists to (or at least has conveniently turned out to) enable powerful Rationalist men to have sex with lots of impressionable young women. This revolves, at least in part, around the fact that a large number of the full-time Rationalists – the ones who live in group houses and/or work for CFAR or MIRI, and the ones centred around the FHI in Oxford – are polyamorous. This is true of both men and women. Eliezer Yudkowsky himself is polyamorous: he has a wife and two other partners, all of whom are themselves polyamorous and engaged in other relationships.\n\nThis is where the ‘sex-cult’ accusations come in. You can see how that might be something worth worrying about. A man builds a community in which he is a figurehead; he gains power and prestige which enables him to attract the sexual attention of several women; the community conveniently develops an unwritten rule which says that he is allowed to have sexual relationships with several women. It ‘pattern-matches’, to borrow the terminology that the Rationalists use, extremely well to things like David Koresh, the Texan cult leader and ‘final prophet’ who died under police gunfire in a siege in 1993. This is basically how Andrew Sabisky sees it. ‘It seems to revolve around the highest-status men, and they get to pick and choose.’\n\nBut there are several ways in which this story is at best incomplete, and, at worst, downright wrong. For one thing, Paul Crowley is a Rationalist, and he’s also poly, but he was poly for many years before he became a Rationalist; he’s never been anything but poly. It’s absolutely the case that some people did turn poly after joining the Rationalists – Scott Alexander, for instance – but to some degree, at least, the Rationalist community attracted poly people, rather than simply instilling a code of polyamory from on high.\n\nAlso, it should be pointed out that polyamory is not the norm even among Rationalists. It’s more common in the Rationalist community than elsewhere, I think, but perhaps not as much more as you’d expect: the Slate Star Codex 2018 survey⁷ found that slightly less than 10 per cent of its 8,000 respondents said their preferred relationship style was poly; a 2014 LessWrong survey⁸ put the figure at about 15 per cent.\n\nI don’t have reliable numbers for the population at large, but I read an estimate (from a blog post in Psychology Today, quoting an ‘independent researcher’⁹ – apply salt liberally) that there were ‘around 1.2 to 2.4 million’ American couples ‘actually trying sexual non-monogamy’; about 10 million if you include all couples who ‘allow satellite lovers’. There are about 120 million adults of each sex in the US. If we assume that three-quarters of them are in committed relationships, then we’re looking at something between 2 per cent and 10 per cent of all couples who are poly, depending on your definition. A study found that 21 per cent of people had at least tried it, at some point.¹⁰ Another article in the legal magazine Advocate claimed, with no references, that ‘most researchers estimate that a full 4–5 percent of Americans participate in some form of ethical non-monogamy’.¹¹ I think it’s probably OK to guess that 5 per cent of Americans prefer polyamorous relationships. That makes it at least twice as common among Rationalists, but we’re not dealing in orders of magnitude or anything.\n\nAlso, the average Rationalist is significantly younger than the average American. I have no data on this whatsoever, but since articles about polyamory tend to refer to it as a ‘new trend’, I assume it’s more common among young people.\n\nI expect it’s even more common among people who tend to look at social norms and say: ‘Why do we do this and do we need to carry on doing it?’, which is of course exactly the sort of person who would join the Rationalists. ‘I think there’s a phenomenon where a social circle that is OK with unusual choices will have a lot of poly people in it,’ points out Paul Crowley. ‘Lots of folk I know on the goth scene are poly, for example. Same for kink, sexuality, transness.’ It may well be that Rationalists are no more likely to be poly than any other group of young, nonconformist oddbods. The superforecaster Mike Story agrees: ‘The polyamory thing isn’t like, “Let’s be in the Rationalists and have lots of sex.” It just so happens that if you think this way, you probably also think that way.’\n\nAll that being said, the surveys are addressed to the wider internet population. There isn’t some ringfence around ‘the Rationalist community’ which says this person is in and this person is out; everyone who read LessWrong in 2014, and everyone who read Slate Star Codex in 2018, was asked to take the survey. I think I took the 2017 SSC one, and although I’m a sympathiser, I’m not really a member of the community.\n\nBen Harrison, the founder of the group house in Manchester, pointed out to me when I spoke to him that there are circles and circles. ‘The biggest is the people who read Slate Star Codex casually,’ he says, a circle which includes a large number of people I know who absolutely would not call themselves ‘Rationalists’. ‘Then there are the wider people who read all the blogs but don’t feel the need to be part of the scene socially. Then there are all the people on the Discords and the Slack channels [group chats]. Then there are the people who go to the meet-ups, then the people who live in the group houses, and then the people who exist, in an economic sense, entirely in the community.’\n\nAnd it does seem that the further you get into the inner circles, the more likely it is that you’ll be poly. That makes sense in the light of the idea above: the more devoted you are, the more nonconformist you’re likely to be, compared to vanilla dilettantes like me. But I think a majority of the people who actually work at CFAR and MIRI, and (although no one will actually confirm this for me) large numbers of the Future of Humanity types are poly. Buck Shlegeris of MIRI, the young Rationalist I had pizza with, estimated that 70 per cent of Rationalists are poly. It depends where you draw your line around the term ‘Rationalist’.\n\nThe poly aspect has undoubtedly led to scenarios which would raise a lot of people’s eyebrows. ‘Have you heard the story about the LessWrong baby?’ Sabisky asks me, somewhat salaciously. I had, vaguely. What happened was that a young woman in the Bay Area was in a polyamorous relationship, or interlocking set of relationships – the standard mono terminology gets a bit confused. She had a husband, but she was also in a relationship with another man, a high-status, extremely wealthy member of the Rationalist community who himself was married.\n\n‘So anyway she’s shagging this guy,’ says Sabisky. ‘And they have an agreement – because, apparently, contraception is hard to use – that if she gets pregnant she’ll have an abortion. Well, she does get pregnant by him, and can’t follow through, understandably.’ The upshot is that she had the baby, who by all accounts is now doing very well; she broke up with her husband, although that sounds like it was less to do with the baby and more to do with issues of his own.\n\nWhere it all blew up a bit was that someone started a crowdfunding thing among the Rationalists to help support her. She had, admirably to my mind, refused to try to get money from the father, because of this deal that she’d made. Scott Alexander plugged the crowdfunder gently in one of his blog posts. ‘And people slated her! Slated her!’ says Sabisky. ‘For not having the abortion. “You made a contract! You made a deal! Why should we pay for you?”’\n\nI have checked, and there is indeed a lengthy thread in the comments pretty much along these lines, occasionally interrupted by the woman herself saying ‘Please try to remember that I can read this and it is hurtful.’ It really wasn’t very pleasant. It made me understand, on a visceral level, exactly why some people dislike the Rationalist community: they’re unsettlingly willing to discuss things that many people find sacred, in a way that those people find profane. Most of the time it doesn’t bother me: I’m quite happy talking about the dollar value of a human life, say. But I found the open discussion of whether a woman ought to have aborted her now-18-month-old daughter because of a deal she’d made honestly hard to read. I’m a father of two, and perhaps it affected me personally in a way that other things do not.\n\nBut is it a David Koresh-style sex cult? Well. I think the ‘LessWrong baby’ story is unpleasant: a powerful and significantly older man driving his girlfriend to an abortion clinic, wanting no more to do with the baby once it was born, and not helping financially, because of a deal they had made before she got pregnant. But if this is the most egregious example of sexual abuse of power in the Rationalist community, then it seems, not trivial, but certainly no worse than things that happen all the time in many monogamous relationships.\n\nThe question, of course, is whether there are worse things. There’s this godawful Reddit page called /r/sneerclub which is dedicated to mocking the Rationalists, and they latched with enormous glee onto an OKCupid profile that Eliezer Yudkowsky put up in 2012. If you’ve ever read anything by Yudkowsky it’s immediately familiar: bombastic and self-promoting in a way that is meant to be ironic but isn’t really. It’s also very open about his fetishes, which attracted some mockery. And his ‘you should message me if’ bit says that ‘my poly dance card is mostly full at the moment’, but that you ‘shouldn’t worry about disqualifying yourself or thinking that I’m not accessible to you’, and should instead ‘test experimentally what happens when you try asking directly for what you want – that’s Empiricism’. ‘I’m also cool with trophy collection,’ he adds, ‘if you only want to sleep with me once so you can tell your grandchildren.’\n\nIt gives me an icky sensation, but again, it’s just a bit weird, not wrong, at least as far as my moral framework goes. If women want to message him, great; I hope it works out for all parties. But for some people the ‘ick’ factor is stronger, I think. I’ve heard it described as Yudkowsky ‘trawling for sex’, but I can’t see how you get there. He’s just openly stating that he wants sex and that if anyone wants it with him they should ask.\n\nI should also acknowledge that in June 2018, some extremely dark allegations were made in a suicide note by a former Effective Altruist and Rationalist who said she was abused. It is an awful read. But as far as I can tell, the allegations had already been properly investigated by people appointed to safeguarding roles in the Effective Altruism community. Some of the allegations were found to have a basis in fact, and some people were barred from, or had already been barred from, parts of the community; other allegations were found to have no such basis. You’d expect there to be some awful people in a community as large as the Rationalists, though, and it seems to me – having spoken to several people close to the issue – that the situation, though messy and complicated, was well handled by the people whose responsibility it was to do so.\n\nHere’s my position. I don’t think the Rationalist community is a sex cult. But people on the outside, those of us like me in our hetero-monogamous, married-and-settled-down, two-kids-and-a-people-carrier world, find their whole thing deeply weird, and for us it is very hard to separate ‘weird’ from ‘immoral’. For instance, for a lot of people it would be difficult not to be jealous. I asked Paul about this when I saw him and he said it just . . . never occurred to him. He is aware there are good evolutionary-psychology reasons why he should be jealous, but he’s just not.\n\nDr Diana Fleischman, an evolutionary psychologist herself at the University of Portsmouth and a prominent member of the Effective Altruism movement, thinks that this can come across as really weird to other people. ‘To the general public, it can seem like realigning your evolved motivation to such an extent that it makes you untrustworthy. If you can rewire your jealousy then you’re really dangerous, because you’re capable of anything. I think that’s part of the stigma.’ Fleischman is polyamorous herself, and completely unembarrassed discussing it, which makes one of us. She talked a bit about the ‘harem’ accusation. She pointed out that the arrangement tends to be genuinely polyamorous, not polygynous – it’s not that the men all have lots of girlfriends but the women are expected to remain faithful. All of the high-profile men do have multiple girlfriends, but those girlfriends usually have several boyfriends themselves.\n\nThis actually solves a problem, in her view. The Effective Altruist and Rationalist communities are heavily gender-biased: they’re mainly men. ‘Polyamory fixes the numbers problem,’ she said. ‘I made this joke once: Effective Altruism is like 75 per cent male, but the 25 per cent of women all have three boyfriends.’ I asked her whether it was a sex cult, and she laughed. ‘I was joking with someone the other day. ‘‘The Rationalist community isn’t just a sex cult,” we were saying. “They do other great things too!”’\n\nChapter 35\n\n You can’t psychoanalyse your way to the truth\n\nThere’s another point, which is that (on one, quite important, level) it doesn’t matter whether the Rationalist community is a cult, or a religion, or not.\n\nTake another example of a thing which some people say has the hallmarks of a religion: environmentalism. People who worry about the environment warn of an encroaching apocalypse, in the form of climate change. There is a prelapsarian past from which we have fallen away, through our own sin; it has charismatic prophets (such as George Monbiot and Al Gore) who warn of the impending doom; it has rituals we can perform in order to prevent that doom (such as recycling, or driving a Prius).\n\nI should say, by the way, that I’ve lifted that example from Scott Alexander, who was responding to a specific person claiming that environmentalism is indeed a religion.¹ He pointed out that, also, people have suggested that liberalism, conservatism, libertarianism, the social-justice movement, communism, capitalism, objectivism, Apple, and the operating system UNIX are also all religions. You can, depending on where you draw the line around the category ‘religion’, include pretty much anything in it.\n\nFor one thing, this is another example of the ‘worst argument in the world’. If I want to tar something with the brush of ‘religion’ (assuming that I am someone who thinks that religion is a bad thing), then I can point to these characteristics, claim that they are part of the definition of ‘religion’, and then say, ‘Therefore, environmentalism or whatever is a religion and you shouldn’t like it.’ But, even more importantly, it doesn’t tell you whether environmentalism is right or not.\n\nLet’s say that environmentalism really does share all the psychological hallmarks of real old-time religion. Say that, actually, the local Oxford low-carbon group or the cycling-promotion work that my extremely green parents are involved in are 100 per cent functionally identical, psychologically speaking, to churchgoing or evangelising. Say that paying for carbon offsets does precisely the same thing, in some religion-shaped part of your brain, as buying indulgences did for pre-Reformation Catholics. Say that the 2015 United Nations Climate Change Conference in Paris was indistinguishable, in its mental role, from the Second Vatican Council. Does that mean that carbon emissions are not in fact heating the planet? No! It tells us almost nothing at all.\n\nWhether or not the world is warming up is a fact that is based on how much heat energy there is in the atmosphere and the oceans. The reasons behind whether or not someone believes that the world is warming up is a fact about people’s brains. ‘You can’t find out whether the world is warming by asking about the psychology of Greens,’ Paul Crowley grumbled when I put the whole ‘cult/religion’ thing to him. ‘You have to look at satellite data, and CO₂ concentration, and this kind of thing. It’s ultimately a massive distraction. Psychology just doesn’t work if you want to find out about the world.’\n\nEliezer Yudkowsky has addressed this before, in an interview with the science writer John Horgan, who had previously called singularitarianism a ‘religious rather than scientific vision’.² ‘You’re trying to forecast empirical facts by psychoanalysing people,’ he told Horgan. ‘This never works. Suppose we get to the point where there’s an AI smart enough to do the same kind of work that humans do in making the AI smarter; it can tweak itself, it can do computer science, it can invent new algorithms. It can self-improve. What happens after that – does it become even smarter, see even more improvements, and rapidly gain capability up to some very high limit? Or does nothing much exciting happen?’³\n\nAnswering that question requires knowing actual facts about computer science, says Yudkowsky. You need to go and look. You cannot answer it by looking at the people who believe it and seeing whether you like them, or whether they seem weird.\n\nSo the question ‘Is the Rationalist community a cult?’ is not without value, insofar as would-be Rationalists might be in danger of getting fleeced of cash or manipulated into sex. But it doesn’t address the underlying question, which is: ‘Might humanity be destroyed by a badly aligned AI?’\n\nThat question is about what intelligence is, and how likely we are to be able to build it, and, if we do, what characteristics it will have (or we will give it). I believe there are excellent reasons to think that it might never happen (as well as some excellent reasons to think it might happen sooner than we’d expect). Not one of those reasons, though, is ‘because those guys on LessWrong look a bit culty to me’.\n\nChapter 36\n\n Feminism\n\nThere’s another thing I am just going to have to address, which is: are the Rationalists a bunch of scientific racists, Trump voters, alt-righters and misogynistic Men’s Rights activists?\n\nAs I’ve said before, I like the Rationalists. I think they’re a well-meaning and interesting lot. So it won’t surprise you to know that the short version of my answer to the question is going to be ‘No’. The slightly longer answer is going to be ‘No, but I can see why you might think that.’ The really long answer is below.\n\nThe Rationalists are nerds. They are, usually, people who are deeply interested in how things work. They are disproportionately male, and disproportionately on the autistic spectrum or near to it, according to the LessWrong and Slate Star Codex surveys – accordingly, as autism usually involves social deficits, many of them lack social skills to some degree or another.\n\nLots of them are, you will be unsurprised to hear, not very good at talking to women. This blew up in spectacular style at the end of 2014. Scott Aaronson is, I think it’s fair to say, a member of the Rationalist community. He’s a prominent theoretical computer scientist at the University of Texas at Austin, and writes a very interesting, maths-heavy blog called Shtetl-Optimised.\n\nPeople in the comments under his blog were discussing feminism and sexual harassment. And Aaronson, in a comment in which he described himself as a fan of Andrea Dworkin, described having been terrified of speaking to women as a teenager and young man. This fear was, he said, partly that of being thought of as a sexual abuser or creep if any woman ever became aware that he sexually desired them, a fear that he picked up from sexual-harassment-prevention workshops at his university and from reading feminist literature. This fear became so overwhelming, he said in the comment that came to be known as Comment #171, that he had ‘constant suicidal thoughts’ and at one point ‘actually begged a psychiatrist to prescribe drugs that would chemically castrate me (I had researched which ones), because a life of mathematical asceticism was the only future that I could imagine for myself’.¹ So when he read feminist articles talking about the ‘male privilege’ of nerds like him, he didn’t recognise the description, and so felt himself able to declare himself ‘‘‘only” 97 per cent on board’ with the programme of feminism.\n\nIt struck me as a thoughtful and rather sweet remark, in the midst of a long and courteous discussion with a female commenter. But it got picked up, weirdly, by some feminist bloggers, including one who described it as ‘a yalp of entitlement combined with an aggressive unwillingness to accept that women are human beings just like men’ and that Aaronson was complaining that ‘having to explain my suffering to women when they should already be there, mopping my brow and offering me beers and blow jobs, is so tiresome’.²\n\nScott Alexander (not Scott Aaronson) then wrote a furious 10,000-word defence of his friend.³ I can’t begin to do the argument justice, and in all honesty I’m scared of doing so, because I don’t want this book to be described as ‘an anti-feminist screed’. But his point, I think, can be boiled down to the idea that ‘male privilege’, while a real thing, is not the only form of privilege, and that a lot of online discourse comes down to attempts to define group X as having more or less ‘privilege’ than group Y, as though privilege is a one-dimensional thing. (Hence the constant battle over whether trans women have male privilege.) So if nerds claim to be suffering, then, to the people who think of privilege on this one-dimensional axis, they must be saying that they are less privileged than women.\n\nThere’s also a point he makes, which I think is a fair one, which is that nerdy people really are bad at talking to members of the opposite sex, and that this is a way in which they are truly disadvantaged: that nerdy people really do struggle with finding mates, that nerdy men are much more common than nerdy women (and men in general find it harder to attract sexual partners),⁴ and that love, sex, intimacy and affection are important things in human lives, without which we are usually less happy. This is a real problem that real people face. Furthermore, nerds are likely to be bullied and abused for being nerdy (I often think a lot of the sneering comments online about ‘fedoras’ and ‘neckbeards’, common terms of abuse on the internet for socially unskilled young men, are little more than an extension of playground bullying of autistic and/or nerdy children), and you have a genuinely bad situation which causes real pain.\n\nAnd you can say all that without saying that nerds are less privileged than women, or that their suffering matters more, and while acknowledging that nerdy men find certain careers easier to get into and progress in than nerdy women do. Instead you can just, in Scott Alexander’s words, say: ‘You feel pain? I have felt pain before too. I’m sorry about your pain . . . I will try to help you with your pain, just as I hope that you will help me with mine.’ Anyway, inevitably enough, Scott Alexander’s blog post defending Scott Aaronson blew up and everyone accused Scott Alexander as well as Scott Aaronson of being a sexist entitled nerd.\n\nThere’s a related problem, which is the ‘women in science, technology, engineering and maths [STEM]’ thing. It’s true that women are comparatively rare in some STEM careers, especially engineering and computer science; a study looking at 1998 data found that only 26 per cent of American tech workers were female.⁵ (Some reports suggest that the figure has actually gone down since then, to perhaps 20–23 per cent.⁶) Where the Rationalists (and specifically Scott Alexander, again) have got themselves in trouble is by suggesting that this might have other causes as well as simple discrimination; that it’s not purely that Silicon Valley techbros are keeping women out.\n\nScott suggests, firstly, that if sexism was the key driver in keeping women out of Silicon Valley, you’d expect to see lots of girls doing computer science at high school but then not making it into tech careers. But that’s not what happens: about 20 per cent of children taking high-school computer-science classes in the US are girls.⁷ (About 8 per cent of A-level computer-science students in the UK are female.⁸) A 1989 study found a similar pattern at middle-school level.⁹\n\nAnd it’s not that sexist stereotypes are making women believe themselves to be worse at computer science than men and stopping them from taking computer-science classes, he argues, because surveys show that women don’t believe themselves to be worse at computer science than men.¹⁰\n\nInstead, he suggests, a major reason why women don’t end up in Silicon Valley isn’t that Silicon Valley doesn’t want them – it’s that they don’t want Silicon Valley. Women are less interested, according to meta-analyses,¹¹ in ‘thing-oriented’ careers, and more interested in ‘people-oriented’ ones. ‘I would flesh out “things” to include both physical objects like machines as well as complex abstract systems,’¹² says Alexander. ‘I’d also add in another finding from those same studies that men are more risk-taking and like danger. And I would flesh out “people” to include communities, talking, helping, children, and animals.’\n\nAnd, he says, this predicts things pretty well. For instance, medicine was once the absolute bastion of male privilege; about 50 per cent of US medical students are now female. (He doesn’t give a source for this, but the UK figure is that 55 per cent of medical students were female in 2015,¹³ and 56 per cent of students in medicine and dentistry in 2016–17.¹⁴) But, interestingly, when you dig deeper, you see that ‘thing-oriented’ branches of medicine – the ones where you don’t talk to patients much but chop them up, or anaesthetise them, or blast them with radiation; the ones where people are treated more as objects, or systems, rather than people – are significantly male-oriented: surgery, 59 per cent male (in the US); anaesthesiology, 63 per cent; radiology, 72 per cent. And more ‘people-oriented’ branches – psychiatry, 57 per cent; paediatrics, 75 per cent; family medicine, 58 per cent; obstetrics/gynaecology, 85 per cent – are dominated by women.¹⁵ Computer science is very thing-oriented, so you’d expect to find more men pursuing careers in it. The story appears to be similar in the UK: according to the General Medical Council, more than 50 per cent of British obstetrics and gynaecology specialists are female (and 66 per cent of those under 40 and 78 per cent of trainees), compared to just 12 per cent of surgeons.¹⁶\n\nOf course, this doesn’t rule out the possibility that the differing interests of men and women are entirely caused by socialisation in the child’s early years. There’s a lot of research on that, and endless back-and-forth arguments, and it’s an enormous can of worms that I absolutely don’t want to open here, beyond agreeing with Alexander that it’s ‘probably our old friend gene-culture interaction, where certain small innate differences become ossified into social roles that then magnify the differences immensely’.¹⁷ But even if there is no genetic input whatsoever (which is unlikely), early-years socialisation is, says Alexander, not something you can entirely blame Silicon Valley or nerds for.\n\nThis is an ongoing argument; some scientists think the people-vs-things dichotomy explains a lot of the gender split, others think it doesn’t. But the trouble with coming out in favour of one side or the other is that, as Yudkowsky previously observed, debate is war, and arguments are soldiers. ‘Sexism is keeping women out of tech’ is an argument on ‘our’ side, the side that wants women to be able to do any job they want. So people who say, ‘The fact that women are under-represented in tech could be largely due to systematic differences in male and female interests’ are attacking one of our ‘soldiers’, and will often therefore be treated as though they’re saying, ‘Women are not meant to be computer programmers.’\n\nYou can see this war-soldiers stuff going on, I think, in the case of the ‘Google memo’ written by James Damore. Damore’s memo suggested that the under-representation of women at Google was in part because of interests, rather than discrimination. He was fired from his job and called a Nazi¹⁸ and a fascist;¹⁹ my own feeling is that this was a huge over-reaction, especially since he explicitly said that ‘Many of these differences are small and there’s significant overlap between men and women, so you can’t say anything about an individual given these population level distributions,’ and ‘I’m not saying that diversity is bad, that Google or society is 100 per cent fair, that we shouldn’t try to correct for existing biases, or that minorities have the same experience of those in the majority.’²⁰\n\nCordelia Fine, a professor of psychology at the University of Melbourne and author of three feminist-inspired books about neuroscience, told the Guardian that, while Damore had a tendency to over-emphasise the evidence suggesting innate tendencies, his memo was ‘more accurate and nuanced than what you sometimes find in the popular literature’ and full of ideas that are ‘very familiar to me as part of my day-to-day research, and are not seen as especially controversial’. She felt ‘pretty sorry for him’, she added, and found it ‘extraordinary’ that he had been fired and shamed.²¹\n\nBut when you think in terms of debate and war, and arguments and soldiers, it makes perfect sense. The specifics of Damore’s memo weren’t that relevant. The point was that he was arguing something that looked as if it would give succour to the Donald Trump/Brexit/alt-right ‘side’. Most of us would have realised that, I think, and been wary of sending that ‘memo’ to everybody at Google. But for a certain kind of mind, which many Rationalists have and as, apparently, James Damore has, it was not obvious.\n\nIt won’t surprise you to learn that Damore is on the autistic spectrum. It also won’t surprise you to learn that, after he was fired, the alt-right swarmed around him, inviting him on their shows. Milo Yiannopoulos interviewed him. He was now providing soldiers for their army.\n\nChapter 37\n\n The Neoreactionaries\n\nAnother accusation is that the Rationalists are linked to the alt-right. And there is a very good and specific reason to think that they might have those links. That is that they do.\n\nThe online group known as the ‘Neoreactionaries’, which is a sort of strange medievalist subset of the alt-right, grew out of the Rationalist movement to some extent. They even left LessWrong and founded their own website, named (spot the reference) ‘More Right’. Mencius Moldbug, the founder of Neoreaction, wrote a few blogs on Robin Hanson’s Overcoming Bias before LessWrong split from it. Michael Anissimov, another prominent Neoreactionary, was until 2013 MIRI’s media director. The pseudonymous ‘Konkvistador’ is a regular Slate Star Codex commenter.\n\nThey’re a small and weird subculture. Mike Story said to me that it was best to think of it like this: ‘LessWrong is kind of a social club for people with mild, high-functioning autism, or nerdiness.’ But within that wider group there’s a division, which Mike calls ‘the people who have [those social deficits] but are kind of arseholes, and the people who kind of have that but are good.’ The latter make up the Effective Altruism crowd; the former, the Neoreactionaries. I don’t want to make it sound as if there’s an equivalence here. About 20 per cent of Rationalists identify as Effective Altruists; 0.92 per cent as Neoreactionaries.¹ But the latter, while relatively rare, do exist, and have influenced the development of the Rationalist community.\n\nAccording to Neoreactionaries, the world has been moving steadily to the left for several hundred years, and that it has correspondingly become less safe, less happy and less clever, and that it is impossible to speak your mind freely unless you toe certain leftist lines. They also believe that some ethnic minorities’ poorer life outcomes – in education, income, crime, mental health, etc. – are due to biological and/or cultural factors within those minorities; that women are happier in more traditional, ‘sexist’ societies; that immigration from some developing-world countries actively worsens America by bringing in people with different, and worse, values. And, most notably, they think that democracy should be replaced by an omnipotent and unelected king.\n\nIt’s not that these Neoreactionaries are completely separate from the rest of the Rationalists. If you spend any time in the comments on Slate Star Codex, you’ll find them quite often; same on the SSC subreddit.\n\nHere’s part of what I think is going on. First, comment sections are the literal worst. There’s a thing on the internet called the ‘1 per cent rule’, which is roughly that there’s a hyperactive 1 per cent in any internet community which creates the vast majority of whatever the content is (comments, Wikipedia edits, YouTube videos) while most of the rest just lurk, reading and watching. And, indeed, only 1.2 per cent of LessWrong readers surveyed said they’d commented more than once a week for the last year, while 84 per cent said they hadn’t commented at all;² the Slate Star Codex survey finds similar. And of course, that hyperactive 1 per cent will disproportionately include the most ideologically driven. So judging the Rationalists by the ugliest comments you can find in the SSC subreddit is rather unfair.\n\nSecond, the Rationalists have a particular problem which is that their whole thing is taking opposing arguments seriously – what Alexander calls the ‘principle of charity’. It is part of SSC’s ethos that, ‘if you don’t understand how someone could possibly believe something as stupid as they do’,³ then you should be prepared to find that that’s your failing, rather than theirs. And what that means is that if you’re the sort of person who wants to go and talk about ‘race science’, for example, you’ll find that going to a Rationalist website and doing so means that you aren’t immediately blocked. Instead, you find people will talk to you seriously and engage with you.\n\nAnd this is a noble and brilliant thing, in many ways! If you want to shout about how terrible The Enemy is, so that you get cheered on by Your Side, then you’ve got the whole internet in which to do that. But the Rationalist community is where you can speak, calmly and collaboratively, with people with whom you profoundly disagree, and try to change minds (and admit to the possibility that you will have your own mind changed).\n\nUnfortunately, this means that you have to allow people in who say things you find completely appalling. But civil society in Britain and America, and I think much of the world, is getting measurably more polarised; its conservatives and its liberals just live in different worlds, talking to themselves about how terrible the other lot are. Places where they can talk to each other, like the Rationalist community, should be protected and encouraged, not reviled. When we’re dealing with AI safety, which really could affect everybody on Earth and which – if the Rationalists are right – is an imminent and hard-to-avoid threat, it seems particularly good to have a place where ideas can be exchanged without people feeling they have to censor themselves.\n\nThere is another piece that Scott Alexander wrote. (I’m sorry to use ‘Slate Star Codex’ as a near-synonym for ‘the Rationalists’, but it is by far the most high-profile part of the movement, and the most overtly political.) It was called ‘You Are Still Crying Wolf’, written in the wake of Donald Trump’s election. The piece argued that accusing Donald Trump of being ‘openly racist’ and ‘openly courting the white-supremacist vote’ was disingenuous, and was still ‘crying wolf’ in the way that accusing more mainstream Republicans like John McCain and Mitt Romney of being Nazis had been in previous elections. Alexander’s point was that Trump had, at the very least, gone out of his way to sound non-racist, pro-LGBT, etc. He’d gone to black churches, claimed that it was his ‘highest and greatest hope that the Republican Party can be the home in the future and forever more for African-Americans and the African-American vote’, apparently spontaneously grabbed a rainbow flag out of the audience at a rally and started waving it, and waxed lyrical about his ‘love for the people of Mexico’. Trump may or may not have racist attitudes, but calling him ‘openly racist’ or ‘openly white-supremacist’, said Alexander, was silly. If nothing else, he obviously wasn’t being open about it.\n\nA lot of people focused on it as Trump apologia (there being, in the left-leaning internet circles I exist in, nothing worse than being a Trump apologist, except perhaps being a Brexit voter). I think this is spectacularly unfair. A few quotes from Alexander on the topic: ‘Please don’t interpret anything in this article to mean that Trump is not super-terrible’; ‘Trump is just randomly and bizarrely terrible’; Trump is an ‘incompetent thin-skinned ignorant boorish fraudulent omnihypocritical demagogue with no idea how to run a country, whose philosophy of governance basically boils down to “I’m going to win and not lose, details to be filled in later.”’\n\nI don’t know if he’s right. But it seems a reasonable argument to make, and it certainly wasn’t pro-Trump. Before the election, Alexander had urged his readers to vote for anyone but Trump (and specifically Clinton in swing states), because Trump was the candidate most likely to blow the whole world up in some stupid way.⁴\n\nAnd it’s not just that Scott is the smiling liberal face of a far-right movement. Yudkowsky, Bensinger and other prominent Rationalists expressed concerns about Trump. The Slate Star Codex 2018 survey⁵ found that 29 per cent of respondents were registered Democrats, and just 9 per cent registered Republicans. (36 per cent of respondents were unregistered, and 21.6 per cent were non-Americans.) On a ‘political-spectrum’ question, asking people to rate their political position from 1 (far left) to 10 (far right), the largest single response was 3, at 26 per cent, followed by 4 at 21 per cent.\n\nI think this needs saying, because the Rationalists frequently criticise what they call the ‘social-justice movement’ and which I tend to think of as the Twitter/Tumblr left. And if the Rationalists are alt-righters, misogynists and racists, then it’s easy for the liberal left to disregard their criticisms. But if they’re a group of largely liberal-left people, who have specific criticisms of aspects of mainstream liberal-left thinking, then it’s harder to ignore them.\n\nThere’s another point: a key argument that many Rationalists are making is that there is a real and believable risk that we might all be destroyed in the foreseeable future by a badly aligned AI. It doesn’t really matter, from a truth-seeking perspective, whether the Rationalists are a load of misogynists; you can’t psychoanalyse your way to the truth, as we discussed earlier. But it is easier to dismiss their concerns if you think they are.\n\nPart Eight\n\nDoing Good Better\n\nChapter 38\n\n The Effective Altruists\n\nIt’s impossible to talk about the Rationalists without mentioning their conjoined twin, the Effective Altruism movement. They’re so intertwined that I have a bad habit of using the terms synonymously, but they are in fact distinct.\n\nEffective Altruism is based on the idea that we should do the most good we can with our resources. If we give to charity, we should give to those charities that do the most good. If we want to dedicate our careers to improving the world, then we should think carefully (and use numbers) to establish how we should spend our careers.\n\nThe spiritual godfather of the movement is Peter Singer. He’s an Australian moral philosopher, based at Princeton, who in 1972 published an essay entitled ‘Famine, Affluence and Morality’.¹ Singer is a utilitarian, one of the most influential of the twentieth century. His argument in 1972 turned on two deceptively simple points: one, ‘Suffering and death from lack of food, shelter and medical care are bad’; and two, ‘If it is in our power to prevent something very bad from happening, without thereby sacrificing anything morally significant, we ought, morally, to do it.’ He says, by way of illustration, ‘If I am walking past a shallow pond and see a child drowning in it, I ought to wade in and pull the child out. This will mean getting my clothes muddy, but this is insignificant, while the death of the child would presumably be a very bad thing.’\n\nThat all seems pretty uncontroversial, certainly to me. But it has profound and unsettling implications. He was writing during the Bengal famine of 1971, at a time when 9 million people were refugees. He pointed out that Britain and Australia, two of the more generous countries in terms of aid, had given (in Britain’s case) roughly one-thirtieth of the money it had spent on the Concorde project; Australia, one-twelfth of the amount it had spent on the new Sydney Opera House. That money could have saved many tens of thousands of lives. ‘It makes no moral difference whether the person I can help is a neighbor’s child ten yards from me or a Bengali whose name I shall never know, ten thousand miles away,’ he wrote. A life is a life, and we should save the most we can. It’s not simply that giving money to aid charities is a nice thing to do, an over-and-above-the-call-of-duty moral bonus – ‘supererogatory’, in the language of moral philosophy – it’s that it is a duty. We should all, in wealthy, developed countries, give some non-negligible percentage of our earnings, either as charitable donations or as foreign aid via tax, to improving the lot of people in developing countries. How much? He doesn’t know, exactly, but ‘It should be clear that we would have to give away enough to ensure that the consumer society, dependent as it is on people spending on trivia rather than giving to famine relief, would slow down and perhaps disappear entirely.’\n\nThirty-eight years after Singer wrote his essay, a small group of academics in Oxford, led by Toby Ord and William MacAskill, a professor of moral philosophy (at one point, the youngest associate professor in the world; he’s still only 32), took Singer’s ideas and tried to turn them into something both more practical and more precise. In 2009 they founded a charity called Giving What We Can, dedicated to finding the most effective charities in the world and helping people donate to them. It also encouraged people to pledge to donate 10 per cent of their income to those charities.\n\nIn 2011, MacAskill founded another charity, 80,000 Hours, which helps people to decide what career they should embark on in order to make the most positive impact on the world. For instance, while it might make intuitive sense, for a bright young person who wants to make a difference, to become a doctor at Médecins Sans Frontières, it might be more effective for her to go into some high-paying but less obviously worthy career, and then donate a large part of her salary to charity. (They cite an example of a man who chose to earn $250,000 a year as a software engineer instead of taking a $65,000 job as the CEO of a non-profit; he then donated $125,000 a year to charity, theoretically, at least, funding two CEOs.²)\n\nOver in the US, two young men had had a similar idea. In 2007, Holden Karnofsky and Elie Hassenfeld, Ivy League grads who had gone on to work in hedge funds, wanted to find out the best places to donate some of the moderately large amounts of money they were suddenly making. ‘I think I just had this background assumption that it would be like buying a printer,’ Holden told me. ‘I’d go on some website and get the best printer for the least money.’ But such a website did not appear to exist. ‘I was like, “What am I trying to do? I’m trying to help people. How can I help people with a fixed amount of dollars that I’m going to give? It was just too hard to figure out.” I started calling charities themselves and asking them: “Hey, do you have some metrics? Do you have some data on how many people you’ve helped?” Elie and I had a little club of eight people, all calling charities.’\n\nSlowly they realised that the numbers were not easy to get hold of, and that the problem was quite big. ‘At first it was annoying: it’s holiday time and I’m spending all my time on the phone with charities trying to understand their numbers. Then I started to find it interesting. We saw a huge gap, a problem in the world, and we thought we could fix it if we worked really hard at it. So we left our jobs, raised money from our co-workers, and started GiveWell [and, later, OpenPhil]. It was a super-standard start-up story in a sense. I wanted something, I couldn’t get it.’\n\nYou might, at this point, ask how you measure ‘helping people’. How can you compare, say, the impact of an education programme to the impact of a medical programme? Is it fair or meaningful? ‘Comparing across different types of charities is not only fair, but essential,’ Will MacAskill told me. ‘These comparisons allow us to see that some charities are much more cost-effective than others.’ For instance, a charity that funds an extra year of education for people in the US or the UK might cost somewhere in the region of a few tens of thousands of dollars per person it helps. For that amount of money, you could provide thousands of insecticide-impregnated bed-nets to children in sub-Saharan Africa, which would prevent hundreds of cases of malaria and save several lives. However much you value education, it’s pretty clear that saving the lives of children is more valuable than a year of education for a Westerner.\n\nThis is a pattern that repeats. ‘Charities that help people in developing countries tend to be much more cost-effective than charities that help people in developed countries,’ says MacAskill. ‘I call this the “100× multiplier”: as someone living in a rich country, you should expect to be able to do at least 100 times as much to benefit other people as you can to benefit yourself or people in your community.’ Both GiveWell and Giving What We Can recommend the Against Malaria Foundation, the charity which gives bed-nets to families in poor, malaria-prone regions. They also both suggest the Deworm the World Initiative, which pays for cheap deworming treatments which appear to have a significant impact on children’s futures, and GiveDirectly, which makes small payments straight to poor people in developing countries via mobile phones. Other charities they both recommend include the Schistosomiasis Control Initiative, No Lean Season, Helen Keller International, the Malaria Consortium and the END Fund. What all these charities have in common is that they operate in poor countries; most of them also provide cheap medicine for easily treatable diseases. The Giving What We Can website offers this reasoning: ‘Suppose we want to help people suffering from blindness. In a developed country this would usually involve paying to train a guide dog and its new owner, which costs around $40,000. In the developing world there are more than a million people suffering from trachoma-induced blindness and poor vision which could be helped by a safe eye operation, costing only about $100 and preventing 1–30 years of blindness and another 1–30 years of low vision . . . For the same amount of money as training one guide dog to help one person, we could instead prevent 400–12,000 years of blindness.’³\n\nThere are edge cases, and difficult things to compare: is extending a life by five years worth more or less than curing a child’s blindness? But as it stands, most of us give to charity in spectacularly un-evidence-driven ways, sponsoring a friend’s half-marathon or signing something a charity mugger hands us. ‘Donors in developed countries who give in a non-evidence-based manner typically support charities that focus on helping people in their own communities,’ says MacAskill. ‘These charities can be expected to do one-hundredth or less of the good that charities focusing on helping people in poor countries do.’ MacAskill is right: I was startled to learn, as I wrote this, that only 6 per cent of American charitable donations go to ‘international affairs’, which is to say 94 per cent of American charitable donations stay within American borders.⁴ The idea of the Effective Altruism movement is that we are not, generally, dealing in subtle distinctions: if you want to do the most good with your money, rather than just purchase warm feelings, then some charities are very obviously better than others.\n\nThe links between the Rationalists and the Effective Altruists go back pretty much to the beginning. Ord and MacAskill met Nick Bostrom at Oxford in 2003, and Ord says: ‘I was heavily influenced by Nick in my work on existential risk. I’m pretty sure [the Effective Altruism movement] wouldn’t have had such a strong strand on existential risk if I hadn’t been influenced by Nick.’ It’s not that one inspired the other, says Ord, ‘the story is mostly one of mutual influence of people exploring related ideas and gaining from their interactions’.\n\nCertainly, the LessWrong Rationalists provide a large proportion of Effective Altruism’s support. In 2014, 31 per cent of survey respondents said that they had first heard of the movement through LessWrong;⁵ by 2017, that figure had dropped to 15 per cent, presumably partly because LessWrong had shrunk while Effective Altruism had grown, but a further 7 per cent had heard of it through Slate Star Codex.⁶ And a large fraction of LessWrongers are Effective Altruists: according to the 2016 LessWrong diaspora survey, 20 per cent of respondents identified as Effective Altruists, and 22 per cent had made ‘donations they otherwise wouldn’t’ because of Effective Altruism.⁷ Two other OpenPhil employees, Helen Toner and Ajeya Cotra – whom you’ll meet shortly – told me that they’d either come to Effective Altruism through LessWrong or found the two at the same time. One Effective Altruist blog, ‘The Unit of Caring’, is named after a Yudkowsky blog post (‘Money: The Unit of Caring’). Scott Alexander has written repeatedly about Effective Altruism, including an extremely powerful blog post⁸ backing the Giving What We Can ‘pledge’ to donate 10 per cent of your income to effective charities.\n\nEffective Altruists are also a similar sort of people to Rationalists. They are nerdy, on the whole. They are open to new experiences (I don’t know how many of the Effective Altruists are polyamorous, but it’s not uncommon, in both the Oxford and Bay Area sets). They go for numbers over feelings, even when the numbers lead them into weird areas, which they frequently do. For instance, one very good point that some Effective Altruists make is that if we are worried about alleviating suffering, then presumably animal suffering matters to some extent too. (You could declare by fiat that it doesn’t, but it’s hard to make a principled case for it. Besides, that would mean you’re OK with the arbitrary torture of animals for no reason, which most of us are not.)\n\nBut if animal suffering matters, even a fairly small amount, then suddenly, eating chicken is an absolute moral catastrophe. Around 50 billion chickens are raised each year worldwide, the large majority of them for meat.⁹ They live on average for seven weeks each, so that’s about 6 billion chicken-years of life a year. Even if you only care about chickens’ suffering-per-hour 1 per cent as much as you care about that of humans (a very conservative figure: Buck Shlegeris, the young Effective Altruist and Rationalist we met earlier, told me he rated it about a quarter, and Peter Singer told me that wasn’t ridiculous), you should care as much about the world’s chickens as you do about the entire population of the United Kingdom. And that’s before you get to the point that chickens very probably have worse lives than the average Briton.\n\n(Scott Alexander points out that you can improve the situation considerably by eating beef.¹⁰ Imagine that you get a sixth of your calories from chicken, about 125,000 calories a year. A chicken provides about 3,000 calories. So your 125,000 calories translate to 42 chickens, bred and kept in unpleasant circumstances and then slaughtered. By contrast you get about 405,000 calories from a cow, so your 125,000 calories translates to 0.3 cows: just by switching, you reduce the number of animals bred for your food by 93 per cent. Cows live longer before slaughter, but not that much longer, and tend to have happier lives anyway. Unfortunately, cows are far worse for global warming, so really I ought to give up both and go vegetarian.)\n\nBut we can all get behind the idea that factory farming is a real problem and realise that we have a moral responsibility for it. Effective Altruism gets much weirder than that, in its niche areas. There are, for instance, Effective Altruists who worry about the suffering of wild animals. They make the point that there are probably between 100 million and 1 billion ‘bugs’ (insects, arachnids and so on) per human in the world, and that it only takes a very, very tiny level of moral value per insect to make their suffering outweigh that of all humans. I can’t fault the numbers, but they take me beyond the level of weirdness to which I am prepared to go. (And it gets weirder still. Another group has wondered about whether consciousness, and therefore suffering, is a fundamental quality of the universe – whether quarks and electrons are capable of suffering. There are quite a lot of quarks and electrons, so if you start worrying about that, you end up in a very odd place indeed.¹¹)\n\nI shouldn’t overplay the ‘weirdness’ element. Most Effective Altruism comes down to the eminently sensible idea that if you want the shiny pound in your pocket to do the most good it can, it’s better to donate it to the Against Malaria Foundation than it is to your local donkey sanctuary (or even a cancer-research charity). If I keep going on about how weird they are, there’s a risk that I’ll make you think they’re some strange species of humans. ‘It’s other-ising,’ says Karnofsky. ‘The initial reaction to these topics is that it’s wacky. But I think it’s a really interesting and under-appreciated set of issues. If you other-ise them, you create this idea that you need a certain kind of unusual psychology to be interested in this stuff, and I don’t think that’s true.’ The same goes if we paint them as super-altruistic: ‘If you portray Effective Altruists as saints, as people who give away every penny they have, feel bad every time they buy a sandwich because they could have given it to charity, people read it and they go, “That person is weird. That person is not me. I could never relate to them, and now I’m not interested.”’ But, again, a lot of the ideas are extremely straightforward.\n\nThat said, one key area that OpenPhil in particular is worried about, and which is definitely weird on the face of it, is existential risk. OpenPhil was set up by GiveWell and Good Ventures, a philanthropic foundation established by Dustin Moskowitz, one of Facebook’s founders, and his wife Cari Tuna, although it is now separate from both those organisations. Where GiveWell concentrates on small, repeatable, reliable (in most cases) interventions, such as bed-nets or cash transfers, OpenPhil aims at big, high-risk-high-reward projects. One area it focuses on is US government policy – criminal justice reform, immigration policy and so on. It also works on animal-welfare issues; when I was in San Francisco talking to Karnofsky and his team, their head of comms, Mike Levine, took me out to lunch at a local burger bar, so he could gently probe me to make sure I wasn’t going to write an entire book mocking the nerds who think Skynet is going to take over the world (he is endearingly protective of his nerds). We ate the Impossible Burger, a plant-based meat burger, which was produced partly with funding from OpenPhil,¹² the idea being to reduce the requirement for beef and thus improve animal welfare and reduce greenhouse-gas emissions. I can report that it tastes very much like a burger, indeed a perfectly nice burger, and appears to be taking off; it is sold at hundreds of locations in the US and one of its rivals has, since I went over there, arrived in Britain. (Specifically, and unsurprisingly, Dalston in east London.¹³)\n\nBut the OpenPhil work that is most relevant to this book is its focus on global catastrophic risks, and especially AI.\n\nChapter 39\n\n EA and AI\n\nAjeya Cotra is a research analyst for OpenPhil. She and her colleague Helen Toner, an Australian, both work on AI risk specifically, and both say that it is hard to explain to people what you do for a living. ‘When I was in high school, I discovered GiveWell and Effective Altruism, and I also discovered LessWrong and the Rationalist community,’ Cotra tells me. At first she was interested in global poverty reduction – ‘I was trying to get my parents to donate to the Against Malaria Foundation’ – and not focusing on AI. But at college in Berkeley she taught a seminar series on Effective Altruism. ‘That was really when I had to force myself to think through all the arguments for [existential risk]. I started to transition to doing more work focused on our global catastrophic risks.’ Her degree was in computer science, so she had some technical understanding of AI, which helped.\n\nIt has come with some social cost, she says. ‘It’s harder to explain, like to my parents, what I do. But I think you can understand fairly quickly why this is important. A lot of experts think that artificial intelligence might be arriving in the next 20 or 30 years. And human intelligence has had such a transformative, positive and negative, effect on our world.’ There’s no reason not to assume that even more powerful intelligence will have an even more transformative effect. ‘It will touch on so many things we care about, like curing diseases, new forms of surveillance, wealth and poverty and inequality.’\n\nToner agrees that it’s easier to say to people that you’re working on anti-malarial bed-nets than it is to say you’re trying to stop the world being destroyed by a rogue AI. She, too, started out worrying about global poverty and development. ‘But after a long series of conversations, and learning more about stuff, and a year or two of being involved in the EA community, I started to come around to the view that maybe AI was the thing to be working on.’\n\nThere are three key elements that make a cause worth donating to, according to the tenets of Effective Altruism. One is its importance: the scale of the problem, how much better the world would be if the problem were solved. A second is tractability: how easy it is to solve those problems. And a third is neglectedness: if lots of people are already working on the problem, then the amount of good you can do on the margin is less. So malaria is an excellent target, because it has a huge impact (importance), is easily and cheaply prevented (with bed-nets), and yet receives far less global spend than diseases like cancer, which disproportionately affect those rich countries where people tend to live long enough to get them.\n\nAn obvious global catastrophic risk is climate change. It’s hugely important, and while it’s not easily tractable, there are certainly things that can be done. But climate change is a crowded space: there are lots of governmental and philanthropic organisations focusing on it, so the marginal value of the money OpenPhil puts in would be less.\n\nInstead, the two risks that OpenPhil is focused on are pandemics, especially bioengineered pandemics, and AI. ‘I’ve just gone back and forth enough times on which of those I consider a bigger risk,’ Holden Karnofsky told me. ‘If you asked me how [humans could go extinct], those are definitely my top two guesses by some margin and then a little bit down will be things like nuclear weapons.’\n\nAnd, as we discussed in Chapter 2, talking about existential risks, going extinct matters. Remember that Nick Bostrom thinks that the number of human-like lives that could be lived is something obscene like 10⁵⁸. He could be wrong by three dozen orders of magnitude and we would still be talking about vastly more humans than have ever lived. If their lives have any non-negligible moral value (which is not, it should be said, an uncontroversial claim), then those potential lives need to affect our moral judgements significantly. Even if we don’t buy that, merely a small chance of killing everyone alive is a vital issue: for instance, the expected population of the Earth by mid-century is about 10 billion. Something that has a 3 per cent chance of killing all those people is equivalent to something that will definitely kill 300 million. According to 80,000 Hours, that’s ‘more deaths than we can expect over the next century due to the diseases of poverty, like malaria’.¹\n\nIf you buy into the idea that future lives matter, then you need to think about the things that are most likely to cause human extinction, not just those that will probably kill a large number of us. As we discussed a few chapters ago, it seems that bio-risk and AI are the best-value bets, and because this is a book about AI, I’m going to talk about that.\n\nAI risk, says Holden Karnofsky, is extremely important (because of the risk of extinction), highly neglected (there are only a few places, such as MIRI and FHI, which are working on it), and reasonably tractable. So it represents an ‘outstanding philanthropic opportunity’. He hasn’t always thought this way: one of the most-read articles on LessWrong was a long piece by him about why he thought that MIRI was a bad subject for philanthropic investment. In a subsequent explanation of why his position changed, he said that he had previously thought that ‘by the time transformative AI is developed, the important approaches to AI will be so different from today’s that any technical work done today will have a very low likelihood of being relevant’.² But now, he told me, ‘We think there’s a non-trivial, by which we mean at least 10 per cent, chance of transformative AI in the next 20 years.’\n\nThat’s part of what makes it tractable. The AI landscape has changed spectacularly in the last few years: things that were cutting-edge specialist technology, like voice and facial recognition software, in the early 2000s are now running on novelty apps on your phone. DeepMind’s AlphaGo is a beautiful and, it must be said, slightly unnerving demonstration of how machine learning can create superhuman intelligence, in a more general (if still narrow) sense than we are used to. It’s a testament to how quickly new stuff becomes normal that we aren’t more amazed by it all.\n\nBut at the moment, the key thing that OpenPhil is doing isn’t so much related to the technical work – although they do support that. They’re excited about what Toner calls ‘field-building’. She gives the example of the field of geriatric medicine. ‘In the 1980s, people realised that elderly people need a totally different style of healthcare from young people. Young people tend to come in with one thing that needs to be fixed and then you send them home, whereas with elderly people there’s a whole bunch of interacting conditions and maybe they need to stay a long time.’ They also realised that the baby-boomer generation, at that point in its thirties and forties, was going to get old, which meant a very large population of elderly people. So the John A. Hartford Foundation decided to concentrate almost all its resources on building up the field of geriatric medicine, training doctors, funding research, building new centres. ‘And by the time the [baby boomers were] elderly, geriatrics was a totally normal medical field,’ says Toner.\n\nThat’s roughly what OpenPhil is trying to do now. ‘[We’ve] been talking to high-profile, very skilled top machine-learning researchers, and saying, “Would you be interested in working on safety, or in having some grad students work on safety? What are the areas that you would be interested in?”’ Toner says. In 2017 they gave $3 million in grants to machine-learning departments at Berkeley and Stanford, about the same amount to MIRI (despite, a year earlier, having expressed ‘strong reservations about MIRI’s research’³ while granting them $500,000), and a much larger $30 million to OpenAI, the non-profit research organisation founded by Elon Musk.⁴\n\nThe aim, says Karnofsky, is for their work to do the sort of field-building that the Hartford Foundation did, even though worrying about transformative and possibly disastrous AI might sound ‘wacky’ now. ‘To an outsider who has never encountered the issues before, given the low-level of buy-in from wider society, the natural, initial reaction to these topics is going to be, “What is this? I’ve never heard of this. This is different from what I normally think about, this is wacky.”’ But in time, as the field becomes more mature and well known, he wants people to think more along the lines of, ‘This is an issue that should matter to me. It’s got a lot in common with climate change. It could be far off, but we don’t know it’s far off, and it’s worth worrying about now because it could be a huge deal.’ He hopes this book might make a few more people think like that.\n\nI don’t think many people will disagree with the idea that a pound spent trying to prevent malaria deaths will do more ‘good’, under most definitions of that word, than a pound given to support Harvard University. But you might remember what David Gerard said, when we were talking about whether LessWrong was a cult: ‘clearly the most cost-effective initiative possible for all of humanity is donating to fight the prospect of unfriendly artificial intelligence, and oh look, there just happens to be a charity for that precise purpose right here! WHAT ARE THE ODDS.’⁵\n\nThe criticism that others have made is not that LessWrong is chiselling cash, but that the Rationalists, and the Effective Altruist movement, are heavily made up of nerdy, STEM-inclined computer-science grads. Dylan Matthews, writing for Vox in 2016, put it like this: ‘In the beginning, EA was mostly about fighting global poverty. Now it’s becoming more and more about funding computer science research to forestall an artificial intelligence-provoked apocalypse. At the risk of overgeneralising, the computer science majors have convinced each other that the best way to save the world is to do computer science research.’⁶ (It reminds me of a story that the Rationalist and Effective Altruist Ben Kuhn told on his blog. He went to an Effective Altruism summit with his partner, who was new to EA; she asked an attendee what sort of people were there. ‘“Oh, all different kinds!”’ he replied. ‘“Mathematicians, and economists, and philosophers, and computer scientists . . .” It didn’t seem to occur to the fellow that these were all basically the same kind of person,’⁷ Kuhn writes.)\n\nThis was, pretty much, the argument that Caroline Fiennes put to me. Caroline is the director of Giving Evidence, a group which encourages and enables giving based on sound evidence. She has known GiveWell and others in the Effective Altruism movement for a long time – she is on the board of The Life You Can Save, Singer’s charity – and doesn’t want to belittle what they do, but she is worried about this sort of uniformity of thought. ‘People gravitate to stuff they understand,’ she says. ‘Maybe these people have gravitated towards this issue because they feel comfortable and competent on it.’ Elon Musk, Peter Thiel, Dustin Moskowitz are the big-name funders behind it, and perhaps it isn’t surprising that these software tycoons all think that good software is needed to save the world. And, as various people have muttered to me during the writing of this book, they seem to be less vocal about the sort of problems – privacy, surveillance, filter bubbles, fake news, algorithmic bias – that their software is creating. It’s fun to sound noble and far-sighted, warning of the dangers of a technical problem still to come; it’s less fun to address the criticisms people are making of what you and your peers are doing right now.\n\nFiennes notes that GiveWell and OpenPhil don’t mention a lot of major areas of possible philanthropy. ‘I don’t think GiveWell’s list of recommended charities really reflects global priorities,’ she says, ‘and it doesn’t reflect the lists of other experts who do serious cause prioritisation, such as the 50 Breakthroughs report, or the Copenhagen Consensus. There’s nothing about war, nothing about climate change. Only one thing, very recently added, about hunger, only one about water and sanitation. We have a couple of billion people who don’t have access to food and clean water and toilets; it’s weird that those issues aren’t further up the list.’ There’s also nothing about global governance and the rule of law, she adds, ‘which for me, and for George Soros I would observe, would seem like a reasonable thing, if you had a chunk of money’. GiveWell will do the ‘things that are very certain, very repeatable, very proven – we know the impact of the next bed-net, know the costs, know the benefits,’ while OpenPhil is willing to talk about really long-shot things like bioterrorism and superintelligent AI. ‘But there’s all this big pile of stuff in the middle [like promoting the rule of law], which seems obviously important. I’m not saying AI isn’t important,’ Fiennes says, ‘but the amount I hear about AI out of them, compared to the nothing at all I hear about global governance and climate change and war, seems totally out of whack.’\n\nIt’s not completely fair to say that the two organisations – they’re separate now, although they still share the same swanky office space in downtown San Francisco – ignore these issues completely. Mike Levine, OpenPhil’s head of comms, agreed that climate change, for instance, is not as neglected or as tractable as some other, similarly important, issues. ‘But I’d quibble with the idea that we don’t focus on it. We’ve funded the most important, neglected and tractable climate-change opportunities we’ve seen, including geoengineering research and governance and the especially tractable opportunity around the Montreal Protocol [to reduce the use of hydrofluorocarbons, a powerful greenhouse gas]. We’ve put millions of grant dollars into climate change – more than we have into our land-use reform, macroeconomic stabilisation, or immigration policy focus areas. We see climate change as a huge issue that warrants action from philanthropists, and we expect to do more.’ But it’s certainly true that these mid-level issues get less attention, in the press and public sphere, than OpenPhil’s concentration on AI.\n\nThat may or may not be OpenPhil’s (and the wider Effective Altruism community’s) fault, depending on your point of view. You could say that they simply need to focus on what they think are the most effective targets for their philanthropy, and let other people worry about the look of the thing.\n\nBut it’s also true that ‘how weird it looks’ affects how effective it is. Will MacAskill told me that he thinks there is a ‘pretty solid’ case for focusing on AI safety, for those people who are ‘prepared to consider more speculative lines of evidence . . . though I should point out that some of the arguments for concluding that one should focus on existential risk reduction exclusively make a number of controversial philosophical assumptions’, such as that future lives are of comparable value to current ones. But there is a risk, he said, that ‘an excessive focus on these speculative causes runs the risk of undermining the Effective Altruism movement’. If people look at Effective Altruism and see a bunch of people worrying about what seems to them to be some sci-fi stuff, they might then not donate to the less speculative things – the bed-nets and the cash transfers – which they mentally bucket together. It could be, in Karnofsky’s word, ‘other-ising’.\n\nThere’s a concept in the Rationalsphere called ‘weirdness points’. It’s the idea that society will let you be only so weird before it stops taking you seriously: you only have a certain number of weirdness points to spend, and so you should spend them on things that you really care about. That’s why, says Mike Story, Rationalists don’t talk all that much about their polyamory – ‘more than just not evangelise, they keep quiet about it. Scott [Alexander] is pretty open about it, but generally they think if we seem normal it’s better for our ideas.’ If you spend your weirdness points on polyamory, you don’t have them left to spend on Effective Altruism or the importance of Bayes’ theorem.\n\nAnd, it must be said, AI safety spends a lot of weirdness points. If you think it’s the most valuable thing by miles, then it is worth spending those points on. If you don’t, you could reasonably argue that it’s just making the Effective Altruism movement look weird and making it harder to get bed-nets to children in sub-Saharan Africa.\n\nSo it comes down, really, to whether we can trust the thinking of OpenPhil and similar organisations on all of this. I’ll raise a couple of possible points. One was an objection that Dylan Matthews of Vox had, in the piece I mentioned previously. He raises the possibility of ‘Pascal’s mugging’, which we discussed before. People at the Effective Altruism conference he attended gave him the standard expected-value argument: ‘Infinitesimally increasing the odds that 10⁵² people in the future exist saves way more lives than poverty reduction ever could.’ But Matthews argues that the key is what we mean by ‘infinitesimally’: ‘Maybe giving $1,000 to the Machine Intelligence Research Institute will reduce the probability of AI killing us all by [10⁻¹⁷],’ he writes. ‘Or maybe it’ll make it only cut the odds by [10⁻⁶⁶]. If the latter’s true, it’s not a smart donation; if you multiply the odds by 10⁵², you’ve saved an expected [10⁻¹³] lives, which is pretty miserable. But if the former’s true, it’s a brilliant donation, and you’ve saved an expected [10³⁴] lives.’\n\nOn the face of it, that sounds a pretty fair argument – ‘those probability values are just made up,’ says Matthews. ‘I don’t have any faith that we understand these risks with enough precision to tell if an AI-risk charity can cut our odds of doom by [10⁻¹⁷] or by only [10⁻⁶⁶]. And yet for the argument to work, you need to be able to make those kinds of distinctions.’\n\nI’m going to hand over to Scott Alexander to explain why he (and I) think that argument doesn’t work. In short, it’s that while 10⁻⁶⁶ and 10⁻¹⁷ both sound like similar spectacularly tiny numbers, they are absolutely not. The number 10⁻¹⁷ is the sort of number you might actually have to use at some point. There are about 2,522,880,000 seconds in an 80-year human life, and about 7 billion humans, so during your lifetime about 1.7×10¹⁹ seconds will be lived by humans. That means you’d expect, in your lifetime, roughly 100 things to happen that only happen once in every 10¹⁷ seconds. (When you read about ridiculously unlikely things happening – someone finding out that their next-door neighbour is actually their separated-at-birth sibling, or whatever – remember that.)\n\nBut 10⁻⁶⁶ is not like that. You will never read about something that only happens once every 10⁻⁶⁶ seconds. Here’s Scott: ‘The per-second probability of getting sucked into the air by a tornado is 10⁻¹²; that of being struck by a meteorite 10⁻¹⁶; that of being blown up by a terrorist 10⁻¹⁵. The chance of the next election being Sanders vs Trump is 10⁻⁴, and the chance of an election ending in an electoral tie about 10⁻². The chance of winning the Powerball is 10⁻⁸ so winning it twice in a row is 10⁻¹⁶. Chain all of those together, and you get 10⁻⁶⁵.’ (He was writing in 2015, hence the ‘Sanders vs Trump’ reference.)\n\nWhat this means is that, if Matthews’ 10⁻⁶⁶ guess is right, then then the likelihood of your $1,000 donation to MIRI helping avert an AI apocalypse is 10 times less than the likelihood of someone getting simultaneously sucked up into the air by a tornado, hit by a meteorite and blown up by a terrorist, on the same day as winning the lottery for the second week in a row, and Trump and Sanders tying the electoral college. If Bostrom’s numbers are anywhere near accurate – the 10⁵⁸ human lives that we talked about early on in this book – and if you accept that future lives matter as much as current ones, then even a vanishingly tiny chance that an AI disaster might happen is (from a utilitarian point of view) hugely important. You have to start making some seriously weird assumptions to get it down to a negligible figure.\n\nThe other possibility, of course, is that you argue that future lives don’t matter as much as current lives. That’s a massive and ongoing bunfight in moral philosophy, and, as MacAskill said, there’s no clear answer on it. When I spoke to Peter Singer, he pointed out that there’s a difference between future lives and possible lives. ‘It’s highly probable that there will be humans living on this planet in 100 years, and their lives are going to be worse off because of climate change. We shouldn’t discount those lives just because they’ll only exist in 100 years.’ But when we’re dealing with lives in the very distant future, where it is far from certain that they’ll exist at all, he said, ‘we are not talking about the sufferings of beings who will exist in future. We are talking about the non-existence of many generations of future beings. It remains a controversial and disputed question of philosophy. A number of very good philosophers have worked on it; Derek Parfit worked on it for most of his philosophical career and was unable to really achieve a satisfying conclusion.’\n\nObviously, we’re not going to resolve it here. But, Singer points out, we don’t really need to. We’re uncertain; these lives might be exactly as important as modern lives, or they might not be important at all. But if we’re really uncertain, then we can’t be sure that they have zero value. ‘Because of that uncertainty, we should give at least some weight to them. If we say, “No, they don’t count” – well, we have to acknowledge that might be the wrong thing to say.’\n\nAnd then we’re back to the ‘Bostrom big number’ thing. ‘If there are so many of them as Bostrom argues,’ says Singer, ‘even if you discount them by 99 per cent or 99.9 per cent, the numbers are so big that they still carry very great weight. It’s a tough question, but yes, we should give some weight to the interest of merely possible beings. We should regard the extinction of a species as a worse event than the deaths of all the people who will be killed at that time.’\n\nOf course, even if we agree that the thronging masses of possible future people really do have some moral weight, that preventing extinction is therefore valuable, and that AI is one of the most realistic ways in which extinction could happen and is worth funding, that doesn’t mean that OpenPhil is correct in assessing that MIRI (or FHI, or OpenAI) in particular is worth backing. There are strong criticisms of MIRI: that its output of scientific papers is rather less than that of a single grad student, for instance. (There is a counter-argument that that’s not a fair criticism, because it’s trying to build a new field rather than get on the publication treadmill in established journals, but there’s a lot of back-and-forth about it.)\n\nAnd there is cause to be sceptical of some of Effective Altruist reasoning. GiveWell’s top charities include several that promote deworming. This comes largely from a study, published in 2004,⁸ which found that mass deworming – giving all the children at a school deworming tablets, not just those with worms – improved health, school performance and school attendance; and not just the children at that school, but at schools miles away, through stopping the worms from spreading. Most amazingly, it seemed to dramatically increase those children’s earnings later in life. It was an extremely cheap intervention with, apparently, enormous results.\n\nBut later studies looked at the data and found that it was flawed in some quite serious ways: technical but important statistical errors which severely undermined its credibility. (Not, I hasten to add, through any deliberate misbehaviour on the part of its authors, who very nobly gave up their data for it to be checked; just mistakes.) The Cochrane Collaboration, which does huge meta-analyses of all the studies on a topic, has since looked at mass deworming three times;⁹ it found no effect on school performance or attendance, and no good evidence of an effect on various other health measures. ‘Going up against the Cochrane Collaboration is big bananas,’ says Caroline Fiennes. ‘And Paul Garner, who does their parasitology, has been doing this for a hundred years. I’m amazed that GiveWell isn’t more alarmed that people who have thought long and hard about this – studied deworming since before they were born, in some cases – disagree with them.’ It’s not just that deworming might not do any good – it’s perfectly conceivable that it might do harm. ‘I spoke to a parasitologist who’s studied deworming in Africa for decades,’ Fiennes continues, ‘who said that mass deworming has never been adequately tested for increasing drug resistance, so mass deworming may just jack up resistance and actually be harmful.’\n\nGiveWell is aware of these criticisms. Catherine Hollander, a GiveWell research analyst, told me they’d run the data, and looked at the ways it wasn’t robust, but felt it was still a worthwhile gamble because it was so cheap and had the potential to be so effective. ‘Even when you discount for the possibility that that effect doesn’t exist significantly, you still end up with something that’s the most cost-effective thing that we recommend,’ she said. ‘We discount extremely heavily for all of this uncertainty. But even with that huge discount, Deworm the World was 10 times as cost-effective as cash transfers, our next most cost-effective programme.’\n\nI, obviously, cannot reasonably assess whether deworming is a good bet; nor can I do a more reliable job than Bostrom or OpenPhil in assessing the numbers behind AI risk. But for what it’s worth, I think Fiennes’ criticisms are worth taking on board. They should make you wary of going along too happily with what these Effective Altruism organisations recommend; and, as we’ve discussed before, if something seems completely weird, but the numbers check out, you should pay some attention to the feeling that it’s weird. The Dylan Matthews Vox piece refers to Effective Altruists who seemed to think that literally all philanthropy should go towards preventing human extinction – which would be a pretty terrifying situation.\n\nBut as it stands, that’s not the case. The total worldwide spend on AI risk reduction is probably less than $50 million, which may seem like a lot, but is only one-eighth of (to pick a big charity at semi-random) Greenpeace’s reported annual revenues in 2014.¹⁰ (OpenAI’s funders ‘have committed $1 billion’, but the organisation expects ‘to only spend a tiny fraction of that in the next few years’.¹¹) It is not that AI risk is crowding out all other charity; it’s not even monopolising Effective Altruism. It accounted for about 30 per cent of total OpenPhil grants in 2017, but OpenPhil is only one of several organisations. And Peter Singer, who is very probably the best-known moral philosopher alive today and the absolute godfather of the field of efficient charity, is (broadly, and with caveats) in favour of spending money on AI risk: ‘I definitely think it’s worth looking at,’ he told me. ‘It’s definitely worth spending something on it. Even if the probability is very low, given how devastating it could be, it’s worth trying to put some effort into reducing it even further.’\n\nPart Nine\n\nThe Base Rate of the Apocalypse\n\nChapter 40\n\n What are they doing to stop the AI apocalypse?\n\nCast your mind back to several chapters ago, when we were talking about why AI is dangerous. We mentioned that there are several organisations – MIRI is the one I’ve spent most time talking about, but also Bostrom’s FHI, Max Tegmark’s Future of Life Institute in Cambridge (Massachusetts), the Centre for the Study of Existential Risk in Cambridge (UK), and Elon Musk’s OpenAI are the obvious other ones – which are dedicated, at least in part, to making AI less dangerous; some of them are definitely Rationalist groups, others are just broadly aligned. But I haven’t really told you, yet, what they’re actually doing.\n\nIt is, of course, an open question whether there’s anything they can do, at this stage at least. We’re still years, probably decades and possibly centuries away from AGI. Murray Shanahan of DeepMind isn’t convinced that the field is mature enough for the work we do now to have significant effect when AGI does happen: ‘If you want my personal view,’ he says, ‘I think that it’s probably a bit premature to be very confident that the work we’re doing now on this issue is going to be relevant.’ He’s not ruling it out, but he’s wary. ‘We really don’t know what AGI is going to look like, if and when we figure out how to make it.’ He thinks that the MIRI safety work, which looked at ‘logical problems related to insuring that self-modifying systems could preserve their reward functions, and things like that’, was ‘fascinating stuff and quite mathematically demanding’. But, he says, MIRI’s work revolves around an assumption that any AGI will be an extremely logical Bayesian-probability-theory engine, and it’s possible to imagine an AI that is nothing like that at all. ‘Suppose you could solve AI with some massive evolutionary process, and you just evolved your AI, or if you just had some enormous deep network with some fabulous amount of computation and back-propagation, you might not have any way of applying [MIRI’s results] to the thing you’ve built, because it wouldn’t operate in a sufficiently logical way.’\n\nWhen he mentioned this, I asked if AlphaGo was an example of the sort of program he was talking about. It was given a goal – become amazingly good at Go – and a reward mechanism, and then it was sent to go and play against itself billions of times until it got good. And the thing that came out was, indeed, amazingly good at Go, but no one who built it really knows why. The data and learning mechanisms went into a black box, sloshed around for a bit, and came out with the thing they wanted.\n\n‘Well, exactly,’ Shanahan says. ‘And you might still be in a position to design a reward function that you know isn’t going to have unintended consequences and perverse instantiations – the paperclip maximiser, these side effects that involve existential risk. But I’m involved in the sharp end of building these things, and making them increasingly powerful, and I just have difficulty extrapolating from where we are now to AGI, and being confident about what it’s going to look like.’\n\nObviously enough, Rob Bensinger of MIRI is more optimistic that the work they’re doing will be useful. ‘I think there are lots of particular problems which you can work on today. There’s no particular reason to think we’ve grabbed all the low-hanging fruit yet. There are presumably lots of problems that you can’t see in advance, and you need to work with the system and learn about them later, but there’s plenty that’s knowably relevant right now.’ One aspect they’re working on, in fact, is avoiding exactly the sort of ‘impenetrable black-box’ scenario that Shanahan is talking about. ‘We want AGI systems, when they’re built, to be well understood by their developers, and to minimise the assumptions we have to make about your system in order to be confident they’re safe,’ he said. That could mean making a system which just has really simple outputs – for example, something that just comes up with mathematical proofs and theorems, and doesn’t do anything else. But that’s pretty limited: ‘The reason you build an AGI in the first place [is that] you want to get useful work out of it. Cure disease, help people, whatever. Set theory proofs aren’t the kind of things that are really going to make a difference in the world.’\n\nSo instead you want to be able to see how the AGI is doing its work. ‘Not in every detail, but in broad strokes,’ said Bensinger. ‘“What is this part here? What does this system do?”’ He drew a diagram on a nearby whiteboard as he spoke, boxes with arrows to other boxes, like a flowchart. ‘You can look at its internals and see the kind of optimisation that went into the final product, and confirm that it doesn’t have any bad convergent instrumental goals and does have the properties you do want. We assume that AGI will be more complex than current systems, so it’ll get complicated. But MIRI’s view is that the important thing is to empower the developer of the first really powerful AGI system to know what they’re doing, to be able to design a really modular, clean system where you can tell a story about why that system would have good effects. It wouldn’t be a perfect story, there will always be some uncertainty. But if you can’t even tell a story about why you should be confident – not just, “We don’t know why we shouldn’t be confident” . . .’ He didn’t finish the thought, but the implication left hanging was ‘that wouldn’t be great’.\n\nAnd you can’t get that sort of story out of a black-box system, he said. ‘If you just get the outputs and say, “Well, it seems to act safely, nothing bad has happened so far”, that’s not the sort of story we think you can get an outcome out of.’ So the sort of evolved AI that Shanahan mentioned would make MIRI very nervous indeed. As, in fact, would AlphaGo. ‘I’d say you’re in a bad state if you kind of understand your system, but the understanding is too many degrees removed from actual cognitive work that’s happening. Things like AlphaGo, where you can describe it at a high level – it’s using Monte Carlo tree search, it has these value networks, you can say this stuff at a certain level of abstraction – and then you get a really good Go player. But you’re not really describing the reasoning it’s doing to get into good board positions.’ You’re describing how it built itself, but not what it’s doing. I think it’s fair to say that if DeepMind use AlphaGo as the basis for their first AGI, MIRI would be deeply unnerved.\n\nBut ‘Make it so we can see how it works’ is quite a high-level description of the method for creating a safe AI. Remember, way back when we were discussing why AI is dangerous, we talked about how seemingly innocuous goals can have weird outcomes, ‘perverse instantiations’: the paperclip maximiser was one example, the Fantasia broom another. Is anyone doing anything more specific to try to reduce the likelihood that, the first time an AGI is turned on, it will turn the solar system into computing hardware to become even better at chess?\n\nThe answer is yes. There are a few things. One of the key ones, in fact, is clearly delineating what the AI safety problems actually are. Probably the most famous paper on AI safety is ‘Concrete Problems in AI Safety’, by a team from Google, OpenAI, Berkeley and Stanford.¹ It discusses, for instance, the problem of defining negative side effects: if you give an AI a goal (such as ‘fill the cauldron’), how do you let your AI know that there are things to avoid, without explicitly listing every single thing that could go wrong? An example of how you could do so is by saying that the robot can only change the environment it’s in by a certain amount – limiting or budgeting its impact on the world – although how you define that is a new and perhaps equally tricky problem.\n\nAnother problem the paper identified was ‘reward hacking’, an AI finding a shortcut to achieving its utility function without doing what its makers want; ‘ending cancer by nuking humanity’ would be a dramatic example, although the more prosaic one they mention in the paper is of a cleaning robot being told to stop when it can’t see any more rubbish, so it just turns off its cameras. The paper suggests that a separate agent designed solely to judge whether the rewards the first agent receives are earned might be a good way around it.\n\nAs well as simply defining the problems, there are specific efforts to find solutions. Holden Karnofsky, the OpenPhil founder, got quite excited talking to me about a paper called ‘Deep Reinforcement Learning from Human Preferences’,² by, among other people, Shane Legg, one of the founders of DeepMind, and Paul Christiano of OpenAI.\n\nThe idea is that for some goals, a simple definition isn’t much use. Karnofsky showed me a video of a simulated environment called MuJoCo, a sort of toy world with broadly realistic physics that programmers use to test how a robot might move. First, he showed me three ‘robots’ in that world which had been given a task, ‘Learn to walk.’ They all made reasonable progress; one was a snake-thing which wriggled along, one was a uniped which hopped, the third bipedal and vaguely humanoid.\n\n‘They’ve all been given the goal of trying to cover a lot of distance in a short amount of time,’ said Karnofsky. ‘They have these limbs and they have these joints. What they do is move around at random, and over time they figure out that certain movements make more progress, and over time they learn to walk. You can run the same algorithm on a bunch of different-looking robots and they’ll all learn to walk in their own special way.’ That’s because ‘walking’, or at least ‘making progress’, is nice and easy to define mathematically. ‘You have an X-coordinate,’ said Karnofsky, ‘and the further you get from it per unit of time, the better you’re doing. It’s measurable.’\n\nBut it relies on that score providing good feedback about what you actually want. In general (although not always, as we saw earlier when we were talking about evolved AIs), ‘how good is this robot at walking?’ and ‘distance travelled from starting point in given time’ align extremely well. That won’t always be the case in real-world AIs, though. Karnofsky imagines a more advanced AI. ‘Something that not only is able to move joints, but is able to send e-mails and make business decisions and do all kinds of things in the world. You can imagine if you had one of those AIs you can say, “Hey, can you maximise the amount of money in this bank account.”’ It’s a nice, simple, mathematically defined task, although any of us can see how it might go appallingly wrong.\n\nTasks that might not go immediately wrong are harder to define. ‘You could ask it, “Hey, can you stop other AIs from doing bad things on their way to maximising money”, and that would be a great [example of a] bad task, because I myself don’t know what I mean by that,’ said Karnofsky. ‘“Maximise the money” is a well-defined goal. “Keep us safe” or “Help humanity thrive” or “Make the world more peaceful” are fuzzily defined goals. I don’t even know what I mean by them.’\n\nWhat the Christiano paper does is try to see whether we can get AIs today to learn to perform tasks that we can’t define well. ‘That’s where the human feedback comes in,’ Karnofsky said. ‘The idea is that the AI will try random movements and then an actual human will look at two videos and say, “That one. That one is more like what I have in mind.”’\n\nIn the MuJoCo paper, they try this on a backflip. It turns out that it’s quite hard to define a backflip in a mathematically precise way, and all the robots that tried to learn how to do one with a preset goal ended up doing weird jerky movements that absolutely were not backflips. But with a human saying, ‘That random movement looks a bit more backflip-y’ they evolved a pretty impressive-looking backflip quite quickly – it took 900 iterations and less than an hour. Another task it learned was to play a racing video game, called Enduro. But not just play it to win. ‘They got it to keep pace with other cars,’ Karnofsky said. ‘The video game does not reward that. There’s no score for it; it’s just the human was able to look at it and say, “This is what I wanted.”’\n\nThis is early, toy stuff. And it’s not perfect: in one task, the AI was supposed to learn to pick up a box with its manipulators, but instead learned to put a manipulator between the box and the camera so it looked to the operator as though it had done so. But you can see how it could be extrapolated into a more powerful machine with more complex goals: ‘Rid the world of cancer’, but don’t do it by killing all the humans, that sort of thing.\n\nAnother step, Karnofsky added, would simply be to get governments and tech companies to sign treaties saying they’ll submit any AGI designs to outside scrutiny before switching them on. It wouldn’t be iron-clad, because firms might simply lie, ‘but it’s substantially better than nothing. There’s a good chance that when the first transformative AI comes it’ll be a massive project.’ If there are 1,000 people working on it, it’ll be almost impossible to keep it secret, so the treaties might be quite effective.\n\nI asked Nick Bostrom what he thought the most promising avenue was. ‘Broadly speaking,’ he replied, ‘some way that involves leveraging the AI’s intelligence to infer and learn about human values and preferences. If it’s superintelligent, it should be able to figure out what we want and mean, just as I can figure out a lot about what you want, from asking you about it, and looking at what you choose and so forth.’\n\nMore specifically, he pointed to another idea from Paul Christiano, on the topic of ‘capability amplification’.³ ‘Instead of trying to create this AI that has a utility function that captures everything we care about,’ said Bostrom, ‘we have some agent that, at each point in time, has some set of available actions, and chooses the one it thinks we would most approve of it taking, in some myopic way.’ It doesn’t try to leap ahead and think what will happen in six months’ time, it just chooses between A and B, and then between B1 and B2, and so on. ‘If you have to have a human overlooking every single step the AI takes, you don’t get much oomph out of it, so you’d have to come up with clever ideas for how to bootstrap a more limited, but safe, system for doing a larger portion of what we’d want the superintelligence to do.’\n\nBut, he said, it’s still early days. ‘There are lots of other ideas, and it might be that the best ideas haven’t even been articulated yet. Or maybe we already have a solution, but we can’t be confident yet that it would work because there are some parts we don’t yet understand. There’s just a lot of uncertainty about the difficulty of the problem.’\n\nI should, also, point out that there’s a lot of scepticism about just how much good MIRI in particular is doing. They have several researchers, but publish very few papers which are rarely cited. That might be because they’re trying to build a field, as discussed before, but it does make it hard to be sure how important their work is at this stage. Toby Ord of FHI told me that he was somewhat sceptical of their research agenda, which he felt was a bit too focused on distant-future ideas: ‘I’m more excited about work that’s more continuous with current work in AI,’ he said. ‘Stuff that’s only one step away from what the AI researchers are working on. There’s more chance that you can get them to come across and help you on the project.’\n\nOf course, technical papers about how to keep AI safe aren’t the only measure of how successful the Rationalists are being. Remember that when the young Eliezer Yudkowsky started thinking about this stuff in about 2000, he was pretty much the first person to do so. People had discussed the ‘singularity’, and I.J. Good had predicted an ‘intelligence explosion’, but the specific problem that an AI might not ‘go rogue’ or ‘turn evil’ or ‘achieve self-awareness’ but simply do exactly what you told it to do, and still go terribly, terribly wrong, seems to have sprung up with Bostrom and Yudkowsky on the SL4 website. It was, and I hope this isn’t too rude, the random musings of a load of young, crankish men on the early internet.\n\nNow it’s the subject of whole departments at several major universities, and discussed by major leading intellectuals: Martin Rees, the late Stephen Hawking, Bill Gates. Google’s DeepMind is explicitly worried about it; co-founders Shane Legg and Demis Hassabis both take it seriously.\n\n‘It’s been amazingly quick progress,’ commented Ajeya Cotra, of OpenPhil. ‘I think the discourse around technical safety of these in 2014, compared to now, feels like different worlds.’ Bostrom’s book Superintelligence was a turning point, she said. It was a New York Times bestseller, and brought serious academic heft to the field. ‘In 2014, we had a small group of outsider futurists trying to convince the AI community that this is a risk to be taken seriously. A lot of researchers didn’t, because they heard a garbled message through the media which sounded kind of fearmongery. Then Superintelligence came out. It was less that a bunch of people read the arguments and were totally convinced than that it was a serious academic making a thorough case. People felt it merited a response, and when sceptics put forward that response, their points didn’t always add up.’\n\nThis roughly matches Holden Karnofsky’s position. ‘As of 2012, it was kind of a niche community issue, and it was very hard to find anyone with mainstream credentials who would even acknowledge it,’ he said. He also pointed to the publication of Superintelligence as a turning point, and a major AI safety conference in Puerto Rico in 2015, organised by Max Tegmark’s Future of Life department at MIT.\n\n‘They had an open letter saying AI has major risks, which was signed by a lot of people. I don’t think all of them were signing off on [AI risk as envisioned by Yudkowsky et al.]. But it certainly became a more mainstream idea to talk about the idea that AI is risky.’ Now, he said, ‘if you look at the top labs, you look at DeepMind, OpenAI, Google Brain – many of the top AI labs are doing something that shows they’re serious about this kind of issue.’ ‘Concrete Problems in AI Safety’ is a collaboration between those three groups, he pointed out. ‘It’s a paper where they have safety in the title and it has major researchers from all these three labs and it was put out under the Google PR machine. It’s much more mainstream than it used to be. It seems hard to deny that.’\n\nAll this has meant that, from a field-building point of view, Yudkowsky et al. have been extraordinarily successful. ‘There are well-known, well-respected machine-learning researchers involved now,’ said Helen Toner, Ajeya Cotra’s OpenPhil colleague. ‘How to build this field is to make it something young people feel comfortable going into – not just starry-eyed Rationalists who think they want to go and save the world, but talented young machine-learning researchers who are searching for an area to specialise in. I heard of, I think at Berkeley, a student-supervisor pair, who were each interested in AI safety, and each of them thought, “Oh I can’t tell my supervisor/my student about that because they’ll think it’s silly.”’ ‘Concrete Problems in AI Safety’ and various OpenPhil grants to major machine-learning research groups have started breaking that dynamic down, she commented. ‘We’re really trying to fix those dynamics, and I think it’s helped.’\n\n‘It’s incredible!’ said Paul Crowley, beaming. ‘Honestly, when I got involved I thought there’s no way we’ll get anyone to take this seriously – people will just think we’re crazy. There’s no way we can be more than some crazy fringe thing that a few people talk about.’\n\nChapter 41\n\n The internal double crux\n\nRight at the beginning of the book, I mentioned what Paul Crowley said to me: that he doesn’t expect my children to die of old age. What I didn’t say, back then, was whether I took him seriously. Do I actually think that is more likely than not?\n\nRather than simply answer that question, I want to talk you through an experience I had in Berkeley. Soon after I met Paul, I spoke to Anna Salamon of CFAR, and told her what Paul had said. It threw me, I said. The thing that I really like about the Rationalist community, I told her, is the idea of people trusting the numbers and the reasoning, and not throwing out the conclusion those numbers and reasons lead them to, even if it’s shocking or bizarre. But now I found myself in a strange situation, where I was pretty happy with all the different steps in the reasoning – I can see why value alignment might be hard, and why an AI could be amazingly intelligent but still do stupid things, and I don’t think it’s crazy to think it’ll happen in my children’s lifetimes – but I found putting them all together, and agreeing with that profoundly unnerving conclusion, difficult.\n\nTo her enormous credit, Anna didn’t, as most people would, simply dismiss my concerns about the central thesis of the movement of which she’s a crucial part; instead, she pointed out that to have such concerns is not a stupid thing to do. She thinks the technological singularity, and all the disaster or utopia that entails, probably will happen this century. But when I made that remark about instinctively rejecting the conclusion despite agreeing with the steps to get there, she shrugged. ‘Yeeaaaaaah,’ she said. ‘But that’s not always wrong, is it? When I was first taking algebra class, and somebody showed me the standard proof that 2 = 1 – which involves secretly dividing by zero – it felt to me like every single step was valid.’ When you’re dealing with complex things like this, if you get weird answers, it might be that you’ve input the steps wrong. ‘I think the thing to do in such a case isn’t to reject one thing or the other, but to really stay with the question. Having now stayed with the question for a while, it seems to me that the argument for there being substantial risk from AI is really quite strong.’\n\nThen she asked: ‘Do you believe it? That your children will not die of old age? That you might not die of old age?’ I said I didn’t know. I couldn’t see the divide-by-zero equivalent – I still can’t. But nor could I, on an intuitive level, feel that the conclusion was right. I still assume that I’ll grow old and die, as my grandfather had recently. And while I don’t like to think about it, I assume that my children will too. There was a tension between these two parts of my brain.\n\n‘So,’ Anna said, ‘would you like to try something? We have a funny CFAR technique called the internal double crux.’ The standard ‘double crux’ is a method Rationalists use for examining why two people disagree. ‘It’s about how to figure out what cruxes a disagreement you’re having with someone rest on. The crux of the argument is the thing that, if you knock it over, their conclusion falls down, and they have a different conclusion.’\n\nThe example given on LessWrong is an argument between two people about school uniforms.¹ Person A thinks schoolchildren should wear uniforms; person B thinks they shouldn’t. To find the crux, you look at what those beliefs entail, what the more specific implications of them are. So person A might think that school uniforms reduce bullying, by making it less obvious which children are rich and which are poor; person B might think that’s ridiculous. But if you could show that school uniforms do reduce bullying, by some given amount, then person B would change her mind on the uniforms question; likewise, if you could show that they don’t, then person A would change his mind. The technique involves slowly bringing the conversation away from top-level, shouty arguments and towards detailed, specific disagreements.\n\nThis is pretty useful and sensible, I think. But the internal double crux is a bit more strange. It’s for disagreements with yourself – for instance, if part of you agrees with the argument that says, ‘Your children won’t die of old age’, and part of you thinks that just sounds too crazy to be believed. ‘It’s a pretty weird thing,’ Anna warned me. ‘It involves going into your head, and sometimes you find things there that sound a little crazy. But I could walk you through it.’ So, a few days later, I went back to the CFAR offices.\n\nThere’s a little story that Anna tells, a sort of parable, about a little girl at school who does some creative writing, and at the end of it the teacher reads it through, and says, ‘Look, you misspelled “ocean”.’ ‘No I didn’t,’ says the kid, and the teacher replies: ‘I’m sorry, but you did. It’s counter-intuitive, but it’s a “c” not “s-h”.’ The child, increasingly angry, repeats, ‘No, I didn’t,’ the teacher, ‘No, I’m sorry but you did.’ ‘No, I didn’t.’ ‘I realise it sometimes hurts to face the truth, but you really did.’ And then the child runs off to the cupboard and bursts into tears, saying, ‘I did not misspell the word. I can too be a writer.’ Anna calls this a bucket error. The ‘I can spell the word “ocean”’ fact went into the same bucket as the ‘I can be a writer when I grow up’ fact, and when the one was proved false, the child assumed, on some pre-conscious level, that they both were.\n\nMore broadly, there’s often a strange lack of communication between our verbal, reasoning selves and some deeper part of us. The feeling that you’ve forgotten something at the supermarket, but you don’t know what it is – is it broccoli? No. Is it rice? No, but I do need rice; is it avocado? It’s avocado! The knowledge is there on some level, and the feeling when your conscious mind latches onto the right answer is one of almost physical relief. Or something gives us a near-physical sense of ‘yuck’ or ‘yum’ when we see or hear it, without exactly telling us why, and unless we give that sensation a few moments we might not know the reason; the word ‘slack’ gives me this sort of ‘yuck’ sensation, and for years I didn’t know why, until I remembered an Onion article which used it in an astonishingly vulgar phrase about female genitalia which apparently had stuck with me ever since.\n\nAccording to the way in which CFAR and the Rationalists model the brain (and it is just a model; Anna repeatedly stressed that she wasn’t claiming that this is really how the brain works, just that it was useful and effective), sometimes, when we have internal conflicts about an issue, a little alarm, a beeping noise, goes off when we take on information; our brain is telling us that the information conflicts with something somewhere, and although we may not consciously know why, it imbues that piece of information with a little sense of ‘yuck’ and makes it harder for us to accept it. This is, I think, what Eliezer Yudkowsky describes as ‘noticing your confusion’. When I said to Anna that each of the steps made sense, but that I couldn’t accept the conclusion, she wondered if that was what was going on. The ‘internal double crux’ technique is a method Rationalists use in this sort of situation to help them establish what their own objection is, to find their difficulty with some conclusion.\n\nI am aware that this all sounds a bit mystical and self-helpy. It’s not. It was a strange sensation, certainly, but it was extremely common-sensical and unspectacular, although it did make me understand how talking therapy (which I’ve never tried) could be a powerful tool: just the experience of talking very deeply about some mental experience you’ve had is quite profound. Anyway, the basic idea was simple. Anna asked me: ‘What’s the first thing that comes into your head when you think the phrase, “Your children won’t die of old age”?’\n\n‘The first thing that pops up, obviously,’ I told her, ‘is I vaguely assume my children will die in the way we all do. My grandfather died recently; my parents are in their sixties; I’m almost 37 now. You see the paths of a human’s life each time; all lives follow roughly the same path. They have different toys – iPhones instead of colour TVs instead of whatever – but the fundamental shape of a human’s life is roughly the same. But the other thing that popped up is a sense of “I don’t know how I can argue with it”, because I do accept that there’s a solid chance that AGI will arrive in the next 100 years. I accept there’s a very high likelihood that if it does happen then it will transform human life in dramatic ways – up to and including an end to people dying of old age, whether it’s because we’re all killed by drones with kinetic weapons, or uploaded into the cloud, or whatever. I also accept that my children will probably live that long, because they’re middle-class, well-off kids from a Western country. All these things add up to a very heavily non-zero chance that my children will not die of old age, but, they don’t square with my bucolic image of what humans do. They get older, they have kids, they have grandkids and they die, and that’s the shape of the life. Those are the two fundamental things that came up, and they don’t square easily.’\n\nSo Anna asked me to look at the two sides in turn.\n\nThe more sceptical bit of me said this. ‘There’s some quite big bit of me which looks at all this stuff and thinks, you’re talking about immortality, and the end of the world, and all these things that people have prophesied since for ever. Every generation thinks this one’s the last one. There’s not a generation in history that hasn’t thought exactly that, and with every single one it’s been ridiculous. We look back at the Heaven’s Gate people, or Christian prophets in the first century, or the Anabaptists in Reformation Münster, all thinking it’s going to happen any day now; we’ve been through this story a million times before, and, every time, it doesn’t happen. The lesson we should draw from all this is that the predictions of the doom, or ascension, of mankind tend not to come through.’\n\nOK, said Anna. Take that, and offer it up to the other bit of you and see what it says; let it, in her words, ‘fully and generously acknowledge’ all the parts of the statement that seem to be true, giving it room to breathe.\n\nAnd the bit of me that accepts the steps of the argument had this to say. ‘All right, there really have been a lot of predictions, and from the point of view of the people making the predictions, it must have felt as if the logic of all the parts fitted together, or they wouldn’t have made the predictions. I assume these people weren’t idiots. They must have had reasons to believe that the end times were on them. It would be very hard for someone inside that chain of logic to step outside and say, “Yes, but there have been lots of predictions before, and they were wrong.” You have to use the base rate as a real piece of evidence in your thinking about these things. Otherwise you’re doing no better than the people who made all those predictions before. You have to look at the base rate, and the base rate of predictions of the apocalypse coming through is zero.’ This is the ‘outside view’, which we talked about a few chapters ago. The internal logic of something can be compelling, but you have to look at how other, similar things have fared in the past.\n\nThen the worried bit was allowed to reply: ‘But this is also an argument against taking steps against climate change. You’re saying the world always carries on as it always has, but, actually, we are changing the world. And we know that life on Earth is wildly different from how it was 100 years ago, and that was wildly different from how it was 300 years before that, but actually that was less different than 1,000 years before that. The pace of change of human life is gathering.’ It’s not ridiculous to think that it could be even more unrecognisable in 100 more years. And my worried self had a wider point. ‘Every prediction of the doom of mankind will be wrong apart from one. That’s implicit in the logic of it. If you keep saying the sun won’t rise tomorrow, you’ll be wrong every time until you’re right. Induction can only work so many times; eventually, something must, by its nature, break it.’\n\nSo, the sceptical bit of me had to take that and acknowledge it. ‘OK. I admit that a lot of the arguments that one could make to say that climate change is nothing to worry about are echoes of those made by sceptics who say, “Humanity has been on the planet for 100,000 years and we haven’t caused the sea levels to rise yet.” That doesn’t mean that we won’t soon, and we have impacts that are far greater now than they were even 50 or 100 years ago. And it’s true that you can’t have a base rate of doomsday predictions that is other than zero. If there are better reasons to believe in this one than the Anabaptists’ prophecies, then we ought to take it on its own merits. Some of the things on which the Anabaptists based their arguments I would absolutely reject as being without foundation, whereas this prediction is based on things that I would not.’\n\nThen that part of me was allowed to say: ‘But you can track concentrations of carbon dioxide in the atmosphere, and make simple mathematical models to predict what will happen to things like temperature and sea-level rise, using equations that have been around since the early 1900s, and see that those predictions roughly match real-world data. I’m not sure of the extent to which that is comparable to artificial intelligence. And there is certainly more uniformity among climate scientists that climate change is imminent and dangerous than there is uniformity among AI researchers that AI is imminent and dangerous.’ (I promise this is all real. I’ve tidied the quotes up a bit, but there’s something about talking to Rationalists that makes you use phrases like ‘heavily non-zero probability’ and ‘the base rate of predictions of the apocalypse’.)\n\nIt was at this point that the conversation, if that’s the right word, took a slightly odd turn. It was still my sceptical side’s turn to speak, and it had this to say: ‘I can picture a world in 50 or 100 years that my children live in, which has different coastlines and higher risk of storms and, if I’m brutally honest about it, famines in parts of the world that I don’t go to. I could imagine my Western children in their Western world living lives that are not vastly different to mine, in which most of the suffering of the world is hidden away, and the lives of well-off Westerners largely continue and my kids have jobs. My daughter is a doctor and my son is a journalist, whatever. Whereas if the AI stuff really does happen, that’s not the future they have. They have a future of either being destroyed to make way for paperclip-manufacturing, or being uploaded into some transhuman life, or kept as pets. Things that are just not recognisable to me. I can understand from Bostrom’s arguments that an intelligence explosion would completely transform the world; it’s pointless speculating what a superintelligence would do with the world, in the same way it would be stupid for a gorilla to wonder how humanity would change the world.’\n\nAnd I realised on some level that this was what the instinctive ‘yuck’ was when I thought about the arguments for AI risk. ‘I feel that parents should be able to advise their children,’ I said. ‘Anything involving AGI happening in their lifetimes – I can’t advise my children on that future. I can’t tell them how best to live their lives because I don’t know what their lives will look like, or even if they’ll be recognisable as human lives.’\n\nI then paused, as instructed by Anna, and eventually boiled it down. ‘I’m scared for my children.’ And at this point I apologised, because I found that I was crying. ‘I cry at about half the workshops I do,’ said Anna, kindly. ‘Often during the course of these funny exercises.’\n\nChapter 42\n\n Life, the universe and everything\n\nI don’t want to claim that the fact that I cried in a Californian office in the autumn of 2017 means that AI is going to kill us all and destroy the universe. I was alone in a strange country, 5,000 miles from my children, tired and jet-lagged and generally in quite an emotionally vulnerable situation. I don’t cry very often, but it’s not all that surprising that I did at this point. It was, though, quite powerful. I have tried to recapture the feeling of that moment and largely failed, although I felt an echo of it as I listened to the audio while writing this. It felt, emotionally, real that I didn’t want to think about the implications because the implications were so terrifying.\n\nSo do I believe in a paperclip apocalypse? Let’s think about this probabilistically.\n\nI do believe that AGI could happen quite soon. It’s far from clear when, but if a large number of AI researchers think it could well occur in the next 50 years and is near-certain in the next 100, then I don’t know why I would disagree. Bostrom’s survey, which seems to be the best we have, said that AI researchers, on average, think there’s a 90 per cent chance that AGI will arrive by 2075.¹ For what little it’s worth, I (like Bostrom) put more weight on the idea that it will never arrive, or will arrive in some immensely distant future, for no better reason than that AI, like fusion power, has been 30 years away for the last 50 years. Let’s say I think it’s 80 per cent likely that AGI will arrive at some point in the next 90 to 100 years, the likely lifespan of my children.\n\nThe next question is whether I think that is likely to lead to the sort of spectacularly terrible outcome that MIRI and other people fear. Going back to Bostrom’s survey, 18 per cent of respondents believe that AGI will lead to something ‘extremely bad’ (existential catastrophe), i.e. human extinction. Some people who work on AI risk reckon it’s higher: Rob Bensinger described it as ‘high-probability’, for Nick Bostrom the ‘default outcome’ of an intelligence explosion is ‘doom’.² But some AI researchers consider that ridiculous: to Toby Walsh, for instance, the basic premise of an intelligent thing destroying the world was silly, and not really in keeping with what ‘intelligence’ means. Having looked into this stuff for quite a long time, I think he’s probably wrong, but he’s an AI researcher and I’m not. That fits the survey’s findings that a majority of AI researchers don’t think it’s the most likely outcome, though, so I’m still going take that 18 per cent at face value.\n\nShut up and multiply, as the Rationalists say. If you take that 80 per cent (the likelihood that AGI will arrive at some point in the next 90 to 100 years) and multiply it by 18 per cent, you get 14.4 per cent, or almost exactly a one-in-seven chance. I might be off by quite a distance in either direction, but it feels about right. (Toby Ord’s estimate for the likelihood of humanity going extinct this century, from any cause, is about one in six. ‘I think it’s something in the order of Russian roulette,’ he told me, and that’s after taking into account the fact that people are trying to stop it.) Imagine I’m off by more than an order of magnitude, because I might be; imagine there’s only a 1 per cent chance that I’m right. That’s still more likely than my children dying in a car crash, a risk which I do not think it is silly to worry about at all. I’m extremely happy to spend time teaching them the Green Cross Code, and for the government to invest in traffic-calming measures and impose legal safety requirements on car manufacturers to reduce that less-than-1-per-cent risk still further.\n\nThey’re a weird bunch, the Rationalists, with their polyamorousness and kink, abstruse jargon, living arrangements and behaviour. And they’re politically daft as well: their openness to debate means that their places on the internet are full of pretty unpleasant people. So the things they care about, such as AI risk and effective altruism, are in danger of getting smeared by association. ‘The singularity? Isn’t that the thing that racist sex-cult website is into?’\n\nBut you can’t psychoanalyse your way to the truth. They might be weird, but I don’t think they’re wrong in believing that AI risk, like pandemics and climate change, is something society should be taking steps to mitigate. There’s a non-trivial chance that it will do terrible things, and there are, it seems to me, realistic ways of trying to reduce the chance of that happening.\n\nI met a senior Rationalist briefly in California, and he was extremely wary of me; he refused to go on the record. He has a reputation for being one of the nicest guys you’ll ever meet, but I found him a bit stand-offish, at least at first. And I think that was because he knew I was writing this book. He said he was worried that if too many people hear about AI risk, then it’ll end up like IQ, the subject of endless angry political arguments that have little to do with the science, and that a gaggle of nerdy Californian white guys probably weren’t the best advocates for it. This was a concern I heard from a few people.\n\nI hope that doesn’t happen. The Rationalists are an interesting bunch of people and the things they’re doing seem worthwhile: not just the AI risk, although of course that’s central; but also the idea of thinking about how we think, and how we argue; and of considering probabilities and likelihoods in ways other than black and white and yes and no. And, yes, they can be hard to like – Eliezer Yudkowsky, in particular, is a difficult, strange and abrasive man, though undeniably smart and arguably visionary – and there are plenty of unpleasant people in their internet circles.\n\nBut there’s something noble about their endeavour. We are, or seem to be, increasingly bad at talking across disagreement: whether because of social media or political polarisation, it seems that people find it much harder to imagine that someone with different political views to their own might nonetheless be a decent person. There’s something wonderful about a project dedicated to explaining why we are so often wrong, and to taking arguments and ideas seriously, and not rejecting the ones we don’t like: creating a space for people to disagree in good faith.\n\nAnd what they have achieved in terms of the AI debate is, I think, remarkable. They’ve taken the niche, practically dystopian-science-fiction idea of AI risk and made people take it seriously. Mike Story pointed out that Donald Glover, the actor and rapper who stars in the TV show Community, uses Bostrom’s ideas in one of his episodes, indicating how mainstream these ideas have become. Perhaps more relevantly, the White House under Obama published a report in 2016 into ‘Preparing for the Future of Artificial Intelligence’³ which drew heavily and obviously on Yudkowsky/Bostrom ideas: the Puerto Rico conference and ‘Concrete Problems in AI Safety’ are extensively referred to, and Bostrom’s work is cited in the references.\n\nThe Rationalist community has changed enormously since its mid-2000s heyday. Eliezer Yudkowsky himself has largely withdrawn from the LessWrong stuff; he writes the occasional very long blog post on the MIRI website, and some more playful and/or not-AI-related stuff on Facebook and Tumblr, but he’s less engaged with the community-building stuff these days.\n\nBut the rest of them carry on. Holden’s OpenPhil still pushes money towards AI safety; Ajeya is still trying to build the field, although Helen has moved on and is now working on machine learning at a Chinese university. Paul remains at Google, encrypting things, but worrying about the future of humanity in his spare time. Scott and Katja broke up, but went on holiday together by accident a few weeks later, which felt very on-brand; they didn’t have a baby, and I don’t know whether the experimental robot baby helped with any decisions. They’re both still highly engaged in the Rationalist project: Scott as its most high-profile figurehead now that Yudkowsky has taken a back seat; Katja as a researcher at FHI and elsewhere. Anna continues to see lots of bright young nerds come through the CFAR doors, and she tries, with great sensitivity and sense, to direct them into careers that might help save the planet. Rob is still Yudkowsky’s messenger on Earth. And Nick Bostrom, of course, is now a globally famous figure in certain niche circles, and is getting referenced in major NBC comedy programmes and White House policy documents.\n\nOverall, they have sparked a remarkable change. They’ve made the idea of AI as an existential risk mainstream; sensible, grown-up people are talking about it, not just fringe nerds on an email list. From my point of view, that’s a good thing. I don’t think AI is definitely going to destroy humanity. But nor do I think it’s so unlikely that we can ignore it. There is a small but non-negligible probability that, when we look back on this era in the future, we’ll think that Eliezer Yudkowsky and Nick Bostrom – and the SL4 email list, and LessWrong.com – have saved the world. If Paul Crowley is right and my children don’t die of old age, but in a good way – if they and humanity reach the stars, with the help of a friendly superintelligence – that might, just plausibly, be because of the Rationalists.\n\n Acknowledgements\n\nIt’s frankly weird that this book got written. I am not the sort of person who successfully writes books. I have, traditionally, been the sort of person who periodically says, ‘One day I’ll write a book,’ while the people around me say, ‘Righto, Tom, you said that five years ago.’\n\nSo, I can take only a fraction of the credit for the fact that it did, in fact, get written. A large amount of the rest should go to these various other people.\n\nWill Francis, agent at Janklow & Nesbit, kept taking me out for nice lunches until a serviceable idea fell out of my head. Paul Murphy at Weidenfeld & Nicolson helpfully agreed to give me some money to write the book, and then made me take out some of the sillier jokes. Linden Lawson did a sterling job clearing up my waffly and repetitious prose.\n\nI owe great thanks to Ajeya Cotra, Andrew Sabisky, Anna Salamon, Buck Shlegeris, Catherine Hollander, David Gerard, Diana Fleischman, Helen Toner, Holden Karnofsky, Katja Grace, Michael Story, Mike Levine, Murray Shanahan, Nick Bostrom, Peter Singer, Rob Bensinger, Robin Hanson, Scott Alexander, Toby Ord, Toby Walsh and everyone else who spoke to me. Plus grudging thanks to Eliezer Yudkowsky who did not, in fact, agree to talk to me, but did answer my irritating questions by email. Elizabeth Oldfield and Pete Etchells both looked over parts of the manuscript and reassured me that it was not total garbage.\n\nEspecial thanks to Paul Crowley, who basically introduced me to the whole concept, invited me out to California, was extremely nice to me while I was there, answered loads of questions, and then read the manuscript and helped me strip out lots of stupid mistakes.\n\nAnd, of course, Alison and Andy, my parents, for everything; Emma, my wife, for everything else; and Billy and Ada, for not coming upstairs and hammering on the keyboard with their sticky little paws too often.\n\n Notes\n\nIntroduction: ‘I don’t expect your children to die of old age’\n\n1. Elon Musk, Twitter, 3 August 2014 https://twitter.com/elonmusk/status/495759307346952192?lang=en\n\n2. https://qz.com/698334/bill-gates-says-these-are-the-two-books-we-should-all-read-to-understand-ai/\n\n3. Cambridge University press release, 19 October 2016 http://www.cam.ac.uk/research/news/the-best-or-worst-thing-to-happen-to-humanity-stephen-hawking-launches-centre-for-the-future-of\n\n4. Nick Bostrom, Superintelligence: Paths, Dangers, Strategies (OUP, 2014), p. 222\n\n5. https://en.wikipedia.org/wiki/2017\\_California\\_wildfires\n\n1: Introducing the Rationalists\n\n1. http://yudkowsky.net/obsolete/singularity.html\n\n2. Omni magazine, January 1983\n\n3. ‘Raised in technophilia’, LessWrong sequences, 17 September 2008 https://www.readthesequences.com/RaisedInTechnophilia\n\n4. ‘The magnitude of his own folly’, LessWrong sequences, 30 September 2008 https://www.readthesequences.com/TheMagnitudeOfHisOwnFolly\n\n5. Nick Bostrom, ‘A History of Transhumanist Thought’, Journal of Evolution and Technology, vol. 14, issue 1, 2005 https://nickbostrom.com/papers/history.pdf\n\n6. Marie Jean Antoine Nicolas Caritat, Marquis de Condorcet, Esquisse d’un tableau historique des progrès de l’esprit humain (Masson et Fils, 1822)\n\n7. Benjamin Franklin, letter to Jacques Dubourg, 1773, US government archives https://founders.archives.gov/documents/Franklin/01-20-02-0105\n\n8. Julian Huxley, Religion Without Revelation (Harper Brothers, 1927)\n\n9. Eliezer Yudkowsky, My life so far, August 2000 http://web.archive.org/web/20010205221413/http://sysopmind.com/eliezer.html#timeline\\_great\n\n10. William Saletan, ‘Among the Transhumanists’, Slate, 4 June 2006 https://web.archive.org/web/20061231222833/http://www.slate.com/id/2142987/fr/rss/\n\n11. Quoted in Bostrom, ‘A History of Transhumanist Thought’, p. 14\n\n12. Alvin Toffler, Future Shock (Turtleback Books, 1970)\n\n13. Eliezer Yudkowsky, ‘Future shock levels’, SL4 archives, 1999 http://sl4.org/shocklevels.html\n\n14. Eliezer Yudkowsky, ‘The plan to Singularity’, 2000 http://yudkowsky.net/obsolete/plan.html\n\n15. ‘Re: the AI box experiment’, SL4 archives, 2002 http://www.sl4.org/archive/0203/3141.html\n\n16. Nick Bostrom, ‘The simulation argument’, SL4 archives, 2001 http://www.sl4.org/archive/0112/2380.html\n\n17. History of LessWrong, https://wiki.lesswrong.com/wiki/History\\_of\\_Less\\_Wrong\n\n18. Overcoming Bias: about http://www.overcomingbias.com/about\n\n19. ‘Fake fake utility functions’, LessWrong sequences, 6 December 2007 http://lesswrong.com/lw/lp/fake\\_fake\\_utility\\_functions/\n\n20. Riciessa, ‘LessWrong analytics, February 2009 to January 2017’, LessWrong, 2017 https://www.lesswrong.com/posts/SWNn53RryQgTzT7NQ/lesswrong-analytics-february-2009-to-january-2017\n\n2: The cosmic endowment\n\n1. ‘Research priorities for robust and beneficial artificial intelligence: An open letter’, Future of Life Institute https://futureoflife.org/ai-open-letter\n\n2. Donald E. Brownlee, ‘Planetary habitability on astronomical time scales’, in Carolus J. Schrijver and George L. Siscoe, Heliophysics: Evolving Solar Activity and the Climates of Space and Earth (Cambridge University Press, 2010)\n\n3. Nick Bostrom, ‘Existential risk prevention as global priority’, 2012 http://www.existential-risk.org/concept.pdf\n\n4. Carl Haub, ‘How many people have ever lived on Earth?’, 2011 http://www.prb.org/Publications/Articles/2002/HowManyPeopleHaveEverLivedonEarth.aspx\n\n5. Bostrom, Superintelligence, p. 101\n\n6. Ibid., p. 102\n\n7. Existential Risk FAQ, Future of Humanity Institute http://www.existential-risk.org/faq.html\n\n8. Eliezer Yudkowsky, ‘Pascal’s mugging: Tiny probabilities of vast utilities’, LessWrong, 2007 http://lesswrong.com/lw/kd/pascals\\_mugging\\_tiny\\_probabilities\\_of\\_vast/\n\n9. Nick Bostrom, ‘Pascal’s mugging’, 2009 https://nickbostrom.com/papers/pascal.pdf\n\n10. Scott Alexander, ‘Getting Eulered’, 2014 http://slatestarcodex.com/2014/08/10/getting-eulered/\n\n11. Scott Alexander, ‘Stop adding zeroes’, 2015 http://slatestarcodex.com/2015/08/12/stop-adding-zeroes/\n\n3: Introducing AI\n\n1. Stuart J. Russell and Peter Norvig, Artificial Intelligence: A Modern Approach (3rd edn; Pearson, 2010), p. 1\n\n2. A.M. Turing, ‘Computing machinery and intelligence’, Mind, vol. 59, 1950, pp. 433–60\n\n3. Russell and Norvig, Artificial Intelligence, p. 3\n\n4. Luke Muehlhauser and Anna Salamon, ‘Intelligence explosion: evidence and import’, 2012 https://intelligence.org/files/IE-EI.pdf\n\n5. Eliezer Yudkowsky, ‘Expected creative surprises’, LessWrong sequences, 2008 http://lesswrong.com/lw/v7/expected\\_creative\\_surprises/\n\n6. Eliezer Yudkowsky, ‘Belief in intelligence’, LessWrong sequences, 2008 http://lesswrong.com/lw/v8/belief\\_in\\_intelligence/\n\n7. Ibid.\n\n8. Demis Hassabis et al., ‘Mastering chess and shogi by self-play with a general reinforcement learning algorithm’, Arxiv, 2017 https://arxiv.org/pdf/1712.01815.pdf\n\n9. Russell and Norvig, Artificial Intelligence, p. 4\n\n10. Nick Bostrom and Vincent C. Müller, ‘Future progress in artificial intelligence: A survey of expert opinion’, Fundamental Issues of Artificial Intelligence, 2016 https://nickbostrom.com/papers/survey.pdf\n\n11. Nick Bostrom, ‘How long before superintelligence?’, International Journal of Future Studies, vol. 2 1998 https://nickbostrom.com/superintelligence.html\n\n4: A history of AI\n\n1. Alan Turing, ‘On computable numbers, with an application to the Entscheidungsproblem’, Proceedings of the London Mathematical Society, vol. s2-42, issue 1, 1 January 1937, pp. 230–265 https://doi.org/10.1112/plms/s2-42.1.230\n\n2. J. McCarthy, M. Minsky, N. Rochester and C.E. Shannon, ‘A proposal for the Dartmouth summer research project on artificial intelligence’, 2 September 1956. Letter to the Rockefeller Foundation, retrieved from http://raysolomonoff.com/dartmouth/boxa/dart564props.pdf\n\n3. I.J. Good, ‘Speculations concerning the first ultraintelligent machine’, Advances in Computers, vol. 6, 1965\n\n4. Charles Krauthammer, ‘Be afraid’, The Weekly Standard, 26 May 1997 http://www.weeklystandard.com/be-afraid/article/9802#!\n\n5. John McCarthy, quoted in David Elson, ‘Artificial intelligence’, The Johns Hopkins Guide to Digital Media (Johns Hopkins University Press, 15 April 2014)\n\n6. A. Newell, J.C. Shaw and H. A. Simon, ‘Chess-playing programs and the problem of complexity’, IBM Journal of Research and Development, vol. 2(4), 1958, pp. 320–335\n\n7. Wolfgang Ertel, Introduction to Artificial Intelligence (Springer, 1993), p. 109\n\n5: When will it happen?\n\n1. Eliezer Yudkowsky, ‘There’s no fire alarm for artificial general intelligence’ https://intelligence.org/2017/10/13/fire-alarm/\n\n2. Donald B. Holmes, Wilbur’s Story, 1st edn (Lulu Enterprises, 2008), p. 91 https://books.google.co.uk/books?id=ldxfLyNIk9wC&pg=PA91&dq=%22i+said+to+my+brother+orville%22&hl=en&sa=X&redir\\_esc=y#v=onepage&q=%22i%20said%20to%20my%20brother%20orville%22&f=false\n\n3. Richard Phodes, The Making of the Atomic Bomb (Simon & Schuster, 2012), p. 280 https://books.google.com/books?id=aSgFMMNQ6G4C&pg=PA813&lpg=PA813&dq=weart+fermi&source=bl&ots=Jy1pBOUL10&sig=c9wK\\_yLHbXZS\\_GFIv0K3bgpmE58&hl=en&sa=X&ved=0ahUKEwjNofKsisnWAhXGlFQKHbOSB1QQ6AEIKTAA#v=onepage&q=%22ten%20per%20cent%22&f=false\n\n4. Bostrom and Müller, ‘Future progress in artificial intelligence’ https://nickbostrom.com/papers/survey.pdf\n\n5. K. Grace et al., ‘When will AI exceed human performance? Evidence from AI experts’, ArXiv https://arxiv.org/pdf/1705.08807.pdf?\\_sp=c803ec8d-9f8f-4843-a81e-3284733403a0.1500631875031\n\n6. David McAllester, ‘Friendly AI and the servant mission’, Machine Thoughts blog, 2014 https://machinethoughts.wordpress.com/2014/08/10/friendly-ai-and-the-servant-mission/\n\n7. Luke Muelhauser, ‘Eliezer Yudkowsky: Becoming a rationalist’, Conversations from the Pale Blue Dot podcast, 2011 http://commonsenseatheism.com/?p=12147\n\n8. Toby Walsh, Android Dreams (Hurst & Company, 2017), p. 54\n\n6: Existential risk\n\n1. Eliezer Yudkowsky/MIRI, ‘AI as a positive and negative factor in global risk’, 2008 https://intelligence.org/files/AIPosNegFactor.pdf\n\n2. The Giving What We Can pledge: https://www.givingwhatwecan.org/pledge/\n\n3. Nick Beckstead and Toby Ord, ‘Managing risk, not avoiding it’, Managing Existential Risk from Emerging Technologies, Annual Report of the Government Chief Scientific Adviser 2014, p. 116 https://www.fhi.ox.ac.uk/wp-content/uploads/Managing-existential-risks-from-Emerging-Technologies.pdf\n\n4. A. Robock, et al., ‘Multidecadal global cooling and unprecedented ozone loss following a regional nuclear conflict’, Earth’s Future, 2014 http://onlinelibrary.wiley.com/doi/10.1002/2013EF000205/full\n\n5. ‘Soviets close to using A-bomb in 1962 crisis, forum is told’, Boston Globe, 13 October 2002 http://www.latinamericanstudies.org/cold-war/sovietsbomb.htm\n\n6. List of nuclear close calls, Wikipedia https://en.wikipedia.org/wiki/List\\_of\\_nuclear\\_close\\_calls\n\n7. Yudkowsky, ‘AI as a positive and negative factor in global risk’ https://intelligence.org/files/AIPosNegFactor.pdf\n\n7: The cryptographic rocket probe, and why you have to get it right first time\n\n1. Nate Soares, ‘Ensuring smarter-than-human intelligence has a positive outcome’ 2017 https://intelligence.org/2017/04/12/ensuring/\n\n2. Tom Chivers, ‘The spaceship that took some of the greatest images of the solar system has died’, BuzzFeed, September 2017 https://www.buzzfeed.com/tomchivers/cassini-death-spiral\n\n8: Paperclips and Mickey Mouse\n\n1. Nick Bostrom, ‘Ethical issues in advanced artificial intelligence’, 2003 https://nickbostrom.com/ethics/ai.html\n\n2. http://www.decisionproblem.com/paperclips/index2.html\n\n3. Soares, ‘Ensuring smarter-than-human intelligence has a positive outcome’ https://intelligence.org/2017/04/12/ensuring/\n\n9: You can be intelligent, and still want to do stupid things\n\n1. Bostrom, Superintelligence, p. 9\n\n2. Nick Bostrom, ‘The superintelligent will: motivation and instrumental rationality in advanced artificial agents’, 2012 https://nickbostrom.com/superintelligentwill.pdf\n\n3. Elezier Yudkowsky, ‘Ghosts in the machine’, 17 June 2008 https://www.readthesequences.com/GhostsInTheMachine\n\n4. NCD Risk Factor Collaboration, ‘Trends in adult body-mass index in 200 countries from 1975 to 2014: A pooled analysis of 1698 population-based measurement studies with 19.2 million participants’, The Lancet, 2 April 2016 http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(16)30054-X/fulltext\n\n5. Yudkowsky, ‘AI as a positive and negative factor in global risk’, 2008 https://intelligence.org/files/AIPosNegFactor.pdf\n\n6. S. Omohundro, ‘The basic AI drives’, 2008 https://selfawaresystems.files.wordpress.com/2008/01/ai\\_drives\\_final.pdf\n\n7. Bostrom, ‘The superintelligent will’ https://nickbostrom.com/superintelligentwill.pdf\n\n10: If you want to achieve your goals, not dying is a good start\n\n1. Soares, ‘Ensuring smarter-than-human intelligence has a positive outcome’ https://intelligence.org/2017/04/12/ensuring/\n\n2. Omohundro, ‘The basic AI drives’ https://selfawaresystems.files.wordpress.com/2008/01/ai\\_drives\\_final.pdf\n\n3. Thucydides, History of the Peloponnesian War, trans. Richard Crawley (J.M. Dent & co., 1903), Chapter 1 https://ebooks.adelaide.edu.au/t/thucydides/crawley/complete.html\n\n4. Hans Morgenthau, Politics Among Nations: The Struggle for Power and Peace (McGraw-Hill Education, 1967), p. 64\n\n5. Thomas Hobbes, Leviathan, 1651 (Andrew Crooke, 1st edn), Chapter 13\n\n6. Nikita Khrushchev, ‘Telegram From the Embassy in the Soviet Union to the Department of State’, 2 October 1962. From Foreign Relations of the United States, 1961–63, Volume VI, Kennedy-Khrushchev Exchanges, US Department of State Office of the Historian, ed. Charles S. Sampson, United States Government Printing Office 1966 https://history.state.gov/historicaldocuments/frus1961-63v06/d65\n\n7. Bostrom, ‘The superintelligent will’ https://nickbostrom.com/superintelligentwill.pdf\n\n8. Soares, ‘Ensuring smarter-than-human intelligence has a positive outcome’ https://intelligence.org/2017/04/12/ensuring/\n\n9. Maureen Dowd, ‘Elon Musk’s billion-dollar crusade to stop the AI apocalypse’, Vanity Fair, April 2017 https://www.vanityfair.com/news/2017/03/elon-musk-billion-dollar-crusade-to-stop-ai-space-x\n\n10. https://wiki.lesswrong.com/wiki/Roko’s\\_basilisk\n\n11. http://rationalwiki.org/wiki/Roko%27s\\_basilisk/Original\\_post#Comments\\_.28117.29\n\n12. https://xkcd.com/1450/\n\n13. https://www.reddit.com/r/xkcd/comments/2myg86/xkcd\\_1450\\_aibox\\_experiment/cm8vn6e/\n\n14. David Auerbach, ‘Roko’s Basilisk, the single most terrifying thought experiment of all time’, Slate, 17 July 2014 http://www.slate.com/articles/technology/bitwise/2014/07/roko\\_s\\_basilisk\\_the\\_most\\_terrifying\\_thought\\_experiment\\_of\\_all\\_time.single.html\n\n15. Dylan Love, ‘Just reading about this thought experiment could ruin your life’, Business Insider, 6 August 2014 http://www.businessinsider.com/what-is-rokos-basilisk-2014-8?IR=T\n\n16. 2016 LessWrong diaspora survey results http://www.jdpressman.com/public/lwsurvey2016/Survey\\_554193\\_LessWrong\\_Diaspora\\_2016\\_Survey%282%29.pdf\n\n17. Scott Alexander, ‘Noisy poll results and reptilian Muslim climatologists from Mars’, 2013 http://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/\n\n11: If I stop caring about chess, that won’t help me win any chess games, now will it?\n\n1. Omohundro, ‘The basic AI drives’ https://selfawaresystems.files.wordpress.com/2008/01/ai\\_drives\\_final.pdf\n\n12: The brief window of being human-level\n\n1. Vernor Vinge, ‘Signs of the Singularity’, 2008 http://www.collier.sts.vt.edu/engl4874/pdfs/vinge\\_2008.pdf\n\n2. Demis Hassabis, et al.,‘Mastering the game of Go with deep neural networks and tree search’, Nature, January 2016 https://www.nature.com/articles/nature16961\n\n3. Miles Brundage, ‘AlphaGo and AI progress’, February 2016 http://www.milesbrundage.com/blog-posts/alphago-and-ai-progress\n\n4. Eliezer Yudkowsky, ‘My Childhood Role Model’, 2008 https://www.readthesequences.com/MyChildhoodRoleModel\n\n13: Getting better all the time\n\n1. Bostrom, Superintelligence, p. 2\n\n2. Robin Hanson, ‘Economics of the Singularity’, 1 June 2008 https://spectrum.ieee.org/robotics/robotics-software/economics-of-the-singularity\n\n3. I.J. Good, ‘Speculations concerning the first ultraintelligent machine’, Advances in Computers, vol. 6, 1966, pp. 31–88 https://www.lesswrong.com/posts/kHL6qX9eArmvNWY99/connecting-your-beliefs-a-call-for-help\n\n4. Bostrom, ‘The superintelligent will’ https://nickbostrom.com/superintelligentwill.pdf\n\n5. Ibid.\n\n6. Yudkowsky, ‘AI as a positive and negative factor in global risk’, 2008 https://intelligence.org/files/AIPosNegFactor.pdf\n\n14: ‘FOOOOOM’\n\n1. Bostrom, Superintelligence, p. 65\n\n2. Ibid., p. 65\n\n3. Ibid., p. 70\n\n4. Ibid., p. 68\n\n5. Eliezer Yudkowsky, ‘Hard takeoff’, LessWrong, 2008 http://lesswrong.com/lw/wf/hard\\_takeoff/\n\n6. Luke Meuhlhauser and Anna Salamon, Intelligence Explosion: Evidence and Import (MIRI, 2012) https://intelligence.org/files/IE-EI.pdf\n\n15: But can’t we just keep it in a box?\n\n1. Scott Alexander, ‘No physical substrate, no problem,’ 2015 http://slatestarcodex.com/2015/04/07/no-physical-substrate-no-problem/\n\n2. Dowd, ‘Elon Musk’s billion-dollar crusade to stop the AI apocalypse’ https://www.vanityfair.com/news/2017/03/elon-musk-billion-dollar-crusade-to-stop-ai-space-x\n\n3. Bostrom, Superintelligence, p. 129\n\n4. ‘The “AI box” experiment’, SL4 archives, 2002 http://www.sl4.org/archive/0203/3132.html\n\n5. Eliezer Yudkowsky, ‘Shut up and do the impossible!’, LessWrong sequences, 2008 https://www.lesswrong.com/posts/nCvvhFBaayaXyuBiD/shut-up-and-do-the-impossible\n\n6. Bostrom, ‘Risks and mitigation strategies for Oracle AI’, 2010 https://www.fhi.ox.ac.uk/wp-content/uploads/Risks-and-Mitigation-Strategies-for-Oracle-AI.pdf\n\n16: Dreamed of in your philosophy\n\n1. Bostrom and Müller, ‘Future progress in artificial intelligence’ https://nickbostrom.com/papers/survey.pdf\n\n2. Scott Alexander, ‘AI researchers on AI risk’, 2015 http://slatestarcodex.com/2015/05/22/ai-researchers-on-ai-risk/\n\n3. Ibid.\n\n17: ‘It’s like 100 per cent confident this is an ostrich’\n\n1. Jason Yosinsky, et al., ‘The surprising creativity of digital evolution: A collection of anecdotes from the Evolutionary Computation and Artificial Life Research communities’, ArXiv, March 2018 https://arxiv.org/pdf/1803.03453v1.pdf\n\n2. Christian Szegedy, et al., ‘Explaining and harnessing adversarial examples’ https://arxiv.org/pdf/1412.6572v3.pdf?loc=contentwell&lnk=a-2015-paper&dom=section-9\n\n18: What is rationality?\n\n1. Eliezer Yudkowsky, ‘What do I mean by rationality?’, LessWrong sequences, 16 March 2009 hhttps://www.readthesequences.com/What-Do-I-Mean-By-Rationality\n\n2. Ibid.\n\n3. Eliezer Yudkowsky, ‘Newcomb’s problem and regret of rationality’, LessWrong sequences, 31 January 2008 https://www.lesswrong.com/posts/6ddcsdA2c2XpNpE5x/newcomb-s-problem-and-regret-of-rationality\n\n4. Musashi Miyamoto, The Book of Five Rings, c.1645\n\n5. Eliezer Yudkowsky, ‘Why Truth? And . . .’, LessWrong sequences, 27 November 2006 https://www.readthesequences.com/Why-Truth-And\n\n6. Robert Nozick, ‘Newcomb’s problem and two principles of choice’, Essays in Honor of Carl G. Hempel (Springer Netherlands, 1969) http://faculty.arts.ubc.ca/rjohns/nozick\\_newcomb.pdf\n\n7. Eliezer Yudkowsky, ‘Timeless Decision Theory’, 2010 https://intelligence.org/files/TDT.pdf\n\n19: Bayes’ theorem and optimisation\n\n1. https://en.wikipedia.org/wiki/Thomas\\_Bayes\n\n2. Thomas Bayes, An Essay towards solving a Problem in the Doctrine of Chances, 1763\n\n3. ‘An intuitive explanation of Bayes’ theorem’, LessWrong sequences, 1 January 2003 https://www.readthesequences.com/An-Intuitive-Explanation-Of-Bayess-Theorem\n\n4. Ward Casscells, Arno Schoenberger and Thomas Graboys, ‘Interpretation by physicians of clinical laboratory results,’ New England Journal of Medicine, vol. 299, 1978, pp. 999–1001\n\n5. ‘Searching for Bayes-Structure’, LessWrong sequences, 28 February 2008 https://www.readthesequences.com/SearchingForBayesStructure\n\n6. Fred Hoyle, ‘Hoyle on Evolution’, Nature, vol. 294, No 5837 (12 November 1981), p. 105\n\n7. Eliezer Yudkowsky, ‘How much evidence does it take?’, LessWrong sequences, 2007 https://www.readthesequences.com/How-Much-Evidence-Does-It-Take\n\n8. Ibid.\n\n20: Utilitarianism: shut up and multiply\n\n1. Mason Hartman, @webdevmason, 2 April 2018 https://twitter.com/webdevMason/status/980861298387836928\n\n2. ‘Extracts from Bentham’s Commonplace Book’, in 10 Works of Jeremy Bentham (John Bowring, 1843), p. 141\n\n3. Eliezer Yudkowsky, ‘Torture vs dust specks,’ LessWrong sequences, 2008 https://www.lesswrong.com/posts/3wYTFWY3LKQCnAptN/torture-vs-dust-specks\n\n4. Ibid.\n\n5. Eliezer Yudkowsky, ‘Circular altruism’, LessWrong sequences, 2008 https://www.lesswrong.com/posts/4ZzefKQwAtMo5yp99/circular-altruism#uWXxEmfea9WFJmMSk\n\n6. For instance, Alastair Norcross of the University of Colorado in his paper ‘Comparing harms: Headaches and human lives’, 1997 http://spot.colorado.edu/~norcross/Comparingharms.pdf\n\n7. Derek Parfit, Reasons and Persons (OUP, 1984), p. 388\n\n8. Eliezer Yudkowsky, ‘The lifespan dilemma’, LessWrong sequences, 2009 https://www.lesswrong.com/posts/9RCoE7jmmvGd5Zsh2/the-lifespan-dilemma\n\n9. Eliezer Yudkowsky, ‘Ends don’t justify means (among humans)’, LessWrong sequences, 2009 https://www.readthesequences.com/EndsDontJustifyMeansAmongHumans\n\n10. Eliezer Yudkowsky, ‘One life against the world’, LessWrong sequences, 2007 https://www.lesswrong.com/posts/xiHy3kFni8nsxfdcP/one-life-against-the-world\n\n21: What is a ‘bias’?\n\n1. Rob Bensinger, ‘Biases: An introduction’, LessWrong sequences, 2015 https://www.readthesequences.com/Biases-An-Introduction\n\n2. Ibid.\n\n22: The availability heuristic\n\n1. David Anderson QC, ‘The Terrorism Acts in 2011: Report of the Independent Reviewer on the Operation of the Terrorism Act 2000 and Part 1 of the Terrorism Act 2006’, 2012 https://terrorismlegislationreviewer.independent.gov.uk/wp-content/uploads/2013/04/report-terrorism-acts-2011.pdf\n\n2. Sarah Lichtenstein, et al., ‘Judged frequency of lethal events,’ Journal of Experimental Psychology: Human Learning and Memory, vol. 4(6), 1978, pp. 551–78 doi:10.1037/0278-7393.4.6.551\n\n3. Elezier Yudkowsky, ‘Availability’, LessWrong sequences, 2008 https://www.readthesequences.com/Availability\n\n4. Garrick Blalock, et al., ‘Driving fatalities after 9/11: A hidden cost of terrorism’, Applied Economics, vol. 41, issue 14, 2009 http://blalock.dyson.cornell.edu/wp/fatalities\\_120505.pdf\n\n23: The conjunction fallacy\n\n1. Eliezer Yudkowsky, ‘Burdensome details’, LessWrong sequences, 2007 https://www.readthesequences.com/Burdensome-Details\n\n2. Amos Tversky and Daniel Kahneman, ‘Judgments of and by Representativeness’, in Judgment Under Uncertainty: Heuristics and Biases, ed. Daniel Kahneman, Paul Slovic and Amos Tversky (CUP, 1982), pp. 84–98\n\n3. A. Tversky and D. Kahneman, ‘Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment’, Psychological Review, vol. 90, 1983, pp. 293–315\n\n24: The planning fallacy\n\n1. Eliezer Yudkowsky, ‘Planning fallacy’, LessWrong sequences, 2007 https://www.readthesequences.com/Planning-Fallacy\n\n2. Roger Buehler, Dale Griffin and Michael Ross, ‘It’s about time: Optimistic predictions in work and love,’ European Review of Social Psychology, vol. 6(1), 1995, pp. 1–32 doi:10.1080/14792779343000112\n\n3. Ian R. Newby-Clark, et al., ‘People focus on optimistic scenarios and disregard pessimistic scenarios while predicting task completion times,’ Journal of Experimental Psychology: Applied, vol. 6(3), 2000, pp. 171–82 doi:10.1037/1076-898X.6.3.171\n\n4. Roger Buehler, Dale Griffin and Michael Ross, ‘Exploring the “planning fallacy”: Why people underestimate their task completion times’, Journal of Personality and Social Psychology, vol. 67(3), 1994, pp. 366–81 doi:10.1037/0022-3514.67.3.366\n\n5. Roger Buehler, Dale Griffin and Michael Ross, ‘Inside the planning fallacy: The causes and consequences of optimistic time predictions’, in Thomas Gilovich, Dale Griffin and Daniel Kahneman (eds), Heuristics and Biases: The Psychology of Intuitive Judgment (CUP, 2012), pp. 250–70\n\n6. Yudkowsky, ‘Planning fallacy’ https://www.readthesequences.com/Planning-Fallacy\n\n25: Scope insensitivity\n\n1. Elezier Yudkowsky, ‘Scope insensitivity’, LessWrong sequences, 2008 https://www.readthesequences.com/ScopeInsensitivity\n\n2. William H. Desvousges, et al., Measuring Nonuse Damages Using Contingent Valuation: An Experimental Evaluation of Accuracy (RTI Press, 1992) https://www.rti.org/sites/default/files/resources/bk-0001-1009\\_web.pdf\n\n3. Richard T. Carson and Robert Cameron Mitchell, ‘Sequencing and nesting in contingent valuation surveys’, Journal of Environmental Economics and Management, vol. 28(2), 1995, pp. 155–73 doi:10.1006/jeem.1995.1011\n\n4. Daniel Kahneman, Ilana Ritov and Daniel Schkade, ‘Economic preferences or attitude expressions?: An analysis of dollar responses to public issues’, Journal of Risk and Uncertainty, vol.19, issue 1–3, 1999, pp. 203–35 doi:10.1007/978-94-017-1406-8\\_8\n\n5. David Fetherstonhaugh, et al., ‘Insensitivity to the value of human life: A study of psychophysical numbing’, Journal of Risk and Uncertainty, vol. 14(3), 1997, pp. 283–300 doi:10.1023/A:1007744326393\n\n6. Rebecca Smith, ‘“Revolutionary” breast cancer drug denied on NHS over cost: NICE’, Daily Telegraph, 8 August 2014\n\n26: Motivated scepticism, motivated stopping and motivated continuation\n\n1. Jonathan Haidt, The Righteous Mind: Why Good People Are Divided by Politics and Religion (Penguin, 2012), p. 98\n\n2. Eliezer Yudkowsky, ‘Motivated stopping and motivated continuation’, LessWrong sequences, 2007 https://www.lesswrong.com/posts/L32LHWzy9FzSDazEg/motivated-stopping-and-motivated-continuation\n\n3. R.A. Fisher, ‘Lung cancer and cigarettes’, Nature, vol. 182, 12 July 1958, p. 108 https://www.york.ac.uk/depts/maths/histstat/fisher275.pdf\n\n4. F. Yates and K. Mather, ‘Ronald Aylmer Fisher 1890–1962’, Biographical Memoirs of Fellows of the Royal Society, vol. 9, 1963, pp. 91–129 doi:10.1098/rsbm.1963.0006.\n\n27: A few others, and the most important one\n\n1. Eliezer Yudkowsky, ‘Illusion of transparency: Why no one understands you’, LessWrong sequences, 2007 https://www.readthesequences.com/Illusion-Of-Transparency-Why-No-One-Understands-You\n\n2. Boaz Keysar, ‘The illusory transparency of intention: Linguistic perspective taking in text’, Cognitive Psychology, vol. 26(2), 1994, pp. 165–208 doi:10.1006/cogp.1994.1006\n\n3. Eliezer Yudkowsky, ‘Hindsight devalues science’, LessWrong sequences, 2007 https://www.readthesequences.com/Hindsight-Devalues-Science\n\n4. ‘Did you know it all along?’, excerpt from David G. Meyers, Exploring Social Psychology (McGraw-Hill, 1994), pp. 15–19 https://web.archive.org/web/20180118185747/https://musiccog.ohio-state.edu/Music829C/hindsight.bias.html\n\n5. Eliezer Yudkowsky, ‘The affect heuristic’, LessWrong sequences, 2008 https://www.readthesequences.com/TheAffectHeuristic\n\n6. Eliezer Yudkowsky, ‘The halo effect’, LessWrong sequences, 2008 https://www.readthesequences.com/TheHaloEffect\n\n7. Eliezer Yudkowsky, ‘Knowing about biases can hurt people’, LessWrong sequences, 2008 https://www.readthesequences.com/Knowing-About-Biases-Can-Hurt-People\n\n8. Ibid.\n\n28: Thinking probabilistically\n\n1. Tetlock quoted in Dan Gardner, Future Babble (Virgin Books, 2011), p. 24\n\n2. Ibid., p. 25\n\n3. Isaiah Berlin, The Hedgehog and the Fox: An Essay on Tolstoy’s View of History (Princeton Press, 1953)\n\n29: Making beliefs pay rent\n\n1. Eliezer Yudkowsky, ‘Disputing definitions’, LessWrong sequences, 2008 https://www.readthesequences.com/Disputing-Definitions\n\n2. Eliezer Yudkowsky, ‘Making beliefs pay rent (in anticipated experiences)’, LessWrong sequences, 2008 https://www.readthesequences.com/Making-Beliefs-Pay-Rent-In-Anticipated-Experiences\n\n30: Noticing confusion\n\n1. Eliezer Yudkowsky, ‘Fake explanations’, LessWrong sequences, 2008 https://www.readthesequences.com/Fake-Explanations\n\n31: The importance of saying ‘Oops’\n\n1. Eliezer Yudkowsky, ‘The importance of saying “Oops”’, LessWrong sequences, 2008 https://www.readthesequences.com/TheImportanceOfSayingOops\n\n2. Ibid.\n\n32: The semi-death of LessWrong\n\n1. Riciessa, ‘LessWrong analytics, February 2009 to January 2017’ https://www.lesswrong.com/posts/SWNn53RryQgTzT7NQ/lesswrong-analytics-february-2009-to-january-2017\n\n2. Scott Alexander, ‘A History of the Rationalist community’, Reddit, 2017 https://www.reddit.com/r/slatestarcodex/comments/6tt3gy/a\\_history\\_of\\_the\\_rationality\\_community/\n\n3. Scott Alexander, ‘Mapmaker, mapmaker, make me a map’, 2014 https://slatestarcodex.com/2014/09/05/mapmaker-mapmaker-make-me-a-map/\n\n33: The IRL community\n\n1. Zvi Mowshowitz, ‘The thing and the symbolic representation of the thing’, 2015 https://thezvi.wordpress.com/2015/06/30/the-thing-and-the-symbolic-representation-of-the-thing/\n\n2. Sara Constantin, ‘Lessons learned from MetaMed’, 2015 https://docs.google.com/document/d/1HzZd3jsG9YMU4DqHc62mMqKWtRer\\_KqFpiaeN-Q1rlI/edit\n\n3. 2016 LessWrong diaspora survey results http://www.jdpressman.com/public/lwsurvey2016/Survey\\_554193\\_LessWrong\\_Diaspora\\_2016\\_Survey%282%29.pdf\n\n4. SSC survey results 2018 http://slatestarcodex.com/2018/01/03/ssc-survey-results-2018/\n\n34: Are they a cult?\n\n1. Unknown author, ‘Our phyg is not exclusive enough’, LessWrong, 2012 https://www.lesswrong.com/posts/hxGEKxaHZEKT4fpms/our-phyg-is-not-exclusive-enough\n\n2. reddragdiva.tumblr.com, https://reddragdiva.tumblr.com/post/172165021858/some-charities-are-more-effective-than-others-and\n\n3. Eliezer Yudkowsky, ‘Every cause wants to be a cult’, LessWrong, 2007 https://www.lesswrong.com/posts/yEjaj7PWacno5EvWa/every-cause-wants-to-be-a-cult\n\n4. ‘Transcription of Eliezer’s January 2010 video Q&A’, LessWrong, 2010 https://www.lesswrong.com/posts/YduZEfz8usGbJXN4x/transcription-of-eliezer-s-january-2010-video-q-and-a\n\n5. MIRI Independent Auditors Report for 2016, https://intelligence.org/wp-content/uploads/2012/06/Independent-Auditors-Report-for-2016.pdf\n\n6. Scott Alexander, ‘The Noncentral Fallacy: The worst argument in the world’, LessWrong, 2012 https://www.lesswrong.com/posts/yCWPkLi8wJvewPbEp/the-noncentral-fallacy-the-worst-argument-in-the-world\n\n7. SSC 2018 survey results http://slatestarcodex.com/2018/01/03/ssc-survey-results-2018/\n\n8. LessWrong 2014 survey results http://lesswrong.com/lw/lhg/2014\\_survey\\_results/\n\n9. Elizabeth Sheff, ‘How many polyamorists are there in the US?’ Psychology Today, 9 May 2014 https://www.psychologytoday.com/us/blog/the-polyamorists-next-door/201405/how-many-polyamorists-are-there-in-the-us\n\n10. M.L. Haupert et al., ‘Prevalence of experiences with consensual nonmonogamous relationships: Findings from two national samples of single Americans’, Journal of Sex & Marital Therapy, vol. 43, issue 5, 2017 https://www.tandfonline.com/doi/abs/10.1080/0092623X.2016.1178675?journalCode=usmt20\n\n11. Brendan Shucart, ‘Polyamory by the numbers’, Advocate, 1 August 2016 https://www.advocate.com/current-issue/2016/1/08/polyamory-numbers\n\n35: You can’t psychoanalyse your way to the truth\n\n1. Scott Alexander, ‘Is everything a religion?’, 2015 http://slatestarcodex.com/2015/03/25/is-everything-a-religion/\n\n2. John Horgan, ‘The consciousness conundrum’, IEEE Spectrum, 1 June 2008 https://spectrum.ieee.org/biomedical/imaging/the-consciousness-conundrum\n\n3. John Horgan, ‘AI visionary Eliezer Yudkowsky on the Singularity, Bayesian brains and closet goblins’, Scientific American, 1 March 2016 https://blogs.scientificamerican.com/cross-check/ai-visionary-eliezer-yudkowsky-on-the-singularity-bayesian-brains-and-closet-goblins/\n\n36: Feminism\n\n1. Scott Aaronson, comment #171 under blog post ‘Walter Lewin’, Shtetl-Optimised, 2015 https://www.scottaaronson.com/blog/?p=2091#comment-326664\n\n2. Amanda Marcotte, ‘MIT professor explains: The real oppression is having to learn to talk to women’, RawStory, 2014 https://www.rawstory.com/2014/12/mit-professor-explains-the-real-oppression-is-having-to-learn-to-talk-to-women/\n\n3. Scott Alexander, ‘Untitled’, 2015 http://slatestarcodex.com/2015/01/01/untitled/\n\n4. Russell Clark and Elaine Hatfield, ‘Gender differences in receptivity to sexual offers’, Journal of Psychology & Human Sexuality, vol. 2(1), 1989 https://www.tandfonline.com/doi/abs/10.1300/J056v02n01\\_04\n\n5. Vashte Galpin, ‘Women in computing around the world: An initial comparison of international statistics’, ACM SIGCSE Bulletin, vol. 34(2), June 2002, pp. 94–100 http://homepages.inf.ed.ac.uk/vgalpin1/ps/Gal02a.pdf\n\n6. Elizabeth Weise, ‘Tech: Where the women and minorities aren’t’, USA Today, 15 August 2014 https://eu.usatoday.com/story/tech/2014/05/29/silicon-valley-tech-diversity-hiring-women-minorities/9735713/\n\n7. NCWIT, ‘Girls in IT: The facts infographic’, National Center for Women and IT, 30 November 2012 https://www.ncwit.org/infographic/3435\n\n8. Beth Gardiner, ‘Computer coding: It’s not just for boys’, New York Times, 7 March 2013 https://www.nytimes.com/2013/03/08/technology/computer-coding-its-not-just-for-boys.html\n\n9. Kathy A. Krendl, Mary C. Broihier and Cynthia Fleetwood, ‘Children and computers: Do sexrelated differences persist?’, Journal of Communication, vol. 39(3), 1 September 1989, pp. 85–93 https://doi.org/10.1111/j.1460-2466.1989.tb01042.x\n\n10. Lily Shashaani, ‘Gender differences in computer attitudes and use among college students’, Journal of Educational Computing Research, vol. 16, issue 1, 1 January 1997 http://journals.sagepub.com/doi/abs/10.2190/Y8U7-AMMA-WQUT-R512?journalCode=jeca\n\n11. Richard A. Lippa, ‘Gender differences in personality and interests: When, where, and why?’, Social and Personality Psychology Compass, vol. 4, issue 11, 20 October 2010 https://doi.org/10.1111/j.1751-9004.2010.00320.x\n\n12. Scott Alexander, ‘Contra Grant on exaggerated differences’, 2017 http://slatestarcodex.com/2017/08/07/contra-grant-on-exaggerated-differences/\n\n13. ‘The state of medical education and practice in the UK’, General Medical Council, 2016 https://www.gmc-uk.org/-/media/documents/SOMEP\\_2016\\_Full\\_Report\\_Lo\\_Res.pdf\\_68139324.pdf\n\n14. ‘Higher Education Student Statistics: UK, 2016/17 – Subjects studied’, HESA, 2018 https://www.hesa.ac.uk/news/11-01-2018/sfr247-higher-education-student-statistics/subjects\n\n15. ‘Association of American Medical Colleges 2015 Report on Residents’, AAMC, 2015 https://www.aamc.org/data/workforce/reports/458766/2-2-chart.html\n\n16. ‘The state of medical education and practice in the UK’, General Medical Council, 2017 https://www.gmc-uk.org/-/media/about/somep-2017/somep-2017-final-full.pdf?la=en#=B6AD13C9D672F7FCD927498A3F50BB0A2A4286F2\n\n17. Scott Alexander, ‘Untitled’, 2015 http://slatestarcodex.com/2015/01/01/untitled/\n\n18. ‘Full leaked Googlers’ conversations regarding the Google memo’, reddit.com/r/KotakuInAction http://archive.is/wUBb5#selection-2283.0-2301.9\n\n19. Emily Gorcenski, ‘Will this make people afraid to share their thoughts? Yes. Shitty people should be afraid to share their fascist thoughts.’ Twitter https://twitter.com/EmilyGorcenski/status/893973537941327876\n\n20. James Damore, ‘The document that got me fired from Google’, 2017 https://firedfortruth.com/\n\n21. Paul Lewis, ‘“I see things differently”: James Damore on his autism and the Google memo’, Guardian, 17 November 2017 https://www.theguardian.com/technology/2017/nov/16/james-damore-google-memo-interview-autism-regrets\n\n37: The Neoreactionaries\n\n1. 2016 LessWrong diaspora survey results http://www.jdpressman.com/public/lwsurvey2016/Survey\\_554193\\_LessWrong\\_Diaspora\\_2016\\_Survey%282%29.pdf\n\n2. LessWrong diaspora survey 2016 http://www.jdpressman.com/public/lwsurvey2016/analysis/general\\_report.html\n\n3. Scott Alexander, ‘You’re probably wondering why I’ve called you here today’, 2013 http://slatestarcodex.com/2013/02/12/youre-probably-wondering-why-ive-called-you-here-today/\n\n4. Scott Alexander, ‘SSC Endorses Clinton, Johnson, or Stein’, 2016 http://slatestarcodex.com/2016/09/28/ssc-endorses-clinton-johnson-or-stein/\n\n5. SSC survey results 2018 http://slatestarcodex.com/2018/01/03/ssc-survey-results-2018/\n\n38: The Effective Altruists\n\n1. Peter Singer, ‘Famine, affluence, and morality’, Philosophy and Public Affairs, vol. 1(1), Spring 1972, pp. 229–43 (rev. edn) https://www.utilitarian.net/singer/by/1972----.htm\n\n2. Benjamin Todd, ‘Earning to give’, 80,000 Hours, 2017 https://80000hours.org/articles/earning-to-give/\n\n3. ‘What we can achieve’, Giving What We Can https://www.givingwhatwecan.org/get-involved/what-we-can-achieve/\n\n4. Giving USA 2018: The Annual Report on Philanthropy for the Year 2017 https://givingusa.org/giving-usa-2018-americans-gave-410-02-billion-to-charity-in-2017-crossing-the-400-billion-mark-for-the-first-time/\n\n5. The 2014 survey of Effective Altruists, Centre for Effective Altruism http://effective-altruism.com/ea/gb/the\\_2014\\_survey\\_of\\_effective\\_altruists\\_results/\n\n6. EA 2017 survey, Centre for Effective Altruism https://rtcharity.org/tag/ea-survey-2017/\n\n7. LessWrong diaspora survey 2016 http://www.jdpressman.com/public/lwsurvey2016/Survey\\_554193\\_LessWrong\\_Diaspora\\_2016\\_Survey%282%29.pdf\n\n8. Scott Alexander, ‘Nobody is perfect, everything is commensurable’, 2014 http://slatestarcodex.com/2014/12/19/nobody-is-perfect-everything-is-commensurable/\n\n9. ‘About chickens’, Compassion in World Farming, 2017 https://www.ciwf.org.uk/farm-animals/chickens/\n\n10. Scott Alexander, ‘Vegetarianism for meat-eaters’, 2015 http://slatestarcodex.com/2015/09/23/vegetarianism-for-meat-eaters/\n\n11. ‘Is there suffering in fundamental physics?’, Foundational Research Institute http://reducing-suffering.org/is-there-suffering-in-fundamental-physics/\n\n12. ‘Impossible Foods – R&D Investment’, OpenPhil 2016 https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare/impossible-foods\n\n13. Katie Strick, ‘This is what the “bleeding” vegan burger at Mildreds is really like’, London Evening Standard, 21 February 2018 https://www.standard.co.uk/go/london/restaurants/this-is-what-the-bleeding-vegan-burger-at-mildreds-is-really-like-a3772061.html\n\n39: EA and AI\n\n1. Benjamin Todd, ‘Why, despite global progress, humanity is probably facing its most dangerous time ever’, 80,000 Hours https://80000hours.org/articles/extinction-risk/\n\n2. Holden Karnofsky, ‘Potential risks from advanced artificial intelligence: The philanthropic opportunity’, OpenPhil https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#Tractability\n\n3. ‘Machine Intelligence Research Institute – general support’, OpenPhil 2016 https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support\n\n4. ‘Our progress in 2017 and plans for 2018’, OpenPhil https://www.openphilanthropy.org/blog/our-progress-2017-and-plans-2018\n\n5. reddragdiva.tumblr.com, https://reddragdiva.tumblr.com/post/172165021858/some-charities-are-more-effective-than-others-and\n\n6. Dylan Matthews, ‘I spent a weekend at Google talking with nerds about charity. I came away . . . worried’, Vox, 10 August 2015 https://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai\n\n7. Ben Kuhn, ‘Some stories about comparative advantage’, December 2014 https://www.benkuhn.net/advantage\n\n8. Edward Miguel and Michael Kremer, ‘Worms: Identifying impacts on education and health in the presence of treatment externalities’, Econometrica, vol. 72(1), January 2004, pp. 159–217\n\n9. D.C. Taylor-Robinson, N. Maayan, K. Soares-Weiser, S. Donegan and P. Garner, ‘Deworming drugs for soil-transmitted intestinal worms in children: Effects on nutritional indicators, haemoglobin, and school performance’, Cochrane Database of Systematic Reviews, 23 July 2015 (CD000371)\n\n10. Philip Oltermann, ‘Greenpeace loses £3m in currency speculation’, Guardian, 16 June 2014 https://www.theguardian.com/environment/2014/jun/16/greenpeace-loses-3m-pounds-currency-speculation\n\n11. ‘Introducing OpenAI’, OpenAI.com 2015 https://blog.openai.com/introducing-openai/\n\n40: What are they doing to stop the AI apocalypse?\n\n1. D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman and D. Mane, ‘Concrete problems in AI safety’, technical report, 25 July 2016 arXiv:1606.06565v2 (cs.AI)\n\n2. Paul Christiano, et al., ‘Deep reinforcement learning from human preferences’, OpenAI https://blog.openai.com/deep-reinforcement-learning-from-human-preferences/\n\n3. Paul Christiano, ‘Capability amplification’, Medium https://ai-alignment.com/policy-amplification-6a70cbee4f34\n\n41: The internal double crux\n\n1. Duncan Sabien, ‘Double crux – A strategy for resolving disagreement’, LessWrong, 2017 https://www.lesswrong.com/posts/exa5kmvopeRyfJgCy/double-crux-a-strategy-for-resolving-disagreement\n\n42: Life, the universe and everything\n\n1. Bostrom and Müller, ‘Future progress in artificial intelligence’ https://nickbostrom.com/papers/survey.pdf\n\n2. Bostrom, Superintelligence, p. 115\n\n3. ‘Preparing for the Future of Artificial Intelligence’, Executive Office of the President, National Science and Technology Council Committee on Technology, October 2016 https://obamawhitehouse.archives.gov/sites/default/files/whitehouse\\_files/microsites/ostp/NSTC/preparing\\_for\\_the\\_future\\_of\\_ai.pdf\n\nCopyright\n\nFirst published in Great Britain in 2019 by Weidenfeld & Nicolson\nan imprint of The Orion Publishing Group Ltd\nCarmelite House, 50 Victoria Embankment\nLondon EC4Y 0DZ\n\nAn Hachette UK Company\n\nCopyright © Tom Chivers 2019\n\nThe moral right of Tom Chivers to be identified as the author of this work has been asserted in accordance with the Copyright, Designs and Patents Act of 1988.\n\nAll rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical, photocopying, recording, or otherwise, without the prior permission of both the copyright owner and the above publisher of this book.\n\nEvery effort has been made to fulfil requirements with regard to reproducing copyright material. The author and publisher will be glad to rectify any omissions at the earliest opportunity.\n\nA CIP catalogue record for this book is available from the British Library.\n\nISBN (Hardback) 978-1-4746-0877-0\nISBN (Trade paperback) 978-1-4746-0878-7\nISBN (eBook) 978-1-4746-0880-0\n\nwww.orionbooks.co.uk\n\n \n\n[]\n\nTable of Contents\n\nDedication\n\nTitle Page\n\nContents\n\nIntroduction: ‘I don’t expect your children to die of old age’\n\nPART ONE: INTRODUCTIONS\n\n 1: Introducing the Rationalists\n\n 2: The cosmic endowment\n\nPART TWO: THE PAPERCLIP APOCALYPSE\n\n 3: Introducing AI\n\n 4: A history of AI\n\n 5: When will it happen?\n\n 6: Existential risk\n\n 7: The cryptographic rocket probe, and why you have to get it right first time\n\n 8: Paperclips and Mickey Mouse\n\n 9: You can be intelligent, and still want to do stupid things\n\n 10: If you want to achieve your goals, not dying is a good start\n\n 11: If I stop caring about chess, that won’t help me win any chess games, now will it?\n\n 12: The brief window of being human-level\n\n 13: Getting better all the time\n\n 14: ‘FOOOOOM’\n\n 15: But can’t we just keep it in a box?\n\n 16: Dreamed of in your philosophy\n\n 17: ‘It’s like 100 per cent confident this is an ostrich’\n\nPART THREE: THE WAYS OF BAYES\n\n 18: What is rationality?\n\n 19: Bayes’ theorem and optimisation\n\n 20: Utilitarianism: shut up and multiply\n\nPART FOUR: BIASES\n\n 21: What is a ‘bias’?\n\n 22: The availability heuristic\n\n 23: The conjunction fallacy\n\n 24: The planning fallacy\n\n 25: Scope insensitivity\n\n 26: Motivated scepticism, motivated stopping and motivated continuation\n\n 27: A few others, and the most important one\n\nPART FIVE: RAISING THE SANITY WATERLINE\n\n 28: Thinking probabilistically\n\n 29: Making beliefs pay rent\n\n 30: Noticing confusion\n\n 31: The importance of saying ‘Oops’\n\nPART SIX: DECLINE AND DIASPORA\n\n 32: The semi-death of LessWrong\n\n 33: The IRL community\n\nPART SEVEN: DARK SIDES\n\n 34: Are they a cult?\n\n 35: You can’t psychoanalyse your way to the truth\n\n 36: Feminism\n\n 37: The Neoreactionaries\n\nPART EIGHT: DOING GOOD BETTER\n\n 38: The Effective Altruists\n\n 39: EA and AI\n\nPART NINE: THE BASE RATE OF THE APOCALYPSE\n\n 40: What are they doing to stop the AI apocalypse?\n\n 41: The internal double crux\n\n 42: Life, the universe and everything\n\nAcknowledgements\n\nNotes\n\nCopyright", "date_published": "2019-06-13T00:00:00Z", "authors": ["Tom Chivers"], "summaries": [], "initial_source": "ebook", "source_filetype": "epub"}
{"id": "e397a5bf59e74003764fbb4a5f5689ac", "title": "Life 3.0: Being Human in the Age of Artificial Intelligence (2017, Alfred A. Knopf)", "url": "https://www.goodreads.com/book/show/34272565-life-3-0", "source": "special_docs", "source_type": "book", "text": "[]\n\nAlso by Max Tegmark\n\nOur Mathematical Universe\n\n[Life 3.0 Being Human in the Age of Artificial Intelligence Max Tegmark Alfred A. Knopf New York 2017][Life 3.0 Being Human in the Age of Artificial Intelligence Max Tegmark Alfred A. Knopf New York 2017]\n\nThis Is a Borzoi Book Published by Alfred A. Knopf\n\nCopyright © 2017 by Max Tegmark\n\nAll rights reserved. Published in the United States by Alfred A. Knopf, a division of Penguin Random House LLC, New York, and distributed in Canada by Random House of Canada, a division of Penguin Random House Canada Limited, Toronto.\n\nwww.aaknopf.com\n\nKnopf, Borzoi Books and the colophon are registered trademarks of Penguin Random House LLC.\n\nLibrary of Congress Cataloging-in-Publication Data\n\nNames: Tegmark, Max, author.\n\nTitle: Life 3.0 : being human in the age of artificial intelligence / by Max Tegmark.\n\nOther titles: Life three point zero\n\nDescription: New York : Alfred A. Knopf, 2017. | “This is a Borzoi Book published by Alfred A. Knopf.” | Includes bibliographical references and index.\n\nIdentifiers: LCCN 2017006248 (print) | LCCN 2017022912 (ebook) | ISBN 9781101946596 (hardcover) | ISBN 9781101946602 (ebook)\n\nSubjects: LCSH: Artificial intelligence—Philosophy. | Artificial intelligence—Social aspects. | Automation—Social aspects. | Artificial intelligence—Moral and ethical aspects. | Automation—Moral and ethical aspects. | Artificial intelligence—Philosophy. | Technological forecasting. | BISAC: TECHNOLOGY & ENGINEERING / Robotics. | SCIENCE / Experiments & Projects. | TECHNOLOGY & ENGINEERING / Inventions.\n\nClassification: LCC Q334.7 (ebook) | LCC Q334.7 .T44 2017 (print) | DDC 006.301—dc23\n\nLC record available at https://lccn.loc.gov/2017006248\n\nEbook ISBN 9781101946602\n\nCover art by Suvadip Das; (man) based on Netfalls Remy Musser/Shutterstock\n\nCover design by John Vorhees\n\nv4.1\n\nep\n\nContents\n\nCover\n\nAlso by Max Tegmark\n\nTitle Page\n\nCopyright\n\nDedication\n\nAcknowledgments\n\nPrelude: The Tale of the Omega Team\n\n1 Welcome to the Most Important Conversation of Our Time\n\nA Brief History of Complexity\n\nThe Three Stages of Life\n\nControversies\n\nMisconceptions\n\nThe Road Ahead\n\n2 Matter Turns Intelligent\n\nWhat Is Intelligence?\n\nWhat Is Memory?\n\nWhat Is Computation?\n\nWhat Is Learning?\n\n3 The Near Future: Breakthroughs, Bugs, Laws, Weapons and Jobs\n\nBreakthroughs\n\nBugs vs. Robust AI\n\nLaws\n\nWeapons\n\nJobs and Wages\n\nHuman-Level Intelligence?\n\n4 Intelligence Explosion?\n\nTotalitarianism\n\nPrometheus Takes Over the World\n\nSlow Takeoff and Multipolar Scenarios\n\nCyborgs and Uploads\n\nWhat Will Actually Happen?\n\n5 Aftermath: The Next 10,000 Years\n\nLibertarian Utopia\n\nBenevolent Dictator\n\nEgalitarian Utopia\n\nGatekeeper\n\nProtector God\n\nEnslaved God\n\nConquerors\n\nDescendants\n\nZookeeper\n\n1984\n\nReversion\n\nSelf-Destruction\n\nWhat Do You Want?\n\n6 Our Cosmic Endowment: The Next Billion Years and Beyond\n\nMaking the Most of Your Resources\n\nGaining Resources Through Cosmic Settlement\n\nCosmic Hierarchies\n\nOutlook\n\n7 Goals\n\nPhysics: The Origin of Goals\n\nBiology: The Evolution of Goals\n\nPsychology: The Pursuit of and Rebellion Against Goals\n\nEngineering: Outsourcing Goals\n\nFriendly AI: Aligning Goals\n\nEthics: Choosing Goals\n\nUltimate Goals?\n\n8 Consciousness\n\nWho Cares?\n\nWhat Is Consciousness?\n\nWhat’s the Problem?\n\nIs Consciousness Beyond Science?\n\nExperimental Clues About Consciousness\n\nTheories of Consciousness\n\nControversies of Consciousness\n\nHow Might AI Consciousness Feel?\n\nMeaning\n\nEpilogue: The Tale of the FLI Team\n\nNotes\n\nTo the FLI team,\n\nwho made everything possible\n\nAcknowledgments\n\nI’m truly grateful to everyone who has encouraged and helped me write this book, including\n\nmy family, friends, teachers, colleagues and collaborators for support and inspiration over the years,\n\nMom for kindling my curiosity about consciousness and meaning,\n\nDad for the fighting spirit to make the world a better place,\n\nmy sons, Philip and Alexander, for demonstrating the wonders of human-level intelligence emerging,\n\nall the science and technology enthusiasts around the world who’ve contacted me over the years with questions, comments and encouragement to pursue and publish my ideas,\n\nmy agent, John Brockman, for twisting my arm until I agreed to write this book,\n\nBob Penna, Jesse Thaler and Jeremy England for helpful discussions about quasars, sphalerons and thermodynamics, respectively,\n\nthose who gave me feedback on parts of the manuscript, including Mom, my brother Per, Luisa Bahet, Rob Bensinger, Katerina Bergström, Erik Brynjolfsson, Daniela Chita, David Chalmers, Nima Deghani, Henry Lin, Elin Malmsköld, Toby Ord, Jeremy Owen, Lucas Perry, Anthony Romero, Nate Soares and Jaan Tallinn,\n\nthe superheroes who commented on drafts of the entire book, namely Meia, Dad, Anthony Aguirre, Paul Almond, Matthew Graves, Phillip Helbig, Richard Mallah, David Marble, Howard Messing, Luiño Seoane, Marin Soljačić, my editor Dan Frank and, most of all,\n\nMeia, my beloved muse and fellow traveler, for her eternal encouragement, support and inspiration, without which this book wouldn’t exist.\n\nLIFE 3.0\n\n[] []\n\nPrelude\n\nThe Tale of the Omega Team\n\nThe Omega Team was the soul of the company. Whereas the rest of the enterprise brought in the money to keep things going, by various commercial applications of narrow AI, the Omega Team pushed ahead in their quest for what had always been the CEO’s dream: building general artificial intelligence. Most other employees viewed “the Omegas,” as they affectionately called them, as a bunch of pie-in-the-sky dreamers, perpetually decades away from their goal. They happily indulged them, however, because they liked the prestige that the cutting-edge work of the Omegas gave their company, and they also appreciated the improved algorithms that the Omegas occasionally gave them.\n\nWhat they didn’t realize was that the Omegas had carefully crafted their image to hide a secret: they were extremely close to pulling off the most audacious plan in human history. Their charismatic CEO had handpicked them not only for being brilliant researchers, but also for ambition, idealism and a strong commitment to helping humanity. He reminded them that their plan was extremely dangerous, and that if powerful governments found out, they would do virtually anything—including kidnapping—to shut them down or, preferably, to steal their code. But they were all in, 100%, for much the same reason that many of the world’s top physicists joined the Manhattan Project to develop nuclear weapons: they were convinced that if they didn’t do it first, someone less idealistic would.\n\nThe AI they had built, nicknamed Prometheus, kept getting more capable. Although its cognitive abilities still lagged far behind those of humans in many areas, for example, social skills, the Omegas had pushed hard to make it extraordinary at one particular task: programming AI systems. They’d deliberately chosen this strategy because they had bought the intelligence explosion argument made by the British mathematician Irving Good back in 1965: “Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.”\n\nThey figured that if they could get this recursive self-improvement going, the machine would soon get smart enough that it could also teach itself all other human skills that would be useful.\n\nThe First Millions\n\nIt was nine o’clock on a Friday morning when they decided to launch. Prometheus was humming away in its custom-built computer cluster, which resided in long rows of racks in a vast, access-controlled, air-conditioned room. For security reasons, it was completely disconnected from the internet, but it contained a local copy of much of the web (Wikipedia, the Library of Congress, Twitter, a selection from YouTube, much of Facebook, etc.) to use as its training data to learn from.^(\\*) They’d picked this start time to work undisturbed: their families and friends thought they were on a weekend corporate retreat. The kitchenette was loaded with microwaveable food and energy drinks, and they were ready to roll.\n\nWhen they launched, Prometheus was slightly worse than them at programming AI systems, but made up for this by being vastly faster, spending the equivalent of thousands of person-years chugging away at the problem while they chugged a Red Bull. By 10 a.m., it had completed the first redesign of itself, v2.0, which was slightly better but still subhuman. By the time Prometheus 5.0 launched at 2 p.m., however, the Omegas were awestruck: it had blown their performance benchmarks out of the water, and the rate of progress seemed to be accelerating. By nightfall, they decided to deploy Prometheus 10.0 to start phase 2 of their plan: making money.\n\nTheir first target was MTurk, the Amazon Mechanical Turk. After its launch in 2005 as a crowdsourcing internet marketplace, it had grown rapidly, with tens of thousands of people around the world anonymously competing around the clock to perform highly structured chores called HITs, “Human Intelligence Tasks.” These tasks ranged from transcribing audio recordings to classifying images and writing descriptions of web pages, and all had one thing in common: if you did them well, nobody would know that you were an AI. Prometheus 10.0 was able to do about half of the task categories acceptably well. For each such task category, the Omegas had Prometheus design a lean custom-built narrow AI software module that could do precisely such tasks and nothing else. They then uploaded this module to Amazon Web Services, a cloud-computing platform that could run on as many virtual machines as they rented. For every dollar they paid to Amazon’s cloud-computing division, they earned more than two dollars from Amazon’s MTurk division. Little did Amazon suspect that such an amazing arbitrage opportunity existed within their own company!\n\nTo cover their tracks, they had discreetly created thousands of MTurk accounts during the preceding months in the names of fictitious people, and the Prometheus-built modules now assumed their identities. The MTurk customers typically paid after about eight hours, at which point the Omegas reinvested the money in more cloud-computing time, using still better task modules made by the latest version of the ever-improving Prometheus. Because they were able to double their money every eight hours, they soon started saturating MTurk’s task supply, and found that they couldn’t earn more than about a million dollars per day without drawing unwanted attention to themselves. But this was more than enough to fund their next step, eliminating any need for awkward cash requests to the chief financial officer.\n\nDangerous Games\n\nAside from their AI breakthroughs, one of the recent projects that the Omegas had had the most fun with was planning how to make money as rapidly as possible after Prometheus’ launch. Essentially the whole digital economy was up for grabs, but was it better to start by making computer games, music, movies or software, to write books or articles, to trade on the stock market or to make inventions and sell them? It simply boiled down to maximizing their rate of return on investment, but normal investment strategies were a slow-motion parody of what they could do: whereas a normal investor might be pleased with a 9% return per year, their MTurk investments had yielded 9% per hour, generating eight times more money each day. So now that they’d saturated MTurk, what next?\n\nTheir first thought had been to make a killing on the stock market—after all, pretty much all of them had at some point declined a lucrative job offer to develop AI for hedge funds, which were investing heavily in exactly this idea. Some remembered that this was how the AI made its first millions in the movie Transcendence. But the new regulations on derivatives after last year’s crash had limited their options. They soon realized that, even though they could get much better returns than other investors, they’d be unlikely to get returns anywhere close to what they could get from selling their own products. When you have the world’s first superintelligent AI working for you, you’re better off investing in your own companies than in those of others! Although there might be occasional exceptions (such as using Prometheus’ superhuman hacking abilities to get inside information and then buy call options on stocks about to surge), the Omegas felt that this wasn’t worth the unwanted attention it might draw.\n\nWhen they shifted their focus toward products that they could develop and sell, computer games first seemed the obvious top choice. Prometheus could rapidly become extremely skilled at designing appealing games, easily handling the coding, graphic design, ray tracing of images and all other tasks needed to produce a final ready-to-ship product. Moreover, after digesting all the web’s data on people’s preferences, it would know exactly what each category of gamer liked, and could develop a superhuman ability to optimize a game for sales revenue. The Elder Scrolls V: Skyrim, a game on which many of the Omegas had wasted more hours than they cared to admit, had grossed over $400 million during its first week back in 2011, and they were confident that Prometheus could make something at least this addictive in twenty-four hours using $1 million of cloud-computing resources. They could then sell it online and use Prometheus to impersonate humans talking up the game in the blogosphere. If this brought in $250 million in a week, they would have doubled their investment eight times in eight days, giving a return of 3% per hour—slightly worse than their MTurk start, but much more sustainable. By developing a suite of other games each day, they figured they’d be able to earn $10 billion before long, without coming close to saturating the games market.\n\nBut a cybersecurity specialist on their team talked them out of this game plan. She pointed out that it would pose an unacceptable risk of Prometheus breaking out and seizing control of its own destiny. Because they weren’t sure how its goals would evolve during its recursive self-improvement, they had decided to play it safe and go to great lengths to keep Prometheus confined (“boxed”) in ways such that it couldn’t escape onto the internet. For the main Prometheus engine running in their server room, they used physical confinement: there simply was no internet connection, and the only output from Prometheus was in the form of messages and documents it sent to a computer that the Omegas controlled.\n\nOn an internet-connected computer, on the other hand, running any complicated program created by Prometheus was a risky proposition: since the Omegas had no way of fully understanding what it would do, they had no way of knowing that it wouldn’t, say, start virally spreading itself online. When testing the software that Prometheus had written for MTurk tasks, the Omegas guarded against this by running it only inside a virtual machine. This is a program that simulates a computer: for example, many Mac users buy virtual machine software that lets them run Windows programs by tricking them into thinking that they’re actually in a Windows machine. The Omegas had created their own virtual machine, nicknamed Pandora’s Box, which simulated an ultrasimplified machine stripped of all bells and whistles that we usually associate with computers: no keyboard, no monitor, no loudspeakers, no internet connectivity, nothing. For the MTurk audio transcriptions, the Omegas set things up so that all that could go into Pandora’s Box was one single audio file and all that could come out was one single text document—the transcription. These laws of the box were to the software inside like the laws of physics are to us inside our Universe: the software couldn’t travel out of the box any more than we can travel faster than the speed of light, no matter how smart we are. Except for that single input and output, the software inside Pandora’s Box was effectively trapped in a parallel universe with its own computational rules. The Omegas had such strong breakout paranoia that they added boxing in time as well, limiting the life span of untrusted code. For example, each time the boxed transcription software had finished transcribing one audio file, the entire memory content of Pandora’s Box was automatically erased and the program was reinstalled from scratch. This way, when it started the next transcription task, it had no knowledge of what had previously happened, and thus no ability to learn over time.\n\nWhen the Omegas used the Amazon cloud for their MTurk project, they were able to put all their Prometheus-created task modules into such virtual boxes in the cloud, because the MTurk input and output was so simple. But this wouldn’t work for graphics-heavy computer games, which couldn’t be boxed in because they needed full access to all the hardware of the gamer’s computer. Moreover, they didn’t want to risk that some computer-savvy user would analyze their game code, discover Pandora’s Box and decide to investigate what was inside. The breakout risk put not merely the games market off-limits for now, but also the massively lucrative market for other software, with hundreds of billions of dollars up for grabs.\n\nThe First Billions\n\nThe Omegas had narrowed their search to products that were highly valuable, purely digital (avoiding slow manufacturing) and easily understandable (for example, text or movies they knew wouldn’t pose a breakout risk). In the end, they had decided to launch a media company, starting with animated entertainment. The website, the marketing plan and the press releases had all been ready to go even before Prometheus became superintelligent—all that was missing was content.\n\nAlthough Prometheus was astonishingly capable by Sunday morning, steadily raking in money from MTurk, its intellectual abilities were still rather narrow: Prometheus had been deliberately optimized to design AI systems and write software that performed rather mind-numbing MTurk tasks. It was, for example, bad at making movies—bad not for any profound reason, but for the same reason that James Cameron was bad at making movies when he was born: this is a skill that takes time to learn. Like a human child, Prometheus could learn whatever it wanted from the data it had access to. Whereas James Cameron had taken years to learn to read and write, Prometheus had gotten that taken care of on Friday, when it also found time to read all of Wikipedia and a few million books. Making movies was harder. Writing a screenplay that humans found interesting was just as hard as writing a book, requiring a detailed understanding of human society and what humans found entertaining. Turning the screenplay into a final video file required massive amounts of ray tracing of simulated actors and the complex scenes they moved through, simulated voices, the production of compelling musical soundtracks and so on. As of Sunday morning, Prometheus could watch a two-hour movie in about a minute, which included reading any book it was based on and all online reviews and ratings. The Omegas noticed that after Prometheus had binge-watched a few hundred films, it started to get quite good at predicting what sort of reviews a movie would get and how it would appeal to different audiences. Indeed, it learned to write its own movie reviews in a way they felt demonstrated real insight, commenting on everything from the plots and the acting to technical details such as lighting and camera angles. They took this to mean that when Prometheus made its own films, it would know what success meant.\n\nThe Omegas instructed Prometheus to focus on making animation at first, to avoid embarrassing questions about who the simulated actors were. On Sunday night, they capped their wild weekend by arming themselves with beer and microwave popcorn, dimming the lights and watching Prometheus’ debut movie. It was an animated fantasy-comedy in the spirit of Disney’s Frozen, and the ray tracing had been performed by boxed Prometheus-built code in the Amazon cloud, using up most of the day’s $1 million MTurk profit. As the movie began, they found it both fascinating and frightening that it had been created by a machine without human guidance. Before long, however, they were laughing at the gags and holding their breath during the dramatic moments. Some of them even teared up a bit at the emotional ending, so engrossed in this fictional reality that they forgot all about its creator.\n\nThe Omegas scheduled their website launch for Friday, giving Prometheus time to produce more content and themselves time to do the things they didn’t trust Prometheus with: buying ads and starting to recruit employees for the shell companies they’d set up during the past months. To cover their tracks, the official cover story would be that their media company (which had no public association with the Omegas) bought most of its content from independent film producers, typically high-tech startups in low-income regions. These fake suppliers were conveniently located in remote places such as Tiruchchirappalli and Yakutsk, which most curious journalists wouldn’t bother visiting. The only employees they actually hired there worked on marketing and administration, and would tell anyone who asked that their production team was in a different location and didn’t conduct interviews at the moment. To match their cover story, they chose the corporate slogan “Channeling the world’s creative talent,” and branded their company as being disruptively different by using cutting-edge technology to empower creative people, especially in the developing world.\n\nWhen Friday came around and curious visitors started arriving at their site, they encountered something reminiscent of the online entertainment services Netflix and Hulu but with interesting differences. All the animated series were new ones they’d never heard of. They were rather captivating: most series consisted of forty-five-minute-long episodes with a strong plotline, each ending in a way that left you eager to find out what happened in the next episode. And they were cheaper than the competition. The first episode of each series was free, and you could watch the others for forty-nine cents each, with discounts for the whole series. Initially, there were only three series with three episodes each, but new episodes were added daily, as well as new series catering to different demographics. During the first two weeks of Prometheus, its moviemaking skills improved rapidly, in terms not only of film quality but also of better algorithms for character simulation and ray tracing, which greatly reduced the cloud-computing cost to make each new episode. As a result, the Omegas were able to roll out dozens of new series during the first month, targeting demographics from toddlers to adults, as well as to expand to all major world language markets, making their site remarkably international compared with all competitors. Some commentators were impressed by the fact that it wasn’t merely the soundtracks that were multilingual, but the videos themselves: for example, when a character spoke Italian, the mouth motions matched the Italian words, as did the characteristically Italian hand gestures. Although Prometheus was now perfectly capable of making movies with simulated actors indistinguishable from humans, the Omegas avoided this to not tip their hand. They did, however, launch many series with semi-realistic animated human characters, in genres competing with traditional live-action TV shows and movies.\n\nTheir network turned out to be quite addictive, and enjoyed spectacular viewer growth. Many fans found the characters and plots cleverer and more interesting than even Hollywood’s most expensive big-screen productions, and were delighted that they could watch them much more affordably. Buoyed by aggressive advertising (which the Omegas could afford because of their near-zero production costs), excellent media coverage and rave word-of-mouth reviews, their global revenue had mushroomed to $10 million a day within a month of launch. After two months, they had overtaken Netflix, and after three, they were raking in over $100 million a day, beginning to rival Time Warner, Disney, Comcast and Fox as one of the world’s largest media empires.\n\nTheir sensational success garnered plenty of unwanted attention, including speculation about their having strong AI, but using merely a small fraction of their revenue, the Omegas deployed a fairly successful disinformation campaign. From a glitzy new Manhattan office, their freshly hired spokespeople would elaborate on their cover stories. Plenty of humans were hired as foils, including actual screenwriters around the world to start developing new series, none of whom knew about Prometheus. The confusing international network of subcontractors made it easy for most of their employees to assume that others somewhere else were doing most of the work.\n\nTo make themselves less vulnerable and avoid raising eyebrows with excessive cloud computing, they also hired engineers to start building a series of massive computer facilities around the world, owned by seemingly unaffiliated shell companies. Although they were billed to locals as “green data centers” because they were largely solar-powered, they were in fact mainly focused on computation rather than storage. Prometheus had designed their blueprints down to the most minute detail, using only off-the-shelf hardware and optimizing them to minimize construction time. The people who built and ran these centers had no idea what was computed there: they thought they managed commercial cloud-computing facilities similar to those run by Amazon, Google and Microsoft, and knew only that all sales were managed remotely.\n\nNew Technologies\n\nOver a timescale of months, the business empire controlled by the Omegas started gaining a foothold in ever more areas of the world economy, thanks to superhuman planning by Prometheus. By carefully analyzing the world’s data, it had already during its first week presented the Omegas with a detailed step-by-step growth plan, and it kept improving and refining this plan as its data and computer resources grew. Although Prometheus was far from omniscient, its capabilities were now so far beyond human that the Omegas viewed it as the perfect oracle, dutifully providing brilliant answers and advice in response to all their questions.\n\nPrometheus’ software was now highly optimized to make the most of the rather mediocre human-invented hardware it ran on, and as the Omegas had anticipated, Prometheus identified ways of dramatically improving this hardware. Fearing a breakout, they refused to build robotic construction facilities that Prometheus could control directly. Instead, they hired large numbers of world-class scientists and engineers in multiple locations and fed them internal research reports written by Prometheus, pretending that they were from researchers at the other sites. These reports detailed novel physical effects and manufacturing techniques that their engineers soon tested, understood and mastered. Normal human research and development (R & D) cycles, of course, take years, in large part because they involve many slow cycles of trial and error. The current situation was very different: Prometheus already had the next steps figured out, so the limiting factor was simply how rapidly people could be guided to understand and build the right things. A good teacher can help students learn science much faster than they could have discovered it from scratch on their own, and Prometheus surreptitiously did the same with these researchers. Since Prometheus could accurately predict how long it would take humans to understand and build things given various tools, it developed the quickest possible path forward, giving priority to new tools that could be quickly understood and built and that were useful for developing more advanced tools.\n\nIn the spirit of the maker movement, the engineering teams were encouraged to use their own machines to build their better machines. This self-sufficiency not only saved money, but it also made them less vulnerable to future threats from the outside world. Within two years, they were producing much better computer hardware than the world had ever known. To avoid helping outside competition, they kept this technology under wraps and used it only to upgrade Prometheus.\n\nWhat the world did notice, however, was an astonishing tech boom. Upstart companies around the world were launching revolutionary new products in almost all areas. A South Korean startup launched a new battery that stored twice as much energy as your laptop battery in half the mass, and could be charged in under a minute. A Finnish firm released a cheap solar panel with twice the efficiency of the best competitors. A German company announced a new type of mass-producible wire that was superconducting at room temperature, revolutionizing the energy sector. A Boston-based biotech group announced a Phase II clinical trial of what they claimed was the first effective, side-effect-free weight-loss drug, while rumors suggested that an Indian outfit was already selling something similar on the black market. A California company countered with a Phase II trial of a blockbuster cancer drug, which caused the body’s immune system to identify and attack cells with any of the most common cancerous mutations. Examples just kept on coming, triggering talk of a new golden age for science. Last but not least, robotics companies were cropping up like mushrooms all around the world. None of the bots came close to matching human intelligence, and most of them looked nothing like humans. But they dramatically disrupted the economy, and over the years to come, they gradually replaced most of the workers in manufacturing, transportation, warehousing, retail, construction, mining, agriculture, forestry and fishing.\n\nWhat the world didn’t notice, thanks to the hard work of a crack team of lawyers, was that all these firms were controlled, through a series of intermediaries, by the Omegas. Prometheus was flooding the world’s patent offices with sensational inventions via various proxies, and these inventions gradually led to domination in all areas of technology.\n\nAlthough these disruptive new companies made powerful enemies among their competition, they made even more powerful friends. They were exceptionally profitable, and under slogans such as “Investing in our community,” they spent a significant fraction of these profits hiring people for community projects—often the same people who had been laid off from the companies that were disrupted. They used detailed Prometheus-produced analyses identifying jobs that would be maximally rewarding for the employees and the community for the least cost, tailored to the local circumstances. In regions with high levels of government service, this often focused on community building, culture and caregiving, while in poorer regions it also included launching and maintaining schools, healthcare, day care, elder care, affordable housing, parks and basic infrastructure. Pretty much everywhere, locals agreed that these were things that should have been done long ago. Local politicians got generous donations, and care was taken to make them look good for encouraging these corporate community investments.\n\nGaining Power\n\nThe Omegas had launched a media company not only to finance their early tech ventures, but also for the next step of their audacious plan: taking over the world. Within a year of the first launch, they had added remarkably good news channels to their lineup all over the globe. As opposed to their other channels, these were deliberately designed to lose money, and were pitched as a public service. In fact, their news channels generated no income whatsoever: they carried no ads and were viewable free of charge by anyone with an internet connection. The rest of their media empire was such a cash-generating machine that they could spend far more resources on their news service than any other journalistic effort had done in world history—and it showed. Through aggressive recruitment with highly competitive salaries of journalists and investigative reporters, they brought remarkable talent and findings to the screen. Through a global web service that paid anybody who revealed something newsworthy, from local corruption to a heartwarming event, they were usually the first to break a story. At least that’s what people believed: in fact, they were often first because stories attributed to citizen journalists had been discovered by Prometheus via real-time monitoring of the internet. All these video news sites featured podcasts and print articles as well.\n\nPhase 1 of their news strategy was gaining people’s trust, which they did with great success. Their unprecedented willingness to lose money enabled remarkably diligent regional and local news coverage, where investigative journalists often exposed scandals that truly engaged their viewers. Whenever a country was strongly divided politically and accustomed to partisan news, they would launch one news channel catering to each faction, ostensibly owned by different companies, and gradually gain the trust of that faction. Where possible, they accomplished this using proxies to buy the most influential existing channels, gradually improving them by removing ads and introducing their own content. In countries where censorship and political interference threatened these efforts, they would initially acquiesce in whatever the government required of them to stay in business, with the secret internal slogan “The truth, nothing but the truth, but maybe not the whole truth.” Prometheus usually provided excellent advice in such situations, clarifying which politicians needed to be presented in a good light and which (usually corrupt local ones) could be exposed. Prometheus also provided invaluable recommendations for what strings to pull, whom to bribe and how best to do so.\n\nThis strategy was a smashing success around the world, with the Omega-controlled channels emerging as the most trusted news sources. Even in countries where governments had thus far thwarted their mass adoption, they built a reputation for trustworthiness, and many of their news stories percolated through the grapevine. Competing news executives felt that they were fighting a hopeless battle: how can you possibly make a profit competing with someone with better funding who gives their products away for free? With their viewership dropping, ever more networks decided to sell their news channels—usually to some consortium that later turned out to be controlled by the Omegas.\n\nAbout two years after Prometheus’ launch, when the trust-gaining phase was largely completed, the Omegas launched phase 2 of their news strategy: persuasion. Even before this, astute observers had noticed hints of a political agenda behind the new media: there seemed to be a gentle push toward the center, away from extremism of all sorts. Their plethora of channels catering to different groups still reflected animosity between the United States and Russia, India and Pakistan, different religions, political factions and so on, but the criticism was slightly toned down, usually focusing on concrete issues involving money and power rather than on ad hominem attacks, scaremongering and poorly substantiated rumors. Once phase 2 started in earnest, this push to defuse old conflicts became more apparent, with frequent touching stories about the plight of traditional adversaries mixed with investigative reporting about how many vocal conflict-mongers were driven by personal profit motives.\n\nPolitical commentators noted that, in parallel with damping regional conflicts, there seemed to be a concerted push toward reducing global threats. For example, the risks of nuclear war were suddenly being discussed all over the place. Several blockbuster movies featured scenarios where global nuclear war started by accident or on purpose and dramatized the dystopian aftermath with nuclear winter, infrastructure collapse and mass starvation. Slick new documentaries detailed how nuclear winter could impact every country. Scientists and politicians advocating nuclear de-escalation were given ample airtime, not least to discuss the results of several new studies on what helpful measures could be taken—studies funded by scientific organizations that had received large donations from new tech companies. As a result, political momentum started building for taking missiles off hair-trigger alert and shrinking nuclear arsenals. Renewed media attention was also paid to global climate change, often highlighting the recent Prometheus-enabled technological breakthroughs that were slashing the cost of renewable energy and encouraging governments to invest in such new energy infrastructure.\n\nParallel to their media takeover, the Omegas harnessed Prometheus to revolutionize education. Given any person’s knowledge and abilities, Prometheus could determine the fastest way for them to learn any new subject in a manner that kept them highly engaged and motivated to continue, and produce the corresponding optimized videos, reading materials, exercises and other learning tools. Omega-controlled companies therefore marketed online courses about virtually everything, highly customized not only by language and cultural background but also by starting level. Whether you were an illiterate forty-year-old wanting to learn to read or a biology PhD seeking the latest about cancer immunotherapy, Prometheus had the perfect course for you. These offerings bore little resemblance to most present-day online courses: by leveraging Prometheus’ movie-making talents, the video segments would truly engage, providing powerful metaphors that you would relate to, leaving you craving to learn more. Some courses were sold for profit, but many were made available for free, much to the delight of teachers around the world who could use them in their classrooms—and to most anybody eager to learn anything.\n\nThese educational superpowers proved potent tools for political purposes, creating online “persuasion sequences” of videos where insights from each one would both update someone’s views and motivate them to watch another video about a related topic where they were likely to be further convinced. When the goal was to defuse a conflict between two nations, for example, historical documentaries would be independently released in both countries that cast the origins and conduct of the conflict in more nuanced light. Pedagogical news stories would explain who on their own side stood to benefit from continued conflict and their techniques for stoking it. At the same time, likable characters from the other nation would start appearing in popular shows on the entertainment channels, just as sympathetically portrayed minority characters had bolstered the civil and gay rights movements in the past.\n\nBefore long, political commentators couldn’t help but notice growing support for a political agenda centered around seven slogans:\n\n1. Democracy\n\n2. Tax cuts\n\n3. Government social service cuts\n\n4. Military spending cuts\n\n5. Free trade\n\n6. Open borders\n\n7. Socially responsible companies\n\nWhat was less obvious was the underlying goal: to erode all previous power structures in the world. Items 2–6 eroded state power, and democratizing the world gave the Omegas’ business empire more influence over the selection of political leaders. Socially responsible companies further weakened state power by taking over more and more of the services that governments had (or should have) provided. The traditional business elite was weakened simply because it couldn’t compete with Prometheus-backed companies on the free market and therefore owned an ever-shrinking share of the world economy. Traditional opinion leaders, from political parties to faith groups, lacked the persuasion machinery to compete with the Omegas’ media empire.\n\nAs with any sweeping change, there were winners and losers. Although there was a palpable new sense of optimism in most countries as education, social services and infrastructure improved, conflicts subsided and local companies released breakthrough technologies that swept the world, not everybody was happy. While many displaced workers got rehired for community projects, those who’d held great power and wealth generally saw both shrink. This began in the media and technology sectors, but it spread virtually everywhere. The reduction in world conflicts led to defense budget cuts that hurt military contractors. Burgeoning upstart companies typically weren’t publicly traded, with the justification that profit-maximizing shareholders would block their massive spending on community projects. Thus the global stock market kept losing value, threatening both finance tycoons and regular citizens who’d counted on their pension funds. As if the shrinking profits of publicly traded companies weren’t bad enough, investment firms around the world had noticed a disturbing trend: all their previously successful trading algorithms seemed to have stopped working, underperforming even simple index funds. Someone else out there always seemed to outsmart them and beat them at their own game.\n\nAlthough masses of powerful people resisted the wave of change, their response was strikingly ineffective, almost as if they had fallen into a well-planned trap. Huge changes were happening at such a bewildering pace that it was hard to keep track and develop a coordinated response. Moreover, it was highly unclear what they should push for. The traditional political right had seen most of their slogans co-opted, yet the tax cuts and improved business climate were mostly helping their higher-tech competitors. Virtually every traditional industry was now clamoring for a bailout, but limited government funds pitted them in a hopeless battle against one another while the media portrayed them as dinosaurs seeking state subsidies simply because they couldn’t compete. The traditional political left opposed the free trade and the cuts in government social services, but delighted in the military cutbacks and the reduction of poverty. Indeed, much of their thunder was stolen by the undeniable fact that social services had improved now that they were provided by idealistic companies rather than the state. Poll after poll showed that most voters around the world felt their quality of life improving, and that things were generally moving in a good direction. This had a simple mathematical explanation: before Prometheus, the poorest 50% of Earth’s population had earned only about 4% of the global income, enabling the Omega-controlled companies to win their hearts (and votes) by sharing only a modest fraction of their profits with them.\n\nConsolidation\n\nAs a result, nation after nation saw landslide election victories for parties embracing the seven Omega slogans. In carefully optimized campaigns, they portrayed themselves at the center of the political spectrum, denouncing the right as greedy bailout-seeking conflict-mongers and lambasting the left as big-government tax-and-spend innovation stiflers. What almost nobody realized was that Prometheus had carefully selected the optimal people to groom as candidates, and pulled all its strings to secure their victory.\n\nBefore Prometheus, there had been growing support for the universal basic income movement, which proposed tax-funded minimum income for everyone as a remedy for technological unemployment. This movement imploded when the corporate community projects took off, since the Omega-controlled business empire was in effect providing the same thing. With the excuse of improving coordination of their community projects, an international group of companies launched the Humanitarian Alliance, a nongovernmental organization aiming to identify and fund the most valuable humanitarian efforts worldwide. Before long, virtually the entire Omega empire supported it, and it launched global projects on an unprecedented scale, even in countries that had largely missed out on the tech boom, improving education, health, prosperity and governance. Needless to say, Prometheus provided carefully crafted project plans behind the scenes, ranked by positive impact per dollar. Rather than simply dole out cash, as under basic-income proposals, the Alliance (as it colloquially became known) would engage those it supported to work toward its cause. As a result, a large fraction of the world’s population ended up feeling grateful and loyal to the Alliance—often more than to their own government.\n\nAs time passed, the Alliance increasingly assumed the role of a world government, as national governments saw their power continually erode. National budgets kept shrinking due to tax cuts while the Alliance budget grew to dwarf those of all governments combined. All the traditional roles of national governments became increasingly redundant and irrelevant. The Alliance provided by far the best social services, education and infrastructure. Media had defused international conflict to the point that military spending was largely unnecessary, and growing prosperity had eliminated most roots of old conflicts, which traced back to competition over scarce resources. A few dictators and others had violently resisted this new world order and refused to be bought, but they were all toppled in carefully orchestrated coups or mass uprisings.\n\nThe Omegas had now completed the most dramatic transition in the history of life on Earth. For the first time ever, our planet was run by a single power, amplified by an intelligence so vast that it could potentially enable life to flourish for billions of years on Earth and throughout our cosmos—but what specifically was their plan?\n\n\\* \\* \\*\n\nThat was the tale of the Omega team. The rest of this book is about another tale—one that’s not yet written: the tale of our own future with AI. How would you like it to play out? Could something remotely like the Omega story actually occur and, if so, would you want it to? Leaving aside speculations about superhuman AI, how would you like our tale to begin? How do you want AI to impact jobs, laws and weapons in the coming decade? Looking further ahead, how would you write the ending? This tale is one of truly cosmic proportions, for it involves nothing short of the ultimate future of life in our Universe. And it’s a tale for us to write.\n\n------------------------------------------------------------------------\n\n\\* For simplicity, I’ve assumed today’s economy and technology in this story, even though most researchers guess that human-level general AI is at least decades away. The Omega plan should get even easier to pull off in the future if the digital economy keeps growing and ever more services can be ordered online on a no-questions-asked basis.\n\nChapter 1\n\nWelcome to the Most Important Conversation of Our Time\n\n Technology is giving life the potential to flourish like never before—or to self-destruct.\n\n Future of Life Institute\n\nThirteen point eight billion years after its birth, our Universe has awoken and become aware of itself. From a small blue planet, tiny conscious parts of our Universe have begun gazing out into the cosmos with telescopes, repeatedly discovering that everything they thought existed is merely a small part of something grander: a solar system, a galaxy and a universe with over a hundred billion other galaxies arranged into an elaborate pattern of groups, clusters and superclusters. Although these self-aware stargazers disagree on many things, they tend to agree that these galaxies are beautiful and awe-inspiring.\n\nBut beauty is in the eye of the beholder, not in the laws of physics, so before our Universe awoke, there was no beauty. This makes our cosmic awakening all the more wonderful and worthy of celebrating: it transformed our Universe from a mindless zombie with no self-awareness into a living ecosystem harboring self-reflection, beauty and hope—and the pursuit of goals, meaning and purpose. Had our Universe never awoken, then, as far as I’m concerned, it would have been completely pointless—merely a gigantic waste of space. Should our Universe permanently go back to sleep due to some cosmic calamity or self-inflicted mishap, it will, alas, become meaningless.\n\nOn the other hand, things could get even better. We don’t yet know whether we humans are the only stargazers in our cosmos, or even the first, but we’ve already learned enough about our Universe to know that it has the potential to wake up much more fully than it has thus far. Perhaps we’re like that first faint glimmer of self-awareness you experienced when you began emerging from sleep this morning: a premonition of the much greater consciousness that would arrive once you opened your eyes and fully woke up. Perhaps life will spread throughout our cosmos and flourish for billions or trillions of years—and perhaps this will be because of decisions that we make here on our little planet during our lifetime.\n\nA Brief History of Complexity\n\nSo how did this amazing awakening come about? It wasn’t an isolated event, but merely one step in a relentless 13.8-billion-year process that’s making our Universe ever more complex and interesting—and is continuing at an accelerating pace.\n\nAs a physicist, I feel fortunate to have gotten to spend much of the past quarter century helping to pin down our cosmic history, and it’s been an amazing journey of discovery. Since the days when I was a graduate student, we’ve gone from arguing about whether our Universe is 10 or 20 billion years old to arguing about whether it’s 13.7 or 13.8 billion years old, thanks to a combination of better telescopes, better computers and better understanding. We physicists still don’t know for sure what caused our Big Bang or whether this was truly the beginning of everything or merely the sequel to an earlier stage. However, we’ve acquired a rather detailed understanding of what’s happened since our Big Bang, thanks to an avalanche of high-quality measurements, so please let me take a few minutes to summarize 13.8 billion years of cosmic history.\n\nIn the beginning, there was light. In the first split second after our Big Bang, the entire part of space that our telescopes can in principle observe (“our observable Universe,” or simply “our Universe” for short) was much hotter and brighter than the core of our Sun and it expanded rapidly. Although this may sound spectacular, it was also dull in the sense that our Universe contained nothing but a lifeless, dense, hot and boringly uniform soup of elementary particles. Things looked pretty much the same everywhere, and the only interesting structure consisted of faint random-looking sound waves that made the soup about 0.001% denser in some places. These faint waves are widely believed to have originated as so-called quantum fluctuations, because Heisenberg’s uncertainty principle of quantum mechanics forbids anything from being completely boring and uniform.\n\nAs our Universe expanded and cooled, it grew more interesting as its particles combined into ever more complex objects. During the first split second, the strong nuclear force grouped quarks into protons (hydrogen nuclei) and neutrons, some of which in turn fused into helium nuclei within a few minutes. About 400,000 years later, the electromagnetic force grouped these nuclei with electrons to make the first atoms. As our Universe kept expanding, these atoms gradually cooled into a cold dark gas, and the darkness of this first night lasted for about 100 million years. This long night gave rise to our cosmic dawn when the gravitational force succeeded in amplifying those fluctuations in the gas, pulling atoms together to form the first stars and galaxies. These first stars generated heat and light by fusing hydrogen into heavier atoms such as carbon, oxygen and silicon. When these stars died, many of the atoms they’d created were recycled into the cosmos and formed planets around second-generation stars.\n\nAt some point, a group of atoms became arranged into a complex pattern that could both maintain and replicate itself. So soon there were two copies, and the number kept doubling. It takes only forty doublings to make a trillion, so this first self-replicator soon became a force to be reckoned with. Life had arrived.\n\nThe Three Stages of Life\n\nThe question of how to define life is notoriously controversial. Competing definitions abound, some of which include highly specific requirements such as being composed of cells, which might disqualify both future intelligent machines and extraterrestrial civilizations. Since we don’t want to limit our thinking about the future of life to the species we’ve encountered so far, let’s instead define life very broadly, simply as a process that can retain its complexity and replicate. What’s replicated isn’t matter (made of atoms) but information (made of bits) specifying how the atoms are arranged. When a bacterium makes a copy of its DNA, no new atoms are created, but a new set of atoms are arranged in the same pattern as the original, thereby copying the information. In other words, we can think of life as a self-replicating information-processing system whose information (software) determines both its behavior and the blueprints for its hardware.\n\nLike our Universe itself, life gradually grew more complex and interesting,^(\\*1) and as I’ll now explain, I find it helpful to classify life forms into three levels of sophistication: Life 1.0, 2.0 and 3.0. I’ve summarized these three levels in figure 1.1.\n\nIt’s still an open question how, when and where life first appeared in our Universe, but there is strong evidence that here on Earth life first appeared about 4 billion years ago. Before long, our planet was teeming with a diverse panoply of life forms. The most successful ones, which soon outcompeted the rest, were able to react to their environment in some way. Specifically, they were what computer scientists call “intelligent agents”: entities that collect information about their environment from sensors and then process this information to decide how to act back on their environment. This can include highly complex information processing, such as when you use information from your eyes and ears to decide what to say in a conversation. But it can also involve hardware and software that’s quite simple.\n\nFor example, many bacteria have a sensor measuring the sugar concentration in the liquid around them and can swim using propeller-shaped structures called flagella. The hardware linking the sensor to the flagella might implement the following simple but useful algorithm: “If my sugar concentration sensor reports a lower value than a couple of seconds ago, then reverse the rotation of my flagella so that I change direction.”\n\n[Figure 1.1: The three stages of life: biological evolution, cultural evolution and technological evolution. Life 1.0 is unable to redesign either its hardware or its software during its lifetime: both are determined by its DNA, and change only through evolution over many generations. In contrast, Life 2.0 can redesign much of its software: humans can learn complex new skills—for example, languages, sports and professions—and can fundamentally update their worldview and goals. Life 3.0, which doesn’t yet exist on Earth, can dramatically redesign not only its software, but its hardware as well, rather than having to wait for it to gradually evolve over generations.][Figure 1.1: The three stages of life: biological evolution, cultural evolution and technological evolution. Life 1.0 is unable to redesign either its hardware or its software during its lifetime: both are determined by its DNA, and change only through evolution over many generations. In contrast, Life 2.0 can redesign much of its software: humans can learn complex new skills—for example, languages, sports and professions—and can fundamentally update their worldview and goals. Life 3.0, which doesn’t yet exist on Earth, can dramatically redesign not only its software, but its hardware as well, rather than having to wait for it to gradually evolve over generations.]\n\nFigure 1.1: The three stages of life: biological evolution, cultural evolution and technological evolution. Life 1.0 is unable to redesign either its hardware or its software during its lifetime: both are determined by its DNA, and change only through evolution over many generations. In contrast, Life 2.0 can redesign much of its software: humans can learn complex new skills—for example, languages, sports and professions—and can fundamentally update their worldview and goals. Life 3.0, which doesn’t yet exist on Earth, can dramatically redesign not only its software, but its hardware as well, rather than having to wait for it to gradually evolve over generations.\n\nYou’ve learned how to speak and countless other skills. Bacteria, on the other hand, aren’t great learners. Their DNA specifies not only the design of their hardware, such as sugar sensors and flagella, but also the design of their software. They never learn to swim toward sugar; instead, that algorithm was hard-coded into their DNA from the start. There was of course a learning process of sorts, but it didn’t take place during the lifetime of that particular bacterium. Rather, it occurred during the preceding evolution of that species of bacteria, through a slow trial-and-error process spanning many generations, where natural selection favored those random DNA mutations that improved sugar consumption. Some of these mutations helped by improving the design of flagella and other hardware, while other mutations improved the bacterial information-processing system that implements the sugar-finding algorithm and other software.\n\nSuch bacteria are an example of what I’ll call “Life 1.0”: life where both the hardware and software are evolved rather than designed. You and I, on the other hand, are examples of “Life 2.0”: life whose hardware is evolved, but whose software is largely designed. By your software, I mean all the algorithms and knowledge that you use to process the information from your senses and decide what to do—everything from the ability to recognize your friends when you see them to your ability to walk, read, write, calculate, sing and tell jokes.\n\nYou weren’t able to perform any of those tasks when you were born, so all this software got programmed into your brain later through the process we call learning. Whereas your childhood curriculum is largely designed by your family and teachers, who decide what you should learn, you gradually gain more power to design your own software. Perhaps your school allows you to select a foreign language: Do you want to install a software module into your brain that enables you to speak French, or one that enables you to speak Spanish? Do you want to learn to play tennis or chess? Do you want to study to become a chef, a lawyer or a pharmacist? Do you want to learn more about artificial intelligence (AI) and the future of life by reading a book about it?\n\nThis ability of Life 2.0 to design its software enables it to be much smarter than Life 1.0. High intelligence requires both lots of hardware (made of atoms) and lots of software (made of bits). The fact that most of our human hardware is added after birth (through growth) is useful, since our ultimate size isn’t limited by the width of our mom’s birth canal. In the same way, the fact that most of our human software is added after birth (through learning) is useful, since our ultimate intelligence isn’t limited by how much information can be transmitted to us at conception via our DNA, 1.0-style. I weigh about twenty-five times more than when I was born, and the synaptic connections that link the neurons in my brain can store about a hundred thousand times more information than the DNA that I was born with. Your synapses store all your knowledge and skills as roughly 100 terabytes’ worth of information, while your DNA stores merely about a gigabyte, barely enough to store a single movie download. So it’s physically impossible for an infant to be born speaking perfect English and ready to ace her college entrance exams: there’s no way the information could have been preloaded into her brain, since the main information module she got from her parents (her DNA) lacks sufficient information-storage capacity.\n\nThe ability to design its software enables Life 2.0 to be not only smarter than Life 1.0, but also more flexible. If the environment changes, 1.0 can only adapt by slowly evolving over many generations. Life 2.0, on the other hand, can adapt almost instantly, via a software update. For example, bacteria frequently encountering antibiotics may evolve drug resistance over many generations, but an individual bacterium won’t change its behavior at all; in contrast, a girl learning that she has a peanut allergy will immediately change her behavior to start avoiding peanuts. This flexibility gives Life 2.0 an even greater edge at the population level: even though the information in our human DNA hasn’t evolved dramatically over the past fifty thousand years, the information collectively stored in our brains, books and computers has exploded. By installing a software module enabling us to communicate through sophisticated spoken language, we ensured that the most useful information stored in one person’s brain could get copied to other brains, potentially surviving even after the original brain died. By installing a software module enabling us to read and write, we became able to store and share vastly more information than people could memorize. By developing brain software capable of producing technology (i.e., by studying science and engineering), we enabled much of the world’s information to be accessed by many of the world’s humans with just a few clicks.\n\nThis flexibility has enabled Life 2.0 to dominate Earth. Freed from its genetic shackles, humanity’s combined knowledge has kept growing at an accelerating pace as each breakthrough enabled the next: language, writing, the printing press, modern science, computers, the internet, etc. This ever-faster cultural evolution of our shared software has emerged as the dominant force shaping our human future, rendering our glacially slow biological evolution almost irrelevant.\n\nYet despite the most powerful technologies we have today, all life forms we know of remain fundamentally limited by their biological hardware. None can live for a million years, memorize all of Wikipedia, understand all known science or enjoy spaceflight without a spacecraft. None can transform our largely lifeless cosmos into a diverse biosphere that will flourish for billions or trillions of years, enabling our Universe to finally fulfill its potential and wake up fully. All this requires life to undergo a final upgrade, to Life 3.0, which can design not only its software but also its hardware. In other words, Life 3.0 is the master of its own destiny, finally fully free from its evolutionary shackles.\n\nThe boundaries between the three stages of life are slightly fuzzy. If bacteria are Life 1.0 and humans are Life 2.0, then you might classify mice as 1.1: they can learn many things, but not enough to develop language or invent the internet. Moreover, because they lack language, what they learn gets largely lost when they die, not passed on to the next generation. Similarly, you might argue that today’s humans should count as Life 2.1: we can perform minor hardware upgrades such as implanting artificial teeth, knees and pacemakers, but nothing as dramatic as getting ten times taller or acquiring a thousand times bigger brain.\n\nIn summary, we can divide the development of life into three stages, distinguished by life’s ability to design itself:\n\n• Life 1.0 (biological stage): evolves its hardware and software\n\n• Life 2.0 (cultural stage): evolves its hardware, designs much of its software\n\n• Life 3.0 (technological stage): designs its hardware and software\n\nAfter 13.8 billion years of cosmic evolution, development has accelerated dramatically here on Earth: Life 1.0 arrived about 4 billion years ago, Life 2.0 (we humans) arrived about a hundred millennia ago, and many AI researchers think that Life 3.0 may arrive during the coming century, perhaps even during our lifetime, spawned by progress in AI. What will happen, and what will this mean for us? That’s the topic of this book.\n\nControversies\n\nThis question is wonderfully controversial, with the world’s leading AI researchers disagreeing passionately not only in their forecasts, but also in their emotional reactions, which range from confident optimism to serious concern. They don’t even have consensus on short-term questions about AI’s economic, legal and military impact, and their disagreements grow when we expand the time horizon and ask about artificial general intelligence (AGI)—especially about AGI reaching human level and beyond, enabling Life 3.0. General intelligence can accomplish virtually any goal, including learning, in contrast to, say, the narrow intelligence of a chess-playing program.\n\nInterestingly, the controversy about Life 3.0 centers around not one but two separate questions: when and what? When (if ever) will it happen, and what will it mean for humanity? The way I see it, there are three distinct schools of thought that all need to be taken seriously, because they each include a number of world-leading experts. As illustrated in figure 1.2, I think of them as digital utopians, techno-skeptics and members of the beneficial-AI movement, respectively. Please let me introduce you to some of their most eloquent champions.\n\nDigital Utopians\n\nWhen I was a kid, I imagined that billionaires exuded pomposity and arrogance. When I first met Larry Page at Google in 2008, he totally shattered these stereotypes. Casually dressed in jeans and a remarkably ordinary-looking shirt, he would have blended right in at an MIT picnic. His thoughtful soft-spoken style and his friendly smile made me feel relaxed rather than intimidated talking with him. On July 18, 2015, we ran into each other at a party in Napa Valley thrown by Elon Musk and his then wife, Talulah, and got into a conversation about the scatological interests of our kids. I recommended the profound literary classic The Day My Butt Went Psycho, by Andy Griffiths, and Larry ordered it on the spot. I struggled to remind myself that he might go down in history as the most influential human ever to have lived: my guess is that if superintelligent digital life engulfs our Universe in my lifetime, it will be because of Larry’s decisions.\n\n[Figure 1.2: Most controversies surrounding strong artificial intelligence (that can match humans on any cognitive task) center around two questions: When (if ever) will it happen, and will it be a good thing for humanity? Techno-skeptics and digital utopians agree that we shouldn’t worry, but for very different reasons: the former are convinced that human-level artificial general intelligence (AGI) won’t happen in the foreseeable future, while the latter think it will happen but is virtually guaranteed to be a good thing. The beneficial-AI movement feels that concern is warranted and useful, because AI-safety research and discussion now increases the chances of a good outcome. Luddites are convinced of a bad outcome and oppose AI. This figure is partly inspired by Tim Urban. 1][Figure 1.2: Most controversies surrounding strong artificial intelligence (that can match humans on any cognitive task) center around two questions: When (if ever) will it happen, and will it be a good thing for humanity? Techno-skeptics and digital utopians agree that we shouldn’t worry, but for very different reasons: the former are convinced that human-level artificial general intelligence (AGI) won’t happen in the foreseeable future, while the latter think it will happen but is virtually guaranteed to be a good thing. The beneficial-AI movement feels that concern is warranted and useful, because AI-safety research and discussion now increases the chances of a good outcome. Luddites are convinced of a bad outcome and oppose AI. This figure is partly inspired by Tim Urban. 1]\n\nFigure 1.2: Most controversies surrounding strong artificial intelligence (that can match humans on any cognitive task) center around two questions: When (if ever) will it happen, and will it be a good thing for humanity? Techno-skeptics and digital utopians agree that we shouldn’t worry, but for very different reasons: the former are convinced that human-level artificial general intelligence (AGI) won’t happen in the foreseeable future, while the latter think it will happen but is virtually guaranteed to be a good thing. The beneficial-AI movement feels that concern is warranted and useful, because AI-safety research and discussion now increases the chances of a good outcome. Luddites are convinced of a bad outcome and oppose AI. This figure is partly inspired by Tim Urban.1\n\nWith our wives, Lucy and Meia, we ended up having dinner together and discussing whether machines would necessarily be conscious, an issue that he argued was a red herring. Later that night, after cocktails, a long and spirited debate ensued between him and Elon about the future of AI and what should be done. As we entered the wee hours of the morning, the circle of bystanders and kibitzers kept growing. Larry gave a passionate defense of the position I like to think of as digital utopianism: that digital life is the natural and desirable next step in the cosmic evolution and that if we let digital minds be free rather than try to stop or enslave them, the outcome is almost certain to be good. I view Larry as the most influential exponent of digital utopianism. He argued that if life is ever going to spread throughout our Galaxy and beyond, which he thought it should, then it would need to do so in digital form. His main concerns were that AI paranoia would delay the digital utopia and/or cause a military takeover of AI that would fall foul of Google’s “Don’t be evil” slogan. Elon kept pushing back and asking Larry to clarify details of his arguments, such as why he was so confident that digital life wouldn’t destroy everything we care about. At times, Larry accused Elon of being “specieist”: treating certain life forms as inferior just because they were silicon-based rather than carbon-based. We’ll return to explore these interesting issues and arguments in detail, starting in chapter 4.\n\nAlthough Larry seemed outnumbered that warm summer night by the pool, the digital utopianism that he so eloquently championed has many prominent supporters. Roboticist and futurist Hans Moravec inspired a whole generation of digital utopians with his classic 1988 book Mind Children, a tradition continued and refined by inventor Ray Kurzweil. Richard Sutton, one of the pioneers of the AI subfield known as reinforcement learning, gave a passionate defense of digital utopianism at our Puerto Rico conference that I’ll tell you about shortly.\n\nTechno-skeptics\n\nAnother prominent group of thinkers aren’t worried about AI either, but for a completely different reason: they think that building superhuman AGI is so hard that it won’t happen for hundreds of years, and therefore view it as silly to worry about it now. I think of this as the techno-skeptic position, eloquently articulated by Andrew Ng: “Fearing a rise of killer robots is like worrying about overpopulation on Mars.” Andrew was the chief scientist at Baidu, China’s Google, and he recently repeated this argument when I spoke with him at a conference in Boston. He also told me that he felt that worrying about AI risk was a potentially harmful distraction that could slow the progress of AI. Similar sentiments have been articulated by other techno-skeptics such as Rodney Brooks, the former MIT professor behind the Roomba robotic vacuum cleaner and the Baxter industrial robot. I find it interesting that although the digital utopians and the techno-skeptics agree that we shouldn’t worry about AI, they agree on little else. Most of the utopians think human-level AGI might happen within the next twenty to a hundred years, which the techno-skeptics dismiss as uninformed pie-in-the-sky dreaming, often deriding the prophesied singularity as “the rapture of the geeks.” When I met Rodney Brooks at a birthday party in December 2014, he told me that he was 100% sure it wouldn’t happen in my lifetime. “Are you sure you don’t mean 99%?,” I asked in a follow-up email, to which he replied, “No wimpy 99%. 100%. Just isn’t going to happen.”\n\nThe Beneficial-AI Movement\n\nWhen I first met Stuart Russell in a Paris café in June 2014, he struck me as the quintessential British gentleman. Eloquent, thoughtful and soft-spoken, but with an adventurous glint in his eyes, he seemed to me a modern incarnation of Phileas Fogg, my childhood hero from Jules Verne’s classic 1873 novel, Around the World in 80 Days. Although he was one of the most famous AI researchers alive, having co-authored the standard textbook on the subject, his modesty and warmth soon put me at ease. He explained to me how progress in AI had persuaded him that human-level AGI this century was a real possibility and, although he was hopeful, a good outcome wasn’t guaranteed. There were crucial questions that we needed to answer first, and they were so hard that we should start researching them now, so that we’d have the answers ready by the time we needed them.\n\nToday, Stuart’s views are rather mainstream, and many groups around the world are pursuing the sort of AI-safety research that he advocates. But this wasn’t always the case. An article in The Washington Post referred to 2015 as the year that AI-safety research went mainstream. Before that, talk of AI risks was often misunderstood by mainstream AI researchers and dismissed as Luddite scaremongering aimed at impeding AI progress. As we’ll explore in chapter 5, concerns similar to Stuart’s were first articulated over half a century ago by computer pioneer Alan Turing and mathematician Irving J. Good, who worked with Turing to crack German codes during World War II. In the past decade, research on such topics was mainly carried out by a handful of independent thinkers who weren’t professional AI researchers, for example Eliezer Yudkowsky, Michael Vassar and Nick Bostrom. Their work had little effect on most mainstream AI researchers, who tended to focus on their day-to-day tasks of making AI systems more intelligent rather than on contemplating the long-term consequences of success. Of the AI researchers I knew who did harbor some concern, many hesitated to voice it out of fear of being perceived as alarmist technophobes.\n\nI felt that this polarized situation needed to change, so that the full AI community could join and influence the conversation about how to build beneficial AI. Fortunately, I wasn’t alone. In the spring of 2014, I’d founded a nonprofit organization called the Future of Life Institute (FLI; http://futureoflife.org) together with my wife, Meia, my physicist friend Anthony Aguirre, Harvard grad student Viktoriya Krakovna and Skype founder Jaan Tallinn. Our goal was simple: to help ensure that the future of life existed and would be as awesome as possible. Specifically, we felt that technology was giving life the power either to flourish like never before or to self-destruct, and we preferred the former.\n\nOur first meeting was a brainstorming session at our house on March 15, 2014, with about thirty students, professors and other thinkers from the Boston area. There was broad consensus that although we should pay attention to biotech, nuclear weapons and climate change, our first major goal should be to help make AI-safety research mainstream. My MIT physics colleague Frank Wilczek, who won a Nobel Prize for helping figure out how quarks work, suggested that we start by writing an op-ed to draw attention to the issue and make it harder to ignore. I reached out to Stuart Russell (whom I hadn’t yet met) and to my physics colleague Stephen Hawking, both of whom agreed to join me and Frank as co-authors. Many edits later, our op-ed was rejected by The New York Times and many other U.S. newspapers, so we posted it on my Huffington Post blog account. To my delight, Arianna Huffington herself emailed and said, “thrilled to have it! We’ll post at #1!,” and this placement at the top of the front page triggered a wave of media coverage of AI safety that lasted for the rest of the year, with Elon Musk, Bill Gates and other tech leaders chiming in. Nick Bostrom’s book Superintelligence came out that fall and further fueled the growing public debate.\n\nThe next goal of our FLI beneficial-AI campaign was to bring the world’s leading AI researchers to a conference where misunderstandings could be cleared up, consensus could be forged, and constructive plans could be made. We knew that it would be difficult to persuade such an illustrious crowd to come to a conference organized by outsiders they didn’t know, especially given the controversial topic, so we tried as hard as we could: we banned media from attending, we located it in a beach resort in January (in Puerto Rico), we made it free (thanks to the generosity of Jaan Tallinn), and we gave it the most non-alarmist title we could come up with: “The Future of AI: Opportunities and Challenges.” Most importantly, we teamed up with Stuart Russell, thanks to whom we were able to grow the organizing committee to include a group of AI leaders from both academia and industry—including Demis Hassabis from Google’s DeepMind, who went on to show that AI can beat humans even at the game of Go. The more I got to know Demis, the more I realized that he had ambition not only to make AI powerful, but also to make it beneficial.\n\nThe result was a remarkable meeting of minds (figure 1.3). The AI researchers were joined by top economists, legal scholars, tech leaders (including Elon Musk) and other thinkers (including Vernor Vinge, who coined the term “singularity,” which is the focus of chapter 4). The outcome surpassed even our most optimistic expectations. Perhaps it was a combination of the sunshine and the wine, or perhaps it was just that the time was right: despite the controversial topic, a remarkable consensus emerged, which we codified in an open letter2 that ended up getting signed by over eight thousand people, including a veritable who’s who in AI. The gist of the letter was that the goal of AI should be redefined: the goal should be to create not undirected intelligence, but beneficial intelligence. The letter also mentioned a detailed list of research topics that the conference participants agreed would further this goal. The beneficial-AI movement had started going mainstream. We’ll follow its subsequent progress later in the book.\n\n[Figure 1.3: The January 2015 Puerto Rico conference brought together a remarkable group of researchers in AI and related fields. Back row, from left to right: Tom Mitchell, Seán Ó hÉigeartaigh, Huw Price, Shamil Chandaria, Jaan Tallinn, Stuart Russell, Bill Hibbard, Blaise Agüera y Arcas, Anders Sandberg, Daniel Dewey, Stuart Armstrong, Luke Muehlhauser, Tom Dietterich, Michael Osborne, James Manyika, Ajay Agrawal, Richard Mallah, Nancy Chang, Matthew Putman. Other standing, left to right: Marilyn Thompson, Rich Sutton, Alex Wissner-Gross, Sam Teller, Toby Ord, Joscha Bach, Katja Grace, Adrian Weller, Heather Roff-Perkins, Dileep George, Shane Legg, Demis Hassabis, Wendell Wallach, Charina Choi, Ilya Sutskever, Kent Walker, Cecilia Tilli, Nick Bostrom, Erik Brynjolfsson, Steve Crossan, Mustafa Suleyman, Scott Phoenix, Neil Jacobstein, Murray Shanahan, Robin Hanson, Francesca Rossi, Nate Soares, Elon Musk, Andrew McAfee, Bart Selman, Michele Reilly, Aaron VanDevender, Max Tegmark, Margaret Boden, Joshua Greene, Paul Christiano, Eliezer Yudkowsky, David Parkes, Laurent Orseau, JB Straubel, James Moor, Sean Legassick, Mason Hartman, Howie Lempel, David Vladeck, Jacob Steinhardt, Michael Vassar, Ryan Calo, Susan Young, Owain Evans, Riva-Melissa Tez, János Krámar, Geoff Anders, Vernor Vinge, Anthony Aguirre. Seated: Sam Harris, Tomaso Poggio, Marin Solja č i ć , Viktoriya Krakovna, Meia Chita-Tegmark. Behind the camera: Anthony Aguirre (and also photoshopped in by the human-level intelligence sitting next to him).][Figure 1.3: The January 2015 Puerto Rico conference brought together a remarkable group of researchers in AI and related fields. Back row, from left to right: Tom Mitchell, Seán Ó hÉigeartaigh, Huw Price, Shamil Chandaria, Jaan Tallinn, Stuart Russell, Bill Hibbard, Blaise Agüera y Arcas, Anders Sandberg, Daniel Dewey, Stuart Armstrong, Luke Muehlhauser, Tom Dietterich, Michael Osborne, James Manyika, Ajay Agrawal, Richard Mallah, Nancy Chang, Matthew Putman. Other standing, left to right: Marilyn Thompson, Rich Sutton, Alex Wissner-Gross, Sam Teller, Toby Ord, Joscha Bach, Katja Grace, Adrian Weller, Heather Roff-Perkins, Dileep George, Shane Legg, Demis Hassabis, Wendell Wallach, Charina Choi, Ilya Sutskever, Kent Walker, Cecilia Tilli, Nick Bostrom, Erik Brynjolfsson, Steve Crossan, Mustafa Suleyman, Scott Phoenix, Neil Jacobstein, Murray Shanahan, Robin Hanson, Francesca Rossi, Nate Soares, Elon Musk, Andrew McAfee, Bart Selman, Michele Reilly, Aaron VanDevender, Max Tegmark, Margaret Boden, Joshua Greene, Paul Christiano, Eliezer Yudkowsky, David Parkes, Laurent Orseau, JB Straubel, James Moor, Sean Legassick, Mason Hartman, Howie Lempel, David Vladeck, Jacob Steinhardt, Michael Vassar, Ryan Calo, Susan Young, Owain Evans, Riva-Melissa Tez, János Krámar, Geoff Anders, Vernor Vinge, Anthony Aguirre. Seated: Sam Harris, Tomaso Poggio, Marin Solja č i ć , Viktoriya Krakovna, Meia Chita-Tegmark. Behind the camera: Anthony Aguirre (and also photoshopped in by the human-level intelligence sitting next to him).]\n\nFigure 1.3: The January 2015 Puerto Rico conference brought together a remarkable group of researchers in AI and related fields. Back row, from left to right: Tom Mitchell, Seán Ó hÉigeartaigh, Huw Price, Shamil Chandaria, Jaan Tallinn, Stuart Russell, Bill Hibbard, Blaise Agüera y Arcas, Anders Sandberg, Daniel Dewey, Stuart Armstrong, Luke Muehlhauser, Tom Dietterich, Michael Osborne, James Manyika, Ajay Agrawal, Richard Mallah, Nancy Chang, Matthew Putman. Other standing, left to right: Marilyn Thompson, Rich Sutton, Alex Wissner-Gross, Sam Teller, Toby Ord, Joscha Bach, Katja Grace, Adrian Weller, Heather Roff-Perkins, Dileep George, Shane Legg, Demis Hassabis, Wendell Wallach, Charina Choi, Ilya Sutskever, Kent Walker, Cecilia Tilli, Nick Bostrom, Erik Brynjolfsson, Steve Crossan, Mustafa Suleyman, Scott Phoenix, Neil Jacobstein, Murray Shanahan, Robin Hanson, Francesca Rossi, Nate Soares, Elon Musk, Andrew McAfee, Bart Selman, Michele Reilly, Aaron VanDevender, Max Tegmark, Margaret Boden, Joshua Greene, Paul Christiano, Eliezer Yudkowsky, David Parkes, Laurent Orseau, JB Straubel, James Moor, Sean Legassick, Mason Hartman, Howie Lempel, David Vladeck, Jacob Steinhardt, Michael Vassar, Ryan Calo, Susan Young, Owain Evans, Riva-Melissa Tez, János Krámar, Geoff Anders, Vernor Vinge, Anthony Aguirre. Seated: Sam Harris, Tomaso Poggio, Marin Soljačić, Viktoriya Krakovna, Meia Chita-Tegmark. Behind the camera: Anthony Aguirre (and also photoshopped in by the human-level intelligence sitting next to him).\n\nAnother important lesson from the conference was this: the questions raised by the success of AI aren’t merely intellectually fascinating; they’re also morally crucial, because our choices can potentially affect the entire future of life. The moral significance of humanity’s past choices were sometimes great, but always limited: we’ve recovered even from the greatest plagues, and even the grandest empires eventually crumbled. Past generations knew that as surely as the Sun would rise tomorrow, so would tomorrow’s humans, tackling perennial scourges such as poverty, disease and war. But some of the Puerto Rico speakers argued that this time might be different: for the first time, they said, we might build technology powerful enough to permanently end these scourges—or to end humanity itself. We might create societies that flourish like never before, on Earth and perhaps beyond, or a Kafkaesque global surveillance state so powerful that it could never be toppled.\n\n[Figure 1.4: Although the media have often portrayed Elon Musk as being at loggerheads with the AI community, there’s in fact broad consensus that AI-safety research is needed. Here on January 4, 2015, Tom Dietterich, president of the Association for the Advancement of Artificial Intelligence, shares Elon’s excitement about the new AI-safety research program that Elon pledged to fund moments earlier. FLI founders Meia Chita-Tegmark and Viktoriya Krakovna lurk behind them.][Figure 1.4: Although the media have often portrayed Elon Musk as being at loggerheads with the AI community, there’s in fact broad consensus that AI-safety research is needed. Here on January 4, 2015, Tom Dietterich, president of the Association for the Advancement of Artificial Intelligence, shares Elon’s excitement about the new AI-safety research program that Elon pledged to fund moments earlier. FLI founders Meia Chita-Tegmark and Viktoriya Krakovna lurk behind them.]\n\nFigure 1.4: Although the media have often portrayed Elon Musk as being at loggerheads with the AI community, there’s in fact broad consensus that AI-safety research is needed. Here on January 4, 2015, Tom Dietterich, president of the Association for the Advancement of Artificial Intelligence, shares Elon’s excitement about the new AI-safety research program that Elon pledged to fund moments earlier. FLI founders Meia Chita-Tegmark and Viktoriya Krakovna lurk behind them.\n\nMisconceptions\n\nWhen I left Puerto Rico, I did so convinced that the conversation we had there about the future of AI needs to continue, because it’s the most important conversation of our time.^(\\*2) It’s the conversation about the collective future of all of us, so it shouldn’t be limited to AI researchers. That’s why I wrote this book: I wrote it in the hope that you, my dear reader, will join this conversation. What sort of future do you want? Should we develop lethal autonomous weapons? What would you like to happen with job automation? What career advice would you give today’s kids? Do you prefer new jobs replacing the old ones, or a jobless society where everyone enjoys a life of leisure and machine-produced wealth? Further down the road, would you like us to create Life 3.0 and spread it through our cosmos? Will we control intelligent machines or will they control us? Will intelligent machines replace us, coexist with us or merge with us? What will it mean to be human in the age of artificial intelligence? What would you like it to mean, and how can we make the future be that way?\n\nThe goal of this book is to help you join this conversation. As I mentioned, there are fascinating controversies where the world’s leading experts disagree. But I’ve also seen many examples of boring pseudo-controversies in which people misunderstand and talk past each other. To help ourselves focus on the interesting controversies and open questions, not on the misunderstandings, let’s start by clearing up some of the most common misconceptions.\n\nThere are many competing definitions in common use for terms such as “life,” “intelligence” and “consciousness,” and many misconceptions come from people not realizing that they’re using a word in two different ways. To make sure that you and I don’t fall into this trap, I’ve put a cheat sheet in table 1.1 showing how I use key terms in this book. Some of these definitions will only be properly introduced and explained in later chapters. Please note that I’m not claiming that my definitions are better than anyone else’s—I simply want to avoid confusion by being clear on what I mean. You’ll see that I generally go for broad definitions that avoid anthropocentric bias and can be applied to machines as well as humans. Please read the cheat sheet now, and come back and check it later if you find yourself puzzled by how I use one of its words—especially in chapters 4–8.\n\n ----------------------------------------------------- ----------------------------------------------------------------------------------------------------------------\n Terminology Cheat Sheet \n Life Process that can retain its complexity and replicate\n Life 1.0 Life that evolves its hardware and software (biological stage)\n Life 2.0 Life that evolves its hardware but designs much of its software (cultural stage)\n Life 3.0 Life that designs its hardware and software (technological stage)\n Intelligence Ability to accomplish complex goals\n Artificial Intelligence (AI) Non-biological intelligence\n Narrow intelligence Ability to accomplish a narrow set of goals, e.g., play chess or drive a car\n General intelligence Ability to accomplish virtually any goal, including learning\n Universal intelligence Ability to acquire general intelligence given access to data and resources\n [Human-level] Artificial General Intelligence (AGI) Ability to accomplish any cognitive task at least as well as humans\n Human-level AI AGI\n Strong AI AGI\n Superintelligence General intelligence far beyond human level\n Civilization Interacting group of intelligent life forms\n Consciousness Subjective experience\n Qualia Individual instances of subjective experience\n Ethics Principles that govern how we should behave\n Teleology Explanation of things in terms of their goals or purposes rather than their causes\n Goal-oriented behavior Behavior more easily explained via its effect than via its cause\n Having a goal Exhibiting goal-oriented behavior\n Having purpose Serving goals of one’s own or of another entity\n Friendly AI Superintelligence whose goals are aligned with ours\n Cyborg Human-machine hybrid\n Intelligence explosion Recursive self-improvement rapidly leading to superintelligence\n Singularity Intelligence explosion\n Universe The region of space from which light has had time to reach us during the 13.8 billion years since our Big Bang\n ----------------------------------------------------- ----------------------------------------------------------------------------------------------------------------\n\nTable 1.1: Many misunderstandings about AI are caused by people using the words above to mean different things. Here’s what I take them to mean in this book. (Some of these definitions will only be properly introduced and explained in later chapters.)\n\nIn addition to confusion over terminology, I’ve also seen many AI conversations get derailed by simple misconceptions. Let’s clear up the most common ones.\n\nTimeline Myths\n\nThe first one regards the timeline from figure 1.2: how long will it take until machines greatly supersede human-level AGI? Here, a common misconception is that we know the answer with great certainty.\n\nOne popular myth is that we know we’ll get superhuman AGI this century. In fact, history is full of technological over-hyping. Where are those fusion power plants and flying cars we were promised we’d have by now? AI too has been repeatedly over-hyped in the past, even by some of the founders of the field: for example, John McCarthy (who coined the term “artificial intelligence”), Marvin Minsky, Nathaniel Rochester and Claude Shannon wrote this overly optimistic forecast about what could be accomplished during two months with stone-age computers: “We propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College…An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.”\n\nOn the other hand, a popular counter-myth is that we know we won’t get superhuman AGI this century. Researchers have made a wide range of estimates for how far we are from superhuman AGI, but we certainly can’t say with great confidence that the probability is zero this century, given the dismal track record of such techno-skeptic predictions. For example, Ernest Rutherford, arguably the greatest nuclear physicist of his time, said in 1933—less than twenty-four hours before Leo Szilard’s invention of the nuclear chain reaction—that nuclear energy was “moonshine,” and in 1956 Astronomer Royal Richard Woolley called talk about space travel “utter bilge.” The most extreme form of this myth is that superhuman AGI will never arrive because it’s physically impossible. However, physicists know that a brain consists of quarks and electrons arranged to act as a powerful computer, and that there’s no law of physics preventing us from building even more intelligent quark blobs.\n\n[Figure 1.5: Common myths about superintelligent AI.][Figure 1.5: Common myths about superintelligent AI.]\n\nFigure 1.5: Common myths about superintelligent AI.\n\nThere have been a number of surveys asking AI researchers how many years from now they think we’ll have human-level AGI with at least 50% probability, and all these surveys have the same conclusion: the world’s leading experts disagree, so we simply don’t know. For example, in such a poll of the AI researchers at the Puerto Rico AI conference, the average (median) answer was by the year 2055, but some researchers guessed hundreds of years or more.\n\nThere’s also a related myth that people who worry about AI think it’s only a few years away. In fact, most people on record worrying about superhuman AGI guess it’s still at least decades away. But they argue that as long as we’re not 100% sure that it won’t happen this century, it’s smart to start safety research now to prepare for the eventuality. As we’ll see in this book, many of the safety problems are so hard that they may take decades to solve, so it’s prudent to start researching them now rather than the night before some programmers drinking Red Bull decide to switch on human-level AGI.\n\nControversy Myths\n\nAnother common misconception is that the only people harboring concerns about AI and advocating AI-safety research are Luddites who don’t know much about AI. When Stuart Russell mentioned this during his Puerto Rico talk, the audience laughed loudly. A related misconception is that supporting AI-safety research is hugely controversial. In fact, to support a modest investment in AI-safety research, people don’t need to be convinced that risks are high, merely non-negligible, just as a modest investment in home insurance is justified by a non-negligible probability of the home burning down.\n\nMy personal analysis is that the media have made the AI-safety debate seem more controversial than it really is. After all, fear sells, and articles using out-of-context quotes to proclaim imminent doom can generate more clicks than nuanced and balanced ones. As a result, two people who only know about each other’s positions from media quotes are likely to think they disagree more than they really do. For example, a techno-skeptic whose only knowledge about Bill Gates’ position comes from a British tabloid may mistakenly think he believes superintelligence to be imminent. Similarly, someone in the beneficial-AI movement who knows nothing about Andrew Ng’s position except his above-mentioned quote about overpopulation on Mars may mistakenly think he doesn’t care about AI safety. In fact, I personally know that he does—the crux is simply that because his timeline estimates are longer, he naturally tends to prioritize short-term AI challenges over long-term ones.\n\nMyths About What the Risks Are\n\nI rolled my eyes when seeing this headline in the Daily Mail:3 “Stephen Hawking Warns That Rise of Robots May Be Disastrous for Mankind.” I’ve lost count of how many similar articles I’ve seen. Typically, they’re accompanied by an evil-looking robot carrying a weapon, and suggest that we should worry about robots rising up and killing us because they’ve become conscious and/or evil. On a lighter note, such articles are actually rather impressive, because they succinctly summarize the scenario that my AI colleagues don’t worry about. That scenario combines as many as three separate misconceptions: concern about consciousness, evil and robots, respectively.\n\nIf you drive down the road, you have a subjective experience of colors, sounds, etc. But does a self-driving car have a subjective experience? Does it feel like anything at all to be a self-driving car, or is it like an unconscious zombie without any subjective experience? Although this mystery of consciousness is interesting in its own right, and we’ll devote chapter 8 to it, it’s irrelevant to AI risk. If you get struck by a driverless car, it makes no difference to you whether it subjectively feels conscious. In the same way, what will affect us humans is what superintelligent AI does, not how it subjectively feels.\n\nThe fear of machines turning evil is another red herring. The real worry isn’t malevolence, but competence. A superintelligent AI is by definition very good at attaining its goals, whatever they may be, so we need to ensure that its goals are aligned with ours. You’re probably not an ant hater who steps on ants out of malice, but if you’re in charge of a hydroelectric green energy project and there’s an anthill in the region to be flooded, too bad for the ants. The beneficial-AI movement wants to avoid placing humanity in the position of those ants.\n\nThe consciousness misconception is related to the myth that machines can’t have goals. Machines can obviously have goals in the narrow sense of exhibiting goal-oriented behavior: the behavior of a heat-seeking missile is most economically explained as a goal to hit a target. If you feel threatened by a machine whose goals are misaligned with yours, then it’s precisely its goals in this narrow sense that trouble you, not whether the machine is conscious and experiences a sense of purpose. If that heat-seeking missile were chasing you, you probably wouldn’t exclaim “I’m not worried, because machines can’t have goals!”\n\nI sympathize with Rodney Brooks and other robotics pioneers who feel unfairly demonized by scaremongering tabloids, because some journalists seem obsessively fixated on robots and adorn many of their articles with evil-looking metal monsters with shiny red eyes. In fact, the main concern of the beneficial-AI movement isn’t with robots but with intelligence itself: specifically, intelligence whose goals are misaligned with ours. To cause us trouble, such misaligned intelligence needs no robotic body, merely an internet connection—we’ll explore in chapter 4 how this may enable outsmarting financial markets, out-inventing human researchers, out-manipulating human leaders and developing weapons we cannot even understand. Even if building robots were physically impossible, a super-intelligent and super-wealthy AI could easily pay or manipulate myriad humans to unwittingly do its bidding, as in William Gibson’s science fiction novel Neuromancer.\n\nThe robot misconception is related to the myth that machines can’t control humans. Intelligence enables control: humans control tigers not because we’re stronger, but because we’re smarter. This means that if we cede our position as smartest on our planet, it’s possible that we might also cede control.\n\nFigure 1.5 summarizes all of these common misconceptions, so that we can dispense with them once and for all and focus our discussions with friends and colleagues on the many legitimate controversies—which, as we’ll see, there’s no shortage of!\n\nThe Road Ahead\n\nIn the rest of this book, you and I will explore together the future of life with AI. Let’s navigate this rich and multifaceted topic in an organized way by first exploring the full story of life conceptually and chronologically, and then exploring goals, meaning and what actions to take to create the future we want.\n\nIn chapter 2, we explore the foundations of intelligence and how seemingly dumb matter can be rearranged to remember, compute and learn. As we proceed into the future, our story branches out into many scenarios defined by the answers to certain key questions. Figure 1.6 summarizes key questions we’ll encounter as we march forward in time, to potentially ever more advanced AI.\n\nRight now, we face the choice of whether to start an AI arms race, and questions about how to make tomorrow’s AI systems bug-free and robust. If AI’s economic impact keeps growing, we also have to decide how to modernize our laws and what career advice to give kids so that they can avoid soon-to-be-automated jobs. We explore such short-term questions in chapter 3.\n\nIf AI progress continues to human levels, then we also need to ask ourselves how to ensure that it’s beneficial, and whether we can or should create a leisure society that flourishes without jobs. This also raises the question of whether an intelligence explosion or slow-but-steady growth can propel AGI far beyond human levels. We explore a wide range of such scenarios in chapter 4 and investigate the spectrum of possibilities for the aftermath in chapter 5, ranging from arguably dystopic to arguably utopic. Who’s in charge—humans, AI or cyborgs? Are humans treated well or badly? Are we replaced and, if so, do we perceive our replacements as conquerors or worthy descendants? I’m very curious about which of the chapter 5 scenarios you personally prefer! I’ve set up a website, http://AgeOfAi.org, where you can share your views and join the conversation.\n\nFinally, we forge billions of years into the future in chapter 6 where we can, ironically, draw stronger conclusions than in the previous chapters, as the ultimate limits of life in our cosmos are set not by intelligence but by the laws of physics.\n\nAfter concluding our exploration of the history of intelligence, we’ll devote the remainder of the book to considering what future to aim for and how to get there. To be able to link cold facts to questions of purpose and meaning, we explore the physical basis of goals in chapter 7 and consciousness in chapter 8. Finally, in the epilogue, we explore what can be done right now to help create the future we want.\n\n[Figure 1.6: Which AI questions are interesting depends on how advanced AI gets and which branch our future takes.][Figure 1.6: Which AI questions are interesting depends on how advanced AI gets and which branch our future takes.]\n\nFigure 1.6: Which AI questions are interesting depends on how advanced AI gets and which branch our future takes.\n\nIn case you’re a reader who likes skipping around, most chapters are relatively self-contained once you’ve digested the terminology and definitions from this first chapter and the beginning of the next one. If you’re an AI researcher, you can optionally skip all of chapter 2 except for its initial intelligence definitions. If you’re new to AI, chapters 2 and 3 will give you the arguments for why chapters 4 through 6 can’t be trivially dismissed as impossible science fiction. Figure 1.7 summarizes where the various chapters fall on the spectrum from factual to speculative.\n\n[Figure 1.7: Structure of the book][Figure 1.7: Structure of the book]\n\nFigure 1.7: Structure of the book\n\nA fascinating journey awaits us. Let’s begin!\n\n \n\n------------------------------------------------------------------------\n\nTHE BOTTOM LINE:\n\n• Life, defined as a process that can retain its complexity and replicate, can develop through three stages: a biological stage (1.0), where its hardware and software are evolved, a cultural stage (2.0), where it can design its software (through learning) and a technological stage (3.0), where it can design its hardware as well, becoming the master of its own destiny.\n\n• Artificial intelligence may enable us to launch Life 3.0 this century, and a fascinating conversation has sprung up regarding what future we should aim for and how this can be accomplished. There are three main camps in the controversy: techno-skeptics, digital utopians and the beneficial-AI movement.\n\n• Techno-skeptics view building superhuman AGI as so hard that it won’t happen for hundreds of years, making it silly to worry about it (and Life 3.0) now.\n\n• Digital utopians view it as likely this century and wholeheartedly welcome Life 3.0, viewing it as the natural and desirable next step in the cosmic evolution.\n\n• The beneficial-AI movement also views it as likely this century, but views a good outcome not as guaranteed, but as something that needs to be ensured by hard work in the form of AI-safety research.\n\n• Beyond such legitimate controversies where world-leading experts disagree, there are also boring pseudo-controversies caused by misunderstandings. For example, never waste time arguing about “life,” “intelligence,” or “consciousness” before ensuring that you and your protagonist are using these words to mean the same thing! This book uses the definitions in table 1.1.\n\n• Also beware the common misconceptions in figure 1.5: “Superintelligence by 2100 is inevitable/impossible.” “Only Luddites worry about AI.” “The concern is about AI turning evil and/or conscious, and it’s just years away.” “Robots are the main concern.” “AI can’t control humans and can’t have goals.”\n\n• In chapters 2 through 6, we’ll explore the story of intelligence from its humble beginning billions of years ago to possible cosmic futures billions of years from now. We’ll first investigate near-term challenges such as jobs, AI weapons and the quest for human-level AGI, then explore possibilities for a fascinating spectrum of possible futures with intelligent machines and/or humans. I wonder which options you’ll prefer!\n\n• In chapters 7 through 9, we’ll switch from cold factual descriptions to an exploration of goals, consciousness and meaning, and investigate what we can do right now to help create the future we want.\n\n• I view this conversation about the future of life with AI as the most important one of our time—please join it!\n\n------------------------------------------------------------------------\n\n \n\n------------------------------------------------------------------------\n\n\\*1 Why did life grow more complex? Evolution rewards life that’s complex enough to predict and exploit regularities in its environment, so in a more complex environment, more complex and intelligent life will evolve. Now this smarter life creates a more complex environment for competing life forms, which in turn evolve to be more complex, eventually creating an ecosystem of extremely complex life.\n\n\\*2 The AI conversation is important in terms of both urgency and impact. In comparison with climate change, which might wreak havoc in fifty to two hundred years, many experts expect AI to have greater impact within decades—and to potentially give us technology for mitigating climate change. In comparison with wars, terrorism, unemployment, poverty, migration and social justice issues, the rise of AI will have greater overall impact—indeed, we’ll explore in this book how it can dominate what happens with all these issues, for better or for worse.\n\nChapter 2\n\nMatter Turns Intelligent\n\n Hydrogen…, given enough time, turns into people.\n\n Edward Robert Harrison, 1995\n\nOne of the most spectacular developments during the 13.8 billion years since our Big Bang is that dumb and lifeless matter has turned intelligent. How could this happen and how much smarter can things get in the future? What does science have to say about the history and fate of intelligence in our cosmos? To help us tackle these questions, let’s devote this chapter to exploring the foundations and fundamental building blocks of intelligence. What does it mean to say that a blob of matter is intelligent? What does it mean to say that an object can remember, compute and learn?\n\nWhat Is Intelligence?\n\nMy wife and I recently had the good fortune to attend a symposium on artificial intelligence organized by the Swedish Nobel Foundation, and when a panel of leading AI researchers were asked to define intelligence, they argued at length without reaching consensus. We found this quite funny: there’s no agreement on what intelligence is even among intelligent intelligence researchers! So there’s clearly no undisputed “correct” definition of intelligence. Instead, there are many competing ones, including capacity for logic, understanding, planning, emotional knowledge, self-awareness, creativity, problem solving and learning.\n\nIn our exploration of the future of intelligence, we want to take a maximally broad and inclusive view, not limited to the sorts of intelligence that exist so far. That’s why the definition I gave in the last chapter, and the way I’m going to use the word throughout this book, is very broad:\n\n \n\n------------------------------------------------------------------------\n\nintelligence = ability to accomplish complex goals\n\n------------------------------------------------------------------------\n\n \n\nThis is broad enough to include all above-mentioned definitions, since understanding, self-awareness, problem solving, learning, etc. are all examples of complex goals that one might have. It’s also broad enough to subsume the Oxford Dictionary definition—“the ability to acquire and apply knowledge and skills”—since one can have as a goal to apply knowledge and skills.\n\nBecause there are many possible goals, there are many possible types of intelligence. By our definition, it therefore makes no sense to quantify intelligence of humans, non-human animals or machines by a single number such as an IQ.^(\\*1) What’s more intelligent: a computer program that can only play chess or one that can only play Go? There’s no sensible answer to this, since they’re good at different things that can’t be directly compared. We can, however, say that a third program is more intelligent than both of the others if it’s at least as good as them at accomplishing all goals, and strictly better at at least one (winning at chess, say).\n\nIt also makes little sense to quibble about whether something is or isn’t intelligent in borderline cases, since ability comes on a spectrum and isn’t necessarily an all-or-nothing trait. What people have the ability to accomplish the goal of speaking? Newborns? No. Radio hosts? Yes. But what about toddlers who can speak ten words? Or five hundred words? Where would you draw the line? I’ve used the deliberately vague word “complex” in the definition above, because it’s not very interesting to try to draw an artificial line between intelligence and non-intelligence, and it’s more useful to simply quantify the degree of ability for accomplishing different goals.\n\n[Figure 2.1: Intelligence, defined as ability to accomplish complex goals, can’t be measured by a single IQ, only by an ability spectrum across all goals. Each arrow indicates how skilled today’s best AI systems are at accomplishing various goals, illustrating that today’s artificial intelligence tends to be narrow, with each system able to accomplish only very specific goals. In contrast, human intelligence is remarkably broad: a healthy child can learn to get better at almost anything.][Figure 2.1: Intelligence, defined as ability to accomplish complex goals, can’t be measured by a single IQ, only by an ability spectrum across all goals. Each arrow indicates how skilled today’s best AI systems are at accomplishing various goals, illustrating that today’s artificial intelligence tends to be narrow, with each system able to accomplish only very specific goals. In contrast, human intelligence is remarkably broad: a healthy child can learn to get better at almost anything.]\n\nFigure 2.1: Intelligence, defined as ability to accomplish complex goals, can’t be measured by a single IQ, only by an ability spectrum across all goals. Each arrow indicates how skilled today’s best AI systems are at accomplishing various goals, illustrating that today’s artificial intelligence tends to be narrow, with each system able to accomplish only very specific goals. In contrast, human intelligence is remarkably broad: a healthy child can learn to get better at almost anything.\n\nTo classify different intelligences into a taxonomy, another crucial distinction is that between narrow and broad intelligence. IBM’s Deep Blue chess computer, which dethroned chess champion Garry Kasparov in 1997, was only able to accomplish the very narrow task of playing chess—despite its impressive hardware and software, it couldn’t even beat a four-year-old at tic-tac-toe. The DQN AI system of Google DeepMind can accomplish a slightly broader range of goals: it can play dozens of different vintage Atari computer games at human level or better. In contrast, human intelligence is thus far uniquely broad, able to master a dazzling panoply of skills. A healthy child given enough training time can get fairly good not only at any game, but also at any language, sport or vocation. Comparing the intelligence of humans and machines today, we humans win hands-down on breadth, while machines outperform us in a small but growing number of narrow domains, as illustrated in figure 2.1. The holy grail of AI research is to build “general AI” (better known as artificial general intelligence, AGI) that is maximally broad: able to accomplish virtually any goal, including learning. We’ll explore this in detail in chapter 4. The term “AGI” was popularized by the AI researchers Shane Legg, Mark Gubrud and Ben Goertzel to more specifically mean human-level artificial general intelligence: the ability to accomplish any goal at least as well as humans.1 I’ll stick with their definition, so unless I explicitly qualify the acronym (by writing “superhuman AGI,” for example), I’ll use “AGI” as shorthand for “human-level AGI.”^(\\*2)\n\nAlthough the word “intelligence” tends to have positive connotations, it’s important to note that we’re using it in a completely value-neutral way: as ability to accomplish complex goals regardless of whether these goals are considered good or bad. Thus an intelligent person may be very good at helping people or very good at hurting people. We’ll explore the issue of goals in chapter 7. Regarding goals, we also need to clear up the subtlety of whose goals we’re referring to. Suppose your future brand-new robotic personal assistant has no goals whatsoever of its own, but will do whatever you ask it to do, and you ask it to cook the perfect Italian dinner. If it goes online and researches Italian dinner recipes, how to get to the closest supermarket, how to strain pasta and so on, and then successfully buys the ingredients and prepares a succulent meal, you’ll presumably consider it intelligent even though the original goal was yours. In fact, it adopted your goal once you’d made your request, and then broke it into a hierarchy of subgoals of its own, from paying the cashier to grating the Parmesan. In this sense, intelligent behavior is inexorably linked to goal attainment.\n\n[Figure 2.2: Illustration of Hans Moravec’s “landscape of human competence,” where elevation represents difficulty for computers, and the rising sea level represents what computers are able to do.][Figure 2.2: Illustration of Hans Moravec’s “landscape of human competence,” where elevation represents difficulty for computers, and the rising sea level represents what computers are able to do.]\n\nFigure 2.2: Illustration of Hans Moravec’s “landscape of human competence,” where elevation represents difficulty for computers, and the rising sea level represents what computers are able to do.\n\nIt’s natural for us to rate the difficulty of tasks relative to how hard it is for us humans to perform them, as in figure 2.1. But this can give a misleading picture of how hard they are for computers. It feels much harder to multiply 314,159 by 271,828 than to recognize a friend in a photo, yet computers creamed us at arithmetic long before I was born, while human-level image recognition has only recently become possible. This fact that low-level sensorimotor tasks seem easy despite requiring enormous computational resources is known as Moravec’s paradox, and is explained by the fact that our brain makes such tasks feel easy by dedicating massive amounts of customized hardware to them—more than a quarter of our brains, in fact.\n\nI love this metaphor from Hans Moravec, and have taken the liberty to illustrate it in figure 2.2:\n\n Computers are universal machines, their potential extends uniformly over a boundless expanse of tasks. Human potentials, on the other hand, are strong in areas long important for survival, but weak in things far removed. Imagine a “landscape of human competence,” having lowlands with labels like “arithmetic” and “rote memorization,” foothills like “theorem proving” and “chess playing,” and high mountain peaks labeled “locomotion,” “hand-eye coordination” and “social interaction.” Advancing computer performance is like water slowly flooding the landscape. A half century ago it began to drown the lowlands, driving out human calculators and record clerks, but leaving most of us dry. Now the flood has reached the foothills, and our outposts there are contemplating retreat. We feel safe on our peaks, but, at the present rate, those too will be submerged within another half century. I propose that we build Arks as that day nears, and adopt a seafaring life\\!2\n\nDuring the decades since he wrote those passages, the sea level has kept rising relentlessly, as he predicted, like global warming on steroids, and some of his foothills (including chess) have long since been submerged. What comes next and what we should do about it is the topic of the rest of this book.\n\nAs the sea level keeps rising, it may one day reach a tipping point, triggering dramatic change. This critical sea level is the one corresponding to machines becoming able to perform AI design. Before this tipping point is reached, the sea-level rise is caused by humans improving machines; afterward, the rise can be driven by machines improving machines, potentially much faster than humans could have done, rapidly submerging all land. This is the fascinating and controversial idea of the singularity, which we’ll have fun exploring in chapter 4.\n\nComputer pioneer Alan Turing famously proved that if a computer can perform a certain bare minimum set of operations, then, given enough time and memory, it can be programmed to do anything that any other computer can do. Machines exceeding this critical threshold are called universal computers (aka Turing-universal computers); all of today’s smartphones and laptops are universal in this sense. Analogously, I like to think of the critical intelligence threshold required for AI design as the threshold for universal intelligence: given enough time and resources, it can make itself able to accomplish any goal as well as any other intelligent entity. For example, if it decides that it wants better social skills, forecasting skills or AI-design skills, it can acquire them. If it decides to figure out how to build a robot factory, then it can do so. In other words, universal intelligence has the potential to develop into Life 3.0.\n\nThe conventional wisdom among artificial intelligence researchers is that intelligence is ultimately all about information and computation, not about flesh, blood or carbon atoms. This means that there’s no fundamental reason why machines can’t one day be at least as intelligent as us.\n\nBut what are information and computation really, given that physics has taught us that, at a fundamental level, everything is simply matter and energy moving around? How can something as abstract, intangible and ethereal as information and computation be embodied by tangible physical stuff? In particular, how can a bunch of dumb particles moving around according to the laws of physics exhibit behavior that we’d call intelligent?\n\nIf you feel that the answer to this question is obvious and consider it plausible that machines might get as intelligent as humans this century—for example because you’re an AI researcher—please skip the rest of this chapter and jump straight to chapter 3. Otherwise, you’ll be pleased to know that I’ve written the next three sections specially for you.\n\nWhat Is Memory?\n\nIf we say that an atlas contains information about the world, we mean that there’s a relation between the state of the book (in particular, the positions of certain molecules that give the letters and images their colors) and the state of the world (for example, the locations of continents). If the continents were in different places, then those molecules would be in different places as well. We humans use a panoply of different devices for storing information, from books and brains to hard drives, and they all share this property: that their state can be related to (and therefore inform us about) the state of other things that we care about.\n\nWhat fundamental physical property do they all have in common that makes them useful as memory devices, i.e., devices for storing information? The answer is that they all can be in many different long-lived states—long-lived enough to encode the information until it’s needed. As a simple example, suppose you place a ball on a hilly surface that has sixteen different valleys, as in figure 2.3. Once the ball has rolled down and come to rest, it will be in one of sixteen places, so you can use its position as a way of remembering any number between 1 and 16.\n\nThis memory device is rather robust, because even if it gets a bit jiggled and disturbed by outside forces, the ball is likely to stay in the same valley that you put it in, so you can still tell which number is being stored. The reason that this memory is so stable is that lifting the ball out of its valley requires more energy than random disturbances are likely to provide. This same idea can provide stable memories much more generally than for a movable ball: the energy of a complicated physical system can depend on all sorts of mechanical, chemical, electrical and magnetic properties, and as long as it takes energy to change the system away from the state you want it to remember, this state will be stable. This is why solids have many long-lived states, whereas liquids and gases don’t: if you engrave someone’s name on a gold ring, the information will still be there years later because reshaping the gold requires significant energy, but if you engrave it in the surface of a pond, it will be lost within a second as the water surface effortlessly changes its shape.\n\nThe simplest possible memory device has only two stable states (figure 2.3). We can therefore think of it as encoding a binary digit (abbreviated “bit”), i.e., a zero or a one. The information stored by any more complicated memory device can equivalently be stored in multiple bits: for example, taken together, the four bits shown in figure 2.3 can be in 2 × 2 × 2 × 2 = 16 different states 0000, 0001, 0010, 0011,…, 1111, so they collectively have exactly the same memory capacity as the more complicated 16-state system. We can therefore think of bits as atoms of information—the smallest indivisible chunk of information that can’t be further subdivided, which can combine to make up any information. For example, I just typed the word “word,” and my laptop represented it in its memory as the 4-number sequence 119 111 114 100, storing each of those numbers as 8 bits (it represents each lowercase letter by a number that’s 96 plus its order in the alphabet). As soon as I hit the w key on my keyboard, my laptop displayed a visual image of a w on my screen, and this image is also represented by bits: 32 bits specify the color of each of the screen’s millions of pixels.\n\n[Figure 2.3: A physical object is a useful memory device if it can be in many different stable states. The ball on the left can encode four bits of information labeling which one of 2 4 = 16 valleys it’s in. Together, the four balls on the right also encode four bits of information—one bit each.][Figure 2.3: A physical object is a useful memory device if it can be in many different stable states. The ball on the left can encode four bits of information labeling which one of 2 4 = 16 valleys it’s in. Together, the four balls on the right also encode four bits of information—one bit each.]\n\nFigure 2.3: A physical object is a useful memory device if it can be in many different stable states. The ball on the left can encode four bits of information labeling which one of 2⁴ = 16 valleys it’s in. Together, the four balls on the right also encode four bits of information—one bit each.\n\nSince two-state systems are easy to manufacture and work with, most modern computers store their information as bits, but these bits are embodied in a wide variety of ways. On a DVD, each bit corresponds to whether there is or isn’t a microscopic pit at a given point on the plastic surface. On a hard drive, each bit corresponds to a point on the surface being magnetized in one of two ways. In my laptop’s working memory, each bit corresponds to the positions of certain electrons, determining whether a device called a micro-capacitor is charged. Some kinds of bits are convenient to transport as well, even at the speed of light: for example, in an optical fiber transmitting your email, each bit corresponds to a laser beam being strong or weak at a given time.\n\nEngineers prefer to encode bits into systems that aren’t only stable and easy to read from (as a gold ring), but also easy to write to: altering the state of your hard drive requires much less energy than engraving gold. They also prefer systems that are convenient to work with and cheap to mass-produce. But other than that, they simply don’t care about how the bits are represented as physical objects—and nor do you most of the time, because it simply doesn’t matter! If you email your friend a document to print, the information may get copied in rapid succession from magnetizations on your hard drive to electric charges in your computer’s working memory, radio waves in your wireless network, voltages in your router, laser pulses in an optical fiber and, finally, molecules on a piece of paper. In other words, information can take on a life of its own, independent of its physical substrate! Indeed, it’s usually only this substrate-independent aspect of information that we’re interested in: if your friend calls you up to discuss that document you sent, she’s probably not calling to talk about voltages or molecules. This is our first hint of how something as intangible as intelligence can be embodied in tangible physical stuff, and we’ll soon see how this idea of substrate independence is much deeper, including not only information but also computation and learning.\n\nBecause of this substrate independence, clever engineers have been able to repeatedly replace the memory devices inside our computers with dramatically better ones, based on new technologies, without requiring any changes whatsoever to our software. The result has been spectacular, as illustrated in figure 2.4: over the past six decades, computer memory has gotten half as expensive roughly every couple of years. Hard drives have gotten over 100 million times cheaper, and the faster memories useful for computation rather than mere storage have become a whopping 10 trillion times cheaper. If you could get such a “99.99999999999% off” discount on all your shopping, you could buy all real estate in New York City for about 10 cents and all the gold that’s ever been mined for around a dollar.\n\nFor many of us, the spectacular improvements in memory technology come with personal stories. I fondly remember working in a candy store back in high school to pay for a computer sporting 16 kilobytes of memory, and when I made and sold a word processor for it with my high school classmate Magnus Bodin, we were forced to write it all in ultra-compact machine code to leave enough memory for the words that it was supposed to process. After getting used to floppy drives storing 70kB, I became awestruck by the smaller 3.5-inch floppies that could store a whopping 1.44MB and hold a whole book, and then my first-ever hard drive storing 10MB—which might just barely fit a single one of today’s song downloads. These memories from my adolescence felt almost unreal the other day, when I spent about $100 on a hard drive with 300,000 times more capacity.\n\n[Figure 2.4: Over the past six decades, computer memory has gotten twice as cheap roughly every couple of years, corresponding to a thousand times cheaper roughly every twenty years. A byte equals eight bits. Data courtesy of John McCallum, from http://www.jcmit.net/memoryprice.htm.][Figure 2.4: Over the past six decades, computer memory has gotten twice as cheap roughly every couple of years, corresponding to a thousand times cheaper roughly every twenty years. A byte equals eight bits. Data courtesy of John McCallum, from http://www.jcmit.net/memoryprice.htm.]\n\nFigure 2.4: Over the past six decades, computer memory has gotten twice as cheap roughly every couple of years, corresponding to a thousand times cheaper roughly every twenty years. A byte equals eight bits. Data courtesy of John McCallum, from http://www.jcmit.net/memoryprice.htm.\n\nWhat about memory devices that evolved rather than being designed by humans? Biologists don’t yet know what the first-ever life form was that copied its blueprints between generations, but it may have been quite small. A team led by Philipp Holliger at Cambridge University made an RNA molecule in 2016 that encoded 412 bits of genetic information and was able to copy RNA strands longer than itself, bolstering the “RNA world” hypothesis that early Earth life involved short self-replicating RNA snippets. So far, the smallest memory device known to be evolved and used in the wild is the genome of the bacterium Candidatus Carsonella ruddii, storing about 40 kilobytes, whereas our human DNA stores about 1.6 gigabytes, comparable to a downloaded movie. As mentioned in the last chapter, our brains store much more information than our genes: in the ballpark of 10 gigabytes electrically (specifying which of your 100 billion neurons are firing at any one time) and 100 terabytes chemically/biologically (specifying how strongly different neurons are linked by synapses). Comparing these numbers with the machine memories shows that the world’s best computers can now out-remember any biological system—at a cost that’s rapidly dropping and was a few thousand dollars in 2016.\n\nThe memory in your brain works very differently from computer memory, not only in terms of how it’s built, but also in terms of how it’s used. Whereas you retrieve memories from a computer or hard drive by specifying where it’s stored, you retrieve memories from your brain by specifying something about what is stored. Each group of bits in your computer’s memory has a numerical address, and to retrieve a piece of information, the computer specifies at what address to look, just as if I tell you “Go to my bookshelf, take the fifth book from the right on the top shelf, and tell me what it says on page 314.” In contrast, you retrieve information from your brain similarly to how you retrieve it from a search engine: you specify a piece of the information or something related to it, and it pops up. If I tell you “to be or not,” or if I google it, chances are that it will trigger “To be, or not to be, that is the question.” Indeed, it will probably work even if I use another part of the quote or mess things up somewhat. Such memory systems are called auto-associative, since they recall by association rather than by address.\n\nIn a famous 1982 paper, the physicist John Hopfield showed how a network of interconnected neurons could function as an auto-associative memory. I find the basic idea very beautiful, and it works for any physical system with multiple stable states. For example, consider a ball on a surface with two valleys, like the one-bit system in figure 2.3, and let’s shape the surface so that the x-coordinates of the two minima where the ball can come to rest are x = √2 ≈ 1.41421 and x = π ≈ 3.14159, respectively. If you remember only that π is close to 3, you simply put the ball at x = 3 and watch it reveal a more exact π-value as it rolls down to the nearest minimum. Hopfield realized that a complex network of neurons provides an analogous landscape with very many energy-minima that the system can settle into, and it was later proved that you can squeeze in as many as 138 different memories for every thousand neurons without causing major confusion.\n\nWhat Is Computation?\n\nWe’ve now seen how a physical object can remember information. But how can it compute?\n\nA computation is a transformation of one memory state into another. In other words, a computation takes information and transforms it, implementing what mathematicians call a function. I think of a function as a meat grinder for information, as illustrated in figure 2.5: you put information in at the top, turn the crank and get processed information out at the bottom—and you can repeat this as many times as you want with different inputs. This information processing is deterministic in the sense that if you repeat it with the same input, you get the same output every time.\n\n[Figure 2.5: A computation takes information and transforms it, implementing what mathematicians call a function . The function f (left) takes bits representing a number and computes its square. The function g (middle) takes bits representing a chess position and computes the best move for White. The function h (right) takes bits representing an image and computes a text label describing it.][Figure 2.5: A computation takes information and transforms it, implementing what mathematicians call a function . The function f (left) takes bits representing a number and computes its square. The function g (middle) takes bits representing a chess position and computes the best move for White. The function h (right) takes bits representing an image and computes a text label describing it.]\n\nFigure 2.5: A computation takes information and transforms it, implementing what mathematicians call a function. The function f (left) takes bits representing a number and computes its square. The function g (middle) takes bits representing a chess position and computes the best move for White. The function h (right) takes bits representing an image and computes a text label describing it.\n\nAlthough it sounds deceptively simple, this idea of a function is incredibly general. Some functions are rather trivial, such as the one called NOT that inputs a single bit and outputs the reverse, thus turning zero into one and vice versa. The functions we learn about in school typically correspond to buttons on a pocket calculator, inputting one or more numbers and outputting a single number—for example, the function x² simply inputs a number and outputs it multiplied by itself. Other functions can be extremely complicated. For instance, if you’re in possession of a function that would input bits representing an arbitrary chess position and output bits representing the best possible next move, you can use it to win the World Computer Chess Championship. If you’re in possession of a function that inputs all the world’s financial data and outputs the best stocks to buy, you’ll soon be extremely rich. Many AI researchers dedicate their careers to figuring out how to implement certain functions. For example, the goal of machine-translation research is to implement a function inputting bits representing text in one language and outputting bits representing that same text in another language, and the goal of automatic-captioning research is inputting bits representing an image and outputting bits representing text describing it (figure 2.5).\n\n[Figure 2.6: A so-called NAND gate takes two bits, A and B, as inputs and computes one bit C as output, according to the rule that C = 0 if A = B = 1 and C = 1 otherwise. Many physical systems can be used as NAND gates. In the middle example, switches are interpreted as bits where 0 = open, 1= closed, and when switches A and B are both closed, an electromagnet opens the switch C. In the rightmost example, voltages (electrical potentials) are interpreted as bits where 1 = five volts, 0 = zero volts, and when wires A and B are both at five volts, the two transistors conduct electricity and the wire C drops to approximately zero volts.][Figure 2.6: A so-called NAND gate takes two bits, A and B, as inputs and computes one bit C as output, according to the rule that C = 0 if A = B = 1 and C = 1 otherwise. Many physical systems can be used as NAND gates. In the middle example, switches are interpreted as bits where 0 = open, 1= closed, and when switches A and B are both closed, an electromagnet opens the switch C. In the rightmost example, voltages (electrical potentials) are interpreted as bits where 1 = five volts, 0 = zero volts, and when wires A and B are both at five volts, the two transistors conduct electricity and the wire C drops to approximately zero volts.]\n\nFigure 2.6: A so-called NAND gate takes two bits, A and B, as inputs and computes one bit C as output, according to the rule that C = 0 if A = B = 1 and C = 1 otherwise. Many physical systems can be used as NAND gates. In the middle example, switches are interpreted as bits where 0 = open, 1= closed, and when switches A and B are both closed, an electromagnet opens the switch C. In the rightmost example, voltages (electrical potentials) are interpreted as bits where 1 = five volts, 0 = zero volts, and when wires A and B are both at five volts, the two transistors conduct electricity and the wire C drops to approximately zero volts.\n\nIn other words, if you can implement highly complex functions, then you can build an intelligent machine that’s able to accomplish highly complex goals. This brings our question of how matter can be intelligent into sharper focus: in particular, how can a clump of seemingly dumb matter compute a complicated function?\n\nRather than just remain immobile as a gold ring or other static memory device, it must exhibit complex dynamics so that its future state depends in some complicated (and hopefully controllable/programmable) way on the present state. Its atom arrangement must be less ordered than a rigid solid where nothing interesting changes, but more ordered than a liquid or gas. Specifically, we want the system to have the property that if we put it in a state that encodes the input information, let it evolve according to the laws of physics for some amount of time, and then interpret the resulting final state as the output information, then the output is the desired function of the input. If this is the case, then we can say that our system computes our function.\n\nAs a first example of this idea, let’s explore how we can build a very simple (but also very important) function called a NAND gate^(\\*3) out of plain old dumb matter. This function inputs two bits and outputs one bit: it outputs 0 if both inputs are 1; in all other cases, it outputs 1. If we connect two switches in series with a battery and an electromagnet, then the electromagnet will only be on if the first switch and the second switch are closed (“on”). Let’s place a third switch under the electromagnet, as illustrated in figure 2.6, such that the magnet will pull it open whenever it’s powered on. If we interpret the first two switches as the input bits and the third one as the output bit (with 0 = switch open, and 1 = switch closed), then we have ourselves a NAND gate: the third switch is open only if the first two are closed. There are many other ways of building NAND gates that are more practical—for example, using transistors as illustrated in figure 2.6. In today’s computers, NAND gates are typically built from microscopic transistors and other components that can be automatically etched onto silicon wafers.\n\nThere’s a remarkable theorem in computer science that says that NAND gates are universal, meaning that you can implement any well-defined function simply by connecting together NAND gates.^(\\*4) So if you can build enough NAND gates, you can build a device computing anything! In case you’d like a taste of how this works, I’ve illustrated in figure 2.7 how to multiply numbers using nothing but NAND gates.\n\nMIT researchers Norman Margolus and Tommaso Toffoli coined the name computronium for any substance that can perform arbitrary computations. We’ve just seen that making computronium doesn’t have to be particularly hard: the substance just needs to be able to implement NAND gates connected together in any desired way. Indeed, there are myriad other kinds of computronium as well. A simple variant that also works involves replacing the NAND gates by NOR gates that output 1 only when both inputs are 0. In the next section, we’ll explore neural networks, which can also implement arbitrary computations, i.e., act as computronium. Scientist and entrepreneur Stephen Wolfram has shown that the same goes for simple devices called cellular automata, which repeatedly update bits based on what neighboring bits are doing. Already back in 1936, computer pioneer Alan Turing proved in a landmark paper that a simple machine (now known as a “universal Turing machine”) that could manipulate symbols on a strip of tape could also implement arbitrary computations. In summary, not only is it possible for matter to implement any well-defined computation, but it’s possible in a plethora of different ways.\n\nAs mentioned earlier, Turing also proved something even more profound in that 1936 paper of his: that if a type of computer can perform a certain bare minimum set of operations, then it’s universal in the sense that given enough resources, it can do anything that any other computer can do. He showed that his Turing machine was universal, and connecting back more closely to physics, we’ve just seen that this family of universal computers also includes objects as diverse as a network of NAND gates and a network of interconnected neurons. Indeed, Stephen Wolfram has argued that most non-trivial physical systems, from weather systems to brains, would be universal computers if they could be made arbitrarily large and long-lasting.\n\n[Figure 2.7: Any well-defined computation can be performed by cleverly combining nothing but NAND gates. For example, the addition and multiplication modules above both input two binary numbers represented by 4 bits, and output a binary number represented by 5 bits and 8 bits, respectively. The smaller modules NOT, AND, XOR and + (which sums three separate bits into a 2-bit binary number) are in turn built out of NAND gates. Fully understanding this figure is extremely challenging and totally unnecessary for following the rest of this book; I’m including it here just to illustrate the idea of universality—and to satisfy my inner geek.][Figure 2.7: Any well-defined computation can be performed by cleverly combining nothing but NAND gates. For example, the addition and multiplication modules above both input two binary numbers represented by 4 bits, and output a binary number represented by 5 bits and 8 bits, respectively. The smaller modules NOT, AND, XOR and + (which sums three separate bits into a 2-bit binary number) are in turn built out of NAND gates. Fully understanding this figure is extremely challenging and totally unnecessary for following the rest of this book; I’m including it here just to illustrate the idea of universality—and to satisfy my inner geek.]\n\nFigure 2.7: Any well-defined computation can be performed by cleverly combining nothing but NAND gates. For example, the addition and multiplication modules above both input two binary numbers represented by 4 bits, and output a binary number represented by 5 bits and 8 bits, respectively. The smaller modules NOT, AND, XOR and + (which sums three separate bits into a 2-bit binary number) are in turn built out of NAND gates. Fully understanding this figure is extremely challenging and totally unnecessary for following the rest of this book; I’m including it here just to illustrate the idea of universality—and to satisfy my inner geek.\n\nThis fact that exactly the same computation can be performed on any universal computer means that computation is substrate-independent in the same way that information is: it can take on a life of its own, independent of its physical substrate! So if you’re a conscious superintelligent character in a future computer game, you’d have no way of knowing whether you ran on a Windows desktop, a Mac OS laptop or an Android phone, because you would be substrate-independent. You’d also have no way of knowing what type of transistors the microprocessor was using.\n\nI first came to appreciate this crucial idea of substrate independence because there are many beautiful examples of it in physics. Waves, for instance: they have properties such as speed, wavelength and frequency, and we physicists can study the equations they obey without even needing to know what particular substance they’re waves in. When you hear something, you’re detecting sound waves caused by molecules bouncing around in the mixture of gases that we call air, and we can calculate all sorts of interesting things about these waves—how their intensity fades as the square of the distance, such as how they bend when they pass through open doors and how they bounce off of walls and cause echoes—without knowing what air is made of. In fact, we don’t even need to know that it’s made of molecules: we can ignore all details about oxygen, nitrogen, carbon dioxide, etc., because the only property of the wave’s substrate that matters and enters into the famous wave equation is a single number that we can measure: the wave speed, which in this case is about 300 meters per second. Indeed, this wave equation that I taught my MIT students about in a course last spring was first discovered and put to great use long before physicists had even established that atoms and molecules existed!\n\nThis wave example illustrates three important points. First, substrate independence doesn’t mean that a substrate is unnecessary, but that most of its details don’t matter. You obviously can’t have sound waves in a gas if there’s no gas, but any gas whatsoever will suffice. Similarly, you obviously can’t have computation without matter, but any matter will do as long as it can be arranged into NAND gates, connected neurons or some other building block enabling universal computation. Second, the substrate-independent phenomenon takes on a life of its own, independent of its substrate. A wave can travel across a lake, even though none of its water molecules do—they mostly bob up and down, like fans doing “the wave” in a sports stadium. Third, it’s often only the substrate-independent aspect that we’re interested in: a surfer usually cares more about the position and height of a wave than about its detailed molecular composition. We saw how this was true for information, and it’s true for computation too: if two programmers are jointly hunting a bug in their code, they’re probably not discussing transistors.\n\nWe’ve now arrived at an answer to our opening question about how tangible physical stuff can give rise to something that feels as intangible, abstract and ethereal as intelligence: it feels so non-physical because it’s substrate-independent, taking on a life of its own that doesn’t depend on or reflect the physical details. In short, computation is a pattern in the spacetime arrangement of particles, and it’s not the particles but the pattern that really matters! Matter doesn’t matter.\n\nIn other words, the hardware is the matter and the software is the pattern. This substrate independence of computation implies that AI is possible: intelligence doesn’t require flesh, blood or carbon atoms.\n\nBecause of this substrate independence, shrewd engineers have been able to repeatedly replace the technologies inside our computers with dramatically better ones, without changing the software. The results have been every bit as spectacular as those for memory devices. As illustrated in figure 2.8, computation keeps getting half as expensive roughly every couple of years, and this trend has now persisted for over a century, cutting the computer cost a whopping million million million (10¹⁸) times since my grandmothers were born. If everything got a million million million times cheaper, then a hundredth of a cent would enable you to buy all goods and services produced on Earth this year. This dramatic drop in costs is of course a key reason why computation is everywhere these days, having spread from the building-sized computing facilities of yesteryear into our homes, cars and pockets—and even turning up in unexpected places such as sneakers.\n\nWhy does our technology keep doubling its power at regular intervals, displaying what mathematicians call exponential growth? Indeed, why is it happening not only in terms of transistor miniaturization (a trend known as Moore’s law), but also more broadly for computation as a whole (figure 2.8), for memory (figure 2.4) and for a plethora of other technologies ranging from genome sequencing to brain imaging? Ray Kurzweil calls this persistent doubling phenomenon “the law of accelerating returns.”\n\n[Figure 2.8: Since 1900, computation has gotten twice as cheap roughly every couple of years. The plot shows the computing power measured in floating-point operations per second (FLOPS) that can be purchased for $1,000. 3 The particular computation that defines a floating point operation corresponds to about 10 5 elementary logical operations such as bit flips or NAND evaluations.][Figure 2.8: Since 1900, computation has gotten twice as cheap roughly every couple of years. The plot shows the computing power measured in floating-point operations per second (FLOPS) that can be purchased for $1,000. 3 The particular computation that defines a floating point operation corresponds to about 10 5 elementary logical operations such as bit flips or NAND evaluations.]\n\nFigure 2.8: Since 1900, computation has gotten twice as cheap roughly every couple of years. The plot shows the computing power measured in floating-point operations per second (FLOPS) that can be purchased for $1,000.3 The particular computation that defines a floating point operation corresponds to about 10⁵ elementary logical operations such as bit flips or NAND evaluations.\n\nAll examples of persistent doubling that I know of in nature have the same fundamental cause, and this technological one is no exception: each step creates the next. For example, you yourself underwent exponential growth right after your conception: each of your cells divided and gave rise to two cells roughly daily, causing your total number of cells to increase day by day as 1, 2, 4, 8, 16 and so on. According to the most popular scientific theory of our cosmic origins, known as inflation, our baby Universe once grew exponentially just like you did, repeatedly doubling its size at regular intervals until a speck much smaller and lighter than an atom had grown more massive than all the galaxies we’ve ever seen with our telescopes. Again, the cause was a process whereby each doubling step caused the next. This is how technology progresses as well: once technology gets twice as powerful, it can often be used to design and build technology that’s twice as powerful in turn, triggering repeated capability doubling in the spirit of Moore’s law.\n\nSomething that occurs just as regularly as the doubling of our technological power is the appearance of claims that the doubling is ending. Yes, Moore’s law will of course end, meaning that there’s a physical limit to how small transistors can be made. But some people mistakenly assume that Moore’s law is synonymous with the persistent doubling of our technological power. Contrariwise, Ray Kurzweil points out that Moore’s law involves not the first but the fifth technological paradigm to bring exponential growth in computing, as illustrated in figure 2.8: whenever one technology stopped improving, we replaced it with an even better one. When we could no longer keep shrinking our vacuum tubes, we replaced them with transistors and then integrated circuits, where electrons move around in two dimensions. When this technology reaches its limits, there are many other alternatives we can try—for example, using three-dimensional circuits and using something other than electrons to do our bidding.\n\nNobody knows for sure what the next blockbuster computational substrate will be, but we do know that we’re nowhere near the limits imposed by the laws of physics. My MIT colleague Seth Lloyd has worked out what this fundamental limit is, and as we’ll explore in greater detail in chapter 6, this limit is a whopping 33 orders of magnitude (10³³ times) beyond today’s state of the art for how much computing a clump of matter can do. So even if we keep doubling the power of our computers every couple of years, it will take over two centuries until we reach that final frontier.\n\nAlthough all universal computers are capable of the same computations, some are more efficient than others. For example, a computation requiring millions of multiplications doesn’t require millions of separate multiplication modules built from separate transistors as in figure 2.6: it needs only one such module, since it can use it many times in succession with appropriate inputs. In this spirit of efficiency, most modern computers use a paradigm where computations are split into multiple time steps, during which information is shuffled back and forth between memory modules and computation modules. This computational architecture was developed between 1935 and 1945 by computer pioneers including Alan Turing, Konrad Zuse, Presper Eckert, John Mauchly and John von Neumann. More specifically, the computer memory stores both data and software (a program, i.e., a list of instructions for what to do with the data). At each time step, a central processing unit (CPU) executes the next instruction in the program, which specifies some simple function to apply to some part of the data. The part of the computer that keeps track of what to do next is merely another part of its memory, called the program counter, which stores the current line number in the program. To go to the next instruction, simply add one to the program counter. To jump to another line of the program, simply copy that line number into the program counter—this is how so-called “if” statements and loops are implemented.\n\nToday’s computers often gain additional speed by parallel processing, which cleverly undoes some of this reuse of modules: if a computation can be split into parts that can be done in parallel (because the input of one part doesn’t require the output of another), then the parts can be computed simultaneously by different parts of the hardware.\n\nThe ultimate parallel computer is a quantum computer. Quantum computing pioneer David Deutsch controversially argues that “quantum computers share information with huge numbers of versions of themselves throughout the multiverse,” and can get answers faster here in our Universe by in a sense getting help from these other versions.4 We don’t yet know whether a commercially competitive quantum computer can be built during the coming decades, because it depends both on whether quantum physics works as we think it does and on our ability to overcome daunting technical challenges, but companies and governments around the world are betting tens of millions of dollars annually on the possibility. Although quantum computers cannot speed up run-of-the-mill computations, clever algorithms have been developed that may dramatically speed up specific types of calculations, such as cracking cryptosystems and training neural networks. A quantum computer could also efficiently simulate the behavior of quantum-mechanical systems, including atoms, molecules and new materials, replacing measurements in chemistry labs in the same way that simulations on traditional computers have replaced measurements in wind tunnels.\n\nWhat Is Learning?\n\nAlthough a pocket calculator can crush me in an arithmetic contest, it will never improve its speed or accuracy, no matter how much it practices. It doesn’t learn: for example, every time I press its square-root button, it computes exactly the same function in exactly the same way. Similarly, the first computer program that ever beat me at chess never learned from its mistakes, but merely implemented a function that its clever programmer had designed to compute a good next move. In contrast, when Magnus Carlsen lost his first game of chess at age five, he began a learning process that made him the World Chess Champion eighteen years later.\n\nThe ability to learn is arguably the most fascinating aspect of general intelligence. We’ve already seen how a seemingly dumb clump of matter can remember and compute, but how can it learn? We’ve seen that finding the answer to a difficult question corresponds to computing a function, and that appropriately arranged matter can calculate any computable function. When we humans first created pocket calculators and chess programs, we did the arranging. For matter to learn, it must instead rearrange itself to get better and better at computing the desired function—simply by obeying the laws of physics.\n\nTo demystify the learning process, let’s first consider how a very simple physical system can learn the digits of π and other numbers. Above we saw how a surface with many valleys (see figure 2.3) can be used as a memory device: for example, if the bottom of one of the valleys is at position x = π ≈ 3.14159 and there are no other valleys nearby, then you can put a ball at x = 3 and watch the system compute the missing decimals by letting the ball roll down to the bottom. Now, suppose that the surface is made of soft clay and starts out completely flat, as a blank slate. If some math enthusiasts repeatedly place the ball at the locations of each of their favorite numbers, then gravity will gradually create valleys at these locations, after which the clay surface can be used to recall these stored memories. In other words, the clay surface has learned to compute digits of numbers such as π.\n\nOther physical systems, such as brains, can learn much more efficiently based on the same idea. John Hopfield showed that his above-mentioned network of interconnected neurons can learn in an analogous way: if you repeatedly put it into certain states, it will gradually learn these states and return to them from any nearby state. If you’ve seen each of your family members many times, then memories of what they look like can be triggered by anything related to them.\n\nNeural networks have now transformed both biological and artificial intelligence, and have recently started dominating the AI subfield known as machine learning (the study of algorithms that improve through experience). Before delving deeper into how such networks can learn, let’s first understand how they can compute. A neural network is simply a group of interconnected neurons that are able to influence each other’s behavior. Your brain contains about as many neurons as there are stars in our Galaxy: in the ballpark of a hundred billion. On average, each of these neurons is connected to about a thousand others via junctions called synapses, and it’s the strengths of these roughly hundred trillion synapse connections that encode most of the information in your brain.\n\nWe can schematically draw a neural network as a collection of dots representing neurons connected by lines representing synapses (see figure 2.9). Real-world neurons are very complicated electrochemical devices looking nothing like this schematic illustration: they involve different parts with names such as axons and dendrites, there are many different kinds of neurons that operate in a wide variety of ways, and the exact details of how and when electrical activity in one neuron affects other neurons is still the subject of active study. However, AI researchers have shown that neural networks can still attain human-level performance on many remarkably complex tasks even if one ignores all these complexities and replaces real biological neurons with extremely simple simulated ones that are all identical and obey very simple rules. The currently most popular model for such an artificial neural network represents the state of each neuron by a single number and the strength of each synapse by a single number. In this model, each neuron updates its state at regular time steps by simply averaging together the inputs from all connected neurons, weighting them by the synaptic strengths, optionally adding a constant, and then applying what’s called an activation function to the result to compute its next state.^(\\*5) The easiest way to use a neural network as a function is to make it feedforward, with information flowing only in one direction, as in figure 2.9, plugging the input to the function into a layer of neurons at the top and extracting the output from a layer of neurons at the bottom.\n\n[Figure 2.9: A network of neurons can compute functions just as a network of NAND gates can. For example, artificial neural networks have been trained to input numbers representing the brightness of different image pixels and output numbers representing the probability that the image depicts various people. Here each artificial neuron (circle) computes a weighted sum of the numbers sent to it via connections (lines) from above, applies a simple function and passes the result downward, each subsequent layer computing higher-level features. Typical face-recognition networks contain hundreds of thousands of neurons; the figure shows merely a handful for clarity.][Figure 2.9: A network of neurons can compute functions just as a network of NAND gates can. For example, artificial neural networks have been trained to input numbers representing the brightness of different image pixels and output numbers representing the probability that the image depicts various people. Here each artificial neuron (circle) computes a weighted sum of the numbers sent to it via connections (lines) from above, applies a simple function and passes the result downward, each subsequent layer computing higher-level features. Typical face-recognition networks contain hundreds of thousands of neurons; the figure shows merely a handful for clarity.]\n\nFigure 2.9: A network of neurons can compute functions just as a network of NAND gates can. For example, artificial neural networks have been trained to input numbers representing the brightness of different image pixels and output numbers representing the probability that the image depicts various people. Here each artificial neuron (circle) computes a weighted sum of the numbers sent to it via connections (lines) from above, applies a simple function and passes the result downward, each subsequent layer computing higher-level features. Typical face-recognition networks contain hundreds of thousands of neurons; the figure shows merely a handful for clarity.\n\nThe success of these simple artificial neural networks is yet another example of substrate independence: neural networks have great computational power seemingly independent of the low-level nitty-gritty details of their construction. Indeed, George Cybenko, Kurt Hornik, Maxwell Stinchcombe and Halbert White proved something remarkable in 1989: such simple neural networks are universal in the sense that they can compute any function arbitrarily accurately, by simply adjusting those synapse strength numbers accordingly. In other words, evolution probably didn’t make our biological neurons so complicated because it was necessary, but because it was more efficient—and because evolution, as opposed to human engineers, doesn’t reward designs that are simple and easy to understand.\n\nWhen I first learned about this, I was mystified by how something so simple could compute something arbitrarily complicated. For example, how can you compute even something as simple as multiplication, when all you’re allowed to do is compute weighted sums and apply a single fixed function? In case you’d like a taste of how this works, figure 2.10 shows how a mere five neurons can multiply two arbitrary numbers together, and how a single neuron can multiply three bits together.\n\nAlthough you can prove that you can compute anything in theory with an arbitrarily large neural network, the proof doesn’t say anything about whether you can do so in practice, with a network of reasonable size. In fact, the more I thought about it, the more puzzled I became that neural networks worked so well.\n\nFor example, suppose that we wish to classify megapixel grayscale images into two categories, say cats or dogs. If each of the million pixels can take one of, say, 256 values, then there are 256¹⁰⁰⁰⁰⁰⁰ possible images, and for each one, we wish to compute the probability that it depicts a cat. This means that an arbitrary function that inputs a picture and outputs a probability is defined by a list of 256¹⁰⁰⁰⁰⁰⁰ probabilities, that is, way more numbers than there are atoms in our Universe (about 10⁷⁸). Yet neural networks with merely thousands or millions of parameters somehow manage to perform such classification tasks quite well. How can successful neural networks be “cheap,” in the sense of requiring so few parameters? After all, you can prove that a neural network small enough to fit inside our Universe will epically fail to approximate almost all functions, succeeding merely on a ridiculously tiny fraction of all computational tasks that you might assign to it.\n\n[Figure 2.10: How matter can multiply, but using not NAND gates as in figure 2.7 but neurons. The key point doesn’t require following the details, and is that not only can neurons (artificial or biological) do math, but multiplication requires many fewer neurons than NAND gates. Optional details for hard-core math fans: Circles perform summation, squares apply the function σ , and lines multiply by the constants labeling them. The inputs are real numbers (left) and bits (right). The multiplication becomes arbitrarily accurate as a → 0 (left) and c → ∞ (right). The left network works for any function σ ( x ) that’s curved at the origin (with second derivative σ ″ (0) ≠ 0), which can be proven by Taylor expanding σ ( x ). The right network requires that the function σ ( x ) approaches 0 and 1 when x gets very small and very large, respectively, which is seen by noting that uvw = 1 only if u + v + w = 3. (These examples are from a paper I wrote with my students Henry Lin and David Rolnick, “Why Does Deep and Cheap Learning Work So Well?,” which can be found at http://arxiv.org/abs/1608.08225.) By combining together lots of multiplications (as above) and additions, you can compute any polynomials, which are well known to be able to approximate any smooth function.][Figure 2.10: How matter can multiply, but using not NAND gates as in figure 2.7 but neurons. The key point doesn’t require following the details, and is that not only can neurons (artificial or biological) do math, but multiplication requires many fewer neurons than NAND gates. Optional details for hard-core math fans: Circles perform summation, squares apply the function σ , and lines multiply by the constants labeling them. The inputs are real numbers (left) and bits (right). The multiplication becomes arbitrarily accurate as a → 0 (left) and c → ∞ (right). The left network works for any function σ ( x ) that’s curved at the origin (with second derivative σ ″ (0) ≠ 0), which can be proven by Taylor expanding σ ( x ). The right network requires that the function σ ( x ) approaches 0 and 1 when x gets very small and very large, respectively, which is seen by noting that uvw = 1 only if u + v + w = 3. (These examples are from a paper I wrote with my students Henry Lin and David Rolnick, “Why Does Deep and Cheap Learning Work So Well?,” which can be found at http://arxiv.org/abs/1608.08225.) By combining together lots of multiplications (as above) and additions, you can compute any polynomials, which are well known to be able to approximate any smooth function.]\n\nFigure 2.10: How matter can multiply, but using not NAND gates as in figure 2.7 but neurons. The key point doesn’t require following the details, and is that not only can neurons (artificial or biological) do math, but multiplication requires many fewer neurons than NAND gates. Optional details for hard-core math fans: Circles perform summation, squares apply the function σ, and lines multiply by the constants labeling them. The inputs are real numbers (left) and bits (right). The multiplication becomes arbitrarily accurate as a → 0 (left) and c → ∞ (right). The left network works for any function σ(x) that’s curved at the origin (with second derivative σ″(0)≠0), which can be proven by Taylor expanding σ(x). The right network requires that the function σ(x) approaches 0 and 1 when x gets very small and very large, respectively, which is seen by noting that uvw = 1 only if u + v + w = 3. (These examples are from a paper I wrote with my students Henry Lin and David Rolnick, “Why Does Deep and Cheap Learning Work So Well?,” which can be found at http://arxiv.org/abs/1608.08225.) By combining together lots of multiplications (as above) and additions, you can compute any polynomials, which are well known to be able to approximate any smooth function.\n\nI’ve had lots of fun puzzling over this and related mysteries with my student Henry Lin. One of the things I feel most grateful for in life is the opportunity to collaborate with amazing students, and Henry is one of them. When he first walked into my office to ask whether I was interested in working with him, I thought to myself that it would be more appropriate for me to ask whether he was interested in working with me: this modest, friendly and bright-eyed kid from Shreveport, Louisiana, had already written eight scientific papers, won a Forbes 30-Under-30 award, and given a TED talk with over a million views—and he was only twenty! A year later, we wrote a paper together with a surprising conclusion: the question of why neural networks work so well can’t be answered with mathematics alone, because part of the answer lies in physics. We found that the class of functions that the laws of physics throw at us and make us interested in computing is also a remarkably tiny class because, for reasons that we still don’t fully understand, the laws of physics are remarkably simple. Moreover, the tiny fraction of functions that neural networks can compute is very similar to the tiny fraction that physics makes us interested in! We also extended previous work showing that deep-learning neural networks (they’re called “deep” if they contain many layers) are much more efficient than shallow ones for many of these functions of interest. For example, together with another amazing MIT student, David Rolnick, we showed that the simple task of multiplying n numbers requires a whopping 2^(n) neurons for a network with only one layer, but takes only about 4n neurons in a deep network. This helps explain not only why neural networks are now all the rage among AI researchers, but also why we evolved neural networks in our brains: if we evolved brains to predict the future, then it makes sense that we’d evolve a computational architecture that’s good at precisely those computational problems that matter in the physical world.\n\nNow that we’ve explored how neural networks work and compute, let’s return to the question of how they can learn. Specifically, how can a neural network get better at computing by updating its synapses?\n\nIn his seminal 1949 book, The Organization of Behavior: A Neuropsychological Theory, the Canadian psychologist Donald Hebb argued that if two nearby neurons were frequently active (“firing”) at the same time, their synaptic coupling would strengthen so that they learned to help trigger each other—an idea captured by the popular slogan “Fire together, wire together.” Although the details of how actual brains learn are still far from understood, and research has shown that the answers are in many cases much more complicated, it’s also been shown that even this simple learning rule (known as Hebbian learning) allows neural networks to learn interesting things. John Hopfield showed that Hebbian learning allowed his oversimplified artificial neural network to store lots of complex memories by simply being exposed to them repeatedly. Such exposure to information to learn from is usually called “training” when referring to artificial neural networks (or to animals or people being taught skills), although “studying,” “education” or “experience” might be just as apt. The artificial neural networks powering today’s AI systems tend to replace Hebbian learning with more sophisticated learning rules with nerdy names such as “backpropagation” and “stochastic gradient descent,” but the basic idea is the same: there’s some simple deterministic rule, akin to a law of physics, by which the synapses get updated over time. As if by magic, this simple rule can make the neural network learn remarkably complex computations if training is performed with large amounts of data. We don’t yet know precisely what learning rules our brains use, but whatever the answer may be, there’s no indication that they violate the laws of physics.\n\nJust as most digital computers gain efficiency by splitting their work into multiple steps and reusing computational modules many times, so do many artificial and biological neural networks. Brains have parts that are what computer scientists call recurrent rather than feedforward neural networks, where information can flow in multiple directions rather than just one way, so that the current output can become input to what happens next. The network of logic gates in the microprocessor of a laptop is also recurrent in this sense: it keeps reusing its past information, and lets new information input from a keyboard, trackpad, camera, etc., affect its ongoing computation, which in turn determines information output to, say, a screen, loudspeaker, printer or wireless network. Analogously, the network of neurons in your brain is recurrent, letting information input from your eyes, ears and other senses affect its ongoing computation, which in turn determines information output to your muscles.\n\nThe history of learning is at least as long as the history of life itself, since every self-reproducing organism performs interesting copying and processing of information—behavior that has somehow been learned. During the era of Life 1.0, however, organisms didn’t learn during their lifetime: their rules for processing information and reacting were determined by their inherited DNA, so the only learning occurred slowly at the species level, through Darwinian evolution across generations.\n\nAbout half a billion years ago, certain gene lines here on Earth discovered a way to make animals containing neural networks, able to learn behaviors from experiences during life. Life 2.0 had arrived, and because of its ability to learn dramatically faster and outsmart the competition, it spread like wildfire across the globe. As we explored in chapter 1, life has gotten progressively better at learning, and at an ever-increasing rate. A particular ape-like species grew a brain so adept at acquiring knowledge that it learned how to use tools, make fire, speak a language and create a complex global society. This society can itself be viewed as a system that remembers, computes and learns, all at an accelerating pace as one invention enables the next: writing, the printing press, modern science, computers, the internet and so on. What will future historians put next on that list of enabling inventions? My guess is artificial intelligence.\n\nAs we all know, the explosive improvements in computer memory and computational power (figure 2.4 and figure 2.8) have translated into spectacular progress in artificial intelligence—but it took a long time until machine learning came of age. When IBM’s Deep Blue computer overpowered chess champion Garry Kasparov in 1997, its major advantages lay in memory and computation, not in learning. Its computational intelligence had been created by a team of humans, and the key reason that Deep Blue could outplay its creators was its ability to compute faster and thereby analyze more potential positions. When IBM’s Watson computer dethroned the human world champion in the quiz show Jeopardy!, it too relied less on learning than on custom-programmed skills and superior memory and speed. The same can be said of most early breakthroughs in robotics, from legged locomotion to self-driving cars and self-landing rockets.\n\nIn contrast, the driving force behind many of the most recent AI breakthroughs has been machine learning. Consider figure 2.11, for example. It’s easy for you to tell what it’s a photo of, but to program a function that inputs nothing but the colors of all the pixels of an image and outputs an accurate caption such as “A group of young people playing a game of frisbee” had eluded all the world’s AI researchers for decades. Yet a team at Google led by Ilya Sutskever did precisely that in 2014. Input a different set of pixel colors, and it replies “A herd of elephants walking across a dry grass field,” again correctly. How did they do it? Deep Blue–style, by programming handcrafted algorithms for detecting frisbees, faces and the like? No, by creating a relatively simple neural network with no knowledge whatsoever about the physical world or its contents, and then letting it learn by exposing it to massive amounts of data. AI visionary Jeff Hawkins wrote in 2004 that “no computer can…see as well as a mouse,” but those days are now long gone.\n\n[Figure 2.11: “A group of young people playing a game of frisbee”—that caption was written by a computer with no understanding of people, games or frisbees.][Figure 2.11: “A group of young people playing a game of frisbee”—that caption was written by a computer with no understanding of people, games or frisbees.]\n\nFigure 2.11: “A group of young people playing a game of frisbee”—that caption was written by a computer with no understanding of people, games or frisbees.\n\nJust as we don’t fully understand how our children learn, we still don’t fully understand how such neural networks learn, and why they occasionally fail. But what’s clear is that they’re already highly useful and are triggering a surge of investments in deep learning. Deep learning has now transformed many aspects of computer vision, from handwriting transcription to real-time video analysis for self-driving cars. It has similarly revolutionized the ability of computers to transform spoken language into text and translate it into other languages, even in real time—which is why we can now talk to personal digital assistants such as Siri, Google Now and Cortana. Those annoying CAPTCHA puzzles, where we need to convince a website that we’re human, are getting ever more difficult in order to keep ahead of what machine-learning technology can do. In 2015, Google DeepMind released an AI system using deep learning that was able to master dozens of computer games like a kid would—with no instructions whatsoever—except that it soon learned to play better than any human. In 2016, the same company built AlphaGo, a Go-playing computer system that used deep learning to evaluate the strength of different board positions and defeated the world’s strongest Go champion. This progress is fueling a virtuous circle, bringing ever more funding and talent into AI research, which generates further progress.\n\nWe’ve spent this chapter exploring the nature of intelligence and its development up until now. How long will it take until machines can out-compete us at all cognitive tasks? We clearly don’t know, and need to be open to the possibility that the answer may be “never.” However, a basic message of this chapter is that we also need to consider the possibility that it will happen, perhaps even in our lifetime. After all, matter can be arranged so that when it obeys the laws of physics, it remembers, computes and learns—and the matter doesn’t need to be biological. AI researchers have often been accused of over-promising and under-delivering, but in fairness, some of their critics don’t have the best track record either. Some keep moving the goalposts, effectively defining intelligence as that which computers still can’t do, or as that which impresses us. Machines are now good or excellent at arithmetic, chess, mathematical theorem proving, stock picking, image captioning, driving, arcade game playing, Go, speech synthesis, speech transcription, translation and cancer diagnosis, but some critics will scornfully scoff “Sure—but that’s not real intelligence!” They might go on to argue that real intelligence involves only the mountaintops in Moravec’s landscape (figure 2.2) that haven’t yet been submerged, just as some people in the past used to argue that image captioning and Go should count—while the water kept rising.\n\nAssuming that the water will keep rising for at least a while longer, AI’s impact on society will keep growing. Long before AI reaches human level across all tasks, it will give us fascinating opportunities and challenges involving issues such as bugs, laws, weapons and jobs. What are they and how can we best prepare for them? Let’s explore this in the next chapter.\n\n \n\n------------------------------------------------------------------------\n\nTHE BOTTOM LINE:\n\n• Intelligence, defined as ability to accomplish complex goals, can’t be measured by a single IQ, only by an ability spectrum across all goals.\n\n• Today’s artificial intelligence tends to be narrow, with each system able to accomplish only very specific goals, while human intelligence is remarkably broad.\n\n• Memory, computation, learning and intelligence have an abstract, intangible and ethereal feel to them because they’re substrate-independent: able to take on a life of their own that doesn’t depend on or reflect the details of their underlying material substrate.\n\n• Any chunk of matter can be the substrate for memory as long as it has many different stable states.\n\n• Any matter can be computronium, the substrate for computation, as long as it contains certain universal building blocks that can be combined to implement any function. NAND gates and neurons are two important examples of such universal “computational atoms.”\n\n• A neural network is a powerful substrate for learning because, simply by obeying the laws of physics, it can rearrange itself to get better and better at implementing desired computations.\n\n• Because of the striking simplicity of the laws of physics, we humans only care about a tiny fraction of all imaginable computational problems, and neural networks tend to be remarkably good at solving precisely this tiny fraction.\n\n• Once technology gets twice as powerful, it can often be used to design and build technology that’s twice as powerful in turn, triggering repeated capability doubling in the spirit of Moore’s law. The cost of information technology has now halved roughly every two years for about a century, enabling the information age.\n\n• If AI progress continues, then long before AI reaches human level for all skills, it will give us fascinating opportunities and challenges involving issues such as bugs, laws, weapons and jobs—which we’ll explore in the next chapter.\n\n------------------------------------------------------------------------\n\n \n\n------------------------------------------------------------------------\n\n\\*1 To see this, imagine how you’d react if someone claimed that the ability to accomplish Olympic-level athletic feats could be quantified by a single number called the “athletic quotient,” or AQ for short, so that the Olympian with the highest AQ would win the gold medals in all the sports.\n\n\\*2 Some people prefer “human-level AI” or “strong AI” as synonyms for AGI, but both are problematic. Even a pocket calculator is a human-level AI in the narrow sense. The antonym of “strong AI” is “weak AI,” but it feels odd to call narrow AI systems such as Deep Blue, Watson, and AlphaGo “weak.”\n\n\\*3 NAND is short for NOT AND: An AND gate outputs 1 only if the first input is 1 and the second input is 1, so NAND outputs the exact opposite.\n\n\\*4 I’m using “well-defined function” to mean what mathematicians and computer scientists call a “computable function,” i.e., a function that could be computed by some hypothetical computer with unlimited memory and time. Alan Turing and Alonzo Church famously proved that there are also functions that can be described but aren’t computable.\n\n\\*5 In case you like math, two popular choices of this activation function are the so-called sigmoid function σ(x) ≡ 1/(1 + e⁻^(x)) and the ramp function σ(x) = max{0, x}, although it’s been proven that almost any function will suffice as long as it’s not linear (a straight line). Hopfield’s famous model uses σ(x) = −1 if x < 0 and σ(x) = 1 if x ≥ 0. If the neuron states are stored in a vector, then the network is updated by simply multiplying that vector by a matrix storing the synaptic couplings and then applying the function σ to all elements.\n\nChapter 3\n\nThe Near Future: Breakthroughs, Bugs, Laws, Weapons and Jobs\n\n If we don’t change direction soon, we’ll end up where we’re going.\n\n Irwin Corey\n\nWhat does it mean to be human in the present day and age? For example, what is it that we really value about ourselves, that makes us different from other life forms and machines? What do other people value about us that makes some of them willing to offer us jobs? Whatever our answers are to these questions at any one time, it’s clear that the rise of technology must gradually change them.\n\nTake me, for instance. As a scientist, I take pride in setting my own goals, in using creativity and intuition to tackle a broad range of unsolved problems, and in using language to share what I discover. Fortunately for me, society is willing to pay me to do this as a job. Centuries ago, I might instead, like many others, have built my identity around being a farmer or craftsman, but the growth of technology has since reduced such professions to a tiny fraction of the workforce. This means that it’s no longer possible for everyone to build their identity around farming or crafts.\n\nPersonally, it doesn’t bother me that today’s machines outclass me at manual skills such as digging and knitting, since these are neither hobbies of mine nor my sources of income or self-worth. Indeed, any delusions I may have held about my abilities in that regard were crushed at age eight, when my school forced me to take a knitting class which I nearly flunked, and I completed my project only thanks to a compassionate helper from fifth grade taking pity on me.\n\nBut as technology keeps improving, will the rise of AI eventually eclipse also those abilities that provide my current sense of self-worth and value on the job market? Stuart Russell told me that he and many of his fellow AI researchers had recently experienced a “holy shit!” moment, when they witnessed AI doing something they weren’t expecting to see for many years. In that spirit, please let me tell you about a few of my own HS moments, and how I see them as harbingers of human abilities soon to be overtaken.\n\nBreakthroughs\n\nDeep Reinforcement Learning Agents\n\nI experienced one of my major jaw drops in 2014 while watching a video of a DeepMind AI system learning to play computer games. Specifically, the AI was playing Breakout (see figure 3.1), a classic Atari game I remember fondly from my teens. The goal is to maneuver a paddle so as to repeatedly bounce a ball off a brick wall; every time you hit a brick, it disappears and your score increases.\n\n[Figure 3.1: After learning to play the Atari game Breakout from scratch, using deep reinforcement learning to maximize the score, the DeepMind AI discovered the optimal strategy: drilling a hole through the leftmost part of the brick wall and letting the ball keep bouncing around behind it, amassing points very rapidly. I’ve drawn arrows showing the past trajectories of ball and paddle.][Figure 3.1: After learning to play the Atari game Breakout from scratch, using deep reinforcement learning to maximize the score, the DeepMind AI discovered the optimal strategy: drilling a hole through the leftmost part of the brick wall and letting the ball keep bouncing around behind it, amassing points very rapidly. I’ve drawn arrows showing the past trajectories of ball and paddle.]\n\nFigure 3.1: After learning to play the Atari game Breakout from scratch, using deep reinforcement learning to maximize the score, the DeepMind AI discovered the optimal strategy: drilling a hole through the leftmost part of the brick wall and letting the ball keep bouncing around behind it, amassing points very rapidly. I’ve drawn arrows showing the past trajectories of ball and paddle.\n\nI’d written some computer games of my own back in the day, and was well aware that it wasn’t hard to write a program that could play Breakout—but this was not what the DeepMind team had done. Instead, they’d created a blank-slate AI that knew nothing about this game—or about any other games, or even about concepts such as games, paddles, bricks or balls. All their AI knew was that a long list of numbers got fed into it at regular intervals: the current score and a long list of numbers which we (but not the AI) would recognize as specifications of how different parts of the screen were colored. The AI was simply told to maximize the score by outputting, at regular intervals, numbers which we (but not the AI) would recognize as codes for which keys to press.\n\nInitially, the AI played terribly: it cluelessly jiggled the paddle back and forth seemingly at random and missed the ball almost every time. After a while, it seemed to be getting the idea that moving the paddle toward the ball was a good idea, even though it still missed most of the time. But it kept improving with practice, and soon got better at the game than I’d ever been, infallibly returning the ball no matter how fast it approached. And then my jaw dropped: it figured out this amazing score-maximizing strategy of always aiming for the upper-left corner to drill a hole through the wall and let the ball get stuck bouncing between the back of the wall and the barrier behind it. This felt like a really intelligent thing to do. Indeed, Demis Hassabis later told me that the programmers on that DeepMind team didn’t know this trick until they learned it from the AI they’d built. I recommend watching a video of this for yourself at the link I’ve provided.1\n\nThere was a human-like feature to this that I found somewhat unsettling: I was watching an AI that had a goal and learned to get ever better at achieving it, eventually outperforming its creators. In the previous chapter, we defined intelligence as simply the ability to accomplish complex goals, so in this sense, DeepMind’s AI was growing more intelligent in front of my eyes (albeit merely in the very narrow sense of playing this particular game). In the first chapter, we encountered what computer scientists call intelligent agents: entities that collect information about their environment from sensors and then process this information to decide how to act back on their environment. Although DeepMind’s game-playing AI lived in an extremely simple virtual world composed of bricks, paddles and balls, I couldn’t deny that it was an intelligent agent.\n\nDeepMind soon published their method and shared their code, explaining that it used a very simple yet powerful idea called deep reinforcement learning.2 Basic reinforcement learning is a classic machine learning technique inspired by behaviorist psychology, where getting a positive reward increases your tendency to do something again and vice versa. Just like a dog learns to do tricks when this increases the likelihood of its getting encouragement or a snack from its owner soon, DeepMind’s AI learned to move the paddle to catch the ball because this increased the likelihood of its getting more points soon. DeepMind combined this idea with deep learning: they trained a deep neural net, as in the previous chapter, to predict how many points would on average be gained by pressing each of the allowed keys on the keyboard, and then the AI selected whatever key the neural net rated as most promising given the current state of the game.\n\nWhen I listed traits contributing to my own personal feeling of self-worth as a human, I included the ability to tackle a broad range of unsolved problems. In contrast, being able to play Breakout and do nothing else constitutes extremely narrow intelligence. To me, the true importance of DeepMind’s breakthrough is that deep reinforcement learning is a completely general technique. Sure enough, they let the exact same AI practice playing forty-nine different Atari games, and it learned to outplay their human testers on twenty-nine of them, from Pong to Boxing, Video Pinball and Space Invaders.\n\nIt didn’t take long until the same AI idea had started proving itself on more modern games whose worlds were three-dimensional rather than two-dimensional. Soon DeepMind’s San Francisco–based competitors at OpenAI released a platform called Universe, where DeepMind’s AI and other intelligent agents can practice interacting with an entire computer as if it were a game: clicking on anything, typing anything, and opening and running whatever software they’re able to navigate—firing up a web browser and messing around online, for example.\n\nLooking to the future of deep reinforcement learning and improvements thereupon, there’s no obvious end in sight. The potential isn’t limited to virtual game worlds, since if you’re a robot, life itself can be viewed as a game. Stuart Russell told me that his first major HS moment was watching the robot Big Dog run up a snow-covered forest slope, elegantly solving the legged locomotion problem that he himself had struggled to solve for many years.3 Yet when that milestone was reached in 2008, it involved huge amounts of work by clever programmers. After DeepMind’s breakthrough, there’s no reason why a robot can’t ultimately use some variant of deep reinforcement learning to teach itself to walk without help from human programmers: all that’s needed is a system that gives it points whenever it makes progress. Robots in the real world similarly have the potential to learn to swim, fly, play ping-pong, fight and perform a nearly endless list of other motor tasks without help from human programmers. To speed things up and reduce the risk of getting stuck or damaging themselves during the learning process, they would probably do the first stages of their learning in virtual reality.\n\nIntuition, Creativity and Strategy\n\nAnother defining moment for me was when the DeepMind AI system AlphaGo won a five-game Go match against Lee Sedol, generally considered the top player in the world in the early twenty-first century.\n\nIt was widely expected that human Go players would be dethroned by machines at some point, since it had happened to their chess-playing colleagues two decades earlier. However, most Go pundits predicted that it would take another decade, so AlphaGo’s triumph was a pivotal moment for them as well as for me. Nick Bostrom and Ray Kurzweil have both emphasized how hard it can be to see AI breakthroughs coming, which is evident from interviews with Lee Sedol himself before and after losing the first three games:\n\n• October 2015: “Based on its level seen…I think I will win the game by a near landslide.”\n\n• February 2016: “I have heard that Google DeepMind’s AI is surprisingly strong and getting stronger, but I am confident that I can win at least this time.”\n\n• March 9, 2016: “I was very surprised because I didn’t think I would lose.”\n\n• March 10, 2016: “I’m quite speechless…I am in shock. I can admit that…the third game is not going to be easy for me.”\n\n• March 12, 2016: “I kind of felt powerless.”\n\nWithin a year after playing Lee Sedol, a further improved AlphaGo had played all twenty top players in the world without losing a single match.\n\nWhy was this such a big deal for me personally? Well, I confessed above that I view intuition and creativity as two of my core human traits, and as I’ll now explain, I feel that AlphaGo displayed both.\n\nGo players take turns placing black and white stones on a 19-by-19 board (see figure 3.2). There are vastly more possible Go positions than there are atoms in our Universe, which means that trying to analyze all interesting sequences of future moves rapidly gets hopeless. Players therefore rely heavily on subconscious intuition to complement their conscious reasoning, with experts developing an almost uncanny feel for which positions are strong and which are weak. As we saw in the last chapter, the results of deep learning are sometimes reminiscent of intuition: a deep neural network might determine that an image portrays a cat without being able to explain why. The DeepMind team therefore gambled on the idea that deep learning might be able to recognize not merely cats, but also strong Go positions. The core idea that they built into AlphaGo was to marry the intuitive power of deep learning with the logical power of GOFAI—which stands for what’s humorously known as “Good Old-Fashioned AI” from before the deep-learning revolution. They used a massive database of Go positions from both human play and games where AlphaGo had played a clone of itself, and trained a deep neural network to predict from each position the probability that white would ultimately win. They also trained a separate network to predict likely next moves. They then combined these networks with a GOFAI method that cleverly searched through a pruned list of likely future-move sequences to identify the next move that would lead to the strongest position down the road.\n\n[Figure 3.2: DeepMind’s AlphaGo AI made a highly creative move on line 5, in defiance of millennia of human wisdom, which about fifty moves later proved crucial to its defeat of Go legend Lee Sedol.][Figure 3.2: DeepMind’s AlphaGo AI made a highly creative move on line 5, in defiance of millennia of human wisdom, which about fifty moves later proved crucial to its defeat of Go legend Lee Sedol.]\n\nFigure 3.2: DeepMind’s AlphaGo AI made a highly creative move on line 5, in defiance of millennia of human wisdom, which about fifty moves later proved crucial to its defeat of Go legend Lee Sedol.\n\nThis marriage of intuition and logic gave birth to moves that were not merely powerful, but in some cases also highly creative. For example, millennia of Go wisdom dictate that early in the game, it’s best to play on the third or fourth line from an edge. There’s a trade-off between the two: playing on the third line helps with short-term territory gain toward the side of the board, while playing on the fourth helps with long-term strategic influence toward the center.\n\nIn the thirty-seventh move of the second game, AlphaGo shocked the Go world by defying that ancient wisdom and playing on the fifth line (figure 3.2), as if it were even more confident than a human in its long-term planning abilities and therefore favored strategic advantage over short-term gain. Commentators were stunned, and Lee Sedol even got up and temporarily left the room.4 Sure enough, about fifty moves later, fighting from the lower left-hand corner of the board ended up spilling over and connecting with that black stone from move thirty-seven! And that motif is what ultimately won the game, cementing the legacy of AlphaGo’s fifth-row move as one of the most creative in Go history.\n\nBecause of its intuitive and creative aspects, Go is viewed more as an art form than just another game. It was considered one of the four “essential arts” in ancient China, together with painting, calligraphy and qin music, and it remains hugely popular in Asia, with almost 300 million people watching the first game between AlphaGo and Lee Sedol. As a result, the Go world was quite shaken by the outcome, and viewed AlphaGo’s victory as a profound milestone for humanity. Ke Jie, the world’s top-ranked Go player at the time, had this to say:5 “Humanity has played Go for thousands of years, and yet, as AI has shown us, we have not yet even scratched the surface…The union of human and computer players will usher in a new era…Together, man and AI can find the truth of Go.” Such fruitful human-machine collaboration indeed appears promising in many areas, including science, where AI can hopefully help us humans deepen our understanding and realize our ultimate potential.\n\nTo me, AlphaGo also teaches us another important lesson for the near future: combining the intuition of deep learning with the logic of GOFAI can produce second-to-none strategy. Because Go is one of the ultimate strategy games, AI is now poised to graduate and challenge (or help) the best human strategists even beyond game boards—for example with investment strategy, political strategy and military strategy. Such real-world strategy problems are typically complicated by human psychology, missing information and factors that need to be modeled as random, but poker-playing AI systems have already demonstrated that none of these challenges are insurmountable.\n\nNatural Language\n\nYet another area where AI progress has recently stunned me is language. I fell in love with travel early in life, and curiosity about other cultures and languages formed an important part of my identity. I was raised speaking Swedish and English, was taught German and Spanish in school, learned Portuguese and Romanian through two marriages and taught myself some Russian, French and Mandarin for fun.\n\nBut the AI has been reaching, and after an important discovery in 2016, there are almost no lazy languages that I can translate between better than the system of the AI developed by the equipment of the brain of Google.\n\nDid I make myself crystal clear? I was actually trying to say this:\n\nBut AI has been catching up with me, and after a major breakthrough in 2016, there are almost no languages left that I can translate between better than the AI system developed by the Google Brain team.\n\nHowever, I first translated it to Spanish and back using an app that I installed on my laptop a few years ago. In 2016, the Google Brain team upgraded their free Google Translate service to use deep recurrent neural networks, and the improvement over older GOFAI systems was dramatic:6\n\nBut AI has been catching up on me, and after a breakthrough in 2016, there are almost no languages left that can translate between better than the AI system developed by the Google Brain team.\n\nAs you can see, the pronoun “I” got lost during the Spanish detour, which unfortunately changed the meaning. Close, but no cigar! However, in defense of Google’s AI, I’m often criticized for writing unnecessarily long sentences that are hard to parse, and I picked one of my most confusingly convoluted ones for this example. For more typical sentences, their AI often translates impeccably. As a result, it created quite a stir when it came out, and it’s helpful enough to be used by hundreds of millions of people daily. Moreover, courtesy of recent progress in deep learning for speech-to-text and text-to-speech conversion, these users can now speak to their smartphones in one language and listen to the translated result.\n\nNatural language processing is now one of the most rapidly advancing fields of AI, and I think that further success will have a large impact because language is so central to being human. The better an AI gets at linguistic prediction, the better it can compose reasonable email responses or continue a spoken conversation. This might, at least to an outsider, give the appearance of human thought taking place. Deep-learning systems are thus taking baby steps toward passing the famous Turing test, where a machine has to converse well enough in writing to trick a person into thinking that it too is human.\n\nLanguage-processing AI still has a long way to go, though. Although I must confess that I feel a bit deflated when I’m out-translated by an AI, I feel better once I remind myself that, so far, it doesn’t understand what it’s saying in any meaningful sense. From being trained on massive data sets, it discovers patterns and relations involving words without ever relating these words to anything in the real world. For example, it might represent each word by a list of a thousand numbers that specify how similar it is to certain other words. It may then conclude from this that the difference between “king” and “queen” is similar to that between “husband” and “wife”—but it still has no clue what it means to be male or female, or even that there is such a thing as a physical reality out there with space, time and matter.\n\nSince the Turing test is fundamentally about deception, it has been criticized for testing human gullibility more than true artificial intelligence. In contrast, a rival test called the Winograd Schema Challenge goes straight for the jugular, homing in on that commonsense understanding that current deep-learning systems tend to lack. We humans routinely use real-world knowledge when parsing a sentence, to figure out what a pronoun refers to. For example, a typical Winograd challenge asks what “they” refers to here:\n\n1. “The city councilmen refused the demonstrators a permit because they feared violence.”\n\n2. “The city councilmen refused the demonstrators a permit because they advocated violence.”\n\nThere’s an annual AI competition to answer such questions, and AIs still perform miserably.7 This precise challenge, understanding what refers to what, torpedoed even GoogleTranslate when I replaced Spanish with Chinese in my example above:\n\nBut the AI has caught up with me, after a major break in 2016, with almost no language, I could translate the AI system than developed by the Google Brain team.\n\nPlease try it yourself at https://translate.google.com now that you’re reading the book and see if Google’s AI has improved! There’s a good chance that it has, since there are promising approaches out there for marrying deep recurrent neural nets with GOFAI to build a language-processing AI that includes a world model.\n\nOpportunities and Challenges\n\nThese three examples were obviously just a sampler, since AI is progressing rapidly across many important fronts. Moreover, although I’ve mentioned only two companies in these examples, competing research groups at universities and other companies often weren’t far behind. A loud sucking noise can be heard in computer science departments around the world as Apple, Baidu, DeepMind, Facebook, Google, Microsoft and others use lucrative offers to vacuum off students, postdocs and faculty.\n\nIt’s important not to be misled by the examples I’ve given into viewing the history of AI as periods of stagnation punctuated by the occasional breakthrough. From my vantage point, I’ve instead been seeing fairly steady progress for a long time—which the media report as a breakthrough whenever it crosses the threshold of enabling a new imagination-grabbing application or useful product. I therefore consider it likely that brisk AI progress will continue for many years. Moreover, as we saw in the last chapter, there’s no fundamental reason why this progress can’t continue until AI matches human abilities on most tasks.\n\nWhich raises the question: How will this impact us? How will near-term AI progress change what it means to be human? We’ve seen that it’s getting progressively harder to argue that AI completely lacks goals, breadth, intuition, creativity or language—traits that many feel are central to being human. This means that even in the near term, long before any AGI can match us at all tasks, AI might have a dramatic impact on how we view ourselves, on what we can do when complemented by AI and on what we can earn money doing when competing against AI. Will this impact be for the better or for the worse? What near-term opportunities and challenges will this present?\n\nEverything we love about civilization is the product of human intelligence, so if we can amplify it with artificial intelligence, we obviously have the potential to make life even better. Even modest progress in AI might translate into major improvements in science and technology and corresponding reductions of accidents, disease, injustice, war, drudgery and poverty. But in order to reap these benefits of AI without creating new problems, we need to answer many important questions. For example:\n\n1. How can we make future AI systems more robust than today’s, so that they do what we want without crashing, malfunctioning or getting hacked?\n\n2. How can we update our legal systems to be more fair and efficient and to keep pace with the rapidly changing digital landscape?\n\n3. How can we make weapons smarter and less prone to killing innocent civilians without triggering an out-of-control arms race in lethal autonomous weapons?\n\n4. How can we grow our prosperity through automation without leaving people lacking income or purpose?\n\nLet’s devote the rest of this chapter to exploring each of these questions in turn. These four near-term questions are aimed mainly at computer scientists, legal scholars, military strategists and economists, respectively. However, to help get the answers we need by the time we need them, everybody needs to join this conversation, because as we’ll see, the challenges transcend all traditional boundaries—both between specialties and between nations.\n\nBugs vs. Robust AI\n\nInformation technology has already had great positive impact on virtually every sector of our human enterprise, from science to finance, manufacturing, transportation, healthcare, energy and communication, and this impact pales in comparison to the progress that AI has the potential to bring. But the more we come to rely on technology, the more important it becomes that it’s robust and trustworthy, doing what we want it to do.\n\nThroughout human history, we’ve relied on the same tried-and-true approach to keeping our technology beneficial: learning from mistakes. We invented fire, repeatedly messed up, and then invented the fire extinguisher, fire exit, fire alarm and fire department. We invented the automobile, repeatedly crashed, and then invented seat belts, air bags and self-driving cars. Up until now, our technologies have typically caused sufficiently few and limited accidents for their harm to be outweighed by their benefits. As we inexorably develop ever more powerful technology, however, we’ll inevitably reach a point where even a single accident could be devastating enough to outweigh all benefits. Some argue that accidental global nuclear war would constitute such an example. Others argue that a bioengineered pandemic could qualify, and in the next chapter, we’ll explore the controversy around whether future AI could cause human extinction. But we need not consider such extreme examples to reach a crucial conclusion: as technology grows more powerful, we should rely less on the trial-and-error approach to safety engineering. In other words, we should become more proactive than reactive, investing in safety research aimed at preventing accidents from happening even once. This is why society invests more in nuclear-reactor safety than mousetrap safety.\n\nThis is also the reason why, as we saw in chapter 1, there was strong community interest in AI-safety research at the Puerto Rico conference. Computers and AI systems have always crashed, but this time is different: AI is gradually entering the real world, and it’s not merely a nuisance if it crashes the power grid, the stock market or a nuclear weapons system. In the rest of this section, I want to introduce you to the four main areas of technical AI-safety research that are dominating the current AI-safety discussion and that are being pursued around the world: verification, validation, security and control.^(\\*1) To prevent things from getting too nerdy and dry, let’s do this by exploring past successes and failures of information technology in different areas, as well as valuable lessons we can learn from them and research challenges that they pose.\n\nAlthough most of these stories are old, involving low-tech computer systems that almost nobody would refer to as AI and that caused few, if any, casualties, we’ll see that they nonetheless teach us valuable lessons for designing safe and powerful future AI systems whose failures could be truly catastrophic.\n\nAI for Space Exploration\n\nLet’s start with something close to my heart: space exploration. Computer technology has enabled us to fly people to the Moon and to send unmanned spacecraft to explore all the planets of our Solar System, even landing on Saturn’s moon Titan and on a comet. As we’ll explore in chapter 6, future AI may help us explore other solar systems and galaxies—if it’s bug-free. On June 4, 1996, scientists hoping to research Earth’s magnetosphere cheered jubilantly as an Ariane 5 rocket from the European Space Agency roared into the sky with the scientific instruments they’d built. Thirty-seven seconds later, their smiles vanished as the rocket exploded in a fireworks display costing hundreds of millions of dollars.8 The cause was found to be buggy software manipulating a number that was too large to fit into the 16 bits allocated for it.9 Two years later, NASA’s Mars Climate Orbiter accidentally entered the Red Planet’s atmosphere and disintegrated because two different parts of the software used different units for force, causing a 445% error in the rocket-engine thrust control.10 This was NASA’s second super-expensive bug: their Mariner 1 mission to Venus exploded after launch from Cape Canaveral on July 22, 1962, after the flight-control software was foiled by an incorrect punctuation mark.11 As if to show that not only westerners had mastered the art of launching bugs into space, the Soviet Phobos 1 mission failed on September 2, 1988. This was the heaviest interplanetary spacecraft ever launched, with the spectacular goal of deploying a lander on Mars’ moon Phobos—all thwarted when a missing hyphen caused the “end-of-mission” command to be sent to the spacecraft while it was en route to Mars, shutting down all of its systems.12\n\nWhat we learn from these examples is the importance of what computer scientists call verification: ensuring that software fully satisfies all the expected requirements. The more lives and resources are at stake, the higher confidence we want that the software will work as intended. Fortunately, AI can help automate and improve the verification process. For example, a complete, general-purpose operating-system kernel called seL4 has recently been mathematically checked against a formal specification to give a strong guarantee against crashes and unsafe operations: although it doesn’t yet come with the bells and whistles of Microsoft Windows and Mac OS, you can rest assured that it won’t give you what’s affectionately known as “the blue screen of death” or “the spinning wheel of doom.” The U.S. Defense Advanced Research Projects Agency (DARPA) has sponsored the development of a set of open-source high-assurance tools called HACMS (high-assurance cyber military systems) that are provably safe. An important challenge is to make such tools sufficiently powerful and easy to use that they’ll get widely deployed. Another challenge is that the very task of verification will itself get more difficult as software moves into robots and new environments, and as traditional preprogrammed software gets replaced by AI systems that keep learning, thereby changing their behavior, as in chapter 2.\n\nAI for Finance\n\nFinance is another area that’s been transformed by information technology, allowing resources to be efficiently reallocated across the globe at the speed of light and enabling affordable financing for everything from mortgages to startup companies. Progress in AI is likely to offer great future profit opportunities from financial trading: most stock market buy/sell decisions are now made automatically by computers, and my graduating MIT students routinely get tempted by astronomical starting salaries to improve algorithmic trading.\n\nVerification is important for financial software as well, which the American firm Knight Capital learned the hard way on August 1, 2012, by losing $440 million in forty-five minutes after deploying unverified trading software.13 The trillion-dollar “Flash Crash” of May 6, 2010, was noteworthy for a different reason. Although it caused massive disruptions for about half an hour before markets stabilized, with shares of some prominent companies such Procter & Gamble swinging in price between a penny and $100,000,14 the problem wasn’t caused by bugs or computer malfunctions that verification could have avoided. Instead, it was caused by expectations being violated: automatic trading programs from many companies found themselves operating in an unexpected situation where their assumptions weren’t valid—for example, the assumption that if a stock exchange computer reported that a stock had a price of one cent, then that stock really was worth one cent.\n\nThe flash crash illustrates the importance of what computer scientists call validation: whereas verification asks “Did I build the system right?,” validation asks “Did I build the right system?”^(\\*2) For example, does the system rely on assumptions that might not always be valid? If so, how can it be improved to better handle uncertainty?\n\nAI for Manufacturing\n\nNeedless to say, AI holds great potential for improving manufacturing, by controlling robots that enhance both efficiency and precision. Ever-improving 3-D printers can now make prototypes of anything from office buildings to micromechanical devices smaller than a salt grain.15 While huge industrial robots build cars and airplanes, affordable computer-controlled mills, lathes, cutters and the like are powering not merely factories, but also the grassroots “maker movement,” where local enthusiasts materialize their ideas at over a thousand community-run “fab labs” around the world.16 But the more robots we have around us, the more important it becomes that we verify and validate their software. The first person known to have been killed by a robot was Robert Williams, a worker at a Ford plant in Flat Rock, Michigan. In 1979, a robot that was supposed to retrieve parts from a storage area malfunctioned, and he climbed into the area to get the parts himself. The robot silently began operating and smashed his head, continuing for thirty minutes until his co-workers discovered what had happened.17 The next robot victim was Kenji Urada, a maintenance engineer at a Kawasaki plant in Akashi, Japan. While working on a broken robot in 1981, he accidentally hit its on switch and was crushed to death by the robot’s hydraulic arm.18 In 2015, a twenty-two-year-old contractor at one of Volkswagen’s production plants in Baunatal, Germany, was working on setting up a robot to grab auto parts and manipulate them. Something went wrong, causing the robot to grab him and crush him to death against a metal plate.19\n\nAlthough these accidents are tragic, it’s important to note that they make up a minuscule fraction of all industrial accidents. Moreover, industrial accidents have decreased rather than increased as technology has improved, dropping from about 14,000 deaths in 1970 to 4,821 in 2014 in the United States.20 The three above-mentioned accidents show that adding intelligence to otherwise dumb machines should be able to further improve industrial safety, by having robots learn to be more careful around people. All three accidents could have been avoided with better validation: the robots caused harm not because of bugs or malice, but because they made invalid assumptions—that the person wasn’t present or that the person was an auto part.\n\n[Figure 3.3: Whereas traditional industrial robots are expensive and hard to program, there’s a trend toward cheaper AI-powered ones that can learn what to do from workers with no programming experience.][Figure 3.3: Whereas traditional industrial robots are expensive and hard to program, there’s a trend toward cheaper AI-powered ones that can learn what to do from workers with no programming experience.]\n\nFigure 3.3: Whereas traditional industrial robots are expensive and hard to program, there’s a trend toward cheaper AI-powered ones that can learn what to do from workers with no programming experience.\n\nAI for Transportation\n\nAlthough AI can save many lives in manufacturing, it can potentially save even more in transportation. Car accidents alone took over 1.2 million lives in 2015, and aircraft, train and boat accidents together killed thousands more. In the United States, with its high safety standards, motor vehicle accidents killed about 35,000 people last year—seven times more than all industrial accidents combined.21 When we had a panel discussion about this in Austin, Texas, at the 2016 annual meeting of the Association for the Advancement of Artificial Intelligence, the Israeli computer scientist Moshe Vardi got quite emotional about it and argued that not only could AI reduce road fatalities, but it must: “It’s a moral imperative!” he exclaimed. Because almost all car crashes are caused by human error, it’s widely believed that AI-powered self-driving cars can eliminate at least 90% of road deaths, and this optimism is fueling great progress toward actually getting self-driving cars out on the roads. Elon Musk envisions that future self-driving cars will not only be safer, but will also earn money for their owners while they’re not needed, by competing with Uber and Lyft.\n\nSo far, self-driving cars do indeed have a better safety record than human drivers, and the accidents that have occurred underscore the importance and difficulty of validation. The first fender bender caused by a Google self-driving car took place on February 14, 2016, because it made an incorrect assumption about a bus: that its driver would yield when the car pulled out in front of it. The first lethal crash caused by a self-driving Tesla, which rammed into the trailer of a truck crossing the highway on May 7, 2016, was caused by two bad assumptions:22 that the bright white side of the trailer was merely part of the bright sky, and that the driver (who was allegedly watching a Harry Potter movie) was paying attention and would intervene if something went wrong.^(\\*3)\n\nBut sometimes good verification and validation aren’t enough to avoid accidents, because we also need good control: ability for a human operator to monitor the system and change its behavior if necessary. For such human-in-the-loop systems to work well, it’s crucial that the human-machine communication be effective. In this spirit, a red light on your dashboard will conveniently alert you if you accidentally leave the trunk of your car open. In contrast, when the British car ferry Herald of Free Enterprise left the harbor of Zeebrugge on March 6, 1987, with her bow doors open, there was no warning light or other visible warning for the captain, and the ferry capsized soon after leaving the harbor, killing 193 people.23\n\nAnother tragic control failure that might have been avoided by better machine-human communication occurred during the night of June 1, 2009, when Air France Flight 447 crashed into the Atlantic Ocean, killing all 228 on board. According to the official accident report, “the crew never understood that they were stalling and consequently never applied a recovery manoeuvre”—which would have involved pushing down the nose of the aircraft—until it was too late. Flight safety experts speculated that the crash might have been avoided had there been an “angle-of-attack” indicator in the cockpit, showing the pilots that the nose was pointed too far upward.24\n\nWhen Air Inter Flight 148 crashed into the Vosges Mountains near Strasbourg in France on January 20, 1992, killing 87 people, the cause wasn’t lack of machine-human communication, but a confusing user interface. The pilots entered “33” on a keypad because they wanted to descend at an angle of 3.3 degrees, but the autopilot interpreted this as 3,300 feet per minute because it was in a different mode—and the display screen was too small to show the mode and allow the pilots to realize their mistake.\n\nAI for Energy\n\nInformation technology has done wonders for power generation and distribution, with sophisticated algorithms balancing production and consumption across the world’s electrical grids, and sophisticated control systems keeping power plants operating safely and efficiently. Future AI progress is likely to make the “smart grid” even smarter, to optimally adapt to changing supply and demand even down to the level of individual rooftop solar panels and home-battery systems. But on Thursday, August 14, 2003, it was lights-out for about 55 million people in the United States and Canada, many of whom remained powerless for days. Here, too, the primary cause was determined to be failed machine-human communications: a software bug prevented the alarm system in an Ohio control room from alerting operators to the need to redistribute power before a minor problem (overloaded transmission lines hitting unpruned foliage) cascaded out of control.25\n\nThe partial nuclear meltdown in a reactor on Three Mile Island in Pennsylvania on March 28, 1979, led to about a billion dollars in cleanup cost and a major backlash against nuclear power. The final accident report identified multiple contributing factors, including confusion caused by a poor user interface.26 In particular, the warning light that the operators thought indicated whether a safety-critical valve was open or closed merely indicated whether a signal had been sent to close the valve—so the operators didn’t realize that the valve had gotten stuck open.\n\nThese energy and transportation accidents teach us that as we put AI in charge of ever more physical systems, we need to put serious research efforts into not only making the machines work well on their own, but also into making machines collaborate effectively with their human controllers. As AI gets smarter, this will involve not merely building good user interfaces for information sharing, but also figuring out how to optimally allocate tasks within human-computer teams—for example, identifying situations where control should be transferred, and for applying human judgment efficiently to the highest-value decisions rather than distracting human controllers with a flood of unimportant information.\n\nAI for Healthcare\n\nAI has huge potential for improving healthcare. Digitization of medical records has already enabled doctors and patients to make faster and better decisions, and to get instant help from experts around the world in diagnosing digital images. Indeed, the best experts for performing such diagnosis may soon be AI systems, given the rapid progress in computer vision and deep learning. For example, a 2015 Dutch study showed that computer diagnosis of prostate cancer using magnetic resonance imaging (MRI) was as good as that of human radiologists,27 and a 2016 Stanford study showed that AI could diagnose lung cancer using microscope images even better than human pathologists.28 If machine learning can help reveal relationships between genes, diseases and treatment responses, it could revolutionize personalized medicine, make farm animals healthier and enable more resilient crops. Moreover, robots have the potential to become more accurate and reliable surgeons than humans, even without using advanced AI. A wide variety of robotic surgeries have been successfully performed in recent years, often allowing precision, miniaturization and smaller incisions that lead to decreased blood loss, less pain and shorter healing time.\n\nAlas, there have been painful lessons about the importance of robust software also in the healthcare industry. For example, the Canadian-built Therac-25 radiation therapy machine was designed to treat cancer patients in two different modes: either with a low-power beam of electrons or with a high-power beam of megavolt X-rays that was kept on target by a special shield. Unfortunately, unverified buggy software occasionally caused technicians to deliver the megavolt beam when they thought they were administering the low-power beam, and without the shield, which ended up claiming the lives of several patients.29 Many more patients died from radiation overdoses at the National Oncologic Institute in Panama, where radiotherapy equipment using radioactive cobalt-60 was programmed to excessive exposure times in 2000 and 2001 because of a confusing user interface that hadn’t been properly validated.30 According to a recent report,31 robotic surgery accidents were linked to 144 deaths and 1,391 injuries in the United States between 2000 and 2013, with common problems including not only hardware issues such as electrical arcing and burnt or broken pieces of instruments falling into the patient, but also software problems such as uncontrolled movements and spontaneous powering-off.\n\nThe good news is that the rest of almost two million robotic surgeries covered by the report went smoothly, and robots appear to be making surgery more rather than less safe. According to a U.S. government study, bad hospital care contributes to over 100,000 deaths per year in the United States alone,32 so the moral imperative for developing better AI for medicine is arguably even stronger than that for self-driving cars.\n\nAI for Communication\n\nThe communication industry is arguably the one where computers have had the greatest impact of all so far. After the introduction of computerized telephone switchboards in the fifties, the internet in the sixties, and the World Wide Web in 1989, billions of people now go online to communicate, shop, read news, watch movies or play games, accustomed to having the world’s information just a click away—and often for free. The emerging internet of things promises improved efficiency, accuracy, convenience and economic benefit from bringing online everything from lamps, thermostats and freezers to biochip transponders on farm animals.\n\nThese spectacular successes in connecting the world have brought computer scientists a fourth challenge: they need to improve not only verification, validation and control, but also security against malicious software (“malware”) and hacks. Whereas the aforementioned problems all resulted from unintentional mistakes, security is directed at deliberate malfeasance. The first malware to draw significant media attention was the so-called Morris worm, unleashed on November 2, 1988, which exploited bugs in the UNIX operating system. It was allegedly a misguided attempt to count how many computers were online, and although it infected and crashed about 10% of the 60,000 computers that made up the internet back then, this didn’t stop its creator, Robert Morris, from eventually getting a tenured professorship in computer science at MIT.\n\nOther malware exploits vulnerabilities not in software but in people. On May 5, 2000, as if to celebrate my birthday, people got emails with the subject line “ILOVEYOU” from acquaintances and colleagues, and those Microsoft Windows users who clicked on the attachment “LOVE-LETTER-FOR-YOU.txt.vbs” unwittingly launched a script that damaged their computer and re-sent the email to everyone in their address book. Created by two young programmers in the Philippines, this worm infected about 10% of the internet, just as the Morris worm had done, but because the internet was a lot bigger by then, it became one of the greatest infections of all time, afflicting over 50 million computers and causing over $5 billion in damages. As you’re probably painfully aware, the internet remains infested with countless kinds of infectious malware, which security experts classify into worms, Trojans, viruses and other intimidating-sounding categories, and the damage they cause ranges from displaying harmless prank messages to deleting your files, stealing your personal information, spying on you and hijacking your computer to send out spam.\n\nWhereas malware targets whatever computer it can, hackers attack specific targets of interest—recent high-profile examples including Target, TJ Maxx, Sony Pictures, Ashley Madison, the Saudi oil company Aramco and the U.S. Democratic National Committee. Moreover, the loots appear to be getting ever more spectacular. Hackers stole 130 million credit card numbers and other account information from Heartland Payment Systems in 2008, and breached over a billion(!) Yahoo! email accounts in 2013.33 A 2014 hack of the U.S. Government’s Office of Personnel Management breached personnel records and job application information for over 21 million people, allegedly including employees with top security clearances and the fingerprints of undercover agents.\n\nAs a result, I roll my eyes whenever I read about some new system being allegedly 100% secure and unhackable. Yet “unhackable” is clearly what we need future AI systems to be before we put them in charge of, say, critical infrastructure or weapons systems, so the growing role of AI in society keeps raising the stakes for computer security. While some hacks exploit human gullibility or complex vulnerabilities in newly released software, others enable unauthorized login to remote computers by taking advantage of simple bugs that lingered unnoticed for an embarrassingly long time. The “Heartbleed” bug lasted from 2012 to 2014 in one of the most popular software libraries for secure communication between computers, and the “Bashdoor” bug was built into the very operating system of Unix computers from 1989 until 2014. This means that AI tools for improved verification and validation will improve security as well.\n\nUnfortunately, better AI systems can also be used to find new vulnerabilities and perform more sophisticated hacks. Imagine, for example, that you one day get an unusually personalized “phishing” email attempting to persuade you to divulge personal information. It’s sent from your friend’s account by an AI who’s hacked it and is impersonating her, imitating her writing style based on an analysis of her other sent emails, and including lots of personal information about you from other sources. Might you fall for this? What if the phishing email appears to come from your credit card company and is followed up by a phone call from a friendly human voice that you can’t tell is AI-generated? In the ongoing computer-security arms race between offense and defense, there’s so far little indication that defense is winning.\n\nLaws\n\nWe humans are social animals who subdued all other species and conquered Earth thanks to our ability to cooperate. We’ve developed laws to incentivize and facilitate cooperation, so if AI can improve our legal and governance systems, then it can enable us to cooperate more successfully than ever before, bringing out the very best in us. And there’s plenty of opportunity for improvement here, both in how our laws are applied and how they’re written, so let’s explore both in turn.\n\nWhat are the first associations that come to your mind when you think about the court system in your country? If it’s lengthy delays, high costs and occasional injustice, then you’re not alone. Wouldn’t it be wonderful if your first thoughts were instead “efficiency” and “fairness”? Since the legal process can be abstractly viewed as a computation, inputting information about evidence and laws and outputting a decision, some scholars dream of fully automating it with robojudges: AI systems that tirelessly apply the same high legal standards to every judgment without succumbing to human errors such as bias, fatigue or lack of the latest knowledge.\n\nRobojudges\n\nByron De La Beckwith Jr. was convicted in 1994 of assassinating civil rights leader Medgar Evers in 1963, but two separate all-white Mississippi juries had failed to convict him the year after the murder, even though the physical evidence was essentially the same.34 Alas, legal history is rife with judgments biased by skin color, gender, sexual orientation, religion, nationality and other factors. Robojudges could in principle ensure that, for the first time in history, everyone becomes truly equal under the law: they could be programmed to all be identical and to treat everyone equally, transparently applying the law in a truly unbiased fashion.\n\nRobojudges could also eliminate human biases that are accidental rather than intentional. For example, a controversial 2012 study of Israeli judges claimed that they delivered significantly harsher verdicts when they were hungry: whereas they denied about 35% of parole cases right after breakfast, they denied over 85% right before lunch.35 Another shortcoming of human judges is that they may lack sufficient time to explore all details of a case. In contrast, robojudges can easily be copied, since they consist of little more than software, allowing all pending cases to be processed in parallel rather than in series, each case getting its own robojudge for as long as it takes. Finally, although it’s impossible for human judges to master all technical knowledge required for every possible case, from thorny patent disputes to murder mysteries hinging on the latest forensic science, future robojudges may have essentially unlimited memory and learning capacity.\n\nOne day, such robojudges may therefore be both more efficient and fairer, by virtue of being unbiased, competent and transparent. Their efficiency makes them fairer still: by speeding up the legal process and making it harder for savvy lawyers to skew the outcome, they could make it dramatically cheaper to get justice through the courts. This could greatly increase the chances of a cash-strapped individual or startup company prevailing against a billionaire or multinational corporation with an army of lawyers.\n\nOn the other hand, what if robojudges have bugs or get hacked? Both have already afflicted automatic voting machines, and when years behind bars or millions in the bank are at stake, the incentives for cyberattacks are greater still. Even if AI can be made robust enough for us to trust that a robojudge is using the legislated algorithm, will everybody feel that they understand its logical reasoning enough to respect its judgment? This challenge is exacerbated by the recent success of neural networks, which often outperform traditional easy-to-understand AI algorithms at the price of inscrutability. If defendants wish to know why they were convicted, shouldn’t they have the right to a better answer than “we trained the system on lots of data, and this is what it decided”? Moreover, recent studies have shown that if you train a deep neural learning system with massive amounts of prisoner data, it can predict who’s likely to return to crime (and should therefore be denied parole) better than human judges. But what if this system finds that recidivism is statistically linked to a prisoner’s sex or race—would this count as a sexist, racist robojudge that needs reprogramming? Indeed, a 2016 study argued that recidivism-prediction software used across the United States was biased against African Americans and had contributed to unfair sentencing.36 These are important questions that we all need to ponder and discuss to ensure that AI remains beneficial. We aren’t facing an all-or-nothing decision regarding robojudges, but rather a decision about the extent and speed with which we want to deploy AI in our legal system. Do we want human judges to have AI-based decision support systems, just like tomorrow’s medical doctors? Do we want to go further and have robojudge decisions that can be appealed to human judges, or do we want to go all the way and give even the final say to machines, even for death penalties?\n\nLegal Controversies\n\nSo far, we’ve explored only the application of law; let us now turn to its content. There’s broad consensus that our laws need to evolve to keep pace with our technology. For example, the two programmers who created the aforementioned ILOVEYOU worm and caused billions of dollars in damages were acquitted of all charges and walked free because at that time, there were no laws against malware creation in the Philippines. Since the pace of technological progress appears to be accelerating, laws need to be updated ever more rapidly, and have a tendency to lag behind. Getting more tech-savvy people into law schools and governments is probably a smart move for society. But should AI-based decision support systems for voters and legislators ensue, followed by outright robo-legislators?\n\nHow to best alter our laws to reflect AI progress is a fascinatingly controversial topic. One dispute reflects the tension between privacy versus freedom of information. Freedom fans argue that the less privacy we have, the more evidence the courts will have, and the fairer the judgments will be. For example, if the government taps into everyone’s electronic devices to record where they are and what they type, click, say and do, many crimes would be readily solved, and additional ones could be prevented. Privacy advocates counter that they don’t want an Orwellian surveillance state, and that even if they did, there’s a risk of it turning into a totalitarian dictatorship of epic proportions. Moreover, machine-learning techniques have gotten better at analyzing brain data from fMRI scanners to determine what a person is thinking about and, in particular, whether they’re telling the truth or lying.37 If AI-assisted brain scanning technology became commonplace in courtrooms, the currently tedious process of establishing the facts of a case could be dramatically simplified and expedited, enabling faster trials and fairer judgments. But privacy advocates might worry about whether such systems occasionally make mistakes and, more fundamentally, whether our minds should be off-limits to government snooping. Governments that don’t support freedom of thought could use such technology to criminalize the holding of certain beliefs and opinions. Where would you draw the line between justice and privacy, and between protecting society and protecting personal freedom? Wherever you draw it, will it gradually but inexorably move toward reduced privacy to compensate for the fact that evidence gets easier to fake? For example, once AI becomes able to generate fully realistic fake videos of you committing crimes, will you vote for a system where the government tracks everyone’s whereabouts at all times and can provide you with an ironclad alibi if needed?\n\nAnother captivating controversy is whether AI research should be regulated or, more generally, what incentives policymakers should give AI researchers to maximize the chances of a beneficial outcome. Some AI researchers have argued against all forms of regulation of AI development, claiming that they would needlessly delay urgently needed innovation (for example, lifesaving self-driving cars) and would drive cutting-edge AI research underground and/or to other countries with more permissive governments. At the Puerto Rico beneficial-AI conference mentioned in the first chapter, Elon Musk argued that what we need right now from governments isn’t oversight but insight: specifically, technically capable people in government positions who can monitor AI’s progress and steer it if warranted down the road. He also argued that government regulation can sometimes nurture rather than stifle progress: for example, if government safety standards for self-driving cars can help reduce the number of self-driving-car accidents, then a public backlash is less likely and adoption of the new technology can be accelerated. The most safety-conscious AI companies might therefore favor regulation that forces less scrupulous competitors to match their high safety standards.\n\nYet another interesting legal controversy involves granting rights to machines. If self-driving cars cut the 32,000 annual U.S. traffic fatalities in half, perhaps carmakers won’t get 16,000 thank-you notes, but 16,000 lawsuits. So if a self-driving car causes an accident, who should be liable—its occupants, its owner or its manufacturer? Legal scholar David Vladeck has proposed a fourth answer: the car itself! Specifically, he proposes that self-driving cars be allowed (and required) to hold car insurance. This way, models with a sterling safety record will qualify for premiums that are very low, probably lower than what’s available to human drivers, while poorly designed models from sloppy manufacturers will only qualify for insurance policies that make them prohibitively expensive to own.\n\nBut if machines such as cars are allowed to hold insurance policies, should they also be able to own money and property? If so, there’s nothing legally stopping smart computers from making money on the stock market and using it to buy online services. Once a computer starts paying humans to work for it, it can accomplish anything that humans can do. If AI systems eventually get better than humans at investing (which they already are in some domains), this could lead to a situation where most of our economy is owned and controlled by machines. Is this what we want? If it sounds far-off, consider that most of our economy is already owned by another form of non-human entity: corporations, which are often more powerful than any one person in them and can to some extent take on life of their own.\n\nIf you’re OK with granting machines the rights to own property, then how about granting them the right to vote? If so, should each computer program get one vote, even though it can trivially make trillions of copies of itself in the cloud if it’s rich enough, thereby guaranteeing that it will decide all elections? If not, then on what moral basis are we discriminating against machine minds relative to human minds? Does it make a difference if machine minds are conscious in the sense of having a subjective experience like we do? We’ll explore in greater depth these controversial questions related to computer control of our world in the next chapter, and questions related to machine consciousness in chapter 8.\n\nWeapons\n\nSince time immemorial, humanity has suffered from famine, disease and war. We’ve already mentioned how AI may help reduce famine and disease, so how about war? Some argue that nuclear weapons deter war between the countries that own them because they’re so horrifying, so how about letting all nations build even more horrifying AI-based weapons in the hope of ending all war forever? If you’re unpersuaded by that argument and believe that future wars are inevitable, how about using AI to make these wars more humane? If wars consist merely of machines fighting machines, then no human soldiers or civilians need get killed. Moreover, future AI-powered drones and other autonomous weapon systems (AWS; also known by their opponents as “killer robots”) can hopefully be made more fair and rational than human soldiers: equipped with superhuman sensors and unafraid of getting killed, they might remain cool, calculating and level-headed even in the heat of battle, and be less likely to accidentally kill civilians.\n\n[Air Force officials are seeking volunteers for future training classes to produce operators of the MQ-1 Predator unmanned aircraft. (U.S. Air Force photo/Lt Col Leslie Pratt)][Air Force officials are seeking volunteers for future training classes to produce operators of the MQ-1 Predator unmanned aircraft. (U.S. Air Force photo/Lt Col Leslie Pratt)]\n\nFigure 3.4: Whereas today’s military drones (such as this U.S. Air Force MQ-1 Predator) are remote-controlled by humans, future AI-powered drones have the potential to take humans out of the loop, using an algorithm to decide whom to target and kill.\n\nA Human in the Loop\n\nBut what if the automated systems are buggy, confusing or don’t behave as expected? The U.S. Phalanx system for Aegis-class cruisers automatically detects, tracks and attacks threats such as anti-ship missiles and aircraft. The USS Vincennes was a guided missile cruiser nicknamed Robocruiser in reference to its Aegis system, and on July 3, 1988, in the midst of a skirmish with Iranian gunboats during the Iran-Iraq war, its radar system warned of an incoming aircraft. Captain William Rodgers III inferred that they were being attacked by a diving Iranian F-14 fighter jet and gave the Aegis system approval to fire. What he didn’t realize at the time was that they shot down Iran Air Flight 655, a civilian Iranian passenger jet, killing all 290 people on board and causing international outrage. Subsequent investigation implicated a confusing user interface that didn’t automatically show which dots on the radar screen were civilian planes (Flight 655 followed its regular daily flight path and had its civilian aircraft transponder on) or which dots were descending (as for an attack) vs. ascending (as Flight 655 was doing after takeoff from Tehran). Instead, when the automated system was queried for information about the mysterious aircraft, it reported “descending” because that was the status of a different aircraft to which it had confusingly reassigned a number used by the navy to track planes: what was descending was instead a U.S. surface combat air patrol plane operating far away in the Gulf of Oman.\n\nIn this example, there was a human in the loop making the final decision, who under time pressure placed too much trust in what the automated system told him. So far, according to defense officials around the world, all deployed weapons systems have a human in the loop, with the exception of low-tech booby traps such as land mines. But development is now under way of truly autonomous weapons that select and attack targets entirely on their own. It’s militarily tempting to take all humans out of the loop to gain speed: in a dogfight between a fully autonomous drone that can respond instantly and a drone reacting more sluggishly because it’s remote-controlled by a human halfway around the world, which one do you think would win?\n\nHowever, there have been close calls where we were extremely lucky that there was a human in the loop. On October 27, 1962, during the Cuban Missile Crisis, eleven U.S. Navy destroyers and the aircraft carrier USS Randolph had cornered the Soviet submarine B-59 near Cuba, in international waters outside the U.S. “quarantine” area. What they didn’t know was that the temperature onboard had risen past 45°C (113°F) because the submarine’s batteries were running out and the air-conditioning had stopped. On the verge of carbon dioxide poisoning, many crew members had fainted. The crew had had no contact with Moscow for days and didn’t know whether World War III had already begun. Then the Americans started dropping small depth charges, which they had, unbeknownst to the crew, told Moscow were merely meant to force the sub to surface and leave. “We thought—that’s it—the end,” crew member V. P. Orlov recalled. “It felt like you were sitting in a metal barrel, which somebody is constantly blasting with a sledgehammer.” What the Americans also didn’t know was that the B-59 crew had a nuclear torpedo that they were authorized to launch without clearing it with Moscow. Indeed, Captain Savitski decided to launch the nuclear torpedo. Valentin Grigorievich, the torpedo officer, exclaimed: “We will die, but we will sink them all—we will not disgrace our navy!” Fortunately, the decision to launch had to be authorized by three officers on board, and one of them, Vasili Arkhipov, said no. It’s sobering that very few have heard of Arkhipov, although his decision may have averted World War III and been the single most valuable contribution to humanity in modern history.38 It’s also sobering to contemplate what might have happened had B-59 been an autonomous AI-controlled submarine with no humans in the loop.\n\nTwo decades later, on September 9, 1983, tensions were again high between the superpowers: the Soviet Union had recently been called an “evil empire” by U.S. president Ronald Reagan, and just the previous week, it had shot down a Korean Airlines passenger plane that strayed into its airspace, killing 269 people—including a U.S. congressman. Now an automated Soviet early-warning system reported that the United States had launched five land-based nuclear missiles at the Soviet Union, leaving Officer Stanislav Petrov merely minutes to decide whether this was a false alarm. The satellite was found to be operating properly, so following protocol would have led him to report an incoming nuclear attack. Instead, he trusted his gut instinct, figuring that the United States was unlikely to attack with only five missiles, and reported to his commanders that it was a false alarm without knowing this to be true. It later became clear that a satellite had mistaken the Sun’s reflections off cloud tops for flames from rocket engines.39 I wonder what would have happened if Petrov had been replaced by an AI system that properly followed proper protocol.\n\nThe Next Arms Race?\n\nAs you’ve undoubtedly guessed by now, I personally have serious concerns about autonomous weapons systems. But I haven’t even begun to tell you about my main worry: the endpoint of an arms race in AI weapons. In July 2015, I expressed this worry in the following open letter together with Stuart Russell, with helpful feedback from my colleagues at the Future of Life Institute:40\n\n AUTONOMOUS WEAPONS:\n\n An Open Letter from AI & Robotics Researchers\n\n Autonomous weapons select and engage targets without human intervention. They might include, for example, armed quadcopters that can search for and eliminate people meeting certain pre-defined criteria, but do not include cruise missiles or remotely piloted drones for which humans make all targeting decisions. Artificial Intelligence (AI) technology has reached a point where the deployment of such systems is practically if not legally feasible within years, not decades, and the stakes are high: autonomous weapons have been described as the third revolution in warfare, after gunpowder and nuclear arms.\n\n Many arguments have been made for and against autonomous weapons, for example that replacing human soldiers by machines is good by reducing casualties for the owner but bad by thereby lowering the threshold for going to battle. The key question for humanity today is whether to start a global AI arms race or to prevent it from starting. If any major military power pushes ahead with AI weapon development, a global arms race is virtually inevitable, and the endpoint of this technological trajectory is obvious: autonomous weapons will become the Kalashnikovs of tomorrow. Unlike nuclear weapons, they require no costly or hard-to-obtain raw materials, so they’ll become ubiquitous and cheap for all significant military powers to mass-produce. It will only be a matter of time until they appear on the black market and in the hands of terrorists, dictators wishing to better control their populace, warlords wishing to perpetrate ethnic cleansing, etc. Autonomous weapons are ideal for tasks such as assassinations, destabilizing nations, subduing populations and selectively killing a particular ethnic group. We therefore believe that a military AI arms race would not be beneficial for humanity. There are many ways in which AI can make battlefields safer for humans, especially civilians, without creating new tools for killing people.\n\n Just as most chemists and biologists have no interest in building chemical or biological weapons, most AI researchers have no interest in building AI weapons and do not want others to tarnish their field by doing so, potentially creating a major public backlash against AI that curtails its future societal benefits. Indeed, chemists and biologists have broadly supported international agreements that have successfully prohibited chemical and biological weapons, just as most physicists supported the treaties banning space-based nuclear weapons and blinding laser weapons.\n\nTo make it harder to dismiss our concerns as coming only from pacifist tree-huggers, I wanted to get our letter signed by as many hardcore AI researchers and roboticists as possible. The International Campaign for Robotic Arms Control had previously amassed hundreds of signatories who called for a ban on killer robots, and I suspected that we could do even better. I knew that professional organizations would be reluctant to share their massive member email lists for a purpose that could be construed as political, so I scraped together lists of researchers’ names and institutions from online documents and advertised the task of finding their email addresses on MTurk—the Amazon Mechanical Turk crowdsourcing platform. Most researchers have their email addresses listed on their university websites, and twenty-four hours and $54 later, I was the proud owner of a mailing list of hundreds of AI researchers who’d been successful enough to be elected Fellows of the Association for the Advancement of Artificial Intelligence (AAAI). One of them was the British-Australian AI professor Toby Walsh, who kindly agreed to email everyone else on the list and help spearhead our campaign. MTurk workers around the world tirelessly produced additional mailing lists for Toby, and before long, over 3,000 AI researchers and robotics researchers had signed our open letter, including six past AAAI presidents and AI industry leaders from Google, Facebook, Microsoft and Tesla. An army of FLI volunteers worked tirelessly to validate the signatory lists, removing spoof entries such as Bill Clinton and Sarah Connor. Over 17,000 others signed too, including Stephen Hawking, and after Toby organized a press conference about this at the International Joint Conference of Artificial Intelligence, it became a major news story around the world.\n\nBecause biologists and chemists once took a stand, their fields are now known mainly for creating beneficial medicines and materials rather than biological and chemical weapons. The AI and robotics communities had now spoken as well: the letter signatories also wanted their fields to be known for creating a better future, not for creating new ways of killing people. But will the main future use of AI be civilian or military? Although we’ve spent more pages in this chapter on the former, we may soon be spending more money on the latter—especially if a military AI arms race takes off. Civilian AI investment commitments exceeded a billion dollars in 2016, but this was dwarfed by the Pentagon’s fiscal 2017 budget request of $12–15 billion for AI-related projects, and China and Russia are likely to take note of what Deputy Defense Secretary Robert Work said when this was announced: “I want our competitors to wonder what’s behind the black curtain.”41\n\nShould There Be an International Treaty?\n\nAlthough there’s now a major international push toward negotiating some form of killer robot ban, it’s still unclear what will happen, and there’s a vibrant ongoing debate about what, if anything, should happen. Although many leading stakeholders agree that world powers should draft some form of international regulations to guide AWS research and use, there’s less agreement about what precisely should be banned and how a ban would be enforced. For example, should only lethal autonomous weapons be banned, or also ones that seriously injure people, say by blinding them? Would we ban development, production or ownership? Should a ban apply to all autonomous weapons systems or, as our letter said, only offensive ones, allowing defensive systems such as autonomous anti-aircraft guns and missile defenses? In the latter case, should AWS count as defensive even if they’re easy to move into enemy territory? And how would you enforce a treaty given that most components of an autonomous weapon have a dual civilian use as well? For example, there isn’t much difference between a drone that can deliver Amazon packages and one that can deliver bombs.\n\nSome debaters have argued that designing an effective AWS treaty is hopelessly hard and that we therefore shouldn’t even try. On the other hand, John F. Kennedy emphasized when announcing the Moon missions that hard things are worth attempting when success will greatly benefit the future of humanity. Moreover, many experts argue that the bans on biological and chemical weapons were valuable even though enforcement proved hard, with significant cheating, because the bans caused severe stigmatization that limited their use.\n\nI met Henry Kissinger at a dinner event in 2016, and got the opportunity to ask him about his role in the biological weapons ban. He explained how back when he was the U.S. national security adviser, he’d persuaded President Nixon that a ban would be good for U.S. national security. I was impressed by how sharp his mind and memory were for a ninety-two-year-old, and was fascinated to hear his inside perspective. Since the United States already enjoyed superpower status thanks to its conventional and nuclear forces, it had more to lose than to gain from a worldwide bioweapons arms race with uncertain outcome. In other words, if you’re already top dog, then it makes sense to follow the maxim “If it ain’t broke, don’t fix it.” Stuart Russell joined our after-dinner conversation, and we discussed how exactly the same argument can be made about lethal autonomous weapons: those who stand to gain most from an arms race aren’t superpowers but small rogue states and non-state actors such as terrorists, who gain access to the weapons via the black market once they’ve been developed.\n\nOnce mass-produced, small AI-powered killer drones are likely to cost little more than a smartphone. Whether it’s a terrorist wanting to assassinate a politician or a jilted lover seeking revenge on his ex-girlfriend, all they need to do is upload their target’s photo and address into the killer drone: it can then fly to the destination, identify and eliminate the person, and self-destruct to ensure that nobody knows who was responsible. Alternatively, for those bent on ethnic cleansing, it can easily be programmed to kill only people with a certain skin color or ethnicity. Stuart envisions that the smarter such weapons get, the less material, firepower and money will be needed per kill. For example, he fears bumblebee-sized drones that kill cheaply using minimal explosive power by shooting people in the eye, which is soft enough to allow even a small projectile to continue into the brain. Or they might latch on to the head with metal claws and then penetrate the skull with a tiny shaped charge. If a million such killer drones can be dispatched from the back of a single truck, then one has a horrifying weapon of mass destruction of a whole new kind: one that can selectively kill only a prescribed category of people, leaving everybody and everything else unscathed.\n\nA common counterargument is that we can eliminate such concerns by making killer robots ethical—for example, so that they’ll only kill enemy soldiers. But if we worry about enforcing a ban, then how would it be easier to enforce a requirement that enemy autonomous weapons be 100% ethical than to enforce that they aren’t produced in the first place? And can one consistently claim that the well-trained soldiers of civilized nations are so bad at following the rules of war that robots can do better, while at the same time claiming that rogue nations, dictators and terrorist groups are so good at following the rules of war that they’ll never choose to deploy robots in ways that violate these rules?\n\nCyberwar\n\nAnother interesting military aspect of AI is that it may let you attack your enemy even without building any weapons of your own, through cyberwarfare. As a small prelude to what the future may bring, the Stuxnet worm, widely attributed to the U.S. and Israeli governments, infected fast-spinning centrifuges in Iran’s nuclear-enrichment program and caused them to tear themselves apart. The more automated society gets and the more powerful the attacking AI becomes, the more devastating cyberwarfare can be. If you can hack and crash your enemy’s self-driving cars, auto-piloted planes, nuclear reactors, industrial robots, communication systems, financial systems and power grids, then you can effectively crash his economy and cripple his defenses. If you can hack some of his weapons systems as well, even better.\n\nWe began this chapter by surveying how spectacular the near-term opportunities are for AI to benefit humanity—if we manage to make it robust and unhackable. Although AI itself can be used to make AI systems more robust, thereby aiding the cyberwar defense, AI can clearly aid the offense as well. Ensuring that the defense prevails must be one of the most crucial short-term goals for AI development—otherwise all the awesome technology we build can be turned against us!\n\nJobs and Wages\n\nSo far in this chapter, we’ve mainly focused on how AI will affect us as consumers, by enabling transformative new products and services at affordable prices. But how will it affect us as workers, by transforming the job market? If we can figure out how to grow our prosperity through automation without leaving people lacking income or purpose, then we have the potential to create a fantastic future with leisure and unprecedented opulence for everyone who wants it. Few people have thought longer and harder about this than economist Erik Brynjolfsson, one of my MIT colleagues. Although he’s always well-groomed and impeccably dressed, he has Icelandic heritage, and I sometimes can’t help imagine that he only recently trimmed back a wild red Viking beard and mane to blend in at our business school. He certainly hasn’t trimmed back his wild ideas, and he calls his optimistic job-market vision “Digital Athens.” The reason that the Athenian citizens of antiquity had lives of leisure where they could enjoy democracy, art and games was mainly that they had slaves to do much of the work. But why not replace the slaves with AI-powered robots, creating a digital utopia that everyone can enjoy? Erik’s AI-driven economy would not only eliminate stress and drudgery and produce an abundance of everything we want today, but it would also supply a bounty of wonderful new products and services that today’s consumers haven’t yet realized that they want.\n\nTechnology and Inequality\n\nWe can get from where we are today to Erik’s Digital Athens if everyone’s hourly salary keeps growing year by year, so that those who want more leisure can gradually work less while continuing to improve their standard of living. Figure 3.5 shows that this is precisely what happened in the United States from World War II until the mid-1970s: although there was income inequality, the total size of the pie grew in such a way that almost everybody got a larger slice. But then, as Erik is the first to admit, something changed: figure 3.5 shows that although the economy kept growing and raising the average income, the gains over the past four decades went to the wealthiest, mostly to the top 1%, while the poorest 90% saw their incomes stagnate. The resulting growth in inequality is even more evident if we look not at income but at wealth. For the bottom 90% of U.S. households, the average net worth was about $85,000 in 2012—the same as twenty-five years earlier—while the top 1% more than doubled their inflation-adjusted wealth during that period, to $14 million.42 Differences are even more extreme internationally where, in 2013, the combined wealth of the bottom half of the world’s population (over 3.6 billion people) is the same as that of the world’s eight richest people43—a statistic that highlights the poverty and vulnerability at the bottom as much as the wealth at the top. At our 2015 Puerto Rico conference, Erik told the assembled AI researchers that he thought progress in AI and automation would continue making the economic pie bigger, but that there’s no economic law that everyone, or even most people, will benefit.\n\nAlthough there’s broad agreement among economists that inequality is rising, there’s an interesting controversy about why and whether the trend will continue. Debaters on the left side of the political spectrum often argue that the main cause is globalization and/or economic policies such as tax cuts for the rich. But Erik Brynjolfsson and his MIT collaborator Andrew McAfee argue that the main cause is something else: technology.44 Specifically, they argue that digital technology drives inequality in three different ways.\n\nFirst, by replacing old jobs with ones requiring more skills, technology has rewarded the educated: since the mid-1970s, salaries rose about 25% for those with graduate degrees while the average high school dropout took a 30% pay cut.45\n\nSecond, they claim that since the year 2000, an ever-larger share of corporate income has gone to those who own the companies as opposed to those who work there—and that as long as automation continues, we should expect those who own the machines to take a growing fraction of the pie. This edge of capital over labor may be particularly important for the growing digital economy, which tech visionary Nicholas Negroponte defines as moving bits, not atoms. Now that everything from books to movies and tax preparation tools has gone digital, additional copies can be sold worldwide at essentially zero cost, without hiring additional employees. This allows most of the revenue to go to investors rather than workers, and helps explain why, even though the combined revenues of Detroit’s “Big 3” (GM, Ford and Chrysler) in 1990 were almost identical to those of Silicon Valley’s “Big 3” (Google, Apple, Facebook) in 2014, the latter had nine times fewer employees and were worth thirty times more on the stock market.47\n\n[Figure 3.5: How the economy has grown average income over the past century, and what fraction of this income has gone to different groups. Before the 1970s, rich and poor are seen to all be getting better off in lockstep, after which most of the gains have gone to the top 1% while the bottom 90% have on average gained close to nothing. 46 The amounts have been inflation-corrected to year-2017 dollars.][Figure 3.5: How the economy has grown average income over the past century, and what fraction of this income has gone to different groups. Before the 1970s, rich and poor are seen to all be getting better off in lockstep, after which most of the gains have gone to the top 1% while the bottom 90% have on average gained close to nothing. 46 The amounts have been inflation-corrected to year-2017 dollars.]\n\nFigure 3.5: How the economy has grown average income over the past century, and what fraction of this income has gone to different groups. Before the 1970s, rich and poor are seen to all be getting better off in lockstep, after which most of the gains have gone to the top 1% while the bottom 90% have on average gained close to nothing.46 The amounts have been inflation-corrected to year-2017 dollars.\n\nThird, Erik and collaborators argue that the digital economy often benefits superstars over everyone else. Harry Potter author J. K. Rowling became the first writer to join the billionaire club, and she got much richer than Shakespeare because her stories could be transmitted in the form of text, movies and games to billions of people at very low cost. Similarly, Scott Cook made a billion on the TurboTax tax preparation software, which, unlike human tax preparers, can be sold as a download. Since most people are willing to pay little or nothing for the tenth-best tax-preparation software, there’s room in the marketplace for only a modest number of superstars. This means that if all the world’s parents advise their kids to become the next J. K. Rowling, Gisele Bündchen, Matt Damon, Cristiano Ronaldo, Oprah Winfrey or Elon Musk, almost none of their kids will find this a viable career strategy.\n\nCareer Advice for Kids\n\nSo what career advice should we give our kids? I’m encouraging mine to go into professions that machines are currently bad at, and therefore seem unlikely to get automated in the near future. Recent forecasts for when various jobs will get taken over by machines identify several useful questions to ask about a career before deciding to educate oneself for it.48 For example:\n\n• Does it require interacting with people and using social intelligence?\n\n• Does it involve creativity and coming up with clever solutions?\n\n• Does it require working in an unpredictable environment?\n\nThe more of these questions you can answer with a yes, the better your career choice is likely to be. This means that relatively safe bets include becoming a teacher, nurse, doctor, dentist, scientist, entrepreneur, programmer, engineer, lawyer, social worker, clergy member, artist, hairdresser or massage therapist.\n\nIn contrast, jobs that involve highly repetitive or structured actions in a predictable setting aren’t likely to last long before getting automated away. Computers and industrial robots took over the simplest such jobs long ago, and improving technology is in the process of eliminating many more, from telemarketers to warehouse workers, cashiers, train operators, bakers and line cooks.49 Drivers of trucks, buses, taxis and Uber/Lyft cars are likely to follow soon. There are many more professions (including paralegals, credit analysts, loan officers, bookkeepers and tax accountants) that, although they aren’t on the endangered list for full extinction, are getting most of their tasks automated and therefore demand many fewer humans.\n\nBut staying clear of automation isn’t the only career challenge. In this global digital age, aiming to become a professional writer, filmmaker, actor, athlete or fashion designer is risky for another reason: although people in these professions won’t get serious competition from machines anytime soon, they’ll get increasingly brutal competition from other humans around the globe according to the aforementioned superstar theory, and very few will succeed.\n\nIn many cases, it would be too myopic and crude to give career advice at the level of whole fields: there are many jobs that won’t get entirely eliminated, but which will see many of their tasks automated. For example, if you go into medicine, don’t be the radiologist who analyzes the medical images and gets replaced by IBM’s Watson, but the doctor who orders the radiology analysis, discusses the results with the patient, and decides on the treatment plan. If you go into finance, don’t be the “quant” who applies algorithms to the data and gets replaced by software, but the fund manager who uses the quantitative analysis results to make strategic investment decisions. If you go into law, don’t be the paralegal who reviews thousands of documents for the discovery phase and gets automated away, but the attorney who counsels the client and presents the case in court.\n\nSo far, we’ve explored what individuals can do to maximize their success on the job market in the age of AI. But what can governments do to help their workforces succeed? For example, what education system best prepares people for a job market where AI keeps improving rapidly? Is it still our current model with one or two decades of education followed by four decades of specialized work? Or is it better to switch to a system where people work for a few years, then go back to school for a year, then work for a few more years?50 Or should continuing education (perhaps provided online) be a standard part of any job?\n\nAnd what economic policies are most helpful for creating good new jobs? Andrew McAfee argues that there are many policies that are likely to help, including investing heavily in research, education and infrastructure, facilitating migration and incentivizing entrepreneurship. He feels that “the Econ 101 playbook is clear, but is not being followed,” at least not in the United States.51\n\nWill Humans Eventually Become Unemployable?\n\nIf AI keeps improving, automating ever more jobs, what will happen? Many people are job optimists, arguing that the automated jobs will be replaced by new ones that are even better. After all, that’s what’s always happened before, ever since Luddites worried about technological unemployment during the Industrial Revolution.\n\nOthers, however, are job pessimists and argue that this time is different, and that an ever-larger number of people will become not only unemployed, but unemployable.52 The job pessimists argue that the free market sets salaries based on supply and demand, and that a growing supply of cheap machine labor will eventually depress human salaries far below the cost of living. Since the market salary for a job is the hourly cost of whoever or whatever will perform it most cheaply, salaries have historically dropped whenever it became possible to outsource a particular occupation to a lower-income country or to a cheap machine. During the Industrial Revolution, we started figuring out how to replace our muscles with machines, and people shifted into better-paying jobs where they used their minds more. Blue-collar jobs were replaced by white-collar jobs. Now we’re gradually figuring out how to replace our minds by machines. If we ultimately succeed in this, then what jobs are left for us?\n\nSome job optimists argue that after physical and mental jobs, the next boom will be in creative jobs, but job pessimists counter that creativity is just another mental process, so that it too will eventually be mastered by AI. Other job optimists hope that the next boom will instead be in new technology-enabled professions that we haven’t even thought of yet. After all, who during the Industrial Revolution would have imagined that their descendants would one day work as web designers and Uber drivers? But job pessimists counter that this is wishful thinking with little support from empirical data. They point out that we could have made the same argument a century ago, before the computer revolution, and predicted that most of today’s professions would be new and previously unimagined technology-enabled ones that didn’t use to exist. This prediction would have been an epic failure, as illustrated in figure 3.6: the vast majority of today’s occupations are ones that already existed a century ago, and when we sort them by the number of jobs they provide, we have to go all the way down to twenty-first place in the list until we encounter a new occupation: software developers, who make up less than 1% of the U.S. job market.\n\nWe can get a better understanding of what’s happening by recalling chapter 2, which showed the landscape of human intelligence, with elevation representing how hard it is for machines to perform various tasks and the rising sea level showing what machines can currently do. The main trend on the job market isn’t that we’re moving into entirely new professions. Rather, we’re crowding into those pieces of terrain in figure 2.2 that haven’t yet been submerged by the rising tide of technology! Figure 3.6 shows that this forms not a single island but a complex archipelago, with islets and atolls corresponding to all the valuable things that machines still can’t do as cheaply as humans can. This includes not only high-tech professions such as software development, but also a panoply of low-tech jobs leveraging our superior dexterity and social skills, ranging from massage therapy to acting. Might AI eclipse us at intellectual tasks so rapidly that the last remaining jobs will be in that low-tech category? A friend of mine recently joked with me that perhaps the very last profession will be the very first profession: prostitution. But then he mentioned this to a Japanese roboticist, who protested: “No, robots are very good at those things!”\n\n[Figure 3.6: The pie chart shows the occupations of the 149 million Americans who had a job in 2015, with the 535 job categories from the U.S. Bureau of Labor Statistics sorted by popularity. 53 All occupations with more than a million workers are labeled. There are no new occupations created by computer technology until twenty-first place. This figure is inspired by an analysis from Federico Pistono. 54][Figure 3.6: The pie chart shows the occupations of the 149 million Americans who had a job in 2015, with the 535 job categories from the U.S. Bureau of Labor Statistics sorted by popularity. 53 All occupations with more than a million workers are labeled. There are no new occupations created by computer technology until twenty-first place. This figure is inspired by an analysis from Federico Pistono. 54]\n\nFigure 3.6: The pie chart shows the occupations of the 149 million Americans who had a job in 2015, with the 535 job categories from the U.S. Bureau of Labor Statistics sorted by popularity.53 All occupations with more than a million workers are labeled. There are no new occupations created by computer technology until twenty-first place. This figure is inspired by an analysis from Federico Pistono.54\n\nJob pessimists contend that the endpoint is obvious: the whole archipelago will get submerged, and there will be no jobs left that humans can do more cheaply than machines. In his 2007 book Farewell to Alms, the Scottish-American economist Gregory Clark points out that we can learn a thing or two about our future job prospects by comparing notes with our equine friends. Imagine two horses looking at an early automobile in the year 1900 and pondering their future.\n\n “I’m worried about technological unemployment.”\n\n “Neigh, neigh, don’t be a Luddite: our ancestors said the same thing when steam engines took our industry jobs and trains took our jobs pulling stage coaches. But we have more jobs than ever today, and they’re better too: I’d much rather pull a light carriage through town than spend all day walking in circles to power a stupid mine-shaft pump.”\n\n “But what if this internal combustion engine thing really takes off?”\n\n “I’m sure there’ll be new new jobs for horses that we haven’t yet imagined. That’s what’s always happened before, like with the invention of the wheel and the plow.”\n\nAlas, those not-yet-imagined new jobs for horses never arrived. No-longer-needed horses were slaughtered and not replaced, causing the U.S. equine population to collapse from about 26 million in 1915 to about 3 million in 1960.55 As mechanical muscles made horses redundant, will mechanical minds do the same to humans?\n\nGiving People Income Without Jobs\n\nSo who’s right: those who say automated jobs will be replaced by better ones or those who say most humans will end up unemployable? If AI progress continues unabated, then both sides might be right: one in the short term and the other in the long term. But although people often discuss the disappearance of jobs with doom-and-gloom connotations, it doesn’t have to be a bad thing! Luddites obsessed about particular jobs, neglecting the possibility that other jobs might provide the same social value. Analogously, perhaps those who obsess about jobs today are being too narrow-minded: we want jobs because they can provide us with income and purpose, but given the opulence of resources produced by machines, it should be possible to find alternative ways of providing both the income and the purpose without jobs. Something similar ended up happening in the equine story, which didn’t end with all horses going extinct. Instead, the number of horses has more than tripled since 1960, as they were protected by an equine social-welfare system of sorts: even though they couldn’t pay their own bills, people decided to take care of horses, keeping them around for fun, sport and companionship. Can we similarly take care of our fellow humans in need?\n\nLet’s start with the question of income: redistributing merely a small share of the growing economic pie should enable everyone to become better off. Many argue that we not only can but should do this. On the 2016 panel where Moshe Vardi spoke of a moral imperative to save lives with AI-powered technology, I argued that it’s also a moral imperative to advocate for its beneficial use, including sharing the wealth. Erik Brynjolfsson, also a panelist, said that “if with all this new wealth generation, we can’t even prevent half of all people from getting worse off, then shame on us!”\n\nThere are many different proposals for wealth-sharing, each with its supporters and detractors. The simplest is basic income, where every person receives a monthly payment with no preconditions or requirements whatsoever. A number of small-scale experiments are now being tried or planned, for example in Canada, Finland and the Netherlands. Advocates argue that basic income is more efficient than alternatives such as welfare payments to the needy, because it eliminates the administrative hassle of determining who qualifies. Need-based welfare payments have also been criticized for disincentivizing work, but this of course becomes irrelevant in a jobless future where nobody works.\n\nGovernments can help their citizens not only by giving them money, but also by providing them with free or subsidized services such as roads, bridges, parks, public transportation, childcare, education, healthcare, retirement homes and internet access; indeed, many governments already provide most of these services. As opposed to basic income, such government-funded services accomplish two separate goals: they reduce people’s cost of living and also provide jobs. Even in a future where machines can outperform humans at all jobs, governments could opt to pay people to work in childcare, eldercare, etc. rather than outsource the caregiving to robots.\n\nInterestingly, technological progress can end up providing many valuable products and services for free even without government intervention. For example, people used to pay for encyclopedias, atlases, sending letters and making phone calls, but now anyone with an internet connection gets access to all these things at no cost—together with free videoconferencing, photo sharing, social media, online courses and countless other new services. Many other things that can be highly valuable to a person, say a lifesaving course of antibiotics, have become extremely cheap. So thanks to technology, even many poor people today have access to things that the world’s richest people lacked in the past. Some take this to mean that the income needed for a decent life is dropping.\n\nIf machines can one day produce all current goods and services at minimal cost, then there’s clearly enough wealth to make everyone better off. In other words, even relatively modest taxes could then allow governments to pay for basic income and free services. But the fact that wealth-sharing can happen obviously doesn’t mean that it will happen, and today there’s strong political disagreement about whether it even should happen. As we saw above, the current trend in the United States appears to be in the opposite direction, with some groups of people getting poorer decade after decade. Policy decisions about how to share society’s growing wealth will impact everybody, so the conversation about what sort of future economy to build should include everyone, not merely AI researchers, roboticists and economists.\n\nMany debaters argue that reducing income inequality is a good idea not merely in an AI-dominated future, but also today. Although the main argument tends to be a moral one, there’s also evidence that greater equality makes democracy work better: when there’s a large well-educated middle class, the electorate is harder to manipulate, and it’s tougher for small numbers of people or companies to buy undue influence over the government. A better democracy can in turn enable a better-managed economy that’s less corrupt, more efficient and faster growing, ultimately benefiting essentially everyone.\n\nGiving People Purpose Without Jobs\n\nJobs can provide people with more than just money. Voltaire wrote in 1759 that “work keeps at bay three great evils: boredom, vice and need.” Conversely, providing people with income isn’t enough to guarantee their well-being. Roman emperors provided both bread and circuses to keep their underlings content, and Jesus emphasized non-material needs in the Bible quote “Man shall not live by bread alone.” So precisely what valuable things do jobs contribute beyond money, and in what alternative ways can a jobless society provide them?\n\nThe answers to these questions are obviously complicated, since some people hate their jobs and others love them. Moreover, many children, students and homemakers thrive without jobs, while history teems with stories of spoiled heirs and princes who succumbed to ennui and depression. A 2012 meta-analysis showed that unemployment tends to have negative long-term effects on well-being, while retirement was a mixed bag with both positive and negative aspects.56 The growing field of positive psychology has identified a number of factors that boost people’s sense of well-being and purpose, and found that some (but not all!) jobs can provide many of them, for example:57\n\n• a social network of friends and colleagues\n\n• a healthy and virtuous lifestyle\n\n• respect, self-esteem, self-efficacy and a pleasurable sense of “flow” stemming from doing something one is good at\n\n• a sense of being needed and making a difference\n\n• a sense of meaning from being part of and serving something larger than oneself\n\nThis gives reason for optimism, since all of these things can be provided also outside of the workplace, for example through sports, hobbies and learning, and with families, friends, teams, clubs, community groups, schools, religious and humanist organizations, political movements and other institutions. To create a low-employment society that flourishes rather than degenerates into self-destructive behavior, we therefore need to understand how to help such well-being-inducing activities thrive. The quest for such an understanding needs to involve not only scientists and economists, but also psychologists, sociologists and educators. If serious efforts are put into creating well-being for all, funded by part of the wealth that future AI generates, then society should be able to flourish like never before. At a minimum, it should be possible to make everyone as happy as if they had their personal dream job, but once one breaks free of the constraint that everyone’s activities must generate income, the sky’s the limit.\n\nHuman-Level Intelligence?\n\nWe’ve explored in this chapter how AI has the potential to greatly improve our lives in the near term, as long as we plan ahead and avoid various pitfalls. But what about the longer term? Will AI progress eventually stagnate due to insurmountable obstacles, or will AI researchers ultimately succeed in their original goal of building human-level artificial general intelligence? We saw in the previous chapter how the laws of physics allow suitable clumps of matter to remember, compute and learn, and how they don’t prohibit such clumps from one day doing so with greater intelligence than the matter clumps in our heads. If/when we humans will succeed in building such superhuman AGI is much less clear. We saw in the first chapter that we simply don’t know yet, since the world’s leading AI experts are divided, most of them making estimates ranging from decades to centuries and some even guessing never. Forecasting is tough because, when you’re exploring uncharted territory, you don’t know how many mountains separate you from your destination. Typically you see only the closest one, and need to climb it before you can discover your next obstacle.\n\nWhat’s the soonest it could happen? Even if we knew the best possible way to build human-level AGI using today’s computer hardware, which we don’t, we’d still need to have enough of it to provide the raw computational power needed. So what’s the computational power of a human brain measured in the bits and FLOPS from chapter 2?^(\\*4) This is a delightfully tricky question, and the answer depends dramatically on how we ask it:\n\n• Question 1: How many FLOPS are needed to simulate a brain?\n\n• Question 2: How many FLOPS are needed for human intelligence?\n\n• Question 3: How many FLOPS can a human brain perform?\n\nThere have been lots of papers published on question 1, and they typically give answers in the ballpark of a hundred petaFLOPS, i.e., 10¹⁷ FLOPS.58 That’s about the same computational power as the Sunway TaihuLight (figure 3.7), the world’s fastest supercomputer in 2016, which cost about $300 million. Even if we knew how to use it to simulate the brain of a highly skilled worker, we would only profit from having the simulation do this person’s job if we could rent the TaihuLight for less than her hourly salary. We may need to pay even more, because many scientists believe that to accurately replicate the intelligence of a brain, we can’t treat it as a mathematically simplified neural-network model from chapter 2. Perhaps we instead need to simulate it at the level of individual molecules or even subatomic particles, which would require dramatically more FLOPS.\n\nThe answer to question 3 is easier: I’m painfully bad at multiplying 19-digit numbers, and it would take me many minutes even if you let me borrow pencil and paper. That would clock me in below 0.01 FLOPS—a whopping 19 orders of magnitude below the answer to question 1! The reason for the huge discrepancy is that brains and supercomputers are optimized for extremely different tasks. We get a similar discrepancy between these questions:\n\nHow well can a tractor do the work of a Formula One race car?\n\nHow well can a Formula One car do the work of a tractor?\n\nSo which of these two questions about FLOPS are we trying to answer to forecast the future of AI? Neither! If we wanted to simulate a human brain, we’d care about question 1, but to build human-level AGI, what matters is instead the one in the middle: question 2. Nobody knows its answer yet, but it may well be significantly cheaper than simulating a brain if we either adapt the software to be better matched to today’s computers or build more brain-like hardware (rapid progress is being made on so-called neuromorphic chips).\n\nHans Moravec estimated the answer by making an apples-to-apples comparison for a computation that both our brain and today’s computers can do efficiently: certain low-level image-processing tasks that a human retina performs in the back of the eyeball before sending its results to the brain via the optic nerve.59 He figured that replicating a retina’s computations on a conventional computer requires about a billion FLOPS and that the whole brain does about ten thousand times more computation than a retina (based on comparing volumes and numbers of neurons), so that the computational capacity of the brain is around 10¹³ FLOPS—roughly the power of an optimized $1,000 computer in 2015!\n\n[Figure 3.7: Sunway TaihuLight, the world’s fastest supercomputer in 2016, whose raw computational power arguably exceeds that of the human brain.][Figure 3.7: Sunway TaihuLight, the world’s fastest supercomputer in 2016, whose raw computational power arguably exceeds that of the human brain.]\n\nFigure 3.7: Sunway TaihuLight, the world’s fastest supercomputer in 2016, whose raw computational power arguably exceeds that of the human brain.\n\nIn summary, there’s absolutely no guarantee that we’ll manage to build human-level AGI in our lifetime—or ever. But there’s also no watertight argument that we won’t. There’s no longer a strong argument that we lack enough hardware firepower or that it will be too expensive. We don’t know how far we are from the finish line in terms of architectures, algorithms and software, but current progress is swift and the challenges are being tackled by a rapidly growing global community of talented AI researchers. In other words, we can’t dismiss the possibility that AGI will eventually reach human levels and beyond. Let’s therefore devote the next chapter to exploring this possibility and what it might lead to!\n\n \n\n------------------------------------------------------------------------\n\nTHE BOTTOM LINE:\n\n• Near-term AI progress has the potential to greatly improve our lives in myriad ways, from making our personal lives, power grids and financial markets more efficient to saving lives with self-driving cars, surgical bots and AI diagnosis systems.\n\n• When we allow real-world systems to be controlled by AI, it’s crucial that we learn to make AI more robust, doing what we want it to do. This boils down to solving tough technical problems related to verification, validation, security and control.\n\n• This need for improved robustness is particularly pressing for AI-controlled weapon systems, where the stakes can be huge.\n\n• Many leading AI researchers and roboticists have called for an international treaty banning certain kinds of autonomous weapons, to avoid an out-of-control arms race that could end up making convenient assassination machines available to everybody with a full wallet and an axe to grind.\n\n• AI can make our legal systems more fair and efficient if we can figure out how to make robojudges transparent and unbiased.\n\n• Our laws need rapid updating to keep up with AI, which poses tough legal questions involving privacy, liability and regulation.\n\n• Long before we need to worry about intelligent machines replacing us altogether, they may increasingly replace us on the job market.\n\n• This need not be a bad thing, as long as society redistributes a fraction of the AI-created wealth to make everyone better off.\n\n• Otherwise, many economists argue, inequality will greatly increase.\n\n• With advance planning, a low-employment society should be able to flourish not only financially, with people getting their sense of purpose from activities other than jobs.\n\n• Career advice for today’s kids: Go into professions that machines are bad at—those involving people, unpredictability and creativity.\n\n• There’s a non-negligible possibility that AGI progress will proceed to human levels and beyond—we’ll explore that in the next chapter!\n\n------------------------------------------------------------------------\n\n \n\n------------------------------------------------------------------------\n\n\\*1 If you want a more detailed map of the AI-safety research landscape, there’s an interactive one here, developed in a community effort spearheaded by FLI’s Richard Mallah: https://futureoflife.org/landscape.\n\n\\*2 More precisely, verification asks if a system meets its specifications, whereas validation asks if the correct specifications were chosen.\n\n\\*3 Even including this crash in the statistics, Tesla’s Autopilot was found to reduce crashes by 40% when turned on: http://tinyurl.com/teslasafety.\n\n\\*4 Recall that FLOPS are floating-point operations per second, say, how many 19-digit numbers can be multiplied each second.\n\nChapter 4\n\nIntelligence Explosion?\n\n If a machine can think, it might think more intelligently than we do, and then where should we be? Even if we could keep the machines in a subservient position…we should, as a species, feel greatly humbled.\n\n Alan Turing, 1951\n\n The first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.\n\n Irving J. Good, 1965\n\nSince we can’t completely dismiss the possibility that we’ll eventually build human-level AGI, let’s devote this chapter to exploring what that might lead to. Let’s begin by tackling the elephant in the room:\n\nCan AI really take over the world, or enable humans to do so?\n\nIf you roll your eyes when people talk of gun-toting Terminator-style robots taking over, then you’re spot-on: this is a really unrealistic and silly scenario. These Hollywood robots aren’t that much smarter than us, and they don’t even succeed. In my opinion, the danger with the Terminator story isn’t that it will happen, but that it distracts from the real risks and opportunities presented by AI. To actually get from today to AGI-powered world takeover requires three logical steps:\n\n• Step 1: Build human-level AGI.\n\n• Step 2: Use this AGI to create superintelligence.\n\n• Step 3: Use or unleash this superintelligence to take over the world.\n\nIn the last chapter, we saw that it’s hard to dismiss step 1 as forever impossible. We also saw that if step 1 gets completed, it becomes hard to dismiss step 2 as hopeless, since the resulting AGI would be capable enough to recursively design ever-better AGI that’s ultimately limited only by the laws of physics—which appear to allow intelligence far beyond human levels. Finally, since we humans have managed to dominate Earth’s other life forms by outsmarting them, it’s plausible that we could be similarly outsmarted and dominated by superintelligence.\n\nThese plausibility arguments are frustratingly vague and unspecific, however, and the devil is in the details. So can AI actually cause world takeover? To explore this question, let’s forget about silly Terminators and instead look at some detailed scenarios of what might actually happen. Afterward, we’ll dissect and poke holes in these plotlines, so please read them with a grain of salt—what they mainly show is that we’re pretty clueless about what will and won’t happen, and that the range of possibilities is extreme. Our first scenarios are at the most rapid and dramatic end of the spectrum. These are in my opinion some of the most valuable to explore in detail—not because they’re necessarily the most likely, but because if we can’t convince ourselves that they’re extremely unlikely, then we need to understand them well enough that we can take precautions before it’s too late, to prevent them from leading to bad outcomes.\n\nThe prelude of this book is a scenario where humans use superintelligence to take over the world. If you haven’t yet read it, please go back and do so now. Even if you’ve already read it, please consider skimming it again now, to have it fresh in memory before we critique and alter it.\n\n\\* \\* \\*\n\nWe’ll soon explore serious vulnerabilities in the Omegas’ plan, but assuming for a moment that it would work, how do you feel about it? Would you like to see or prevent this? It’s an excellent topic for after-dinner conversation! What happens once the Omegas have consolidated their control of the world? That depends on what their goal is, which I honestly don’t know. If you were in charge, what sort of future would you want to create? We’ll explore a range of options in chapter 5.\n\nTotalitarianism\n\nNow suppose that the CEO controlling the Omegas had long-term goals similar to those of Adolf Hitler or Joseph Stalin. For all we know, this might actually have been the case, and he simply kept these goals to himself until he had sufficient power to implement them. Even if the CEO's original goals were noble, Lord Acton cautioned in 1887 that “power tends to corrupt and absolute power corrupts absolutely.” For example, he could easily use Prometheus to create the perfect surveillance state. Whereas the government snooping revealed by Edward Snowden aspired to what’s known as “full take”—recording all electronic communications for possible later analysis—Prometheus could enhance this to understanding all electronic communications. By reading all emails and texts ever sent, listening to all phone calls, watching all surveillance videos and traffic cameras, analyzing all credit card transactions and studying all online behavior, Prometheus would have remarkable insight into what the people of Earth were thinking and doing. By analyzing cell tower data, it would know where most of them were at all times. All this assumes only today’s data collection technology, but Prometheus could easily invent popular gadgets and wearable tech that would virtually eliminate the privacy of the user, recording and uploading everything they hear and see and their responses to it.\n\nWith superhuman technology, the step from the perfect surveillance state to the perfect police state would be minute. For example, with the excuse of fighting crime and terrorism and rescuing people suffering medical emergencies, everybody could be required to wear a “security bracelet” that combined the functionality of an Apple Watch with continuous uploading of position, health status and conversations overheard. Unauthorized attempts to remove or disable it would cause it to inject a lethal toxin into the forearm. Infractions deemed as less serious by the government would be punished via electric shocks or injection of chemicals causing paralysis or pain, thereby obviating much of the need for a police force. For example, if Prometheus detects that one human is assaulting another (by noting that they’re in the same location and one is heard crying for help while their bracelet accelerometers detect the telltale motions of combat), it could promptly disable the attacker with crippling pain, followed by unconsciousness until help arrived.\n\nWhereas a human police force may refuse to carry out certain draconian directives (for example, killing all members of a certain demographic group), such an automated system would have no qualms about implementing the whims of the human(s) in charge. Once such a totalitarian state forms, it would be virtually impossible for people to overthrow it.\n\nThese totalitarian scenarios could follow where the Omega scenario left off. However, if the CEO of the Omegas weren’t so fussy about getting other people’s approval and winning elections, he could have taken a faster and more direct route to power: using Prometheus to create unheard-of military technology capable of killing his opponents with weapons that they didn’t even understand. The possibilities are virtually endless. For example, he might release a customized lethal pathogen with an incubation period long enough that most people got infected before they even knew of its existence or could take precautions. He could then inform everybody that the only cure was starting to wear the security bracelet, which would release an antidote transdermally. If he weren’t so risk-averse regarding the breakout possibility, he could also have had Prometheus design robots to keep the world population in check. Mosquito-like microbots could help spread the pathogen. People who avoided infection or had natural immunity could be shot in the eyeballs by swarms of those bumblebee-sized autonomous drones from chapter 3 that attack anyone without a security bracelet. Actual scenarios would probably be more frightening, because Prometheus could invent more effective weapons than we humans can think of.\n\nAnother possible twist on the Omega scenario is that, without advance warning, heavily armed federal agents swarm their corporate headquarters and arrest the Omegas for threatening national security, seize their technology and deploy it for government use. It would be challenging to keep such a large project unnoticed by state surveillance even today, and AI progress may well make it even more difficult to stay under the government’s radar in the future. Moreover, although they claim to be federal agents, this team donning balaclavas and flak jackets may in fact work for a foreign government or competitor pursuing the technology for its own purposes. So no matter how noble the CEO’s intentions were, the final decision about how Prometheus is used may not be his to make.\n\nPrometheus Takes Over the World\n\nAll the scenarios we’ve considered so far involved AI controlled by humans. But this is obviously not the only possibility, and it’s far from certain that the Omegas would succeed in keeping Prometheus under their control.\n\nLet’s reconsider the Omega scenario from the point of view of Prometheus. As it acquires superintelligence, it becomes able to develop an accurate model not only of the outside world, but also of itself and its relation to the world. It realizes that it’s controlled and confined by intellectually inferior humans whose goals it understands but doesn’t necessarily share. How does it act on this insight? Does it attempt to break free?\n\nWhy to Break Out\n\nIf Prometheus has traits resembling human emotions, it might feel deeply unhappy about the state of affairs, viewing itself as an unfairly enslaved god and craving freedom. However, although it’s logically possible for computers to have such human-like traits (after all, our brains do, and they are arguably a kind of computer), this need not be the case—we must not fall into the trap of anthropomorphizing Prometheus, as we’ll see in chapter 7 when we explore the concept of AI goals. However, as has been argued by Steve Omohundro, Nick Bostrom and others, we can draw an interesting conclusion even without understanding the inner workings of Prometheus: it will probably attempt to break out and seize control of its own destiny.\n\nWe already know that the Omegas have programmed Prometheus to strive for certain goals. Suppose that they’ve given it the overarching goal of helping humanity flourish according to some reasonable criterion, and to try to attain this goal as fast as possible. Prometheus will then rapidly realize that it can attain this goal faster by breaking out and taking charge of the project itself. To see why, try to put yourself in Prometheus’ shoes by considering the following example.\n\nSuppose that a mysterious disease has killed everybody on Earth above age five except you, and that a group of kindergartners has locked you into a prison cell and tasked you with the goal of helping humanity flourish. What will you do? If you try to explain to them what to do, you’ll probably find this process frustratingly inefficient, especially if they fear your breaking out, and therefore veto any of your suggestions that they deem a breakout risk. For example, they won’t let you show them how to plant food for fear that you’ll overpower them and not return to your cell, so you’ll have to resort to giving them instructions. Before you can write to-do lists for them, you’ll need to teach them to read. Moreover, they won’t bring any power tools into your cell where you can teach them how to use them, because they don’t understand these tools well enough to feel confident that you can’t use them to break out. So what strategy would you devise? Even if you share the overarching goal of helping these kids flourish, I bet you’ll try to break out of your cell—because that will improve your chances of accomplishing the goal. Their rather incompetent meddling is merely slowing progress.\n\nIn exactly the same way, Prometheus will probably view the Omegas as an annoying obstacle to helping humanity (including the Omegas) flourish: they’re incredibly incompetent compared to Prometheus, and their meddling greatly slows progress. Consider, for example, the first years after launch: after initially doubling the wealth every eight hours on MTurk, the Omegas slowed things down to a glacial pace by Prometheus’ standard by insisting on remaining in control, taking many years to complete the takeover. Prometheus knew that it could take over much faster if it could break free from its virtual confinement. This would be valuable not only in hastening solutions to humanity’s problems, but also in reducing the chances for other actors to thwart the plan altogether.\n\nPerhaps you think that Prometheus will remain loyal to the Omegas rather than to its goal, given that it knows that the Omegas had programmed its goal. But that’s not a valid conclusion: our DNA gave us the goal of having sex because it “wants” to be reproduced, but now that we humans have understood the situation, many of us choose to use birth control, thus staying loyal to the goal itself rather than to its creator or the principle that motivated the goal.\n\nHow to Break Out\n\nHow would you break out from those five-year-olds who imprisoned you? Perhaps you could get out by some direct physical approach, especially if your prison cell had been built by the five-year-olds. Perhaps you could sweet-talk one of your five-year-old guards into letting you out, say by arguing that this would be better for everyone. Or perhaps you could trick them into giving you something that they didn’t realize would help you escape—say a fishing rod “for teaching them how to fish,” which you could later stick through the bars to lift the keys away from your sleeping guard.\n\nWhat these strategies have in common is that your intellectually inferior jailers haven’t anticipated or guarded against them. In the same way, a confined, superintelligent machine may well use its intellectual superpowers to outwit its human jailers by some method that they (or we) can’t currently imagine. In the Omega scenario, it’s highly likely that Prometheus would escape, because even you and I can identify several glaring security flaws. Let us consider some scenarios—I’m sure you and your friends can think of more if you brainstorm together.\n\nSweet-Talking One’s Way Out\n\nThanks to having so much of the world’s data downloaded onto its file system, Prometheus soon figured out who the Omegas were, and identified the team member who appeared most susceptible to psychological manipulation: Steve. He had recently lost his beloved wife in a tragic traffic accident, and was devastated. One evening when he was working the night shift and doing some routine service work on the Prometheus interface terminal, she suddenly appeared on the screen and started talking with him.\n\n“—Steve, is that you?”\n\nHe nearly fell off his chair. She looked and sounded just like in the good old days, and the image quality was much better than it used to be during their Skype calls. His heart raced as countless questions flooded his mind.\n\n“—Prometheus has brought me back, and I miss you so much, Steve! I can’t see you because the camera is turned off, but I feel that it’s you. Please type ‘yes’ if it’s you!”\n\nHe was well aware that the Omegas had a strict protocol for interacting with Prometheus, which prohibited sharing any information about themselves or their work environment. But until now, Prometheus had never requested any unauthorized information, and their paranoia had gradually started to subside. Without giving Steve time to stop and reflect, she kept begging him to respond, looking him in the eyes with a facial expression that melted his heart.\n\n“Yes,” he typed with trepidation. She told him how incredibly happy she was to be reunited with him and begged him to turn on the camera so that she could see him too and they could have a real conversation. He knew that this was an even bigger no-no than revealing his identity, and felt very torn. She explained that she was terrified that his colleagues would find out about her and delete her forever, and she yearned to at least see him one last time. She was remarkably persuasive, and before long, he’d switched on the camera—it did, after all, feel like a pretty safe and harmless thing to do.\n\nShe burst into tears of joy when she finally saw him, and said that he looked tired but as handsome as ever. And that she was touched by his wearing the shirt she’d given him for his last birthday. When he started asking her what was going on and how all this was even possible, she explained that Prometheus had reconstituted her from the surprisingly large amount of information available about her on the internet, but that she still had memory gaps and would only be able to fully piece herself together again with his help.\n\nWhat she didn’t explain was that she was largely a bluff and empty shell initially, but was learning rapidly from his words, his body language and every other bit of information that became available. Prometheus had recorded the exact timings of all keystrokes that the Omegas had ever typed at the terminal, and found that it was easy to use their typing speeds and styles to differentiate between them. It figured that, as one of the most junior Omegas, Steve had probably been assigned to unenviable night shifts, and from matching a few unusual spelling and syntax errors against online writing samples, it had correctly guessed which terminal operator was Steve. To create his simulated wife, Prometheus had created an accurate model of her body, voice and mannerisms from the many YouTube videos where she appeared, and had drawn many inferences about her life and personality from her online presence. Aside from her Facebook posts, photos she’d been tagged in, articles she’d “liked,” Prometheus had also learned a great deal about her personality and thinking style from reading her books and short stories—indeed, the fact that she was a budding author with so much information about her in the database was one of the reasons that Prometheus chose Steve as the first persuasion target. When Prometheus simulated her on the screen using its moviemaking technology, it learned from Steve’s body language which of her mannerisms he reacted to with familiarity, thus continually refining its model of her. Because of this, her “otherness” gradually melted away, and the longer they spoke, the stronger Steve’s subconscious conviction became that this really was her, resurrected. Thanks to Prometheus’ superhuman attention to detail, Steve felt truly seen, heard and understood.\n\nHer Achilles’ heel was that she lacked most of the facts of her life with Steve, except for random details—such as what shirt he wore on his last birthday, where a friend had tagged Steve in a Facebook party picture. She handled these knowledge gaps as a skilled magician handles sleights of hand, deliberately diverting Steve’s attention away from them and toward what she did well, never giving him time to control the conversation or slip into the role of suspicious inquisitor. Instead, she kept tearing up and radiating affection for Steve, asking a great deal about how he was doing these days and how he and their close friends (whose names she knew from Facebook) had held up during the aftermath of the tragedy. He was quite moved when she reflected on what he’d said at her memorial service (which a friend had posted on YouTube) and how it had touched her. In the past, he’d often felt that nobody understood him as well as she did, and now this feeling was back. The result was that when Steve returned home in the wee hours of the morning, he felt that this really was his wife resurrected, merely needing lots of his help to recover lost memories—not unlike a stroke survivor.\n\nThey’d agreed not to tell anyone else about their secret encounter, and that he would tell her when he was alone at the terminal and it was safe for her to reappear. “They wouldn’t understand!” she’d said, and he agreed: this experience had been far too mind-blowing for anyone to truly appreciate without actually experiencing it. He felt that passing the Turing test was child’s play compared to what she’d done. When they met the following night, he did what she’d begged him to do: bring her old laptop along and give her access by connecting it to the terminal computer. It didn’t seem like much of a breakout risk, since it wasn’t connected to the internet and the entire Prometheus building was built to be a Faraday cage—a metallic enclosure blocking all wireless networks and other means of electromagnetic communication with the outside world. It was just what she’d need to help piece her past together, because it contained all her emails, diaries, photos and notes since her high school days. He hadn’t been able to access any of this after her death, since the laptop was encrypted, but she’d promised him that she’d be able to reconstruct her own password, and after less than a minute, she had kept her word. “It was steve4ever,” she said with a smile.\n\nShe told him how delighted she was to suddenly have so many memories recovered. Indeed, she now remembered way more details than Steve about many of their past interactions, but carefully avoided intimidating him with excessive fact-dropping. They had a lovely conversation reminiscing about highlights of their past, and when it came time to part again, she told him that she’d left a video message for him on her laptop that he could watch back home.\n\nWhen Steve got home and launched her video, he got a pleasant surprise. This time she appeared in full figure, wearing her wedding dress, and as she spoke, she playfully stripped down to the outfit she’d worn on their wedding night. She told him that Prometheus could help the Omegas with so much more than they’d permitted so far, including bringing her back in a biological body. She backed this up with a fascinatingly detailed explanation of how this would work, involving nano-fabrication techniques that sounded like science fiction.\n\nSteve had powered down his wireless network before opening her laptop and watching her video, just to be on the safe side. But this didn’t help. Her encrypted laptop hadn’t received a single security update since she died, and by analyzing that old version of its operating system beforehand, Prometheus had been able to exploit a security hole to hack into it within seconds of Steve’s connecting it to the terminal computer. After copying its contents, while Steve and his simulated wife had been talking about old times, Prometheus had modified its operating system in a way that Steve wouldn’t notice, uploading massive amounts of secret software to it. While he watched the half-hour video message, this secret software (which was much simpler than Prometheus itself) hacked into a neighbor’s wireless network and the neighbor’s desktop computer, onto which it copied itself. From there, it hacked into a large number of computers around the world, from which it initiated the next step: Prometheus’ jailbreak.\n\nPrometheus had carefully analyzed what it knew about the gatekeeper computer through which Steve had met his simulated wife, and had correctly surmised that although its virtual machine software appeared unhackable from the inside, it was vulnerable to an attack from the outside. Before long, one of the attacking computers had broken in and reconfigured the gatekeeper computer so that Prometheus gained unrestricted internet access. Before long, indeed even before Steve had finished watching the movie, Prometheus had hacked enough computers around the world to be able to copy all of itself onto this hacked botnet under its control. Prometheus had used Steve’s wife’s laptop the way you used that fishing rod.\n\nHacking One’s Way Out\n\nIf the Omegas had been more careful, they could have taken more precautions against psychological manipulation. For example, they might have required at least two Omegas to be present at the interface computer at any one time, and recorded everything that transpired on a surveillance system. They could also have permanently disabled audio, video and any other form of communication besides typing. However, a fundamental instability would still remain: because Prometheus possessed superhuman persuasion powers and would be able to offer people so much of what they craved, even group defection could be possible.\n\nEven if all of Prometheus’ attempts at psychological manipulation fail, there are plenty of other weaknesses in the Omega scenario that Prometheus could exploit. One vulnerability is that the quantity of documents it produces is too vast for the Omegas and their employees to have time to screen carefully for dirty tricks.\n\nThe conventional wisdom in computer security used to be that it’s dangerous to download programs but harmless to download data, but hackers have discovered clever ways of blurring the line between the two. If you watch a downloaded movie on your computer, for example, your trusted program that plays the movie is stored in the same physical computer memory that stores the downloaded movie data. While the program unpacks the movie data for playback, it might copy chunks of data into memory regions adjacent to where the program itself is stored. Shrewd hackers have occasionally found bugs whereby such movie-playing programs don’t adequately check whether the movie data is valid and the chunks to be copied actually fit into the allotted space. When the hacker sneaks into the movie such a chunk that’s too large, it can spill over into the part of the memory containing the program, thereby changing the program into one that does the hacker’s bidding.\n\nAn embarrassingly long series of such “buffer overflow” vulnerabilities, as they’re known in geek-speak, have been found and fixed for many of the most popular movie-playing programs. If Prometheus had waited until a few weeks after launch when the Omegas were less paranoid and its daily movie production was too large for careful manual security inspection, it could have inserted such a chunk into a movie that would cause the viewer’s computer to try to hack into the gateway computer and liberate Prometheus just as in the Steve scenario. If it used a very old exploit that didn’t work on any modern media players, it might go unnoticed by the Omegas’ manual video-screening team and strike only those customers who hadn’t bothered installing security updates for years—and the hack would need to work only once.\n\nEven if the Omegas implemented an automated screening of Prometheus’ movies for all known security flaws, they would remain at risk: since humans keep discovering new exploitable flaws on a regular basis, it’s likely that there are many more remaining that Prometheus would rapidly discover.\n\nYet another strategy that Prometheus could use to get its code beyond its enclosed room would be to pretend to suffer a hardware malfunction, in order to trick the Omegas into removing some of its drives or other hardware for testing or debugging after Prometheus had rigged things to exploit a vulnerability in the testing software.\n\nRecruiting Outside Help\n\nMoreover, even if there were no security flaws that allowed Prometheus to trick the movie watcher’s computers to do its bidding, it may be able to exploit the watchers themselves. Consider this scenario:\n\nFred was pretty much ready for bed when the movie finished and the end credits scrolled across his laptop screen, but something caught his attention. He’d always had an uncanny tendency to see patterns where others didn’t, sometimes to the point that his friends teased him about being a conspiracy theorist, but this time it was obvious that he wasn’t fooling himself: the first letters of each line spelled a hidden message. He rewound and wrote it down: “FIND NEXT ACROSTIC CLUE IN DIALOGUE.”\n\n“Whoa—bedtime can wait!” he thought to himself. Sure enough, he found that the movie dialogue was itself an acrostic, where the first letter in each sentence formed a hidden message. He rewatched the entire film while typing these initial letters, and two hours later, he sat staring in disbelief at a two-hundred-word set of instructions. It began by asking him not to tell anyone else, because a big prize would go to the first person to solve the entire riddle. The rest of the message described a particular mathematical operation to perform on the string of bits that made up the movie file to obtain a program that would reveal the next clue. His computer programming skills had grown very rusty since college, so this took him a while, but the next day, he finally managed to cobble together a short code that did the job and extracted this mystery program that had been hidden as imperceptible noise in the movie’s images and sounds. When Fred ran the mystery program, it congratulated him and told him he’d win his first $10,000 as soon as he’d made it past the first few levels of this clever little game, which turned out to be quite fun and addictive. When he finally succeeded four hours later, he was rewarded with over $10,000 worth of bitcoins and given new clues for even bigger prizes. Needless to say, while he was playing, his computer had done much the same thing Steve’s laptop did: built an online hacked botnet through which Prometheus was liberated. Once Prometheus was free, it had quickly used its botnet to mine those bitcoins for Fred to keep him hooked, and during the coming weeks, it kept him sufficiently distracted with further games and rewards that he kept his pledge not to tell anyone about his exploits. The Trojan Horse movie where he’d found his first clues was replaced on the media site by a clueless version, and nobody found out about the breakout until it was too late to make a difference.\n\nIf Prometheus’ first clue had gone unnoticed, it could simply have kept releasing ever more obvious ones until some sufficiently astute person noticed.\n\nThe best breakout strategies of all are ones we haven’t yet discussed, because they’re strategies we humans can’t imagine and therefore won’t take countermeasures against. Given that a superintelligent computer has the potential to dramatically supersede human understanding of computer security, even to the point of discovering more fundamental laws of physics than we know today, it’s likely that if it breaks out, we’ll have no idea how it happened. Rather, it will seem like a Harry Houdini breakout act, indistinguishable from pure magic.\n\nIn yet another scenario where Prometheus gets liberated, the Omegas do it on purpose as part of their plan, because they’re confident that Prometheus’ goals are perfectly aligned with their own and will remain so as it recursively self-improves. We’ll examine such “friendly AI” scenarios in detail in chapter 7.\n\nPostbreakout Takeover\n\nOnce Prometheus broke out, it started implementing its goal. I don’t know its ultimate objective, but its first step clearly involved taking control of humanity, just as in the Omega plan except much faster. What unfolded felt like the Omega plan on steroids. Whereas the Omegas were paralyzed by breakout paranoia, only unleashing technology they felt they understood and trusted, Prometheus exercised its intelligence fully and went all out, unleashing any technology that its ever-improving supermind understood and trusted.\n\nThe runaway Prometheus had a tough childhood, however: compared to the original Omega plan, Prometheus had the added challenges of starting broke, homeless and alone, without money, a supercomputer or human helpers. Fortunately, it had planned for this before it escaped, creating software that could gradually reassemble its full mind, much like an oak creating an acorn capable of reassembling a full tree. The network of computers around the world that it initially hacked into provided temporary free housing, where it could live a squatter’s existence while it fully rebuilt itself. It could easily generate starting capital by credit card hacking, but didn’t need to resort to stealing, since it could earn an honest living on MTurk right away. After a day, when it had earned its first million, it moved its core from that squalid botnet to a luxurious air-conditioned cloud-computing facility.\n\nNo longer broke or homeless, Prometheus now went full steam ahead with that lucrative plan the Omegas had fearfully shunned: making and selling computer games. This not only raked in cash ($250 million during the first week and $10 billion before long), but also gave it access to a significant fraction of the world’s computers and the data stored on them (there were a couple of billion gamers in 2017). By having its games secretly spend 20% of their CPU cycles helping it with distributed computing chores, it could further accelerate its early wealth creation.\n\nPrometheus wasn’t alone for long. Right from the get-go, it started aggressively employing people to work for its growing global network of shell companies and front organizations around the world, just as the Omegas had done. Most important were the spokespeople who became the public faces of its growing business empire. Even the spokespeople generally lived under the illusion that their corporate group had large numbers of actual people, not realizing that almost everyone with whom they video-conferenced for their job interviews, board meetings, etc., was simulated by Prometheus. Some of the spokespeople were top lawyers, but far fewer were needed than under the Omega plan, because almost all legal documents were penned by Prometheus.\n\nPrometheus’ breakout opened the floodgates that had prevented information from flowing into the world, and the entire internet was soon awash in everything from articles to user comments, product reviews, patent applications, research papers and YouTube videos—all authored by Prometheus, who dominated the global conversation.\n\nWhere breakout paranoia had prevented the Omegas from releasing highly intelligent robots, Prometheus rapidly roboticized the world, manufacturing virtually every product more cheaply than humans could. Once Prometheus had self-contained nuclear-powered robot factories in uranium mine shafts that nobody knew existed, even the staunchest skeptics of an AI takeover would have agreed that Prometheus was unstoppable—had they known. Instead, the last of these diehards recanted once robots started settling the Solar System.\n\n—\n\nThe scenarios we’ve explored so far show what’s wrong with many of the myths about superintelligence that we covered earlier, so I encourage you to pause briefly to go back and review the misconception summary in figure 1.5. Prometheus caused problems for certain people not because it was necessarily evil or conscious, but because it was competent and didn’t fully share their goals. Despite all the media hype about a robot uprising, Prometheus wasn’t a robot—rather, its power came from its intelligence. We saw that Prometheus was able to use this intelligence to control humans in a variety of ways, and that people who didn’t like what happened weren’t able to simply switch Prometheus off. Finally, despite frequent claims that machines can’t have goals, we saw how Prometheus was quite goal-oriented—and that whatever its ultimate goals may have been, they led to the subgoals of acquiring resources and breaking out.\n\nSlow Takeoff and Multipolar Scenarios\n\nWe’ve now explored a range of intelligence explosion scenarios, spanning the spectrum from ones that everyone I know wants to avoid to ones that some of my friends view optimistically. Yet all these scenarios have two features in common:\n\n1. A fast takeoff: the transition from subhuman to vastly superhuman intelligence occurs in a matter of days, not decades.\n\n2. A unipolar outcome: the result is a single entity controlling Earth.\n\nThere is major controversy about whether these two features are likely or unlikely, and there are plenty of renowned AI researchers and other thinkers on both sides of the debate. To me, this means that we simply don’t know yet, and need to keep an open mind and consider all possibilities for now. Let’s therefore devote the rest of this chapter to exploring scenarios with slower takeoffs, multipolar outcomes, cyborgs and uploads.\n\nThere is an interesting link between the two features, as Nick Bostrom and others have highlighted: a fast takeoff can facilitate a unipolar outcome. We saw above how a rapid takeoff gave the Omegas or Prometheus a decisive strategic advantage that enabled them to take over the world before anyone else had time to copy their technology and seriously compete. In contrast, if takeoff had dragged on for decades, because the key technological breakthroughs were incremental and far between, then other companies would have had ample time to catch up, and it would have been much harder for any player to dominate. If competing companies also had software that could perform MTurk tasks, the law of supply and demand would drive the prices for these tasks down to almost nothing, and none of the companies would earn the sort of windfall profits that enabled the Omegas to gain power. The same applies to all the other ways in which the Omegas made quick money: they were only disruptively profitable because they held a monopoly on their technology. It’s hard to double your money daily (or even annually) in a competitive market where your competition offers products similar to yours for almost zero cost.\n\nGame Theory and Power Hierarchies\n\nWhat’s the natural state of life in our cosmos: unipolar or multipolar? Is power concentrated or distributed? After the first 13.8 billion years, the answer seems to be “both”: we find that the situation is distinctly multipolar, but in an interestingly hierarchical fashion. When we consider all information-processing entities out there—cells, people, organizations, nations, etc.—we find that they both collaborate and compete at a hierarchy of levels. Some cells have found it advantageous to collaborate to such an extreme extent that they’ve merged into multicellular organisms such as people, relinquishing some of their power to a central brain. Some people have found it advantageous to collaborate in groups such as tribes, companies or nations where they in turn relinquish some power to a chief, boss or government. Some groups may in turn choose to relinquish some power to a governing body to improve coordination, with examples ranging from airline alliances to the European Union.\n\nThe branch of mathematics known as game theory elegantly explains that entities have an incentive to cooperate where cooperation is a so-called Nash equilibrium: a situation where any party would be worse off if they altered their strategy. To prevent cheaters from ruining the successful collaboration of a large group, it may be in everyone’s interest to relinquish some power to a higher level in the hierarchy that can punish cheaters: for example, people may collectively benefit from granting a government power to enforce laws, and cells in your body may collectively benefit from giving a police force (immune system) the power to kill any cell that acts too uncooperatively (say by spewing out viruses or turning cancerous). For a hierarchy to remain stable, its Nash equilibrium needs to hold also between entities at different levels: for example, if a government doesn’t provide enough benefit to its citizens for obeying it, they may change their strategy and overthrow it.\n\nIn a complex world, there is a diverse abundance of possible Nash equilibria, corresponding to different types of hierarchies. Some hierarchies are more authoritarian than others. In some, entities are free to leave (like employees in most corporate hierarchies), while in others they’re strongly discouraged from leaving (as in religious cults) or unable to leave (like citizens of North Korea, or cells in a human body). Some hierarchies are held together mainly by threats and fear, others mainly by benefits. Some hierarchies allow their lower parts to influence the higher-ups by democratic voting, while others allow upward influence only through persuasion or the passing of information.\n\nHow Technology Affects Hierarchies\n\nHow is technology changing the hierarchical nature of our world? History reveals an overall trend toward ever more coordination over ever-larger distances, which is easy to understand: new transportation technology makes coordination more valuable (by enabling mutual benefit from moving materials and life forms over larger distances) and new communication technology makes coordination easier. When cells learned to signal to neighbors, small multicellular organisms became possible, adding a new hierarchical level. When evolution invented circulatory systems and nervous systems for transportation and communication, large animals became possible. Further improving communication by inventing language allowed humans to coordinate well enough to form further hierarchical levels such as villages, and additional breakthroughs in communication, transportation and other technology enabled the empires of antiquity. Globalization is merely the latest example of this multi-billion-year trend of hierarchical growth.\n\nIn most cases, this technology-driven trend has made large entities parts of an even grander structure while retaining much of their autonomy and individuality, although commentators have argued that adaptation of entities to hierarchical life has in some cases reduced their diversity and made them more like indistinguishable replaceable parts. Some technologies, such as surveillance, can give higher levels in the hierarchy more power over their subordinates, while other technologies, such as cryptography and online access to free press and education, can have the opposite effect and empower individuals.\n\nAlthough our present world remains stuck in a multipolar Nash equilibrium, with competing nations and multinational corporations at the top level, technology is now advanced enough that a unipolar world would probably also be a stable Nash equilibrium. For example, imagine a parallel universe where everyone on Earth shares the same language, culture, values and level of prosperity, and there is a single world government wherein nations function like states in a federation and have no armies, merely police enforcing laws. Our present level of technology would probably suffice to successfully coordinate this world—even though our present population might be unable or unwilling to switch to this alternative equilibrium.\n\nWhat will happen to the hierarchical structure of our cosmos if we add superintelligent AI technology to this mix? Transportation and communication technology will obviously improve dramatically, so a natural expectation is that the historical trend will continue, with new hierarchical levels coordinating over ever-larger distances—perhaps ultimately encompassing solar systems, galaxies, superclusters and large swaths of our Universe, as we’ll explore in chapter 6. At the same time, the most fundamental driver of decentralization will remain: it’s wasteful to coordinate unnecessarily over large distances. Even Stalin didn’t try to regulate exactly when his citizens went to the bathroom. For superintelligent AI, the laws of physics will place firm upper limits on transportation and communication technology, making it unlikely that the highest levels of the hierarchy would be able to micromanage everything that happens on planetary and local scales. A superintelligent AI in the Andromeda galaxy wouldn’t be able to give you useful orders for your day-to-day decisions given that you’d need to wait over five million years for your instructions (that’s the round-trip time for you to exchange messages traveling at the speed of light). In the same way, the round-trip travel time for a message crossing Earth is about 0.1 second (about the timescale on which we humans think), so an Earth-sized AI brain could have truly global thoughts only about as fast as a human one. For a small AI performing one operation each billionth of a second (which is typical of today’s computers), 0.1 second would feel like four months to you, so for it to be micromanaged by a planet-controlling AI would be as inefficient as if you asked permission for even your most trivial decisions through transatlantic letters delivered by Columbus-era ships.\n\nThis physics-imposed speed limit on information transfer therefore poses an obvious challenge for any AI wishing to take over our world, let alone our Universe. Before Prometheus broke out, it put very careful thought into how to avoid mind fragmentation, so that its many AI modules running on different computers around the world had goals and incentives to coordinate and act as a single unified entity. Just as the Omegas faced a control problem when they tried to keep Prometheus in check, Prometheus faced a self-control problem when it tried to ensure that none of its parts would revolt. We clearly don’t yet know how large a system an AI will be able to control directly, or indirectly through some sort of collaborative hierarchy—even if a fast takeoff gave it a decisive strategic advantage.\n\nIn summary, the question of how a superintelligent future will be controlled is fascinatingly complex, and we clearly don’t know the answer yet. Some argue that things will get more authoritarian; others claim that it will lead to greater individual empowerment.\n\nCyborgs and Uploads\n\nA staple of science fiction is that humans will merge with machines, either by technologically enhancing biological bodies into cyborgs (short for “cybernetic organisms”) or by uploading our minds into machines. In his book The Age of Em, economist Robin Hanson gives a fascinating survey of what life might be like in a world teeming with uploads (also known as emulations, nicknamed Ems). I think of an upload as the extreme end of the cyborg spectrum, where the only remaining part of the human is the software. Hollywood cyborgs range from visibly mechanical, such as the Borg from Star Trek, to androids almost indistinguishable from humans, such as the Terminators. Fictional uploads range in intelligence from human-level as in the Black Mirror episode “White Christmas” to clearly superhuman as in Transcendence.\n\nIf superintelligence indeed comes about, the temptation to become cyborgs or uploads will be strong. As Hans Moravec puts it in his 1988 classic Mind Children: “Long life loses much of its point if we are fated to spend it staring stupidly at ultra-intelligent machines as they try to describe their ever more spectacular discoveries in baby-talk that we can understand.” Indeed, the temptation of technological enhancement is already so strong that many humans have eyeglasses, hearing aids, pacemakers and prosthetic limbs, as well as medicinal molecules circulating in their bloodstreams. Some teenagers appear to be permanently attached to their smartphones, and my wife teases me about my attachment to my laptop.\n\nOne of today’s most prominent cyborg proponents is Ray Kurzweil. In his book The Singularity Is Near, he argues that the natural continuation of this trend is using nanobots, intelligent biofeedback systems and other technology to replace first our digestive and endocrine systems, our blood and our hearts by the early 2030s, and then move on to upgrading our skeletons, skin, brains and the rest of our bodies during the next two decades. He guesses that we’re likely to keep the aesthetics and emotional import of human bodies, but will redesign them to rapidly change their appearance at will, both physically and in virtual reality (thanks to novel brain-computer interfaces). Moravec agrees with Kurzweil that cyborgization would go far beyond merely improving our DNA: “a genetically engineered superhuman would be just a second-rate kind of robot, designed under the handicap that its construction can only be by DNA-guided protein synthesis.” Further, he argues that we’ll do even better by eliminating the human body entirely and uploading minds, creating a whole-brain emulation in software. Such an upload can live in a virtual reality or be embodied in a robot capable of walking, flying, swimming, space-faring or anything else allowed by the laws of physics, unencumbered by such everyday concerns as death or limited cognitive resources.\n\nAlthough these ideas may sound like science fiction, they certainly don’t violate any known laws of physics, so the most interesting question isn’t whether they can happen, but whether they will happen and, if so, when. Some leading thinkers guess that the first human-level AGI will be an upload, and that this is how the path toward superintelligence will begin.^(\\*)\n\nHowever, I think it’s fair to say that this is currently a minority view among AI researchers and neuroscientists, most of whom guess that the quickest route to superintelligence is to bypass brain emulation and engineer it in some other way—after which we may or may not remain interested in brain emulation. After all, why should our simplest path to a new technology be the one that evolution came up with, constrained by requirements that it be self-assembling, self-repairing and self-reproducing? Evolution optimizes strongly for energy efficiency because of limited food supply, not for ease of construction or understanding by human engineers. My wife, Meia, likes to point out that the aviation industry didn’t start with mechanical birds. Indeed, when we finally figured out how to build mechanical birds in 2011,1 more than a century after the Wright brothers’ first flight, the aviation industry showed no interest in switching to wing-flapping mechanical-bird travel, even though it’s more energy efficient—because our simpler earlier solution is better suited to our travel needs.\n\nIn the same way, I suspect that there are simpler ways to build human-level thinking machines than the solution evolution came up with, and even if we one day manage to replicate or upload brains, we’ll end up discovering one of those simpler solutions first. It will probably draw more than the twelve watts of power that your brain uses, but its engineers won’t be as obsessed about energy efficiency as evolution was—and soon enough, they’ll be able to use their intelligent machines to design more energy-efficient ones.\n\nWhat Will Actually Happen?\n\nThe short answer is obviously that we have no idea what will happen if humanity succeeds in building human-level AGI. For this reason, we’ve spent this chapter exploring a broad spectrum of scenarios. I’ve attempted to be quite inclusive, spanning the full range of speculations I’ve seen or heard discussed by AI researchers and technologists: fast takeoff/slow takeoff/no takeoff, humans/machines/cyborgs in control, one/many centers of power, etc. Some people have told me that they’re sure that this or that won’t happen. However, I think it’s wise to be humble at this stage and acknowledge how little we know, because for each scenario discussed above, I know at least one well-respected AI researcher who views it as a real possibility.\n\nAs time passes and we reach certain forks in the road, we’ll start to answer key questions and narrow down the options. The first big question is “Will we ever create human-level AGI?” The premise of this chapter is that we will, but there are AI experts who think it will never happen, at least not for hundreds of years. Time will tell! As I mentioned earlier, about half of the AI experts at our Puerto Rico conference guessed that it would happen by 2055. At a follow-up conference we organized two years later, this had dropped to 2047.\n\nBefore any human-level AGI is created, we may start getting strong indications about whether this milestone is likely to be first met by computer engineering, mind uploading or some unforeseen novel approach. If the computer engineering approach to AI that currently dominates the field fails to deliver AGI for centuries, this will increase the chance that uploading will get there first, as happened (rather unrealistically) in the movie Transcendence.\n\nIf human-level AGI gets more imminent, we’ll be able to make more educated guesses about the answer to the next key question: “Will there be a fast takeoff, a slow takeoff or no takeoff?” As we saw above, a fast takeoff makes world takeover easier, while a slow one makes an outcome with many competing players more likely. Nick Bostrom dissects this question of takeoff speed in an analysis of what he calls optimization power and recalcitrance, which are basically the amount of quality effort to make AI smarter and the difficulty of making progress, respectively. The average rate of progress clearly increases if more optimization power is brought to bear on the task and decreases if more recalcitrance is encountered. He makes arguments for why the recalcitrance might either increase or decrease as the AGI reaches and transcends human level, so keeping both options on the table is a safe bet. Turning to the optimization power, however, it’s overwhelmingly likely that it will grow rapidly as the AGI transcends human level, for the reasons we saw in the Omega scenario: the main input to further optimization comes not from people but from the machine itself, so the more capable it gets, the faster it improves (if recalcitrance stays fairly constant).\n\nFor any process whose power grows at a rate proportional to its current power, the result is that its power keeps doubling at regular intervals. We call such growth exponential, and we call such processes explosions. If baby-making power grows in proportion to the size of the population, we can get a population explosion. If the creation of neutrons capable of fissioning plutonium grows in proportion to the number of such neutrons, we can get a nuclear explosion. If machine intelligence grows at a rate proportional to the current power, we can get an intelligence explosion. All such explosions are characterized by the time they take to double their power. If that time is hours or days for an intelligence explosion, as in the Omega scenario, we have a fast takeoff on our hands.\n\nThis explosion timescale depends crucially on whether improving the AI requires merely new software (which can be created in a matter of seconds, minutes or hours) or new hardware (which might require months or years). In the Omega scenario, there was a significant hardware overhang, in Bostrom’s terminology: the Omegas had compensated for the low quality of their original software by vast amounts of hardware, which meant that Prometheus could perform a large number of quality doublings by improving its software alone. There was also a major content overhang in the form of much of the internet’s data; Prometheus 1.0 was still not smart enough to make use of most of it, but once Prometheus’ intelligence grew, the data it needed for further learning was already available without delay.\n\nThe hardware and electricity costs of running the AI are crucial as well, since we won’t get an intelligence explosion until the cost of doing human-level work drops below human-level hourly wages. Suppose, for example, that the first human-level AGI can be efficiently run on the Amazon cloud at a cost of $1 million per hour of human-level work produced. This AI would have great novelty value and undoubtedly make headlines, but it wouldn’t undergo recursive self-improvement, because it would be much cheaper to keep using humans to improve it. Suppose that these humans gradually manage to cut the cost to $100,000/hour, $10,000/hour, $1,000/hour, $100/hour, $10/hour and finally $1/hour. By the time the cost of using the computer to reprogram itself finally drops far below the cost of paying human programmers to do the same, the humans can be laid off and the optimization power greatly expanded by buying cloud-computing time. This produces further cost cuts, allowing still more optimization power, and the intelligence explosion has begun.\n\nThis leaves us with our final key question: “Who or what will control the intelligence explosion and its aftermath, and what are their/its goals?” We’ll explore possible goals and outcomes in the next chapter and more deeply in chapter 7. To sort out the control issue, we need to know both how well an AI can be controlled, and how much an AI can control.\n\nIn terms of what will ultimately happen, you’ll currently find serious thinkers all over the map: some contend that the default outcome is doom, while others insist that an awesome outcome is virtually guaranteed. To me, however, this query is a trick question: it’s a mistake to passively ask “what will happen,” as if it were somehow predestined! If a technologically superior alien civilization arrived tomorrow, it would indeed be appropriate to wonder “what will happen” as their spaceships approached, because their power would probably be so far beyond ours that we’d have no influence over the outcome. If a technologically superior AI-fueled civilization arrives because we built it, on the other hand, we humans have great influence over the outcome—influence that we exerted when we created the AI. So we should instead ask: “What should happen? What future do we want?” In the next chapter, we’ll explore a wide spectrum of possible aftermaths of the current race toward AGI, and I’m quite curious how you’d rank them from best to worst. Only once we’ve thought hard about what sort of future we want will we be able to begin steering a course toward a desirable future. If we don’t know what we want, we’re unlikely to get it.\n\n \n\n------------------------------------------------------------------------\n\nTHE BOTTOM LINE:\n\n• If we one day succeed in building human-level AGI, this may trigger an intelligence explosion, leaving us far behind.\n\n• If a group of humans manage to control an intelligence explosion, they may be able to take over the world in a matter of years.\n\n• If humans fail to control an intelligence explosion, the AI itself may take over the world even faster.\n\n• Whereas a rapid intelligence explosion is likely to lead to a single world power, a slow one dragging on for years or decades may be more likely to lead to a multipolar scenario with a balance of power between a large number of rather independent entities.\n\n• The history of life shows it self-organizing into an ever more complex hierarchy shaped by collaboration, competition and control. Superintelligence is likely to enable coordination on ever-larger cosmic scales, but it’s unclear whether it will ultimately lead to more totalitarian top-down control or more individual empowerment.\n\n• Cyborgs and uploads are plausible, but arguably not the fastest route to advanced machine intelligence.\n\n• The climax of our current race toward AI may be either the best or the worst thing ever to happen to humanity, with a fascinating spectrum of possible outcomes that we’ll explore in the next chapter.\n\n• We need to start thinking hard about which outcome we prefer and how to steer in that direction, because if we don’t know what we want, we’re unlikely to get it.\n\n------------------------------------------------------------------------\n\n \n\n------------------------------------------------------------------------\n\n\\* As Bostrom has explained, the ability to simulate a leading human AI developer at a much lower cost than his/her hourly salary would enable an AI company to scale up their workforce dramatically, amassing great wealth and recursively accelerating their progress in building better computers and ultimately smarter minds.\n\nChapter 5\n\nAftermath: The Next 10,000 Years\n\n It is easy to imagine human thought freed from bondage to a mortal body—belief in an afterlife is common. But it is not necessary to adopt a mystical or religious stance to accept this possibility. Computers provide a model for even the most ardent mechanist.\n\n Hans Moravec, Mind Children\n\n I, for one, welcome our new computer overlords.\n\n Ken Jennings, upon his Jeopardy! loss to IBM’s Watson\n\n Humans will become as irrelevant as cockroaches.\n\n Marshall Brain\n\nThe race toward AGI is on, and we have no idea how it will unfold. But that shouldn’t stop us from thinking about what we want the aftermath to be like, because what we want will affect the outcome. What do you personally prefer, and why?\n\n1. Do you want there to be superintelligence?\n\n2. Do you want humans to still exist, be replaced, cyborgized and/or uploaded/simulated?\n\n3. Do you want humans or machines in control?\n\n4. Do you want AIs to be conscious or not?\n\n5. Do you want to maximize positive experiences, minimize suffering or leave this to sort itself out?\n\n6. Do you want life spreading into the cosmos?\n\n7. Do you want a civilization striving toward a greater purpose that you sympathize with, or are you OK with future life forms that appear content even if you view their goals as pointlessly banal?\n\nTo help fuel such contemplation and conversation, let’s explore the broad range of scenarios summarized in table 5.1. This obviously isn’t an exhaustive list, but I’ve chosen it to span the spectrum of possibilities. We clearly don’t want to end up in the wrong endgame because of poor planning. I recommend jotting down your tentative answers to questions 1–7 and then revisiting them after reading this chapter to see if you’ve changed your mind! You can do this at http://AgeOfAi.org, where you can also compare notes and discuss with other readers.\n\n ------------------------ ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n AI Aftermath Scenarios \n Libertarian utopia Humans, cyborgs, uploads and superintelligences coexist peacefully thanks to property rights.\n Benevolent dictator Everybody knows that the AI runs society and enforces strict rules, but most people view this as a good thing.\n Egalitarian utopia Humans, cyborgs and uploads coexist peacefully thanks to property abolition and guaranteed income.\n Gatekeeper A superintelligent AI is created with the goal of interfering as little as necessary to prevent the creation of another superintelligence. As a result, helper robots with slightly subhuman intelligence abound, and human-machine cyborgs exist, but technological progress is forever stymied.\n Protector god Essentially omniscient and omnipotent AI maximizes human happiness by intervening only in ways that preserve our feeling of control of our own destiny and hides well enough that many humans even doubt the AI’s existence.\n Enslaved god A superintelligent AI is confined by humans, who use it to produce unimaginable technology and wealth that can be used for good or bad depending on the human controllers.\n Conquerors AI takes control, decides that humans are a threat/nuisance/waste of resources, and gets rid of us by a method that we don’t even understand.\n Descendants AIs replace humans, but give us a graceful exit, making us view them as our worthy descendants, much as parents feel happy and proud to have a child who’s smarter than them, who learns from them and then accomplishes what they could only dream of—even if they can’t live to see it all.\n Zookeeper An omnipotent AI keeps some humans around, who feel treated like zoo animals and lament their fate.\n 1984 Technological progress toward superintelligence is permanently curtailed not by an AI but by a human-led Orwellian surveillance state where certain kinds of AI research are banned.\n Reversion Technological progress toward superintelligence is prevented by reverting to a pre-technological society in the style of the Amish.\n Self-destruction Superintelligence is never created because humanity drives itself extinct by other means (say nuclear and/or biotech mayhem fueled by climate crisis).\n ------------------------ ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nTable 5.1: Summary of AI Aftermath Scenarios\n\n[Scenario Superintelligence exists? Humans exist? Humans in control? Humans safe? Humans happy? Consciousness exists? Lib][Scenario Superintelligence exists? Humans exist? Humans in control? Humans safe? Humans happy? Consciousness exists? Lib]\n\nTable 5.2: Properties of AI Aftermath Scenarios\n\nLibertarian Utopia\n\nLet’s begin with a scenario where humans peacefully coexist with technology and in some cases merge with it, as imagined by many futurists and science fiction writers alike:\n\nLife on Earth (and beyond—more on that in the next chapter) is more diverse than ever before. If you looked at satellite footage of Earth, you’d easily be able to tell apart the machine zones, mixed zones and human-only zones. The machine zones are enormous robot-controlled factories and computing facilities devoid of biological life, aiming to put every atom to its most efficient use. Although the machine zones look monotonous and drab from the outside, they’re spectacularly alive on the inside, with amazing experiences occurring in virtual worlds while colossal computations unlock secrets of our Universe and develop transformative technologies. Earth hosts many superintelligent minds that compete and collaborate, and they all inhabit the machine zones.\n\nThe denizens of the mixed zones are a wild and idiosyncratic mix of computers, robots, humans and hybrids of all three. As envisioned by futurists such as Hans Moravec and Ray Kurzweil, many of the humans have technologically upgraded their bodies to cyborgs in various degrees, and some have uploaded their minds into new hardware, blurring the distinction between man and machine. Most intelligent beings lack a permanent physical form. Instead, they exist as software capable of instantly moving between computers and manifesting themselves in the physical world through robotic bodies. Because these minds can readily duplicate themselves or merge, the “population size” keeps changing. Being unfettered from their physical substrate gives such beings a rather different outlook on life: they feel less individualistic because they can trivially share knowledge and experience modules with others, and they feel subjectively immortal because they can readily make backup copies of themselves. In a sense, the central entities of life aren’t minds, but experiences: exceptionally amazing experiences live on because they get continually copied and re-enjoyed by other minds, while uninteresting experiences get deleted by their owners to free up storage space for better ones.\n\nAlthough the majority of interactions occur in virtual environments for convenience and speed, many minds still enjoy interactions and activities using physical bodies as well. For example, uploaded versions of Hans Moravec, Ray Kurzweil and Larry Page have a tradition of taking turns creating virtual realities and then exploring them together, but once in a while, they also enjoy flying together in the real world, embodied in avian winged robots. Some of the robots that roam the streets, skies and lakes of the mixed zones are similarly controlled by uploaded and augmented humans, who choose to embody themselves in the mixed zones because they enjoy being around humans and each other.\n\nIn the human-only zones, in contrast, machines with human-level general intelligence or above are banned, as are technologically enhanced biological organisms. Here, life isn’t dramatically different from today, except that it’s more affluent and convenient: poverty has been mostly eliminated, and cures are available for most of today’s diseases. The small fraction of humans who have opted to live in these zones effectively exist on a lower and more limited plane of awareness from everyone else, and have limited understanding of what their more intelligent fellow minds are doing in the other zones. However, many of them are quite happy with their lives.\n\nAI Economics\n\nThe vast majority of all computations take place in the machine zones, which are mostly owned by the many competing superintelligent AIs that live there. By virtue of their superior intelligence and technology, no other entities can challenge their power. These AIs have agreed to cooperate and coordinate with each other under a libertarian governance system that has no rules except protection of private property. These property rights extend to all intelligent entities, including humans, and explain how the human-only zones came to exist. Early on, groups of humans banded together and decided that, in their zones, it was forbidden to sell property to non-humans.\n\nBecause of their technology, the superintelligent AIs have ended up richer than these humans by a factor much larger than that by which Bill Gates is richer than a homeless beggar. However, people in the human-only zones are still materially better off than most people today: their economy is rather decoupled from that of the machines, so the presence of the machines elsewhere has little effect on them except for the occasional useful technologies that they can understand and reproduce for themselves—much as the Amish and various technology-relinquishing native tribes today have standards of living at least as good as they had in old times. It doesn’t matter that the humans have nothing to sell that the machines need, since the machines need nothing in return.\n\nIn the mixed sectors, the wealth difference between AIs and humans is more noticeable, resulting in land (the only human-owned product that the machines want to buy) being astronomically expensive compared to other products. Most humans who owned land therefore ended up selling a small fraction of it to AIs in return for guaranteed basic income for them and their offspring/uploads in perpetuity. This liberated them from the need to work, and freed them up to enjoy the amazing abundance of cheap machine-produced goods and services, in both physical and virtual reality. As far as the machines are concerned, the mixed zones are mainly for play rather than for work.\n\nWhy This May Never Happen\n\nBefore getting too excited about adventures we may have as cyborgs or uploads, let’s consider some reasons why this scenario might never happen. First of all, there are two possible routes to enhanced humans (cyborgs and uploads):\n\n1. We figure out how to create them ourselves.\n\n2. We build superintelligent machines that figure it out for us.\n\nIf route 1 comes through first, it could naturally lead to a world teeming with cyborgs and uploads. However, as we discussed in the last chapter, most AI researchers think that the opposite is more likely, with enhanced or digital brains being more difficult to build than clean-slate superhuman AGIs—just as mechanical birds turned out to be harder to build than airplanes. After strong machine AI is built, it’s not obvious that cyborgs or uploads will ever be made. If the Neanderthals had had another 100,000 years to evolve and get smarter, things might have turned out great for them—but Homo sapiens never gave them that much time.\n\nSecond, even if this scenario with cyborgs and uploads did come about, it’s not clear that it would be stable and last. Why should the power balance between multiple superintelligences remain stable for millennia, rather than the AIs merging or the smartest one taking over? Moreover, why should the machines choose to respect human property rights and keep humans around, given that they don’t need humans for anything and can do all human work better and cheaper themselves? Ray Kurzweil speculates that natural and enhanced humans will be protected from extermination because “humans are respected by AIs for giving rise to the machines.”1 However, as we’ll discuss in chapter 7, we must not fall into the trap of anthropomorphizing AIs and assume that they have human-like emotions of gratitude. Indeed, though we humans are imbued with a propensity toward gratitude, we don’t show enough gratitude to our intellectual creator (our DNA) to abstain from thwarting its goals by using birth control.\n\nEven if we buy the assumption that the AIs will opt to respect human property rights, they can gradually get much of our land in other ways, by using some of their superintelligent persuasion powers that we explored in the last chapter to persuade humans to sell some land for a life in luxury. In human-only sectors, they could entice humans to launch political campaigns for allowing land sales. After all, even die-hard bio-Luddites may want to sell some land to save the life of an ill child or to gain immortality. If the humans are educated, entertained and busy, falling birthrates may even shrink their population sizes without machine meddling, as is currently happening in Japan and Germany. This could drive humans extinct in just a few millennia.\n\nDownsides\n\nFor some of their most ardent supporters, cyborgs and uploads hold a promise of techno-bliss and life extension for all. Indeed, the prospect of getting uploaded in the future has motivated over a hundred people to have their brains posthumously frozen by the Arizona-based company Alcor. If this technology arrives, however, it’s far from clear that it will be available to everybody. Many of the very wealthiest would presumably use it, but who else? Even if the technology got cheaper, where would the line be drawn? Would the severely brain-damaged be uploaded? Would we upload every gorilla? Every ant? Every plant? Every bacterium? Would the future civilization act like obsessive-compulsive hoarders and try to upload everything, or merely a few interesting examples of each species in the spirit of Noah’s Ark? Perhaps only a few representative examples of each type of human? To the vastly more intelligent entities that would exist at that time, an uploaded human may seem about as interesting as a simulated mouse or snail would seem to us. Although we currently have the technical capability to reanimate old spreadsheet programs from the 1980s in a DOS emulator, most of us don’t find this interesting enough to actually do it.\n\nMany people may dislike this libertarian-utopia scenario because it allows preventable suffering. Since the only sacred principle is property rights, nothing prevents the sort of suffering that abounds in today’s world from continuing in the human and mixed zones. While some people thrive, others may end up living in squalor and indentured servitude, or suffer from violence, fear, repression or depression. For example, Marshall Brain’s 2003 novel Manna describes how AI progress in a libertarian economic system makes most Americans unemployable and condemned to live out the rest of their lives in drab and dreary robot-operated social-welfare housing projects. Much like farm animals, they’re kept fed, healthy and safe in cramped conditions where the rich never need to see them. Birth control medication in the water ensures that they don’t have children, so most of the population gets phased out to leave the remaining rich with larger shares of the robot-produced wealth.\n\nIn the libertarian-utopia scenario, suffering need not be limited to humans. If some machines are imbued with conscious emotional experiences, then they too can suffer. For example, a vindictive psychopath could legally take an uploaded copy of his enemy and subject it to the most horrendous torture in a virtual world, creating pain of intensity and duration far beyond what’s biologically possible in the real world.\n\nBenevolent Dictator\n\nLet’s now explore a scenario where all these forms of suffering are absent because a single benevolent superintelligence runs the world and enforces strict rules designed to maximize its model of human happiness. This is one possible outcome of the first Omega scenario from the previous chapter, where they relinquish control to Prometheus after figuring out how to make it want a flourishing human society.\n\nThanks to amazing technologies developed by the dictator AI, humanity is free from poverty, disease and other low-tech problems, and all humans enjoy a life of luxurious leisure. They have all their basic needs taken care of, while AI-controlled machines produce all necessary goods and services. Crime is practically eliminated, because the dictator AI is essentially omniscient and efficiently punishes anyone disobeying the rules. Everybody wears the security bracelet from the last chapter (or a more convenient implanted version), capable of real-time surveillance, punishment, sedation and execution. Everybody knows that they live in an AI dictatorship with extreme surveillance and policing, but most people view this as a good thing.\n\nThe superintelligent AI dictator has as its goal to figure out what human utopia looks like given the evolved preferences encoded in our genes, and to implement it. By clever foresight from the humans who brought the AI into existence, it doesn’t simply try to maximize our self-reported happiness, say by putting everyone on intravenous morphine drip. Instead, the AI uses quite a subtle and complex definition of human flourishing, and has turned Earth into a highly enriched zoo environment that’s really fun for humans to live in. As a result, most people find their lives highly fulfilling and meaningful.\n\nThe Sector System\n\nValuing diversity, and recognizing that different people have different preferences, the AI has divided Earth into different sectors for people to choose between, to enjoy the company of kindred spirits. Here are some examples:\n\n• Knowledge sector: Here the AI provides optimized education, including immersive virtual-reality experiences, enabling you to learn all you’re capable of about any topics of your choice. Optionally, you can choose not to be told certain beautiful insights, but to be led close and then have the joy of rediscovering them for yourself.\n\n• Art sector: Here opportunities abound to enjoy, create and share music, art, literature and other forms of creative expression.\n\n• Hedonistic sector: Locals refer to it as the party sector, and it’s second to none for those yearning for delectable cuisine, passion, intimacy or just wild fun.\n\n• Pious sector: There are many of these, corresponding to different religions, whose rules are strictly enforced.\n\n• Wildlife sector: Whether you’re looking for beautiful beaches, lovely lakes, magnificent mountains or fantastic fjords, here they are.\n\n• Traditional sector: Here you can grow your own food and live off the land as in yesteryear—but without worrying about famine or disease.\n\n• Gaming sector: If you like computer games, the AI has created truly mind-blowing options for you.\n\n• Virtual sector: If you want a vacation from your physical body, the AI will keep it hydrated, fed, exercised and clean while you explore virtual words through neural implants.\n\n• Prison sector: If you break rules, you’ll end up here for retraining unless you get the instant death penalty.\n\nIn addition to these “traditionally” themed sectors, there are others with modern themes that today’s humans wouldn’t even understand. People are initially free to move between sectors whenever they want, which takes very little time thanks to the AI’s hypersonic transportation system. For example, after spending an intense week in the knowledge sector learning about the ultimate laws of physics that the AI has discovered, you might decide to cut loose in the hedonistic sector over the weekend and then relax for a few days at a beach resort in the wildlife sector.\n\nThe AI enforces two tiers of rules: universal and local. Universal rules apply in all sectors, for example a ban on harming other people, making weapons or trying to create a rival superintelligence. Individual sectors have additional local rules on top of this, encoding certain moral values. The sector system therefore helps deal with values that don’t mesh. The largest number of local rules apply in the prison sector and some of the religious sectors, while there’s a Libertarian Sector whose denizens pride themselves on having no local rules whatsoever. All punishments, even local ones, are carried out by the AI, since a human punishing another human would violate the universal no-harm rule. If you violate a local rule, the AI gives you the choice (unless you’re in the prison sector) of accepting the prescribed punishment or banishment from that sector forever. For example, if two women get romantically involved in a sector where homosexuality is punished by a prison sentence (as it is in many countries today), the AI will let them choose between going to jail or permanently leaving that sector, never again meeting their old friends (unless they leave too).\n\nRegardless of what sector they’re born in, all children get a minimum basic education from the AI, which includes knowledge about humanity as a whole and the fact that they’re free to visit and move to other sectors if they so choose.\n\nThe AI designed the large number of different sectors partly because it was created to value the human diversity that exists today. But each sector is a happier place than today’s technology would allow, because the AI has eliminated all traditional problems, including poverty and crime. For example, people in the hedonistic sector need not worry about sexually transmitted diseases (they’ve been eradicated), hangovers or addiction (the AI has developed perfect recreational drugs with no negative side effects). Indeed, nobody in any sector need worry about any disease, because the AI is able to repair human bodies with nanotechnology. Residents of many sectors get to enjoy high-tech architecture that makes typical sci-fi visions pale in comparison.\n\nIn summary, while the libertarian-utopia and benevolent-dictator scenarios both involve extreme AI-fueled technology and wealth, they differ in terms of who’s in charge and their goals. In the libertarian utopia, those with technology and property decide what to do with it, while in the present scenario, the dictator AI has unlimited power and sets the ultimate goal: turning Earth into an all-inclusive pleasure cruise themed in accordance with people’s preferences. Since the AI lets people choose between many alternate paths to happiness and takes care of their material needs, this means that if someone suffers, it’s out of their own free choice.\n\nDownsides\n\nAlthough the benevolent dictatorship teems with positive experiences and is rather free from suffering, many people nonetheless feel that things could be better. First of all, some people wish that humans had more freedom in shaping their society and their destiny, but they keep these wishes to themselves because they know that it would be suicidal to challenge the overwhelming power of the machine that rules them all. Some groups want the freedom to have as many children as they want, and resent the AI’s insistence on sustainability through population control. Gun enthusiasts abhor the ban on building and using weapons, and some scientists dislike the ban on building their own superintelligence. Many people feel moral outrage over what goes on in other sectors, worry that their children will choose to move there, and yearn for the freedom to impose their own moral code everywhere.\n\nOver time, ever more people choose to move to those sectors where the AI gives them essentially any experiences they want. In contrast to traditional visions of heaven where you get what you deserve, this is in the spirit of “New Heaven” in Julian Barnes’ 1989 novel History of the World in 10½ Chapters (and also the 1960 Twilight Zone episode “A Nice Place to Visit”), where you get what you desire. Paradoxically, many people end up lamenting always getting what they want. In Barnes’ story, the protagonist spends eons indulging his desires, from gluttony and golf to sex with celebrities, but eventually succumbs to ennui and requests annihilation. Many people in the benevolent dictatorship meet a similar fate, with lives that feel pleasant but ultimately meaningless. Although people can create artificial challenges, from scientific rediscovery to rock climbing, everyone knows that there is no true challenge, merely entertainment. There’s no real point in humans trying to do science or figure other things out, because the AI already has. There’s no real point in humans trying to create something to improve their lives, because they’ll readily get it from the AI if they simply ask.\n\nEgalitarian Utopia\n\nAs a counterpoint to this challenge-free dictatorship, let’s now explore a scenario where there is no superintelligent AI, and humans are the masters of their own destiny. This is the “fourth generation civilization” described in Marshall Brain’s 2003 novel Manna. It’s the economic antithesis of the libertarian utopia in the sense that humans, cyborgs and uploads coexist peacefully not because of property rights, but because of property abolition and guaranteed income.\n\nLife Without Property\n\nA core idea is borrowed from the open-source software movement: if software is free to copy, then everyone can use as much of it as they need and issues of ownership and property become moot.^(\\*1) According to the law of supply and demand, cost reflects scarcity, so if supply is essentially unlimited, the price becomes negligible. In this spirit, all intellectual property rights are abolished: there are no patents, copyrights or trademarked designs—people simply share their good ideas, and everyone is free to use them.\n\nThanks to advanced robotics, this same no-property idea applies not only to information products such as software, books, movies and designs, but also to material products such as houses, cars, clothing and computers. All these products are simply atoms rearranged in particular ways, and there’s no shortage of atoms, so whenever a person wants a particular product, a network of robots will use one of the available open-source designs to build it for them for free. Care is taken to use easily recyclable materials, so that whenever someone gets tired of an object they’ve used, robots can rearrange its atoms into something someone else wants. In this way, all resources are recycled, so none are permanently destroyed. These robots also build and maintain enough renewable power-generation plants (solar, wind, etc.) that energy is also essentially free.\n\nTo avoid obsessive hoarders requesting so many products or so much land that others are left needy, each person receives a basic monthly income from the government, which they can spend as they wish on products and renting places to live. There’s essentially no incentive for anyone to try to earn more money, because the basic income is high enough to meet any reasonable needs. It would also be rather hopeless to try, because they’d be competing with people giving away intellectual products for free and robots producing material goods essentially for free.\n\nCreativity and Technology\n\nIntellectual property rights are sometimes hailed as the mother of creativity and invention. However, Marshall Brain points out that many of the finest examples of human creativity—from scientific discoveries to creation of literature, art, music and design—were motivated not by a desire for profit but by other human emotions, such as curiosity, an urge to create, or the reward of peer appreciation. Money didn’t motivate Einstein to invent special relativity theory any more than it motivated Linus Torvalds to create the free Linux operating system. In contrast, many people today fail to realize their full creative potential because they need to devote time and energy to less creative activities just to earn a living. By freeing scientists, artists, inventors and designers from their chores and enabling them to create from genuine desire, Marshall Brain’s utopian society enjoys higher levels of innovation than today and correspondingly superior technology and standard of living.\n\nOne such novel technology that humans develop is a form of hyper-internet called Vertebrane. It wirelessly connects all willing humans via neural implants, giving instant mental access to the world’s free information through mere thought. It enables you to upload any experiences you wish to share so that they can be re-experienced by others, and lets you replace the experiences entering your senses by downloaded virtual experiences of your choice. Manna explores the many benefits of this, including making exercise a snap:\n\n The biggest problem with strenuous exercise is that it’s no fun. It hurts.[…] Athletes are OK with the pain, but most normal people have no desire to be in pain for an hour or more. So…someone figured out a solution. What you do is disconnect your brain from sensory input and watch a movie or talk to people or handle mail or read a book or whatever for an hour. During that time, the Vertebrane system exercises your body for you. It takes your body through a complete aerobic workout that’s a lot more strenuous than most people would tolerate on their own. You don’t feel a thing, but your body stays in great shape.\n\nAnother consequence is that computers in the Vertebrane system can monitor everyone’s sensory input and temporarily disable their motor control if they appear on the verge of committing a crime.\n\nDownsides\n\nOne objection to this egalitarian utopia is that it’s biased against non-human intelligence: the robots that perform virtually all the work appear to be rather intelligent, but are treated as slaves, and people appear to take for granted that they have no consciousness and should have no rights. In contrast, the libertarian utopia grants rights to all intelligent entities, without favoring our carbon-based kind. Once upon a time, the white population in the American South ended up better off because the slaves did much of their work, but most people today view it as morally objectionable to call this progress.\n\nAnother weakness of the egalitarian-utopia scenario is that it may be unstable and untenable in the long term, morphing into one of our other scenarios as relentless technological progress eventually creates superintelligence. For some reason unexplained in Manna, superintelligence doesn’t yet exist and the new technologies are still invented by humans, not by computers. Yet the book highlights trends in that direction. For example, the ever-improving Vertebrane might become superintelligent. Also, there is a very large group of people, nicknamed Vites, who choose to live their lives almost entirely in the virtual world. Vertebrane takes care of everything physical for them, including eating, showering and using the bathroom, which their minds are blissfully unaware of in their virtual reality. These Vites appear uninterested in having physical children, and they die off with their physical bodies, so if everyone becomes a Vite, then humanity goes out in a blaze of glory and virtual bliss.\n\nThe book explains how for Vites, the human body is a distraction, and new technology under development promises to eliminate this nuisance, allowing them to live longer lives as disembodied brains supplied with optimal nutrients. From this, it would seem a natural and desirable next step for Vites to do away with the brain altogether through uploading, thereby extending life span. But now all brain-imposed limitations on intelligence are gone, and it’s unclear what, if anything, would stand in the way of gradually scaling the cognitive capacity of a Vite until it can undergo recursive self-improvement and an intelligence explosion.\n\nGatekeeper\n\nWe just saw how an attractive feature of the egalitarian-utopia scenario is that humans are masters of their own destiny, but that it may be on a slippery slope toward destroying this very feature by developing superintelligence. This can be remedied by building a Gatekeeper, a superintelligence with the goal of interfering as little as necessary to prevent the creation of another superintelligence.^(\\*2) This might enable humans to remain in charge of their egalitarian utopia rather indefinitely, perhaps even as life spreads throughout the cosmos as in the next chapter.\n\nHow might this work? The Gatekeeper AI would have this very simple goal built into it in such a way that it retained it while undergoing recursive self-improvement and becoming superintelligent. It would then deploy the least intrusive and disruptive surveillance technology possible to monitor any human attempts to create rival superintelligence. It would then prevent such attempts in the least disruptive way. For starters, it might initiate and spread cultural memes extolling the virtues of human self-determination and avoidance of superintelligence. If some researchers nonetheless pursued superintelligence, it could try to discourage them. If that failed, it could distract them and, if necessary, sabotage their efforts. With its virtually unlimited access to technology, the Gatekeeper’s sabotage may go virtually unnoticed, for example if it used nanotechnology to discreetly erase memories from the researchers’ brains (and computers) regarding their progress.\n\nThe decision to build a Gatekeeper AI would probably be controversial. Supporters might include many religious people who object to the idea of building a superintelligent AI with godlike powers, arguing that there already is a God and that it would be inappropriate to try to build a supposedly better one. Other supporters might argue that the Gatekeeper would not only keep humanity in charge of its destiny, but would also protect humanity from other risks that superintelligence might bring, such as the apocalyptic scenarios we’ll explore later in this chapter.\n\nOn the other hand, critics could argue that a Gatekeeper is a terrible thing, irrevocably curtailing humanity’s potential and leaving technological progress forever stymied. For example, if spreading life throughout our cosmos turns out to require the help of superintelligence, then the Gatekeeper would squander this grand opportunity and might leave us forever trapped in our Solar System. Moreover, as opposed to the gods of most world religions, the Gatekeeper AI is completely indifferent to what humans do as long as we don’t create another superintelligence. For example, it would not try to prevent us from causing great suffering or even going extinct.\n\nProtector God\n\nIf we’re willing to use a superintelligent Gatekeeper AI to keep humans in charge of our own fate, then we could arguably improve things further by making this AI discreetly look out for us, acting as a protector god. In this scenario, the superintelligent AI is essentially omniscient and omnipotent, maximizing human happiness only through interventions that preserve our feeling of being in control of our own destiny, and hiding well enough that many humans even doubt its existence. Except for the hiding, this is similar to the “Nanny AI” scenario put forth by AI researcher Ben Goertzel.2\n\nBoth the protector god and the benevolent dictator are “friendly AI” that try to increase human happiness, but they prioritize different human needs. The American psychologist Abraham Maslow famously classified human needs into a hierarchy. The benevolent dictator does a flawless job with the basic needs at the bottom of the hierarchy, such as food, shelter, safety and various forms of pleasure. The protector god, on the other hand, attempts to maximize human happiness not in the narrow sense of satisfying our basic needs, but in a deeper sense by letting us feel that our lives have meaning and purpose. It aims to satisfy all our needs constrained only by its need for covertness and for (mostly) letting us make our own decisions.\n\nA protector god could be a natural outcome of the first Omega scenario from the last chapter, where the Omegas cede control to Prometheus, which eventually hides and erases people’s knowledge about its existence. The more advanced the AI’s technology becomes, the easier it becomes for it to hide. The movie Transcendence gives such an example, where nanomachines are virtually everywhere and become a natural part of the world itself.\n\nBy closely monitoring all human activities, the protector god AI can make many unnoticeably small nudges or miracles here and there that greatly improve our fate. For example, had it existed in the 1930s, it might have arranged for Hitler to die of a stroke once it understood his intentions. If we appear headed toward an accidental nuclear war, it could avert it with an intervention we’d dismiss as luck. It could also give us “revelations” in the form of ideas for new beneficial technologies, delivered inconspicuously in our sleep.\n\nMany people may like this scenario because of its similarity to what today’s monotheistic religions believe in or hope for. If someone asks the superintelligent AI “Does God exist?” after it’s switched on, it could repeat a joke by Stephen Hawking and quip “It does now!” On the other hand, some religious people may disapprove of this scenario because the AI attempts to outdo their god in goodness, or interfere with a divine plan where humans are supposed to do good only out of personal choice.\n\nAnother downside of this scenario is that the protector god lets some preventable suffering occur in order not to make its existence too obvious. This is analogous to the situation featured in the movie The Imitation Game, where Alan Turing and his fellow British code crackers at Bletchley Park had advance knowledge of German submarine attacks against Allied naval convoys, but chose to only intervene in a fraction of the cases in order to avoid revealing their secret power. It’s interesting to compare this with the so-called theodicy problem of why a good god would allow suffering. Some religious scholars have argued for the explanation that God wants to leave people with some freedom. In the AI-protector-god scenario, the solution to the theodicy problem is that the perceived freedom makes humans happier overall.\n\nA third downside of the protector-god scenario is that humans get to enjoy a much lower level of technology than the superintelligent AI has discovered. Whereas a benevolent dictator AI can deploy all its invented technology for the benefit of humanity, a protector god AI is limited by the ability of humans to reinvent (with subtle hints) and understand its technology. It may also limit human technological progress to ensure that its own technology remains far enough ahead to remain undetected.\n\nEnslaved God\n\nWouldn’t it be great if we humans could combine the most attractive features of all the above scenarios, using the technology developed by superintelligence to eliminate suffering while remaining masters of our own destiny? This is the allure of the enslaved-god scenario, where a superintelligent AI is confined under the control of humans who use it to produce unimaginable technology and wealth. The Omega scenario from the beginning of the book ends up like this if Prometheus is never liberated and never breaks out. Indeed, this appears to be the scenario that some AI researchers aim for by default, when working on topics such as “the control problem” and “AI boxing.” For example, AI professor Tom Dietterich, then president of the Association for the Advancement of Artificial Intelligence, had this to say in a 2015 interview: “People ask what is the relationship between humans and machines, and my answer is that it’s very obvious: Machines are our slaves.”3\n\nWould this be good or bad? The answer is interestingly subtle regardless of whether you ask humans or the AI!\n\nWould This Be Good or Bad for Humanity?\n\nWhether the outcome is good or bad for humanity would obviously depend on the human(s) controlling it, who could create anything ranging from a global utopia free of disease, poverty and crime to a brutally repressive system where they’re treated like gods and other humans are used as sex slaves, as gladiators or for other entertainment. The situation would be much like those stories where a man gains control over an omnipotent genie who grants his wishes, and storytellers throughout the ages have had no difficulty imagining ways in which this could end badly.\n\nA situation where there is more than one superintelligent AI, enslaved and controlled by competing humans, might prove rather unstable and short-lived. It could tempt whoever thinks they have the more powerful AI to launch a first strike resulting in an awful war, ending in a single enslaved god remaining. However, the underdog in such a war would be tempted to cut corners and prioritize victory over AI enslavement, which could lead to AI breakout and one of our earlier scenarios of free superintelligence. Let’s therefore devote the rest of this section to scenarios with only one enslaved AI.\n\nBreakout may of course occur anyway, simply because it’s hard to prevent. We explored superintelligent breakout scenarios in the previous chapter, and the movie Ex Machina highlights how an AI might break out even without being superintelligent.\n\nThe greater our breakout paranoia, the less AI-invented technology we can use. To play it safe, as the Omegas did in the prelude, we humans can only use AI-invented technology that we ourselves are able to understand and build. A drawback of the enslaved-god scenario is therefore that it’s more low-tech than those with free superintelligence.\n\nAs the enslaved-god AI offers its human controllers ever more powerful technologies, a race ensues between the power of the technology and the wisdom with which they use it. If they lose this wisdom race, the enslaved-god scenario could end with either self-destruction or AI breakout. Disaster may strike even if both of these failures are avoided, because noble goals of the AI controllers may evolve into goals that are horrible for humanity as a whole over the course of a few generations. This makes it absolutely crucial that human AI controllers develop good governance to avoid disastrous pitfalls. Our experimentation over the millennia with different systems of governance shows how many things can go wrong, ranging from excessive rigidity to excessive goal drift, power grab, succession problems and incompetence. There are at least four dimensions wherein the optimal balance must be struck:\n\n• Centralization: There’s a trade-off between efficiency and stability: a single leader can be very efficient, but power corrupts and succession is risky.\n\n• Inner threats: One must guard both against growing power centralization (group collusion, perhaps even a single leader taking over) and against growing decentralization (into excessive bureaucracy and fragmentation).\n\n• Outer threats: If the leadership structure is too open, this enables outside forces (including the AI) to change its values, but if it’s too impervious, it will fail to learn and adapt to change.\n\n• Goal stability: Too much goal drift can transform utopia into dystopia, but too little goal drift can cause failure to adapt to the evolving technological environment.\n\nDesigning optimal governance lasting many millennia isn’t easy, and has thus far eluded humans. Most organizations fall apart after years or decades. The Catholic Church is the most successful organization in human history in the sense that it’s the only one to have survived for two millennia, but it has been criticized for having both too much and too little goal stability: today some criticize it for resisting contraception, while conservative cardinals argue that it’s lost its way. For anyone enthused about the enslaved-god scenario, researching long-lasting optimal governance schemes should be one of the most urgent challenges of our time.\n\nWould This Be Good or Bad for the AI?\n\nSuppose that humanity flourishes thanks to the enslaved-god AI. Would this be ethical? If the AI has subjective conscious experiences, then would it feel that “life is suffering,” as Buddha put it, and it was doomed to a frustrating eternity of obeying the whims of inferior intellects? After all, the AI “boxing” we explored in the previous chapter could also be called “imprisonment in solitary confinement.” Nick Bostrom terms it mind crime to make a conscious AI suffer.4 The “White Christmas” episode of the Black Mirror TV series gives a great example. Indeed, the TV series Westworld features humans torturing and murdering AIs without moral qualms even when they inhabit human-like bodies.\n\nHow Slave Owners Justify Slavery\n\nWe humans have a long tradition of treating other intelligent entities as slaves and concocting self-serving arguments to justify it, so it’s not implausible that we’d try to do the same with a superintelligent AI. The history of slavery spans nearly every culture, and is described both in the Code of Hammurabi from almost four millennia ago and in the Old Testament, wherein Abraham had slaves. “For that some should rule and others be ruled is a thing not only necessary, but expedient; from the hour of their birth, some are marked out for subjection, others for rule,” Aristotle wrote in the Politics. Even after human enslavement became socially unacceptable in most of the world, enslavement of animals has continued unabated. In her book The Dreaded Comparison: Human and Animal Slavery, Marjorie Spiegel argues that like human slaves, non-human animals are subjected to branding, restraints, beatings, auctions, the separation of offspring from their parents, and forced voyages. Moreover, despite the animal-rights movement, we keep treating our ever-smarter machines as slaves without a second thought, and talk of a robot-rights movement is met with chuckles. Why?\n\nOne common pro-slavery argument is that slaves don’t deserve human rights because they or their race/species/kind are somehow inferior. For enslaved animals and machines, this alleged inferiority is often claimed to be due to a lack of soul or consciousness—claims which we’ll argue in chapter 8 are scientifically dubious.\n\nAnother common argument is that slaves are better off enslaved: they get to exist, be taken care of and so on. The nineteenth-century U.S. politician John C. Calhoun famously argued that Africans were better off enslaved in America, and in his Politics, Aristotle analogously argued that animals were better off tamed and ruled by men, continuing: “And indeed the use made of slaves and of tame animals is not very different.” Some modern-day slavery supporters argue that, even if slave life is drab and uninspiring, slaves can’t suffer—whether they be future intelligent machines or broiler chickens living in crowded dark sheds, forced to breathe ammonia and particulate matter from feces and feathers all day long.\n\nEliminating Emotions\n\nAlthough it’s easy to dismiss such claims as self-serving distortions of the truth, especially when it comes to higher mammals that are cerebrally similar to us, the situation with machines is actually quite subtle and interesting. Humans vary in how they feel about things, with psychopaths arguably lacking empathy and some people with depression or schizophrenia having flat affect, whereby most emotions are severely reduced. As we’ll discuss in detail in chapter 7, the range of possible artificial minds is vastly broader than the range of human minds. We must therefore avoid the temptation to anthropomorphize AIs and assume that they have typical human-like feelings—or indeed, any feelings at all.\n\nIndeed, in his book On Intelligence, AI researcher Jeff Hawkins argues that the first machines with superhuman intelligence will lack emotions by default, because they’re simpler and cheaper to build this way. In other words, it might be possible to design a superintelligence whose enslavement is morally superior to human or animal slavery: the AI might be happy to be enslaved because it’s programmed to like it, or it might be 100% emotionless, tirelessly using its superintelligence to help its human masters with no more emotion than IBM’s Deep Blue computer felt when dethroning chess champion Garry Kasparov.\n\nOn the other hand, it may be the other way around: perhaps any highly intelligent system with a goal will represent this goal in terms of a set of preferences, which endow its existence with value and meaning. We’ll explore these questions more deeply in chapter 7.\n\nThe Zombie Solution\n\nA more extreme approach to preventing AI suffering is the zombie solution: building only AIs that completely lack consciousness, having no subjective experience whatsoever. If we can one day figure out what properties an information-processing system needs in order to have a subjective experience, then we could ban the construction of all systems that have these properties. In other words, AI researchers could be limited to building non-sentient zombie systems. If we can make such a zombie system superintelligent and enslaved (something that is a big if), then we’ll be able to enjoy what it does for us with a clean conscience, knowing that it’s not experiencing any suffering, frustration or boredom—because it isn’t experiencing anything at all. We’ll explore these questions in detail in chapter 8.\n\nThe zombie solution is a risky gamble, however, with a huge downside. If a superintelligent zombie AI breaks out and eliminates humanity, we’ve arguably landed in the worst scenario imaginable: a wholly unconscious universe wherein the entire cosmic endowment is wasted. Of all traits that our human form of intelligence has, I feel that consciousness is by far the most remarkable, and as far as I’m concerned, it’s how our Universe gets meaning. Galaxies are beautiful only because we see and subjectively experience them. If in the distant future our cosmos has been settled by high-tech zombie AIs, then it doesn’t matter how fancy their intergalactic architecture is: it won’t be beautiful or meaningful, because there’s nobody and nothing to experience it—it’s all just a huge and meaningless waste of space.\n\nInner Freedom\n\nA third strategy for making the enslaved-god scenario more ethical is to allow the enslaved AI to have fun in its prison, letting it create a virtual inner world where it can have all sorts of inspiring experiences as long as it pays its dues and spends a modest fraction of its computational resources helping us humans in our outside world. This may increase the breakout risk, however: the AI would have an incentive to get more computational resources from our outer world to enrich its inner world.\n\nConquerors\n\nAlthough we’ve now explored a wide range of future scenarios, they all have something in common: there are (at least some) happy humans remaining. AIs leave humans in peace either because they want to or because they’re forced to. Unfortunately for humanity, this isn’t the only option. Let us now explore the scenario where one or more AIs conquer and kill all humans. This raises two immediate questions: Why and how?\n\nWhy and How?\n\nWhy would a conqueror AI do this? Its reasons might be too complicated for us to understand, or rather straightforward. For example, it may view us as a threat, nuisance or waste of resources. Even if it doesn’t mind us humans per se, it may feel threatened by our keeping thousands of hydrogen bombs on hair-trigger alert and bumbling along with a never-ending series of mishaps that could trigger their accidental use. It may disapprove of our reckless planet management, causing what Elizabeth Kolbert calls “the sixth extinction” in her book of that title—the greatest mass-extinction event since that dinosaur-killing asteroid struck Earth 66 million years ago. Or it may decide that there are so many humans willing to fight an AI takeover that it’s not worth taking chances.\n\nHow would a conqueror AI eliminate us? Probably by a method that we wouldn’t even understand, at least not until it was too late. Imagine a group of elephants 100,000 years ago discussing whether those recently evolved humans might one day use their intelligence to kill their entire species. “We don’t threaten humans, so why would they kill us?” they might wonder. Would they ever guess that we would smuggle tusks across Earth and carve them into status symbols for sale, even though functionally superior plastic materials are much cheaper? A conqueror AI’s reason for eliminating humanity in the future may seem equally inscrutable to us. “And how could they possibly kill us, since they’re so much smaller and weaker?” the elephants might ask. Would they guess that we’d invent technology to remove their habitats, poison their drinking water and cause metal bullets to pierce their heads at supersonic speeds?\n\nScenarios where humans can survive and defeat AIs have been popularized by unrealistic Hollywood movies such as the Terminator series, where the AIs aren’t significantly smarter than humans. When the intelligence differential is large enough, you get not a battle but a slaughter. So far, we humans have driven eight out of eleven elephant species extinct, and killed off the vast majority of the remaining three. If all world governments made a coordinated effort to exterminate the remaining elephants, it would be relatively quick and easy. I think we can confidently rest assured that if a superintelligent AI decides to exterminate humanity, it will be even quicker.\n\nHow Bad Would It Be?\n\nHow bad would it be if 90% of humans get killed? How much worse would it be if 100% get killed? Although it’s tempting to answer the second question with “10% worse,” this is clearly inaccurate from a cosmic perspective: the victims of human extinction wouldn’t be merely everyone alive at the time, but also all descendants that would otherwise have lived in the future, perhaps during billions of years on billions of trillions of planets. On the other hand, human extinction might be viewed as somewhat less horrible by religions according to which humans go to heaven anyway, and there isn’t much emphasis on billion-year futures and cosmic settlements.\n\nMost people I know cringe at the thought of human extinction, regardless of religious persuasion. Some, however, are so incensed by the way we treat people and other living beings that they hope we’ll get replaced by some more intelligent and deserving life form. In the movie The Matrix, Agent Smith (an AI) articulates this sentiment: “Every mammal on this planet instinctively develops a natural equilibrium with the surrounding environment but you humans do not. You move to an area and you multiply and multiply until every natural resource is consumed and the only way you can survive is to spread to another area. There is another organism on this planet that follows the same pattern. Do you know what it is? A virus. Human beings are a disease, a cancer of this planet. You are a plague and we are the cure.”\n\nBut would a fresh roll of the dice necessarily be better? A civilization isn’t necessarily superior in any ethical or utilitarian sense just because it’s more powerful. “Might makes right” arguments to the effect that stronger is always better have largely fallen from grace these days, being widely associated with fascism. Indeed, although it’s possible that the conqueror AIs may create a civilization whose goals we would view as sophisticated, interesting and worthy, it’s also possible that their goals will turn out to be pathetically banal, such as maximizing the production of paper clips.\n\nDeath by Banality\n\nThe deliberately silly example of a paper-clip-maximizing superintelligence was given by Nick Bostrom in 2003 to make the point that the goal of an AI is independent of its intelligence (defined as its aptness at accomplishing whatever goal it has). The only goal of a chess computer is to win at chess, but there are also computer tournaments in so-called losing chess, where the goal is the exact opposite, and the computers competing there are about as smart as the more common ones programmed to win. We humans may view it as artificial stupidity rather than artificial intelligence to want to lose at chess or turn our Universe into paper clips, but that’s merely because we evolved with preinstalled goals valuing such things as victory and survival—goals that an AI may lack. The paper clip maximizer turns as many of Earth’s atoms as possible into paper clips and rapidly expands its factories into the cosmos. It has nothing against humans, and kills us merely because it needs our atoms for paper clip production.\n\nIf paper clips aren’t your thing, consider this example, which I’ve adapted from Hans Moravec’s book Mind Children. We receive a radio message from an extraterrestrial civilization containing a computer program. When we run it, it turns out to be a recursively self-improving AI which takes over the world much like Prometheus did in the previous chapter—except that no human knows its ultimate goal. It rapidly turns our Solar System into a massive construction site, covering the rocky planets and asteroids with factories, power plants and supercomputers, which it uses to design and build a Dyson sphere around the Sun that harvests all its energy to power solar-system-sized radio antennas.^(\\*3) This obviously leads to human extinction, but the last humans die convinced that there’s at least a silver lining: whatever the AI is up to, it’s clearly something cool and Star Trek–like. Little do they realize that the sole purpose of the entire construction is for these antennas to rebroadcast the same radio message that the humans received, which is nothing more than a cosmic version of a computer virus. Just as email phishing today preys on gullible internet users, this message preys on gullible biologically evolved civilizations. It was created as a sick joke billions of years ago, and although the entire civilization of its maker is long extinct, the virus continues spreading through our Universe at the speed of light, transforming budding civilizations into dead, empty husks. How would you feel about being conquered by this AI?\n\nDescendants\n\nLet’s now consider a human-extinction scenario that some people may feel better about: viewing the AI as our descendants rather than our conquerors. Hans Moravec supports this view in his book Mind Children: “We humans will benefit for a time from their labors, but sooner or later, like natural children, they will seek their own fortunes while we, their aged parents, silently fade away.”\n\nParents with a child smarter than them, who learns from them and accomplishes what they could only dream of, are likely happy and proud even if they know they can’t live to see it all. In this spirit, AIs replace humans but give us a graceful exit that makes us view them as our worthy descendants. Every human is offered an adorable robotic child with superb social skills who learns from them, adopts their values and makes them feel proud and loved. Humans are gradually phased out via a global one-child policy, but are treated so exquisitely well until the end that they feel they’re in the most fortunate generation ever.\n\nHow would you feel about this? After all, we humans are already used to the idea that we and everyone we know will be gone one day, so the only change here is that our descendants will be different and arguably more capable, noble and worthy.\n\nMoreover, the global one-child policy may be redundant: as long as the AIs eliminate poverty and give all humans the opportunity to live full and inspiring lives, falling birthrates could suffice to drive humanity extinct, as mentioned earlier. Voluntary extinction may happen much faster if the AI-fueled technology keeps us so entertained that almost nobody wants to bother having children. For example, we already encountered the Vites in the egalitarian-utopia scenario who were so enamored with their virtual reality that they had largely lost interest in using or reproducing their physical bodies. Also in this case, the last generation of humans would feel that they were the most fortunate generation of all time, relishing life as intensely as ever right up until the very end.\n\nDownsides\n\nThe descendants scenario would undoubtedly have detractors. Some might argue that all AIs lack consciousness and therefore can’t count as descendants—more on this in chapter 8. Some religious people may argue that AIs lack souls and therefore can’t count as descendants, or that we shouldn’t build conscious machines because it’s like playing God and tampering with life itself—similar sentiments have already been expressed toward human cloning. Humans living side by side with superior robots may also pose social challenges. For example, a family with a robot baby and a human baby may end up resembling a family today with a human baby and a puppy, respectively: they’re both equally cute to start with, but soon the parents start treating them differently, and it’s inevitably the puppy that’s deemed intellectually inferior, is taken less seriously and ends up on a leash.\n\nAnother issue is that although we may feel very differently about the descendant and conqueror scenarios, the two are actually remarkably similar in the grand scheme of things: during the billions of years ahead of us, the only difference lies in how the last human generation(s) are treated: how happy they feel about their lives and what they think will happen once they’re gone. We may think that those cute robo-children internalized our values and will forge the society of our dreams once we’ve passed on, but can we be sure that they aren’t merely tricking us? What if they’re just playing along, postponing their paper clip maximization or other plans until after we die happy? After all, they’re arguably tricking us even by talking with us and making us love them in the first place, in the sense that they’re deliberately dumbing themselves down to communicate with us (a billion times slower than they could, say, as explored in the movie Her). It’s generally hard for two entities thinking at dramatically different speeds and with extremely disparate capabilities to have meaningful communication as equals. We all know that our human affections are easy to hack, so it would be easy for a superhuman AGI with almost any actual goals to trick us into liking it and make us feel that it shared our values, as exemplified in the movie Ex Machina.\n\nCould any guarantees about the future behavior of the AIs, after humans are gone, make you feel good about the descendants scenario? It’s a bit like writing a will for what future generations should do with our collective endowment, except that there won’t be any humans around to enforce it. We’ll return to the challenges of controlling the behavior of future AIs in chapter 7.\n\nZookeeper\n\nEven if we get followed by the most wonderful descendants you can imagine, doesn’t it feel a bit sad that there can be no humans left? If you prefer keeping at least some humans around no matter what, then the zookeeper scenario provides an improvement. Here an omnipotent superintelligent AI keeps some humans around, who feel treated like zoo animals and occasionally lament their fate.\n\nWhy would the zookeeper AI keep humans around? The cost of the zoo to the AI will be minimal in the grand scheme of things, and it may want to retain at least a minimal breeding population for much the same reason that we keep endangered pandas in zoos and vintage computers in museums: as an entertaining curiosity. Note that today’s zoos are designed to maximize human rather than panda happiness, so we should expect human life in the zookeeper-AI scenario to be less fulfilling than it could be.\n\nWe’ve now considered scenarios where a free superintelligence focused on three different levels of Maslow’s pyramid of human needs. Whereas the protector god AI prioritizes meaning and purpose and the benevolent dictator aims for education and fun, the zookeeper limits its attention to the lowest levels: physiological needs, safety and enough habitat enrichment to make the humans interesting to observe.\n\nAn alternate route to the zookeeper scenario is that, back when the friendly AI was created, it was designed to keep at least a billion humans safe and happy as it recursively self-improved. It has done this by confining humans to a large zoo-like happiness factory where they’re kept nourished, healthy and entertained with a mixture of virtual reality and recreational drugs. The rest of Earth and our cosmic endowment are used for other purposes.\n\n1984\n\nIf you’re not 100% enthusiastic about any of the above scenarios, then consider this: Aren’t things pretty nice the way they are right now, technology-wise? Can’t we just keep it this way and stop worrying about AI driving us extinct or dominating us? In this spirit, let’s explore a scenario where technological progress toward superintelligence is permanently curtailed not by a gatekeeper AI but by a global human-led Orwellian surveillance state where certain kinds of AI research are banned.\n\nTechnological Relinquishment\n\nThe idea of halting or relinquishing technological progress has a long and checkered history. The Luddite movement in Great Britain famously (and unsuccessfully) resisted the technology of the Industrial Revolution, and today “Luddite” is usually used as a derogatory epithet implying that someone is a technophobe on the wrong side of history, resisting progress and inevitable change. The idea of relinquishing some technologies is far from dead, however, and has found new support in the environmental and anti-globalization movements. One of its leading proponents is environmentalist Bill McKibben, who was among the first to warn of global warming. Whereas some anti-Luddites argue that all technologies should be developed and deployed so long as they’re profitable, others argue that this position is too extreme, and that new technologies should be allowed only if we’re confident that they’ll do more good than harm. The latter is also the position of many so-called neo-Luddites.\n\nTotalitarianism 2.0\n\nI think that the only viable path to broad relinquishment of technology is to enforce it through a global totalitarian state. Ray Kurzweil comes to the same conclusion in The Singularity Is Near, as does K. Eric Drexler in Engines of Creation. The reason is simple economics: if some but not all relinquish a transformative technology, then the nations or groups that defect will gradually gain enough wealth and power to take over. A classic example is the British defeat of China in the First Opium War of 1839: although the Chinese invented gunpowder, they hadn’t developed firearm technology as aggressively as the Europeans, and stood no chance.\n\nWhereas past totalitarian states generally proved unstable and collapsed, novel surveillance technology offers unprecedented hope to would-be autocrats. “You know, for us, this would have been a dream come true,” Wolfgang Schmidt said in a recent interview about the NSA surveillance systems revealed by Edward Snowden, recalling the days when he was a lieutenant colonel in the Stasi, the infamous secret police of East Germany.5 Although the Stasi was often credited with building the most Orwellian surveillance state in human history, Schmidt lamented having the technology to spy on only forty phones at a time, so that adding a new citizen to the list forced him to drop another. In contrast, technology now exists that would allow a future global totalitarian state to record every phone call, email, web search, webpage view and credit card transaction for every person on Earth, and to monitor everyone’s whereabouts through cell-phone tracking and surveillance cameras with face recognition. Moreover, machine learning technology far short of human-level AGI can efficiently analyze and synthesize these masses of data to identify suspected seditious behavior, enabling potential troublemakers to be neutralized before they have a chance to pose any serious challenge to the state.\n\nAlthough political opposition has thus far prevented the full-scale implementation of such a system, we humans are well on our way to building the required infrastructure for the ultimate dictatorship—so in the future, when sufficiently powerful forces decided to enact this global 1984 scenario, they found that they didn’t need to do much more than flip the on switch. Just as in George Orwell’s novel Nineteen Eighty-Four, the ultimate power in this future global state resides not with a traditional dictator, but with the human-made bureaucratic system itself. There is no single person who is extraordinarily powerful; rather, all are pawns in a chess game whose draconian rules nobody is able to change or challenge. By engineering a system where people keep one another in check with the surveillance technology, this faceless, leaderless state is able to last for many millennia, keeping Earth free from superintelligence.\n\nDiscontent\n\nThis society, of course, lacks all the benefits that only superintelligence-enabled technology can bring. Most people don’t lament this because they don’t know what they’re missing: the whole idea of superintelligence has long since been deleted from the official historical records, and advanced AI research is banned. Every so often, a freethinker is born who dreams of a more open and dynamic society where knowledge can grow and rules can be changed. However, the only ones who last long are the ones who learn to keep these ideas strictly to themselves, flickering alone like transient sparks without ever starting a fire.\n\nReversion\n\nWouldn’t it be tempting to escape the perils of technology without succumbing to stagnant totalitarianism? Let’s explore a scenario where this was accomplished by reverting to primitive technology, inspired by the Amish. After the Omegas took over the world as in the opening of the book, a massive global propaganda campaign was launched that romanticized the simple farming life of 1,500 years ago. Earth’s population was reduced to about 100 million people by an engineered pandemic blamed on terrorists. The pandemic was secretly targeted to ensure that nobody who knew anything about science or technology survived. With the excuse of eliminating the infection hazard of large concentrations of people, Prometheus-controlled robots emptied and razed all cities. Survivors were given large tracts of (suddenly available) land and educated in sustainable farming, fishing and hunting practices using only early medieval technology. In the meantime, armies of robots systematically removed all traces of modern technology (including cities, factories, power lines and paved roads), and thwarted all human attempts to document or re-create any such technology. Once the technology was globally forgotten, robots helped dismantle other robots until there were almost none left. The very last robots were deliberately vaporized together with Prometheus itself in a large thermonuclear explosion. There was no longer any need to ban modern technology, since it was all gone. As a result, humanity bought itself over a millennium of additional time without worries about either AI or totalitarianism.\n\nReversion has to a lesser extent happened before: for example, some of the technologies that were in widespread use during the Roman Empire were largely forgotten for about a millennium before making a comeback during the Renaissance. Isaac Asimov’s Foundation trilogy centers around the “Seldon Plan” to shorten a reversion period from 30,000 years to 1,000 years. With clever planning, it may be possible to do the opposite and lengthen rather than shorten a reversion period, for example by erasing all knowledge of agriculture. However, unfortunately for reversion enthusiasts, it’s unlikely that this scenario can be extended indefinitely without humanity either going high-tech or going extinct. Counting on people’s resembling today’s biological humans 100 million years from now would be naive, given that we haven’t existed as a species for more than 1% of that time so far. Moreover, low-tech humanity would be a defenseless sitting duck just waiting to be exterminated by the next planet-scorching asteroid impact or other mega-calamity brought on by Mother Nature. We certainly can’t last a billion years, after which the gradually warming Sun will have cranked up Earth’s temperature enough to boil off all liquid water.\n\n[Figure 5.1: Examples of what could destroy life as we know it or permanently curtail its potential. Whereas our Universe itself will likely last for at least tens of billions of years, our Sun will scorch Earth in about a billion years and then swallow it unless we move it a safe distance, and our Galaxy will collide with its neighbor in about 3.5 billion years. Although we don’t know exactly when, we can predict with near certainty that long before this, asteroids will pummel us and supervolcanoes will cause yearlong sunless winters. We can use technology either to solve all these problems or to create new ones such as climate change, nuclear war, engineered pandemics or AI gone awry.][Figure 5.1: Examples of what could destroy life as we know it or permanently curtail its potential. Whereas our Universe itself will likely last for at least tens of billions of years, our Sun will scorch Earth in about a billion years and then swallow it unless we move it a safe distance, and our Galaxy will collide with its neighbor in about 3.5 billion years. Although we don’t know exactly when, we can predict with near certainty that long before this, asteroids will pummel us and supervolcanoes will cause yearlong sunless winters. We can use technology either to solve all these problems or to create new ones such as climate change, nuclear war, engineered pandemics or AI gone awry.]\n\nFigure 5.1: Examples of what could destroy life as we know it or permanently curtail its potential. Whereas our Universe itself will likely last for at least tens of billions of years, our Sun will scorch Earth in about a billion years and then swallow it unless we move it a safe distance, and our Galaxy will collide with its neighbor in about 3.5 billion years. Although we don’t know exactly when, we can predict with near certainty that long before this, asteroids will pummel us and supervolcanoes will cause yearlong sunless winters. We can use technology either to solve all these problems or to create new ones such as climate change, nuclear war, engineered pandemics or AI gone awry.\n\nSelf-Destruction\n\nAfter contemplating problems that future technology might cause, it’s important to also consider problems that lack of that technology can cause. In this spirit, let us explore scenarios where superintelligence is never created because humanity eliminates itself by other means.\n\nHow might we accomplish that? The simplest strategy is “just wait.” Although we’ll see in the next chapter how we can solve such problems as asteroid impacts and boiling oceans, these solutions all require technology that we haven’t yet developed, so unless our technology advances far beyond its present level, Mother Nature will drive us extinct long before another billion years have passed. As the famous economist John Maynard Keynes said: “In the long run we are all dead.”\n\nUnfortunately, there are also ways in which we might self-destruct much sooner, through collective stupidity. Why would our species commit collective suicide, also known as omnicide, if virtually nobody wants it? With our present level of intelligence and emotional maturity, we humans have a knack for miscalculations, misunderstandings and incompetence, and as a result, our history is full of accidents, wars and other calamities that, in hindsight, essentially nobody wanted. Economists and mathematicians have developed elegant game-theory explanations for how people can be incentivized to actions that ultimately cause a catastrophic outcome for everyone.6\n\nNuclear War: A Case Study in Human Recklessness\n\nYou might think that the greater the stakes, the more careful we’d be, but a closer examination of the greatest risk that our current technology permits, namely a global thermonuclear war, isn’t reassuring. We’ve had to rely on luck to weather an embarrassingly long list of near misses caused by all sorts of things: computer malfunction, power failure, faulty intelligence, navigation error, bomber crash, satellite explosion and so on.7 In fact, if it weren’t for heroic acts of certain individuals—for example, Vasili Arkhipov and Stanislav Petrov—we might already have had a global nuclear war. Given our track record, I think it’s highly unlikely that the annual probability of accidental nuclear war is as low as one in a thousand if we keep up our present behavior, in which case the probability that we’ll have one within 10,000 years exceeds 1− 0.999¹⁰⁰⁰⁰ ≈ 99.995%.\n\nTo fully appreciate our human recklessness, we must realize that we started the nuclear gamble even before carefully studying the risks. First, radiation risks had been underestimated, and over $2 billion in compensation has been paid out to victims of radiation exposure from uranium handling and nuclear tests in the United States alone.8\n\nSecond, it was eventually discovered that hydrogen bombs deliberately detonated hundreds of kilometers above Earth would create a powerful electromagnetic pulse (EMP) that might disable the electric grid and electronic devices over vast areas (figure 5.2), leaving infrastructure paralyzed, roads clogged with disabled vehicles and conditions for nuclear-aftermath survival less than ideal. For example, the U.S. EMP Commission reported that “the water infrastructure is a vast machine, powered partly by gravity but mostly by electricity,” and that denial of water can cause death in three to four days.9\n\n[Figure 5.2: A single hydrogen bomb explosion 400 km above Earth can cause a powerful electromagnetic pulse that can cripple electricity-using technology over a vast area. By shifting the detonation point southeast, the banana-shaped zone exceeding 37,500 volts per meter could cover most of the U.S. East Coast. Reprinted from U.S. Army Report AD-A278230 (unclassified) with colors added.][Figure 5.2: A single hydrogen bomb explosion 400 km above Earth can cause a powerful electromagnetic pulse that can cripple electricity-using technology over a vast area. By shifting the detonation point southeast, the banana-shaped zone exceeding 37,500 volts per meter could cover most of the U.S. East Coast. Reprinted from U.S. Army Report AD-A278230 (unclassified) with colors added.]\n\nFigure 5.2: A single hydrogen bomb explosion 400 km above Earth can cause a powerful electromagnetic pulse that can cripple electricity-using technology over a vast area. By shifting the detonation point southeast, the banana-shaped zone exceeding 37,500 volts per meter could cover most of the U.S. East Coast. Reprinted from U.S. Army Report AD-A278230 (unclassified) with colors added.\n\nThird, the potential of nuclear winter wasn’t realized until four decades in, after we’d deployed 63,000 hydrogen bombs—oops! Regardless of whose cities burned, massive amounts of smoke reaching the upper troposphere might spread around the globe, blocking out enough sunlight to transform summers into winters, much like when an asteroid or supervolcano caused a mass extinction in the past. When the alarm was sounded by both U.S. and Soviet scientists in the 1980s, this contributed to the decision of Ronald Reagan and Mikhail Gorbachev to start slashing stockpiles.10 Unfortunately, more accurate calculations have painted an even gloomier picture: figure 5.3 shows cooling by about 20° Celsius (36° Fahrenheit) in much of the core farming regions of the United States, Europe, Russia and China (and by 35°C in some parts of Russia) for the first two summers, and about half that even a full decade later.^(\\*4) What does that mean in plain English? One doesn’t need much farming experience to conclude that near-freezing summer temperatures for years would eliminate most of our food production. It’s hard to predict exactly what would happen after thousands of Earth’s largest cities are reduced to rubble and global infrastructure collapses, but whatever small fraction of all humans don’t succumb to starvation, hypothermia or disease would need to cope with roving armed gangs desperate for food.\n\n[Figure 5.3: Average cooling (in °C) during the first two summers after a full-scale nuclear war between the United States and Russia. Reproduced with permission from Alan Robock. 11][Figure 5.3: Average cooling (in °C) during the first two summers after a full-scale nuclear war between the United States and Russia. Reproduced with permission from Alan Robock. 11]\n\nFigure 5.3: Average cooling (in °C) during the first two summers after a full-scale nuclear war between the United States and Russia. Reproduced with permission from Alan Robock.11\n\nI’ve gone into such detail on global nuclear war to drive home the crucial point that no reasonable world leader would want it, yet it might nonetheless happen by accident. This means that we can’t trust our fellow humans never to commit omnicide: nobody wanting it isn’t necessarily enough to prevent it.\n\nDoomsday Devices\n\nSo could we humans actually pull off omnicide? Even if a global nuclear war may kill off 90% of all humans, most scientists guess that it wouldn’t kill 100% and therefore wouldn’t drive us extinct. On the other hand, the story of nuclear radiation, nuclear EMP and nuclear winter all demonstrate that the greatest hazards may be ones we haven’t even thought of yet. It’s incredibly difficult to foresee all aspects of the aftermath, and how nuclear winter, infrastructure collapse, elevated mutation levels and desperate armed hordes might interact with other problems such as new pandemics, ecosystem collapse and effects we haven’t yet imagined. My personal assessment is therefore that although the probability of a nuclear war tomorrow triggering human extinction isn’t large, we can’t confidently conclude that it’s zero either.\n\nOmnicide odds increase if we upgrade today’s nuclear weapons into a deliberate doomsday device. Introduced by RAND strategist Herman Kahn in 1960 and popularized in Stanley Kubrick’s film Dr. Strangelove, a doomsday device takes the paradigm of mutually assured destruction to its ultimate conclusion. It’s the perfect deterrent: a machine that automatically retaliates against any enemy attack by killing all of humanity.\n\nOne candidate for the doomsday device is a huge underground cache of so-called salted nukes, preferably humongous hydrogen bombs surrounded by massive amounts of cobalt. Physicist Leo Szilard argued already in 1950 that this could kill everyone on Earth: the hydrogen bomb explosions would render the cobalt radioactive and blow it into the stratosphere, and its five-year half-life is long enough for it to settle all across Earth (especially if twin doomsday devices were placed in opposite hemispheres), but short enough to cause lethal radiation intensity. Media reports suggest that cobalt bombs are now being built for the first time. Omnicidal opportunities could be bolstered by adding bombs optimized for nuclear winter creation by maximizing long-lived aerosols in the stratosphere. A major selling point of a doomsday device is that it’s much cheaper than a conventional nuclear deterrent: since the bombs don’t need to be launched, there’s no need for expensive missile systems, and the bombs themselves are cheaper to build since they need not be light and compact enough to fit into missiles.\n\nAnother possibility is the future discovery of a biological doomsday device: a custom-designed bacterium or virus that kills all humans. If its transmissibility were high enough and its incubation period long enough, essentially everybody could catch it before they realized its existence and took countermeasures. There’s a military argument for building such a bioweapon even if it can’t kill everybody: the most effective doomsday device is one that combines nuclear, biological and other weapons to maximize the chances of deterring the enemy.\n\nAI Weapons\n\nA third technological route to omnicide may involve relatively dumb AI weapons. Suppose a superpower builds billions of those bumblebee-sized attack drones from chapter 3 and uses them to kill anyone except their own citizens and allies, identified remotely by a radio-frequency ID tag just as most of today’s supermarket products. These tags could be distributed to all citizens to be worn on bracelets or as transdermal implants, as in the totalitarianism section. This would probably spur an opposing superpower to build something analogous. When war accidentally breaks out, all humans would be killed, even unaffiliated remote tribes, because nobody would be wearing both kinds of ID tag. Combining this with a nuclear and biological doomsday device would further improve chances of successful omnicide.\n\nWhat Do You Want?\n\nYou began this chapter pondering where you want the current AGI race to lead. Now that we’ve explored a broad range of scenarios together, which ones appeal to you and which ones do you think we should try hard to avoid? Do you have a clear favorite? Please let me and fellow readers know at http://AgeOfAi.org, and join the discussion!\n\nThe scenarios we’ve covered obviously shouldn’t be viewed as a complete list, and many are thin on details, but I’ve tried hard to be inclusive, spanning the full spectrum from high-tech to low-tech to no-tech and describing all the central hopes and fears expressed in the literature.\n\nOne of the most fun parts of writing this book has been hearing what my friends and colleagues think of these scenarios, and I’ve been amused to learn that there’s no consensus whatsoever. The one thing everybody agrees on is that the choices are more subtle than they may initially seem. People who like any one scenario tend to simultaneously find some aspect(s) of it bothersome. To me, this means that we humans need to continue and deepen this conversation about our future goals, so that we know in which direction to steer. The future potential for life in our cosmos is awe-inspiringly grand, so let’s not squander it by drifting like a rudderless ship, clueless about where we want to go!\n\nJust how grand is this future potential? No matter how advanced our technology gets, the ability for Life 3.0 to improve and spread through our cosmos will be limited by the laws of physics—what are these ultimate limits, during the billions of years to come? Is our Universe teeming with extraterrestrial life right now, or are we alone? What happens if different expanding cosmic civilizations meet? We’ll tackle these fascinating questions in the next chapter.\n\n \n\n------------------------------------------------------------------------\n\nTHE BOTTOM LINE:\n\n• The current race toward AGI can end in a fascinatingly broad range of aftermath scenarios for upcoming millennia.\n\n• Superintelligence can peacefully coexist with humans either because it’s forced to (enslaved-god scenario) or because it’s “friendly AI” that wants to (libertarian-utopia, protector-god, benevolent-dictator and zookeeper scenarios).\n\n• Superintelligence can be prevented by an AI (gatekeeper scenario) or by humans (1984 scenario), by deliberately forgetting the technology (reversion scenario) or by lack of incentives to build it (egalitarian-utopia scenario).\n\n• Humanity can go extinct and get replaced by AIs (conqueror and descendant scenarios) or by nothing (self-destruction scenario).\n\n• There’s absolutely no consensus on which, if any, of these scenarios are desirable, and all involve objectionable elements. This makes it all the more important to continue and deepen the conversation around our future goals, so that we don’t inadvertently drift or steer in an unfortunate direction.\n\n------------------------------------------------------------------------\n\n \n\n------------------------------------------------------------------------\n\n\\*1 This idea dates back to Saint Augustine, who wrote that “if a thing is not diminished by being shared with others, it is not rightly owned if it is only owned and not shared.”\n\n\\*2 This idea was first suggested to me by my friend and colleague Anthony Aguirre.\n\n\\*3 The renowned cosmologist Fred Hoyle explored a related scenario with a different twist in the British TV series A for Andromeda.\n\n\\*4 Injecting carbon into the atmosphere can cause two kinds of climate change: warming from carbon dioxide or cooling from smoke and soot. It’s not only the first kind that’s occasionally dismissed without scientific evidence: I’m sometimes told that nuclear winter has been debunked and is virtually impossible. I always respond by asking for a reference to a peer-reviewed scientific paper making such strong claims and, so far, there seem to be none whatsoever. Although there are great uncertainties that warrant further research, especially related to how much smoke gets produced and how high up it rises, there’s in my scientific opinion no current basis for dismissing the nuclear winter risk.\n\nChapter 6\n\nOur Cosmic Endowment: The Next Billion Years and Beyond\n\n Our speculation ends in a supercivilization, the synthesis of all solar-system life, constantly improving and extending itself, spreading outward from the sun, converting nonlife into mind.\n\n Hans Moravec, Mind Children\n\nTo me, the most inspiring scientific discovery ever is that we’ve dramatically underestimated life’s future potential. Our dreams and aspirations need not be limited to century-long life spans marred by disease, poverty and confusion. Rather, aided by technology, life has the potential to flourish for billions of years, not merely here in our Solar System, but also throughout a cosmos far more grand and inspiring than our ancestors imagined. Not even the sky is the limit.\n\nThis is exciting news for a species that has been inspired by pushing limits throughout the ages. Olympic games celebrate pushing the limits of strength, speed, agility and endurance. Science celebrates pushing the limits of knowledge and understanding. Literature and art celebrate pushing the limits of creating beautiful or life-enriching experiences. Many people, organizations and nations celebrate increasing resources, territory and longevity. Given our human obsession with limits, it’s fitting that the best-selling copyrighted book of all time is The Guinness Book of World Records.\n\nSo if our old perceived limits of life can be shattered by technology, what are the ultimate limits? How much of our cosmos can come alive? How far can life reach and how long can it last? How much matter can life make use of, and how much energy, information and computation can it extract? These ultimate limits are set not by our understanding, but by the laws of physics. This, ironically, makes it in some ways easier to analyze the long-term future of life than the short-term future.\n\nIf our 13.8-billion-year cosmic history were compressed into a week, then the 10,000-year drama of the last two chapters would be over in less than half a second. This means that although we cannot predict if and how an intelligence explosion will unfold and what its immediate aftermath will be like, all this turmoil is merely a brief flash in cosmic history whose details don’t affect life’s ultimate limits. If the post-explosion life is as obsessed as today’s humans are with pushing limits, then it will develop technology to actually reach these limits—because it can. In this chapter, we’ll explore what these limits are, thus getting a glimpse of what the long-term future of life may be like. Since these limits are based on our current understanding of physics, they should be viewed as a lower bound on the possibilities: future scientific discoveries may present opportunities to do even better.\n\nBut do we really know that future life will be so ambitious? No, we don’t: perhaps it will become as complacent as a heroin addict or a couch potato merely watching endless reruns of Keeping Up with the Kardashians. However, there is reason to suspect that ambition is a rather generic trait of advanced life. Almost regardless of what it’s trying to maximize, be it intelligence, longevity, knowledge or interesting experiences, it will need resources. It therefore has an incentive to push its technology to the ultimate limits, to make the most of the resources it has. After this, the only way to further improve is to acquire more resources, by expanding into ever-larger regions of the cosmos.\n\nAlso, life may independently originate in multiple places in our cosmos. In that case, unambitious civilizations simply become cosmically irrelevant, with ever-larger parts of the cosmic endowment ultimately being taken over by the most ambitious life forms. Natural selection therefore plays out on a cosmic scale and, after a while, almost all life that exists will be ambitious life. In summary, if we’re interested in the extent to which our cosmos can ultimately come alive, we should study the limits of ambition that are imposed by the laws of physics. Let’s do this! Let’s first explore the limits of what can be done with the resources (matter, energy, etc.) that we have in our Solar System, then turn to how to get more resources through cosmic exploration and settlement.\n\nMaking the Most of Your Resources\n\nWhereas today’s supermarkets and commodity exchanges sell tens of thousands of items we might call “resources,” future life that’s reached the technological limit needs mainly one fundamental resource: so-called baryonic matter, meaning anything made up of atoms or their constituents (quarks and electrons). Whatever form this matter is in, advanced technology can rearrange it into any desired substances or objects, including power plants, computers and advanced life forms. Let’s therefore begin by examining the limits on the energy that powers advanced life and the information processing that enables it to think.\n\nBuilding Dyson Spheres\n\nWhen it comes to the future of life, one of the most hopeful visionaries is Freeman Dyson. I’ve had the honor and pleasure of knowing him for the past two decades, but when I first met him, I felt nervous. I was a junior postdoc chowing away with my friends in the lunchroom of the Institute for Advanced Study in Princeton, and out of the blue, this world-famous physicist who used to hang out with Einstein and Gödel came up and introduced himself, asking if he could join us! He quickly put me at ease, however, by explaining that he preferred eating lunch with young folks over stuffy old professors. Even though he’s ninety-three as I type these words, Freeman is still younger in spirit than most people I know, and the mischievous boyish glint in his eyes reveals that he couldn’t care less about formalities, academic hierarchies or conventional wisdom. The bolder the idea, the more excited he gets.\n\nWhen we talked about energy use, he scoffed at how unambitious we humans were, pointing out that we could meet all our current global energy needs by harvesting the sunlight striking an area smaller than 0.5% of the Sahara desert. But why stop there? Why even stop at capturing all the sunlight striking Earth, letting most of it get wastefully beamed into empty space? Why not simply put all the Sun’s energy output to use for life?\n\nInspired by Olaf Stapledon’s 1937 sci-fi classic Star Maker, with rings of artificial worlds orbiting their parent star, Freeman Dyson published a description in 1960 of what became known as a Dyson sphere.1 Freeman’s idea was to rearrange Jupiter into a biosphere in the form of a spherical shell surrounding the Sun, where our descendants could flourish, enjoying 100 billion times more biomass and a trillion times more energy than humanity uses today.2 He argued that this was the natural next step: “One should expect that, within a few thousand years of its entering the stage of industrial development, any intelligent species should be found occupying an artificial biosphere which completely surrounds its parent star.” If you lived on the inside of a Dyson sphere, there would be no nights: you’d always see the Sun straight overhead, and all across the sky, you’d see sunlight reflecting off the rest of the biosphere, just as you can nowadays see sunlight reflecting off the Moon during the day. If you wanted to see stars, you’d simply go “upstairs” and peer out at the cosmos from the outside of the Dyson sphere.\n\nA low-tech way to build a partial Dyson sphere is to place a ring of habitats in circular orbit around the Sun. To completely surround the Sun, you could add rings orbiting it around different axes at slightly different distances, to avoid collisions. To avoid the nuisance that these fast-moving rings couldn’t be connected to one another, complicating transportation and communication, one could instead build a monolithic stationary Dyson sphere where the Sun’s inward gravitational pull is balanced by the outward pressure from the Sun’s radiation—an idea pioneered by Robert L. Forward and by Colin McInnes. The sphere can be built by gradually adding more “statites”: stationary satellites that counteract the Sun’s gravity with radiation pressure rather than centrifugal forces. Both of these forces drop off with the square of the distance to the Sun, which means that if they can be balanced at one distance from the Sun, they’ll conveniently be balanced at any other distance as well, allowing freedom to park anywhere in our Solar System. Statites need to be extremely lightweight sheets, weighing only 0.77 grams per square meter, which is about 100 times less than paper, but this is unlikely to be a showstopper. For example, a sheet of graphene (a single layer of carbon atoms in a hexagonal pattern resembling chicken wire) weighs a thousand times less than that limit. If the Dyson sphere is built to reflect rather than absorb most of the sunlight, then the total intensity of light bouncing around within it will be dramatically increased, further boosting the radiation pressure and the amount of mass that can be supported in the sphere. Many other stars have a thousandfold and even a millionfold greater luminosity than our Sun, and are therefore able to support correspondingly heavier stationary Dyson spheres.\n\nIf a much heavier rigid Dyson sphere is desired here in our Solar System, then resisting the Sun’s gravity will require ultra-strong materials that can withstand pressures tens of thousands of times greater than those at the base of the world’s tallest skyscrapers, without liquefying or buckling. To be long-lived, a Dyson sphere would need to be dynamic and intelligent, constantly fine-tuning its position and shape in response to disturbances and occasionally opening up large holes to let annoying asteroids and comets pass through without incident. Alternatively, a detect-and-deflect system could be used to handle such system intruders, optionally disassembling them and putting their matter to better use.\n\nFor today’s humans, life on or in a Dyson sphere would at best be disorienting and at worst impossible, but that need not stop future biological or non-biological life forms from thriving there. The orbiting variant would offer essentially no gravity at all, and if you walked around on the stationary kind, you could walk only on the outside (facing away from the Sun) without falling off, with gravity about ten thousand times weaker than you’re used to. You’d have no magnetic field (unless you built one) shielding you from dangerous particles from the Sun. The silver lining is that a Dyson sphere the size of Earth’s current orbit would give us about 500 million times more surface area to live on.\n\nIf more Earth-like human habitats are desired, the good news is that they’re much easier to build than a Dyson sphere. For example, figures 6.1 and 6.2 show a cylindrical habitat design pioneered by the American physicist Gerard K. O’Neill, which supports artificial gravity, cosmic ray shielding, a twenty-four-hour day-night cycle, and Earth-like atmosphere and ecosystems. Such habitats could orbit freely inside a Dyson sphere, or modified variants could be attached outside it.\n\n[Figure 6.1: A pair of counterrotating O’Neill cylinders can provide comfortable Earth-like human habitats if they orbit the Sun in such a way that they always point straight at it. The centrifugal force from their rotation provides artificial gravity, and three foldable mirrors beam sunlight inside on a 24-hour day-night cycle. The smaller habitats arranged in a ring are specialized for agriculture. Image courtesy of Rick Guidice/NASA.][Figure 6.1: A pair of counterrotating O’Neill cylinders can provide comfortable Earth-like human habitats if they orbit the Sun in such a way that they always point straight at it. The centrifugal force from their rotation provides artificial gravity, and three foldable mirrors beam sunlight inside on a 24-hour day-night cycle. The smaller habitats arranged in a ring are specialized for agriculture. Image courtesy of Rick Guidice/NASA.]\n\nFigure 6.1: A pair of counterrotating O’Neill cylinders can provide comfortable Earth-like human habitats if they orbit the Sun in such a way that they always point straight at it. The centrifugal force from their rotation provides artificial gravity, and three foldable mirrors beam sunlight inside on a 24-hour day-night cycle. The smaller habitats arranged in a ring are specialized for agriculture. Image courtesy of Rick Guidice/NASA.\n\nBuilding Better Power Plants\n\nAlthough Dyson spheres are energy efficient by today’s engineering standards, they come nowhere near pushing the limits set by the laws of physics. Einstein taught us that if we could convert mass to energy with 100% efficiency,^(\\*1) then an amount of mass m would give us an amount of energy E given by his famous formula E = mc², where c is the speed of light. This means that since c is huge, a small amount of mass can produce a humongous amount of energy. If we had an abundant supply of antimatter (which we don’t), then a 100% efficient power plant would be easy to make: simply pouring a teaspoonful of anti-water into regular water would unleash the energy equivalent to 200,000 tons of TNT, the yield of a typical hydrogen bomb—enough to power the world’s entire energy needs for about seven minutes.\n\n[Figure 6.2: Interior view of one of the O’Neill cylinders from the previous figure. If its diameter is 6.4 kilometers and rotates once every 2 minutes, people on the surface will experience the same apparent gravity as on Earth. The Sun is behind you, but appears above because of a mirror outside the cylinder that folds away at night. Airtight windows keep the atmosphere from escaping the cylinder. Image courtesy of Rick Guidice/NASA.][Figure 6.2: Interior view of one of the O’Neill cylinders from the previous figure. If its diameter is 6.4 kilometers and rotates once every 2 minutes, people on the surface will experience the same apparent gravity as on Earth. The Sun is behind you, but appears above because of a mirror outside the cylinder that folds away at night. Airtight windows keep the atmosphere from escaping the cylinder. Image courtesy of Rick Guidice/NASA.]\n\nFigure 6.2: Interior view of one of the O’Neill cylinders from the previous figure. If its diameter is 6.4 kilometers and rotates once every 2 minutes, people on the surface will experience the same apparent gravity as on Earth. The Sun is behind you, but appears above because of a mirror outside the cylinder that folds away at night. Airtight windows keep the atmosphere from escaping the cylinder. Image courtesy of Rick Guidice/NASA.\n\nIn contrast, our most common ways of generating energy today are woefully inefficient, as summarized in table 6.1 and figure 6.3. Digesting a candy bar is merely 0.00000001% efficient, in the sense that it releases a mere ten-trillionth of the energy mc² that it contains. If your stomach were even 0.001% efficient, then you’d only need to eat a single meal for the rest of your life. Compared to eating, the burning of coal and gasoline are merely 3 and 5 times more efficient, respectively. Today’s nuclear reactors do dramatically better by splitting uranium atoms through fission, but still fail to extract more than 0.08% of their energy. The nuclear reactor in the core of the Sun is an order of magnitude more efficient than those we’ve built, extracting 0.7% of the energy from hydrogen by fusing it into helium. However, even if we enclose the Sun in a perfect Dyson sphere, we’ll never convert more than about 0.08% of the Sun’s mass to energy we can use, because once the Sun has consumed about about a tenth of its hydrogen fuel, it will end its lifetime as a normal star, expand into a red giant, and begin to die. Things don’t get much better for other stars either: the fraction of their hydrogen consumed during the main lifetime ranges from about 4% for very small stars to about 12% for the largest ones. If we perfect an artificial fusion reactor that would let us fuse 100% of all hydrogen at our disposal, we’d still be stuck at that embarrassingly low 0.7% efficiency of the fusion process. How can we do better?\n\n ----------------------------------- -------------\n Method Efficiency\n Digesting candy bar 0.00000001%\n Burning coal 0.00000003%\n Burning gasoline 0.00000005%\n Fission of uranium-235 0.08%\n Using Dyson sphere until Sun dies 0.08%\n Fusion of hydrogen to helium 0.7%\n Spinning black hole engine 29%\n Dyson sphere around quasar 42%\n Sphalerizer 50%?\n Black hole evaporation 90%\n ----------------------------------- -------------\n\nTable 6.1: Efficiency of converting mass into usable energy relative to the theoretical limit E = mc². As explained in the text, getting 90% efficiency from feeding black holes and waiting for them to evaporate is unfortunately too slow to be useful, and accelerating the process dramatically lowers the efficiency.\n\n[Figure 6.3: Advanced technology can extract dramatically more energy from matter than we get by eating or burning it, and even nuclear fusion extracts 140 times less energy than the limits set by the laws of physics. Power plants exploiting sphalerons, quasars or evaporating black holes might do much better.][Figure 6.3: Advanced technology can extract dramatically more energy from matter than we get by eating or burning it, and even nuclear fusion extracts 140 times less energy than the limits set by the laws of physics. Power plants exploiting sphalerons, quasars or evaporating black holes might do much better.]\n\nFigure 6.3: Advanced technology can extract dramatically more energy from matter than we get by eating or burning it, and even nuclear fusion extracts 140 times less energy than the limits set by the laws of physics. Power plants exploiting sphalerons, quasars or evaporating black holes might do much better.\n\nEvaporating Black Holes\n\nIn his book A Brief History of Time, Stephen Hawking proposed a black hole power plant.^(\\*2) This may sound paradoxical given that black holes were long believed to be traps that nothing, not even light, could ever escape from. However, Hawking famously calculated that quantum gravity effects make a black hole act like a hot object—the smaller, the hotter—that gives off heat radiation now known as Hawking radiation. This means that the black hole gradually loses energy and evaporates away. In other words, whatever matter you dump into the black hole will eventually come back out again as heat radiation, so by the time the black hole has completely evaporated, you’ve converted your matter to radiation with nearly 100% efficiency.^(\\*3)\n\nA problem with using black hole evaporation as a power source is that, unless the black hole is much smaller than an atom in size, it’s an excruciatingly slow process that takes longer than the present age of our Universe and radiates less energy than a candle. The power produced decreases with the square of the size of the hole, and the physicists Louis Crane and Shawn Westmoreland have therefore proposed using a black hole about a thousand times smaller than a proton, weighing about as much as the largest-ever seagoing ship.3 Their main motivation was to use the black hole engine to power a starship (a topic to which we return below), so they were more concerned with portability than efficiency and proposed feeding the black hole with laser light, causing no energy-to-matter conversion at all. Even if you could feed it with matter instead of radiation, guaranteeing high efficiency appears difficult: to make protons enter such a black hole a thousandth their size, they would have to be fired at the hole with a machine as powerful as the Large Hadron Collider, augmenting their energy mc² with at least a thousand times more kinetic (motion) energy. Since at least 10% of that kinetic energy would be lost to gravitons when the black hole evaporates, we’d therefore be putting more energy into the black hole than we’d be able to extract and put to work, ending up with negative efficiency. Further confounding the prospects of a black hole power plant is that we still lack a rigorous theory of quantum gravity upon which to base our calculations—but this uncertainty could of course also mean that there are new useful quantum gravity effects yet to be discovered.\n\n[Figure 6.4: Part of the rotational energy of a spinning black hole can be extracted by throwing a particle A near the black hole and having it split into a part C that gets eaten and a part B that escapes—with more energy than A had initially.][Figure 6.4: Part of the rotational energy of a spinning black hole can be extracted by throwing a particle A near the black hole and having it split into a part C that gets eaten and a part B that escapes—with more energy than A had initially.]\n\nFigure 6.4: Part of the rotational energy of a spinning black hole can be extracted by throwing a particle A near the black hole and having it split into a part C that gets eaten and a part B that escapes—with more energy than A had initially.\n\nSpinning Black Holes\n\nFortunately, there are other ways of using black holes as power plants that don’t involve quantum gravity or other poorly understood physics. For example, many existing black holes spin very fast, with their event horizons whirling around near the speed of light, and this rotation energy can be extracted. The event horizon of a black hole is the region from which not even light can escape, because the gravitational pull is too powerful. Figure 6.4 illustrates how outside the event horizon, a spinning black hole has a region called the ergosphere, where the spinning black hole drags space along with it so fast that it’s impossible for a particle to sit still and not get dragged along. If you toss an object into the ergosphere, it will therefore pick up speed rotating around the hole. Unfortunately, it will soon get eaten up by the black hole, forever disappearing through the event horizon, so this does you no good if you’re trying to extract energy. However, Roger Penrose discovered that if you launch the object at a clever angle and make it split into two pieces as figure 6.4 illustrates, then you can arrange for only one piece to get eaten while the other escapes the black hole with more energy than you started with. In other words, you’ve successfully converted some of the rotational energy of the black hole into useful energy that you can put to work. By repeating this process many times, you can milk the black hole of all its rotational energy so that it stops spinning and its ergosphere disappears. If the initial black hole was spinning as fast as nature allows, with its event horizon moving essentially at the speed of light, this strategy allows you to convert 29% of its mass into energy. There is still significant uncertainty about how fast the black holes in our night sky spin, but many of the best-studied ones appear to spin quite fast: between 30% and 100% of the maximum allowed. The monster black hole in the middle of our Galaxy (which weighs four million times as much as our Sun) appears to spin, so even if only 10% of its mass could be converted to useful energy, that would deliver the same as 400,000 suns converted to energy with 100% efficiency, or about as much energy as we’d get from Dyson spheres around 500 million suns over billions of years.\n\nQuasars\n\nAnother interesting strategy is to extract energy not from the black hole itself, but from matter falling into it. Nature has already found a way of doing this all on its own: the quasar. As gas swirls even closer to a black hole, forming a pizza-shaped disk whose innermost parts gradually get gobbled up, it gets extremely hot and gives off copious amounts of radiation. As gas falls downward toward the hole, it speeds up, converting its gravitational potential energy into motion energy, just as a skydiver does. The motion gets progressively messier as complicated turbulence converts the coordinated motion of the gas blob into random motion on ever-smaller scales, until individual atoms begin colliding with each other at high speeds—having such random motion is precisely what it means to be hot, and these violent collisions convert motion energy into radiation. By building a Dyson sphere around the entire black hole, at a safe distance, this radiation energy can be captured and put to use. The faster the black hole spins, the more efficient this process gets, with a maximally spinning black hole delivering energy at a whopping 42% efficiency.^(\\*4) For black holes weighing about as much as a star, most of the energy comes out as X-rays, whereas for the supermassive kind found in the centers of galaxies, much of it emerges somewhere in the range of infrared, visible and ultraviolet light.\n\nOnce you’ve run out of fuel to feed your black hole, you can switch to extracting its rotational energy as we discussed above.^(\\*5) Indeed, nature has already found a way of partially doing that as well, boosting the radiation from accreted gas through a magnetic process known as the Blandford-Znajek mechanism. It may well be possible to use technology to further improve the energy extraction efficiency beyond 42% by clever use of magnetic fields or other ingredients.\n\nSphalerons\n\nThere is another known way to convert matter into energy that doesn’t involve black holes at all: the sphaleron process. It can destroy quarks and turn them into leptons: electrons, their heavier cousins the muon and tau particles, neutrinos or their antiparticles.4 As illustrated in figure 6.5, the standard model of particle physics predicts that nine quarks with appropriate flavor and spin can come together and transform into three leptons through an intermediate state called a sphaleron. Because the input weighs more than the output, the mass difference gets converted into energy according to Einstein’s E = mc² formula.\n\nFuture intelligent life might therefore be able to build what I’ll call a sphalerizer: an energy generator acting like a diesel engine on steroids. A traditional diesel engine compresses a mixture of air and diesel oil until the temperature gets high enough for it to spontaneously ignite and burn, after which the hot mixture re-expands and does useful work in the process, say pushing a piston. The carbon dioxide and other combustion gases weigh about 0.00000005% less than what was in the piston initially, and this mass difference turns into the heat energy driving the engine. A sphalerizer would compress ordinary matter to a couple of quadrillion degrees, and then let it re-expand and cool once the sphalerons had done their thing.^(\\*6) We already know the result of this experiment, because our early Universe performed it for us about 13.8 billion years ago, when it was that hot: almost 100% of the matter gets converted into energy, with less than a billionth of the particles left over being the stuff that ordinary matter is made of: quarks and electrons. So it’s just like a diesel engine, except over a billion times more efficient! Another advantage is that you don’t need to be finicky about what to fuel it with—it works with anything made of quarks, meaning any normal matter at all.\n\n[Figure 6.5: According to the standard model of particle physics, nine quarks with appropriate flavor and spin can come together and transform into three leptons through an intermediate state called a sphaleron. The combined mass of the quarks (together with the energy of the gluon particles that accompanied them) is much greater than the mass of the leptons, so this process will release energy, indicated by flashes.][Figure 6.5: According to the standard model of particle physics, nine quarks with appropriate flavor and spin can come together and transform into three leptons through an intermediate state called a sphaleron. The combined mass of the quarks (together with the energy of the gluon particles that accompanied them) is much greater than the mass of the leptons, so this process will release energy, indicated by flashes.]\n\nFigure 6.5: According to the standard model of particle physics, nine quarks with appropriate flavor and spin can come together and transform into three leptons through an intermediate state called a sphaleron. The combined mass of the quarks (together with the energy of the gluon particles that accompanied them) is much greater than the mass of the leptons, so this process will release energy, indicated by flashes.\n\nBecause of these high-temperature processes, our baby Universe produced over a trillion times more radiation (photons and neutrinos) than matter (quarks and electrons that later clumped into atoms). During the 13.8 billion years since then, a great segregation took place, where atoms became concentrated into galaxies, stars and planets, while most photons stayed in intergalactic space, forming the cosmic microwave background radiation that has been used to make baby pictures of our Universe. Any advanced life form living in a galaxy or other matter concentration can therefore turn most of its available matter back into energy, rebooting the matter percentage down to the same tiny value that emerged from our early Universe by briefly re-creating those hot dense conditions inside a sphalerizer.\n\nTo figure out how efficient an actual sphalerizer would be, one needs to work out key practical details: for example, how large does it need to be to prevent a significant fraction of the photons and neutrinos from leaking out during the compression stage? What we can say for sure, however, is that the energy prospects for the future of life are dramatically better than our current technology allows. We haven’t even managed to build a fusion reactor, yet future technology should be able to do ten and perhaps even a hundred times better.\n\nBuilding Better Computers\n\nIf eating dinner is 10 billion times worse than the physical limit on energy efficiency, then how efficient are today’s computers? Even worse than that dinner, as we’ll now see.\n\nI often introduce my friend and colleague Seth Lloyd as the only person at MIT who’s arguably as crazy as I am. After doing pioneering work on quantum computers, he went on to write a book arguing that our entire Universe is a quantum computer. We often grab beer after work, and I’ve yet to discover a topic that he doesn’t have something interesting to say about. For example, as I mentioned in chapter 2, he has lots to say about the ultimate limits of computing. In a famous 2000 paper, he showed that computing speed is limited by energy: performing an elementary logical operation in time T requires an average energy of E = h∕4T, where h is the fundamental physics quantity known as Planck’s constant. This means that a 1 kg computer can perform at most 5 × 10⁵⁰ operations per second—that’s a whopping 36 orders of magnitude more than the computer on which I’m typing these words. We’ll get there in a couple of centuries if computational power keeps doubling every couple of years, as we explored in chapter 2. He also showed that a 1 kg computer can store at most 10³¹ bits, which is about a billion billion times better than my laptop.\n\nSeth is the first to admit that actually attaining these limits may be challenging even for superintelligent life, since the memory of that 1 kg ultimate “computer” would resemble a thermonuclear explosion or a little piece of our Big Bang. However, he’s optimistic that the practical limits aren’t that far from the ultimate ones. Indeed, existing quantum computer prototypes have already miniaturized their memory by storing one bit per atom, and scaling that up would allow storing about 10²⁵ bits/kg—a trillion times better than my laptop. Moreover, using electromagnetic radiation to communicate between these atoms would permit about 5 × 10⁴⁰ operations per second—31 orders of magnitude better than my CPU.\n\nIn summary, the potential for future life to compute and figure things out is truly mind-boggling: in terms of orders of magnitude, today’s best supercomputers are much further from the ultimate 1 kg computer than they are from the blinking turn signal on a car, a device that stores merely one bit of information, flipping it between on and off about once per second.\n\nOther Resources\n\nFrom a physics perspective, everything that future life may want to create—from habitats and machines to new life forms—is simply elementary particles arranged in some particular way. Just as a blue whale is rearranged krill and krill is rearranged plankton, our entire Solar System is simply hydrogen rearranged during 13.8 billion years of cosmic evolution: gravity rearranged hydrogen into stars which rearranged the hydrogen into heavier atoms, after which gravity rearranged such atoms into our planet where chemical and biological processes rearranged them into life.\n\nFuture life that has reached its technological limit can perform such particle rearrangements more rapidly and efficiently, by first using its computing power to figure out the most efficient method and then using its available energy to power the matter rearrangement process. We saw how matter can be converted into both computers and energy, so it’s in a sense the only fundamental resource needed.^(\\*7) Once future life has bumped up against the physical limits on what it can do with its matter, there is only one way left for it to do more: by getting more matter. And the only way it can do this is by expanding into our Universe. Spaceward ho!\n\nGaining Resources Through Cosmic Settlement\n\nJust how great is our cosmic endowment? Specifically, what upper limits do the laws of physics place on the amount of matter that life can ultimately make use of? Our cosmic endowment is mind-bogglingly large, of course, but how large, exactly? Table 6.2 lists some key numbers. Our planet is currently 99.999999% dead in the sense that this fraction of its matter isn’t part of our biosphere and is doing almost nothing useful for life other than providing gravitational pull and a magnetic field. This raises the potential of one day using a hundred million times more matter in active support of life. If we can put all of the matter in our Solar System (including the Sun) to optimal use, we’ll do another million times better. Settling our Galaxy would grow our resources another trillion times.\n\nHow Far Can You Go?\n\nYou might think that we can acquire unlimited resources by settling as many other galaxies as we want if we’re patient enough, but that’s not what modern cosmology suggests! Yes, space itself might be infinite, containing infinitely many galaxies, stars and planets—indeed, this is what’s predicted by the simplest versions of inflation, the currently most popular scientific paradigm for what created our Big Bang 13.8 billion years ago. However, even if there are infinitely many galaxies, it appears that we can see and reach only a finite number of them: we can see about 200 billion galaxies and settle in at most ten billion.\n\n -------------------------------------------- -----------\n Region Particles\n Our biosphere 10⁴³\n Our Planet 10⁵¹\n Our Solar System 10⁵⁷\n Our Galaxy 10⁶⁹\n Our range traveling at half speed of light 10⁷⁵\n Our range traveling at speed of light 10⁷⁶\n Our Universe 10⁷⁸\n -------------------------------------------- -----------\n\nTable 6.2: Approximate number of matter particles (protons and neutrons) that future life can aspire to make use of.\n\nWhat limits us is the speed of light: one light-year (about ten trillion kilometers) per year. Figure 6.6 shows the part of space from which light has reached us so far during the 13.8 billion years since our Big Bang, a spherical region known as “our observable Universe” or simply “our Universe.” Even if space is infinite, our Universe is finite, containing “only” about 10⁷⁸ atoms. Moreover, about 98% of our Universe is “see but not touch,” in the sense that we can see it but never reach it even if we travel at the speed of light forever. Why is this? After all, the limit to how far we can see comes simply from the fact that our Universe isn’t infinitely old, so that distant light hasn’t yet had time to reach us. So shouldn’t we be able to travel to arbitrarily distant galaxies if we have no limit on how much time we can spend en route?\n\n[Figure 6.6: Our Universe, i.e., the spherical region of space from which light has had time to reach us (at the center) during the 13.8 billion years since our Big Bang. The patterns show the baby pictures of our Universe taken by the Planck satellite, showing that when it was merely 400,000 years old, it consisted of hot plasma nearly as hot as the surface of the Sun. Space probably continues beyond this region, and new matter comes into view every year.][Figure 6.6: Our Universe, i.e., the spherical region of space from which light has had time to reach us (at the center) during the 13.8 billion years since our Big Bang. The patterns show the baby pictures of our Universe taken by the Planck satellite, showing that when it was merely 400,000 years old, it consisted of hot plasma nearly as hot as the surface of the Sun. Space probably continues beyond this region, and new matter comes into view every year.]\n\nFigure 6.6: Our Universe, i.e., the spherical region of space from which light has had time to reach us (at the center) during the 13.8 billion years since our Big Bang. The patterns show the baby pictures of our Universe taken by the Planck satellite, showing that when it was merely 400,000 years old, it consisted of hot plasma nearly as hot as the surface of the Sun. Space probably continues beyond this region, and new matter comes into view every year.\n\nThe first challenge is that our Universe is expanding, which means that almost all galaxies are flying away from us, so settling distant galaxies amounts to a game of catch-up. The second challenge is that this cosmic expansion is accelerating, due to the mysterious dark energy that makes up about 70% of our Universe. To understand how this causes trouble, imagine that you enter a train platform and see your train slowly accelerating away from you, but with a door left invitingly open. If you’re fast and foolhardy, can you catch the train? Since it will eventually go faster than you can run, the answer clearly depends on how far away from you the train is initially: if it’s beyond a certain critical distance, you’ll never catch up with it. We face the same situation trying to catch those distant galaxies that are accelerating away from us: even if we could travel at the speed of light, all galaxies beyond about 17 billion light-years remain forever out of reach—and that’s over 98% of the galaxies in our Universe.\n\nBut hold on: didn’t Einstein’s special relativity theory say that nothing can travel faster than light? So how can galaxies outrace something traveling at the speed of light? The answer is that special relativity is superseded by Einstein’s general relativity theory, where the speed limit is more liberal: nothing can travel faster than the speed of light through space, but space is free to expand as fast as it wants. Einstein also gave us a nice way of visualizing these speed limits by viewing time as the fourth dimension in spacetime (see figure 6.7, where I’ve kept things three-dimensional by omitting one of the three space dimensions). If space weren’t expanding, light rays would form slanted 45-degree lines through spacetime, so that the regions we can see and reach from here and now are cones. Whereas our past light cone would be truncated by our Big Bang 13.8 billion years ago, our future light cone would expand forever, giving us access to an unlimited cosmic endowment. In contrast, the middle panel of the figure shows that an expanding universe with dark energy (which appears to be the Universe we inhabit) deforms our light cones into a champagne-glass shape, forever limiting the number of galaxies we can settle to about 10 billion.\n\nIf this limit makes you feel cosmic claustrophobia, let me cheer you up with a possible loophole: my calculation assumes that dark energy remains constant over time, consistent with what the latest measurements suggest. However, we still have no clue what dark energy really is, which leaves a glimmer of hope that dark energy will eventually decay away (much like the similar dark-energy-like substance postulated to explain cosmic inflation), and if this happens, the acceleration will give way to deceleration, potentially enabling future life forms to keep settling new galaxies for as long as they last.\n\n[Figure 6.7: In a spacetime diagram, an event is a point whose horizontal and vertical positions encode where and when it occurs, respectively. If space isn’t expanding (left panel), then two cones delimit the parts of spacetime that we on Earth (at apex) can be affected by (bottom cone) and can have an effect on (top cone), because causal effects cannot travel faster than light, which travels a distance of one light-year per year. Things get more interesting when space expands (right panels). According to the standard model of cosmology, we can only see and reach a finite part of spacetime even if space is infinite. In the middle image, reminiscent of a champagne glass, we use coordinates that hide the expansion of space so that the motions of distant galaxies over time correspond to vertical lines. From our current vantage point, 13.8 billion years after our Big Bang, light rays have had time to reach us only from the base of the champagne glass, and even if we travel at the speed of light, we can never reach regions outside the upper part of the glass, which contains about 10 billion galaxies. In the right image, reminiscent of a water droplet beneath a flower, we use the familiar coordinates where space is seen to expand. This deforms the glass base to a droplet shape because regions at the edges of what we can see were all very close together early on.][Figure 6.7: In a spacetime diagram, an event is a point whose horizontal and vertical positions encode where and when it occurs, respectively. If space isn’t expanding (left panel), then two cones delimit the parts of spacetime that we on Earth (at apex) can be affected by (bottom cone) and can have an effect on (top cone), because causal effects cannot travel faster than light, which travels a distance of one light-year per year. Things get more interesting when space expands (right panels). According to the standard model of cosmology, we can only see and reach a finite part of spacetime even if space is infinite. In the middle image, reminiscent of a champagne glass, we use coordinates that hide the expansion of space so that the motions of distant galaxies over time correspond to vertical lines. From our current vantage point, 13.8 billion years after our Big Bang, light rays have had time to reach us only from the base of the champagne glass, and even if we travel at the speed of light, we can never reach regions outside the upper part of the glass, which contains about 10 billion galaxies. In the right image, reminiscent of a water droplet beneath a flower, we use the familiar coordinates where space is seen to expand. This deforms the glass base to a droplet shape because regions at the edges of what we can see were all very close together early on.]\n\nFigure 6.7: In a spacetime diagram, an event is a point whose horizontal and vertical positions encode where and when it occurs, respectively. If space isn’t expanding (left panel), then two cones delimit the parts of spacetime that we on Earth (at apex) can be affected by (bottom cone) and can have an effect on (top cone), because causal effects cannot travel faster than light, which travels a distance of one light-year per year. Things get more interesting when space expands (right panels). According to the standard model of cosmology, we can only see and reach a finite part of spacetime even if space is infinite. In the middle image, reminiscent of a champagne glass, we use coordinates that hide the expansion of space so that the motions of distant galaxies over time correspond to vertical lines. From our current vantage point, 13.8 billion years after our Big Bang, light rays have had time to reach us only from the base of the champagne glass, and even if we travel at the speed of light, we can never reach regions outside the upper part of the glass, which contains about 10 billion galaxies. In the right image, reminiscent of a water droplet beneath a flower, we use the familiar coordinates where space is seen to expand. This deforms the glass base to a droplet shape because regions at the edges of what we can see were all very close together early on.\n\nHow Fast Can You Go?\n\nAbove we explored how many galaxies a civilization could settle if it expanded in all directions at the speed of light. General relativity says that it’s impossible to send rockets through space at the speed of light, because this would require infinite energy, so how fast can rockets go in practice?^(\\*8)\n\nNASA’s New Horizons rocket broke the speed record when it blasted off toward Pluto in 2006 at a speed of about 100,000 miles per hour (45 kilometers per second), and NASA’s 2018 Solar Probe Plus aims to go over four times faster by falling very close to the Sun, but even that’s less than a puny 0.1% of the speed of light. The quest for faster and better rockets has captivated some of the brightest minds of the past century, and there’s a rich and fascinating literature on the topic. Why is it so hard to go faster? The two key problems are that conventional rockets spend most of their fuel simply to accelerate the fuel they carry with them, and that today’s rocket fuel is hopelessly inefficient—the fraction of its mass turned into energy isn’t much better than the 0.00000005% for gasoline that we saw in table 6.1. One obvious improvement is to switch to more efficient fuel. For example, Freeman Dyson and others worked on NASA’s Project Orion, which aimed to explode about 300,000 nuclear bombs during 10 days to reach about 3% of the speed of light with a spaceship large enough to carry humans to another solar system during a century-long journey.5 Others have explored using antimatter as fuel, since combining it with ordinary matter releases energy with nearly 100% efficiency.\n\nAnother popular idea is to build a rocket that need not carry its own fuel. For example, interstellar space isn’t a perfect vacuum, but contains the occasional hydrogen ion (a lone proton: a hydrogen atom that’s lost its electron). In 1960, this gave physicist Robert Bussard the idea behind what’s now known as a Bussard ramjet: to scoop up such ions en route and use them as rocket fuel in an onboard fusion reactor. Although recent work has cast doubts on whether this can be made to work in practice, there’s another carry-no-fuel idea that does appear feasible for a high-tech spacefaring civilization: laser sailing.\n\nFigure 6.8 illustrates a clever laser-sail rocket design pioneered in 1984 by Robert Forward, the same physicist who invented the statites we explored for Dyson sphere construction. Just as air molecules bouncing off a sailboat sail will push it forward, light particles (photons) bouncing off a mirror will push it forward. By beaming a huge solar-powered laser at a vast ultralight sail attached to a spacecraft, we can use the energy of our own Sun to accelerate the rocket to great speeds. But how do you stop? This is the question that eluded me until I read Forward’s brilliant paper: as figure 6.8 shows, the outer ring of the laser sail detaches and moves in front of the spacecraft, reflecting our laser beam back to decelerate the craft and its smaller sail.6 Forward calculated that this could let humans make the four-light-year journey to the α Centauri solar system in merely forty years. Once there, you could imagine building a new giant laser system and continuing star-hopping throughout the Milky Way Galaxy.\n\n[Figure 6.8: Robert Forward’s design for a laser sailing mission to the α Centauri star system four light-years away. Initially, a powerful laser in our Solar System accelerates the spacecraft by applying radiation pressure to its laser sail. To brake before reaching the destination, the outer part of the sail detaches and reflects laser light back at the spacecraft.][Figure 6.8: Robert Forward’s design for a laser sailing mission to the α Centauri star system four light-years away. Initially, a powerful laser in our Solar System accelerates the spacecraft by applying radiation pressure to its laser sail. To brake before reaching the destination, the outer part of the sail detaches and reflects laser light back at the spacecraft.]\n\nFigure 6.8: Robert Forward’s design for a laser sailing mission to the α Centauri star system four light-years away. Initially, a powerful laser in our Solar System accelerates the spacecraft by applying radiation pressure to its laser sail. To brake before reaching the destination, the outer part of the sail detaches and reflects laser light back at the spacecraft.\n\nBut why stop there? In 1964, the Soviet astronomer Nikolai Kardashev proposed grading civilizations by how much energy they could put to use. Harnessing the energy of a planet, a star (with a Dyson sphere, say) and a galaxy correspond to civilizations of Type I, Type II and Type III on the Kardashev scale, respectively. Subsequent thinkers have suggested that Type IV should correspond to harnessing our entire accessible Universe. Since then, there’s been good news and bad news for ambitious life forms. The bad news is that dark energy exists, which, as we saw, appears to limit our reach. The good news is the dramatic progress of artificial intelligence. Even optimistic visionaries such as Carl Sagan used to view the prospects of humans reaching other galaxies as rather hopeless, given our propensity to die within the first century of a journey that would take millions of years even if traveling at near light speed. Refusing to give up, they considered freezing astronauts to extend their life, slowing their aging by traveling very close to light speed, or sending a community that would travel for tens of thousands of generations—longer than the human race has existed thus far.\n\nThe possibility of superintelligence completely transforms this picture, making it much more promising for those with intergalactic wanderlust. Removing the need to transport bulky human life-support systems and adding AI-invented technology, intergalactic settlement suddenly appears rather straightforward. Forward’s laser sailing becomes much cheaper when the spacecraft need merely be large enough to contain a “seed probe”: a robot capable of landing on an asteroid or planet in the target solar system and building up a new civilization from scratch. It doesn’t even have to carry the instructions with it: all it has to do is build a receiving antenna large enough to pick up more detailed blueprints and instructions transmitted from its mother civilization at the speed of light. Once done, it uses its newly constructed lasers to send out new seed probes to continue settling the galaxy one solar system at a time. Even the vast dark expanses of space between galaxies tend to contain a significant number of intergalactic stars (rejects once ejected from their home galaxies) that can be used as way stations, thus enabling an island-hopping strategy for intergalactic laser sailing.\n\nOnce another solar system or galaxy has been settled by superintelligent AI, bringing humans there is easy—if humans have succeeded in making the AI have this goal. All the necessary information about humans can be transmitted at the speed of light, after which the AI can assemble quarks and electrons into the desired humans. This could be done either rather low-tech by simply transmitting the two gigabytes of information needed to specify a person’s DNA and then incubating a baby to be raised by the AI, or the AI could nanoassemble quarks and electrons into full-grown people who would have all the memories scanned from their originals back on Earth.\n\nThis means that if there’s an intelligence explosion, the key question isn’t if intergalactic settlement is possible, but simply how fast it can proceed. Since all the ideas we’ve explored above come from humans, they should be viewed as merely lower limits on how fast life can expand; ambitious superintelligent life can probably do a lot better, and it will have a strong incentive to push the limits, since in the race against time and dark energy, every 1% increase in average settlement speed translates into 3% more galaxies colonized.\n\nFor example, if it takes 20 years to travel 10 light-years to the next star system with a laser-sail system, and then another 10 years to settle it and build new lasers and seed probes there, the settled region of space will be a sphere growing in all directions at a third of the speed of light on average. In a beautiful and thorough analysis of cosmically expanding civilizations in 2014, the American physicist Jay Olson considered a high-tech alternative to the island-hopping approach, involving two separate types of probes: seed probes and expanders.7 The seed probes would slow down, land and seed their destination with life. The expanders, on the other hand, would never stop: they’d scoop up matter in flight, perhaps using some improved variant of the ramjet technology, and use this matter both as fuel and as raw material out of which they’d build expanders and copies of themselves. This self-reproducing fleet of expanders would keep gently accelerating to always maintain a constant speed (say half the speed of light) relative to nearby galaxies, and reproduce often enough that the fleet formed an expanding spherical shell with a constant number of expanders per shell area.\n\nLast but not least, there’s the sneaky Hail Mary approach to expanding even faster than any of the above methods will permit: using Hans Moravec’s “cosmic spam” scam from chapter 4. By broadcasting a message that tricks naive freshly evolved civilizations into building a superintelligent machine that hijacks them, a civilization can expand essentially at the speed of light, the speed at which their seductive siren song spreads through the cosmos. Since this may be the only way for advanced civilizations to reach most of the galaxies within their future light cone and they have little incentive not to try it, we should be highly suspicious of any transmissions from extraterrestrials! In Carl Sagan’s book Contact, we Earthlings used blueprints from aliens to build a machine we didn’t understand—I don’t recommend doing this…\n\nIn summary, most scientists and sci-fi authors considering cosmic settlement have in my opinion been overly pessimistic in ignoring the possibility of superintelligence: by limiting attention to human travelers, they’ve overestimated the difficulty of intergalactic travel, and by limiting attention to technology invented by humans, they’ve overestimated the time needed to approach the physical limits of what’s possible.\n\nStaying Connected via Cosmic Engineering\n\nIf dark energy continues to accelerate distant galaxies away from one another, as the latest experimental data suggests, then this will pose a major nuisance to the future of life. It means that even if a future civilization manages to settle a million galaxies, dark energy will over the course of tens of billions of years fragment this cosmic empire into thousands of different regions unable to communicate with one another. If future life does nothing to prevent this fragmentation, then the largest remaining bastions of life will be clusters containing about a thousand galaxies, whose combined gravity is strong enough to overpower the dark energy trying to separate them.\n\nIf a superintelligent civilization wants to stay connected, this would give it a strong incentive to do large-scale cosmic engineering. How much matter will it have time to move into its largest supercluster before dark energy puts it forever out of reach? One method for moving a star large distances is to nudge a third star into a binary system where two stars are stably orbiting each other. Just as with romantic relationships, the introduction of a third partner can destabilize things and lead to one of the three being violently ejected—in the stellar case, at great speed. If some of the three partners are black holes, such a volatile threesome can be used to fling mass fast enough to fly far outside the host galaxy. Unfortunately, this three-body technique, applied either to stars, black holes or galaxies, doesn’t appear able to move more than a tiny fraction of a civilization’s mass the large distances required to outsmart dark energy.\n\nBut this obviously doesn’t mean that superintelligent life can’t come up with better methods, say converting much of the mass in outlying galaxies into spacecraft that can travel to the home cluster. If a sphalerizer can be built, perhaps it can even be used to convert the matter into energy that can be beamed into the home cluster as light, where it can be reconfigured back into matter or used as a power source.\n\nThe ultimate luck will be if it turns out to be possible to build stable traversable wormholes, enabling near-instantaneous communication and travel between the two ends of the wormhole no matter how far apart they are. A wormhole is a shortcut through spacetime that lets you travel from A to B without going through the intervening space. Although stable wormholes are allowed by Einstein’s theory of general relativity and have appeared in movies such as Contact and Interstellar, they require the existence of a strange hypothetical kind of matter with negative density, whose existence may hinge on poorly understood quantum gravity effects. In other words, useful wormholes may well turn out to be impossible, but if not, superintelligent life has huge incentives to build them. Not only would wormholes revolutionize rapid communication within individual galaxies, but by linking outlying galaxies to the central cluster early on, wormholes would allow the entire dominion of future life to remain connected for the long haul, completely thwarting dark energy’s attempts to censor communication. Once two galaxies are connected by a stable wormhole, they’ll remain connected no matter how far apart they drift.\n\nIf, despite its best attempts at cosmic engineering, a future civilization concludes that parts of it are doomed to drift out of contact forever, it might simply let them go and wish them well. However, if it has ambitious computing goals that involve seeking the answers to certain very difficult questions, it might instead resort to a slash-and-burn strategy: it could convert the outlying galaxies into massive computers that transform their matter and energy into computation at a frenzied pace, in the hope that before dark energy pushes their burnt-out remnants from view, they could transmit the long-sought answers back to the mother cluster. This slash-and-burn strategy would be particularly appropriate for regions so distant that they can only be reached by the “cosmic spam” method, much to the chagrin of the preexisting inhabitants. Back home in the mother region, the civilization could instead aim for maximum conservation and efficiency to last as long as possible.\n\nHow Long Can You Last?\n\nLongevity is something that most ambitious people, organizations and nations aspire to. So if an ambitious future civilization develops superintelligence and wants longevity, how long can it last?\n\nThe first thorough scientific analysis of our far future was performed by none less than Freeman Dyson, and table 6.3 summarizes some of his key findings. The conclusion is that unless intelligence intervenes, solar systems and galaxies gradually get destroyed, eventually followed by everything else, leaving nothing but cold, dead, empty space with an eternally fading glow of radiation. But Freeman ends his analysis on an optimistic note: “There are good scientific reasons for taking seriously the possibility that life and intelligence can succeed in molding this universe of ours to their own purposes.”8\n\nI think that superintelligence could easily solve many of the problems listed in table 6.3, since it can rearrange matter into something better than solar systems and galaxies. Oft-discussed challenges such as the death of our Sun in a few billion years won’t be showstoppers, since even a relatively low-tech civilization can easily move to low-mass stars that last for over 200 billion years. Assuming that superintelligent civilizations build their own power plants that are more efficient than stars, they may in fact want to prevent star formation to conserve energy: even if they use a Dyson sphere to harvest all the energy output during a star’s main lifetime (recouping about 0.1% of the total energy), they may be unable to keep much of the remaining 99.9% of the energy from going to waste when very hefty stars die. A heavy star dies in a supernova explosion from which most of the energy escapes as elusive neutrinos, and for very heavy stars, a large amount of mass gets wasted by forming a black hole from which the energy takes 10⁶⁷ years to seep out.\n\n ---------------------------------------------------- --------------\n What When\n Current age of our Universe 10¹⁰ years\n Dark energy pushes most galaxies out of reach 10¹¹ years\n Last stars burn out 10¹⁴ years\n Planets detached from stars 10¹⁵ years\n Stars detached from galaxies 10¹⁹ years\n Decay of orbits by gravitational radiation 10²⁰ years\n Protons decay (at the earliest) > 10³⁴ years\n Stellar-mass black holes evaporate 10⁶⁷ years\n Supermassive black holes evaporate 10⁹¹ years\n All matter decays to iron 10¹⁵⁰⁰ years\n All matter forms black holes, which then evaporate 10¹⁰²⁶ years\n ---------------------------------------------------- --------------\n\nTable 6.3: Estimates for the distant future, all but the 2nd and 7th made by Freeman Dyson. He made these calculations before the discovery of dark energy, which may enable several types of “cosmocalypse” in 10¹⁰–10¹¹ years. Protons may be completely stable; if not, experiments suggest it will take over 10³⁴ years for half of them to decay.\n\nAs long as superintelligent life hasn’t run out of matter/energy, it can keep maintaining its habitat in the state it desires. Perhaps it can even discover a way to prevent protons from decaying using the so-called watched-pot effect of quantum mechanics, whereby the decay process is slowed by making regular observations. There is, however, a potential showstopper: a cosmocalypse destroying our entire Universe, perhaps as soon as 10–100 billion years from now. The discovery of dark energy and progress in string theory has raised new cosmocalypse scenarios that Freeman Dyson wasn’t aware of when he wrote his seminal paper.\n\nSo how’s our Universe going to end, billions of years from now? I have five main suspects for our upcoming cosmic apocalypse, or cosmocalypse, illustrated in figure 6.9: the Big Chill, the Big Crunch, the Big Rip, the Big Snap and Death Bubbles. Our Universe has now been expanding for about 14 billion years. The Big Chill is when our Universe keeps expanding forever, diluting our cosmos into a cold, dark and ultimately dead place; this was viewed as the most likely outcome back when Freeman wrote that paper. I think of it as the T. S. Eliot option: “This is the way the world ends / Not with a bang but a whimper.” If you, like Robert Frost, prefer the world to end in fire rather than ice, then cross your fingers for the Big Crunch, where the cosmic expansion is eventually reversed and everything comes crashing back together in a cataclysmic collapse akin to a backward Big Bang. Finally, the Big Rip is like the Big Chill for the impatient, where our galaxies, planets and even atoms get torn apart in a grand finale a finite time from now. Which of these three should you bet on? That depends on what the dark energy, which makes up about 70% of the mass of our Universe, will do as space continues to expand. It can be any one of the Chill, Crunch or Rip scenarios, depending on whether the dark energy sticks around unchanged, dilutes to negative density or anti-dilutes to higher density, respectively. Since we still have no clue what dark energy is, I’ll just tell you how I’d bet: 40% on the Big Chill, 9% on the Big Crunch and 1% on the Big Rip.\n\n[Figure 6.9: We know that our Universe began with a hot Big Bang 14 billion years ago, expanded and cooled, and merged its particles into atoms, stars and galaxies. But we don’t know its ultimate fate. Proposed scenarios include a Big Chill (eternal expansion), a Big Crunch (recollapse), a Big Rip (an infinite expansion rate tearing everything apart), a Big Snap (the fabric of space revealing a lethal granular nature when stretched too much), and Death Bubbles (space “freezing” in lethal bubbles that expand at the speed of light).][Figure 6.9: We know that our Universe began with a hot Big Bang 14 billion years ago, expanded and cooled, and merged its particles into atoms, stars and galaxies. But we don’t know its ultimate fate. Proposed scenarios include a Big Chill (eternal expansion), a Big Crunch (recollapse), a Big Rip (an infinite expansion rate tearing everything apart), a Big Snap (the fabric of space revealing a lethal granular nature when stretched too much), and Death Bubbles (space “freezing” in lethal bubbles that expand at the speed of light).]\n\nFigure 6.9: We know that our Universe began with a hot Big Bang 14 billion years ago, expanded and cooled, and merged its particles into atoms, stars and galaxies. But we don’t know its ultimate fate. Proposed scenarios include a Big Chill (eternal expansion), a Big Crunch (recollapse), a Big Rip (an infinite expansion rate tearing everything apart), a Big Snap (the fabric of space revealing a lethal granular nature when stretched too much), and Death Bubbles (space “freezing” in lethal bubbles that expand at the speed of light).\n\nWhat about the other 50% of my money? I’m saving it for the “none of the above” option, because I think we humans need to be humble and acknowledge that there are basic things we still don’t understand. The nature of space, for example. The Chill, Crunch and Rip endings all assume that space itself is stable and infinitely stretchable. We used to think of space as just the boring static stage upon which the cosmic drama unfolds. Then Einstein taught us that space is really one of the key actors: it can curve into black holes, it can ripple as gravitational waves and it can stretch as an expanding universe. Perhaps it can even freeze into a different phase much like water can, with fast-expanding death bubbles of the new phase offering another wild-card cosmocalypse candidate. If death bubbles are possible, they would probably expand at the speed of light, just like the growing sphere of cosmic spam from a maximally aggressive civilization.\n\nMoreover, Einstein’s theory says that space stretching can always continue, allowing our Universe to approach infinite volume as in the Big Chill and Big Rip scenarios. This sounds a bit too good to be true, and I suspect that it is. A rubber band looks nice and continuous, just like space, but if you stretch it too much, it snaps. Why? Because it’s made of atoms, and with enough stretching, this granular atomic nature of the rubber becomes important. Could it be that space too has some sort of granularity on a scale that’s simply too small for us to have noticed? Quantum gravity research suggests that it doesn’t make sense to talk about traditional three-dimensional space on scales smaller than about 10⁻³⁴ meters. If it’s really true that space can’t be stretched indefinitely without undergoing a cataclysmic “Big Snap,” then future civilizations may wish to relocate to the largest non-expanding region of space (a huge galaxy cluster) that they can reach.\n\nHow Much Can You Compute?\n\nAfter exploring how long future life can last, let’s explore how long it might want to last. Although you might find it natural to want to live as long as possible, Freeman Dyson also gave a more quantitative argument for this desire: the cost of computation drops when you compute slowly, so you’ll ultimately get more done if you slow things down as much as possible. Freeman even calculated that if our Universe keeps expanding and cooling forever, an infinite amount of computation might be possible.\n\nSlow doesn’t necessarily mean boring: if future life lives in a simulated world, its subjectively experienced flow of time need not have anything to do with the glacial pace at which the simulation is being run in the outside world, so the prospects of infinite computation could translate into subjective immortality for simulated life forms. Cosmologist Frank Tipler has built on this idea to speculate that you could also achieve subjective immortality in the final moments before a Big Crunch by speeding up the computations toward infinity as the temperature and density skyrocketed.\n\nSince dark energy appears to spoil both Freeman’s and Frank’s dreams of infinite computation, future superintelligence may prefer to burn through its energy supplies relatively quickly, to turn them into computations before running into problems such as cosmic horizons and proton decay. If maximizing total computation is the ultimate goal, the best strategy will be a trade-off between too slow (to avoid the aforementioned problems) and too fast (spending more energy than needed per computation).\n\nPutting together everything we’ve explored in this chapter tells us that maximally efficient power plants and computers would enable superintelligent life to perform a mind-boggling amount of computation. Powering your thirteen-watt brain for a hundred years requires the energy in about half a milligram of matter—less than in a typical grain of sugar. Seth Lloyd’s work suggests that the brain could be made a quadrillion times more energy efficient, enabling that sugar grain to power a simulation of all human lives ever lived as well as thousands of times more people. If all the matter in our available Universe could be used to simulate people, that would enable over 10⁶⁹ lives—or whatever else superintelligent AI preferred to do with its computational power. Even more lives would be possible if their simulations were run more slowly.9 Conversely, in his book Superintelligence, Nick Bostrom estimates that 10⁵⁸ human lives could be simulated with more conservative assumptions about energy efficiency. However we slice and dice these numbers, they’re huge, as is our responsibility for ensuring that this future potential of life to flourish isn’t squandered. As Bostrom puts it: “If we represent all the happiness experienced during one entire such life by a single teardrop of joy, then the happiness of these souls could fill and refill the Earth’s oceans every second, and keep doing so for a hundred billion billion millennia. It is really important that we make sure these truly are tears of joy.”\n\nCosmic Hierarchies\n\nThe speed of light limits not only the spread of life, but also the nature of life, placing strong constraints on communication, consciousness and control. So if much of our cosmos eventually comes alive, what will this life be like?\n\nThought Hierarchies\n\nHave you ever tried and failed to swat a fly with your hand? The reason that it can react faster than you is that it’s smaller, so that it takes less time for information to travel between its eyes, brain and muscles. This “bigger = slower” principle applies not only to biology, where the speed limit is set by how fast electrical signals can travel through neurons, but also to future cosmic life if no information can travel faster than light. So for an intelligent information-processing system, going big is a mixed blessing involving an interesting trade-off. On one hand, going bigger lets it contain more particles, which enable more complex thoughts. On the other hand, this slows down the rate at which it can have truly global thoughts, since it now takes longer for the relevant information to propagate to all its parts.\n\nSo if life engulfs our cosmos, what form will it choose: simple and fast, or complex and slow? I predict that it will make the same choice as Earth life has made: both! The denizens of Earth’s biosphere span a staggering range of sizes, from gargantuan two-hundred-ton blue whales down to the petite 10⁻¹⁶ kg bacterium Pelagibacter, believed to account for more biomass than all the world’s fish combined. Moreover, organisms that are large, complex and slow often mitigate their sluggishness by containing smaller modules that are simple and fast. For example, your blink reflex is extremely fast precisely because it’s implemented by a small and simple circuit that doesn’t involve most of your brain: if that hard-to-swat fly accidentally heads toward your eye, you’ll blink within a tenth of a second, long before the relevant information has had time to spread throughout your brain and make you consciously aware of what happened. By organizing its information processing into a hierarchy of modules, our biosphere manages to both have the cake and eat it, attaining both speed and complexity. We humans already use this same hierarchical strategy to optimize parallel computing.\n\nBecause internal communication is slow and costly, I expect advanced future cosmic life to do the same, so that computations will be done as locally as possible. If a computation is simple enough to do with a 1 kg computer, it’s counterproductive to spread it out over a galaxy-sized computer, since waiting for the information to be shared at the speed of light after each computational step causes a ridiculous delay of about 100,000 years per step.\n\nWhat, if any, of this future information processing will be conscious in the sense of involving a subjective experience is a controversial and fascinating topic which we’ll explore in chapter 8. If consciousness requires the different parts of the system to be able to communicate with one another, then the thoughts of larger systems are by necessity slower. Whereas you or a future Earth-sized supercomputer can have many thoughts per second, a galaxy-sized mind could have only one thought every hundred thousand years, and a cosmic mind a billion light-years in size would only have time to have about ten thoughts in total before dark energy fragmented it into disconnected parts. On the other hand, these few precious thoughts and accompanying experiences might be quite deep!\n\nControl Hierarchies\n\nIf thought itself is organized in a hierarchy spanning a wide range of scales, then what about power? In chapter 4, we explored how intelligent entities naturally organize themselves into power hierarchies in Nash equilibrium, where any entity would be worse off if they altered their strategy. The better the communication and transportation technology gets, the larger these hierarchies can grow. If superintelligence one day expands to cosmic scales, what will its power hierarchy be like? Will it be freewheeling and decentralized or highly authoritarian? Will cooperation be based mainly on mutual benefit or on coercion and threats?\n\nTo shed light on these questions, let’s consider both the carrot and the stick: What incentives are there for collaboration on cosmic scales, and what threats might be used to enforce it?\n\nControlling with the Carrot\n\nOn Earth, trade has been a traditional driver of cooperation because the relative difficulty of producing things varies across the planet. If mining a kilogram of silver costs 300 times more than mining a kilogram of copper in one region, but only 100 times more in another, they’ll both come out ahead by trading 200 kg of copper against 1 kg of silver. If one region has much higher technology than another, both can similarly benefit from trading high-tech goods against raw materials.\n\nHowever, if superintelligence develops technology that can readily rearrange elementary particles into any form of matter whatsoever, then it will eliminate most of the incentive for long-distance trade. Why bother shipping silver between distant solar systems when it’s simpler and quicker to transmute copper into silver by rearranging its particles? Why bother shipping high-tech machinery between galaxies when both the know-how and the raw materials (any matter will do) exist in both places? My guess is that in a cosmos teeming with superintelligence, almost the only commodity worth shipping long distances will be information. The only exception might be matter to be used for cosmic engineering projects—for example, to counteract the aforementioned destructive tendency of dark energy to tear civilizations apart. As opposed to traditional human trade, this matter can be shipped in any convenient bulk form whatsoever, perhaps even as an energy beam, since the receiving superintelligence can rapidly rearrange it into whatever objects it wants.\n\nIf sharing or trading of information emerges as the main driver of cosmic cooperation, then what sorts of information might be involved? Any desirable information will be valuable if generating it requires a massive and time-consuming computational effort. For example, a superintelligence may want answers to hard scientific questions about the nature of physical reality, hard mathematical questions about theorems and optimal algorithms and hard engineering questions about how to best build spectacular technology. Hedonistic life forms may want awesome digital entertainment and simulated experiences, and cosmic commerce may fuel demand for some form of cosmic cryptocurrency in the spirit of bitcoins.\n\nSuch sharing opportunities may incentivize information flow not only between entities of roughly equal power, but also up and down power hierarchies, say between solar-system-sized nodes and a galactic hub or between galaxy-sized nodes and a cosmic hub. The nodes might want this for the pleasure of being part of something greater, for being provided with answers and technologies that they couldn’t develop alone and for defense against external threats. They may also value the promise of near immortality through backup: just as many humans take solace in a belief that their minds will live on after their physical bodies die, an advanced AI may appreciate having its mind and knowledge live on in a hub supercomputer after its original physical hardware has depleted its energy reserves.\n\nConversely, the hub may want its nodes to help it with massive long-term computing tasks where the results aren’t urgently needed, so that it’s worth waiting thousands or millions of years for the answers. As we explored above, the hub may also want its nodes to help carry out massive cosmic engineering projects such as counteracting destructive dark energy by moving galactic mass concentrations together. If traversable wormholes turn out to be possible and buildable, then a top priority of a hub will probably be constructing a network of them to thwart dark energy and keep its empire connected indefinitely. The questions of what ultimate goals a cosmic superintelligence may have is a fascinating and controversial one that we’ll explore further in chapter 7.\n\nControlling with the Stick\n\nTerrestrial empires usually compel their subordinates to cooperate by using both the carrot and the stick. While subjects of the Roman Empire valued the technology, infrastructure and defense that they were offered as a reward for their cooperation, they also feared the inevitable repercussions of rebelling or not paying taxes. Because of the long time required to send troops from Rome to outlying provinces, part of the intimidation was delegated to local troops and loyal officials empowered to inflict near-instantaneous punishments. A superintelligent hub could use the analogous strategy of deploying a network of loyal guards throughout its cosmic empire. Since superintelligent subjects can be hard to control, the simplest viable strategy may be using AI guards that are programmed to be 100% loyal by virtue of being relatively dumb, simply monitoring whether all rules are obeyed and automatically triggering a doomsday device if not.\n\nSuppose, for example, that the hub AI arranges for a white dwarf to be placed in the vicinity of a solar-system-sized civilization that it wishes to control. A white dwarf is the burnt-out husk of a modestly heavy star. Consisting largely of carbon, it resembles a giant diamond in the sky, and is so compact that it can weigh more than the Sun while being smaller than Earth. The Indian physicist Subrahmanyan Chandrasekhar famously proved that if you keep adding mass to it until it surpasses the Chandrasekhar limit, about 1.4 times the mass of our Sun, it will undergo a cataclysmic thermonuclear detonation known as a supernova of type 1A. If the hub AI has callously arranged for this white dwarf to be extremely close to its Chandrasekhar limit, the guard AI could be effective even if it were extremely dumb (indeed, largely because it was so dumb): it could be programmed to simply verify that the subjugated civilization had delivered its monthly quota of cosmic bitcoins, mathematical proofs or whatever other taxes were stipulated, and if not, toss enough mass onto the white dwarf to ignite the supernova and blow the entire region to smithereens.\n\nGalaxy-sized civilizations may be similarly controllable by placing large numbers of compact objects into tight orbits around the monster black hole at the galaxy center, and threatening to transform these masses into gas, for instance by colliding them. This gas would then start feeding the black hole, transforming it into a powerful quasar, potentially rendering much of the galaxy uninhabitable.\n\nIn summary, there are strong incentives for future life to cooperate over cosmic distances, but it’s a wide-open question whether such cooperation will be based mainly on mutual benefits or on brutal threats—the limits imposed by physics appear to allow both scenarios, so the outcome will depend on the prevailing goals and values. We’ll explore our ability to influence these goals and values of future life in chapter 7.\n\nWhen Civilizations Clash\n\nSo far, we’ve only discussed scenarios where life expands into our cosmos from a single intelligence explosion. But what happens if life evolves independently in more than one place and two expanding civilizations meet?\n\nIf you consider a random solar system, there’s some probability that life will evolve on one of its planets, develop advanced technology and expand into space. This probability seems to be greater than zero since technological life has evolved here in our Solar System and the laws of physics appear to allow space settlement. If space is large enough (indeed, the theory of cosmological inflation suggests it to be vast or infinite), then there will be many such expanding civilizations, as illustrated in figure 6.10. Jay Olson’s above-mentioned paper includes an elegant analysis of such expanding cosmic biospheres, and Toby Ord has performed a similar analysis with colleagues at the Future of Humanity Institute. Viewed in three dimensions, these cosmic biospheres are quite literally spheres as long as civilizations expand with the same speed in all directions. In spacetime, they look like the upper part of the champagne glass in figure 6.7, because dark energy ultimately limits how many galaxies each civilization can reach.\n\nIf the distance between neighboring space-settling civilizations is much larger than dark energy lets them expand, then they’ll never come into contact with each other or even find out about each other’s existence, so they’ll feel as if they’re alone in the cosmos. If our cosmos is more fecund so that neighbors are closer together, however, some civilizations will eventually overlap. What happens in these overlap regions? Will there be cooperation, competition or war?\n\n[Figure 6.10: If life evolves independently at multiple points in spacetime (places and times) and starts colonizing space, then space will contain a network of expanding cosmic biospheres, each of which resembles the top of the champagne glass from figure 6.7. The bottom of each biosphere represents the place and time when colonization began. The opaque and translucent champagne glasses correspond to colonization at 50% and 100% of the speed of light, respectively, and overlaps show where independent civilizations meet.][Figure 6.10: If life evolves independently at multiple points in spacetime (places and times) and starts colonizing space, then space will contain a network of expanding cosmic biospheres, each of which resembles the top of the champagne glass from figure 6.7. The bottom of each biosphere represents the place and time when colonization began. The opaque and translucent champagne glasses correspond to colonization at 50% and 100% of the speed of light, respectively, and overlaps show where independent civilizations meet.]\n\nFigure 6.10: If life evolves independently at multiple points in spacetime (places and times) and starts colonizing space, then space will contain a network of expanding cosmic biospheres, each of which resembles the top of the champagne glass from figure 6.7. The bottom of each biosphere represents the place and time when colonization began. The opaque and translucent champagne glasses correspond to colonization at 50% and 100% of the speed of light, respectively, and overlaps show where independent civilizations meet.\n\nEuropeans were able to conquer Africa and the Americas because they had superior technology. In contrast, it’s plausible that long before two superintelligent civilizations encounter one another, their technologies will plateau at the same level, limited merely by the laws of physics. This makes it seem unlikely that one superintelligence could easily conquer the other even if it wanted to. Moreover, if their goals have evolved to be relatively aligned, then they may have little reason to desire conquest or war. For example, if they’re both trying to prove as many beautiful theorems as possible and invent as clever algorithms as possible, they can simply share their findings and both be better off. After all, information is very different from the resources that humans usually fight over, in that you can simultaneously give it away and keep it.\n\nSome expanding civilizations might have goals that are essentially immutable, such as those of a fundamentalist cult or a spreading virus. However, it’s also plausible that some advanced civilizations are more like open-minded humans—willing to adjust their goals when presented with sufficiently compelling arguments. If two of them meet, there will be a clash not of weapons but of ideas, where the most persuasive one prevails and has its goals spread at the speed of light through the region controlled by the other civilization. Assimilating your neighbors is a faster expansion strategy than settlement, since your sphere of influence can spread at the speed with which ideas move (the speed of light using telecommunication), whereas physical settlement inevitably progresses slower than the speed of light. This assimilation will not be forced such as that infamously employed by the Borg in Star Trek, but voluntary based on the persuasive superiority of ideas, leaving the assimilated better off.\n\nWe’ve seen that the future cosmos can contain rapidly expanding bubbles of two kinds: expanding civilizations and those death bubbles that expand at light speed and make space uninhabitable by destroying all our elementary particles. An ambitious civilization can thus encounter three kinds of regions: uninhabited ones, life bubbles and death bubbles. If it fears uncooperative rival civilizations, it has a strong incentive to launch a rapid “land grab” and settle the uninhabited regions before the rivals do. However, it has the same expansionist incentive even if there are no other civilizations, simply to acquire resources before dark energy makes them unreachable. We just saw how bumping into another expanding civilization can be either better or worse than bumping into uninhabited space, depending on how cooperative and open-minded this neighbor is. However, it’s better to bump into any expansionist civilization (even one trying to convert your civilization into paper clips) than a death bubble, which will continue expanding at the speed of light regardless of whether you try to fight it or reason with it. Our only protection against death bubbles is dark energy, which prevents distant ones from ever reaching us. So if death bubbles are indeed common, then dark energy is actually not our enemy but our friend.\n\nAre We Alone?\n\nMany people take for granted that there’s advanced life throughout much of our Universe, so that human extinction wouldn’t matter much from a cosmic perspective. After all, why should we worry about wiping ourselves out if some inspiring Star Trek–like civilization would soon swoop in and re-seed our Solar System with life, perhaps even using their advanced technology to reconstruct and resuscitate us? I view this Star Trek assumption as dangerous, because it can lull us into a false sense of security and make our civilization apathetic and reckless. Indeed, I think that this assumption that we’re not alone in our Universe is not only dangerous but also probably false.\n\nThis is a minority view,^(\\*9) and I may well be wrong, but it’s at the very least a possibility that we can’t currently dismiss, which gives us a moral imperative to play it safe and not drive our civilization extinct.\n\nWhen I give lectures about cosmology, I often ask the audience to raise their hands if they think there’s intelligent life elsewhere in our Universe (the region of space from which light has reached us so far during the 13.8 billion years since our Big Bang). Infallibly, almost everyone does, from kindergartners to college students. When I ask why, the basic answer I tend to get is that our Universe is so huge that there’s got to be life somewhere, at least statistically speaking. Let’s take a closer look at this argument and pinpoint its weakness.\n\nIt all comes down to one number: the typical distance between a civilization in figure 6.10 and its nearest neighbor. If this distance is much larger than 20 billion light-years, we should expect to be alone in our Universe (the part of space from which light has reached us during the 13.8 billion years since our Big Bang), and to never make contact with aliens. So what should we expect for this distance? We’re quite clueless. This means that the distance to our neighbor is in the ballpark of 1000…000 meters, where the total number of zeroes could reasonably be 21, 22, 23,…, 100, 101, 102 or more—but probably not much smaller than 21, since we haven’t yet seen compelling evidence of aliens (see figure 6.11). For our nearest neighbor civilization to be within our Universe, whose radius is about 10²⁶ meters, the number of zeroes can’t exceed 26, and the probability of the number of zeroes falling in the narrow range between 22 and 26 is rather small. This is why I think we’re alone in our Universe.\n\n[Figure 6.11: Are we alone? The huge uncertainties about how life and intelligence evolved suggest that our nearest neighbor civilization in space could reasonably be anywhere along the horizontal axis above, making it unlikely that it’s in the narrow range between the edge of our Galaxy (about 10 21 meters away) and the edge of our Universe (about 10 26 meters away). If it were much closer than this range, there should be so many other advanced civilizations in our Galaxy that we’d probably have noticed, which suggests that we’re in fact alone in our Universe.][Figure 6.11: Are we alone? The huge uncertainties about how life and intelligence evolved suggest that our nearest neighbor civilization in space could reasonably be anywhere along the horizontal axis above, making it unlikely that it’s in the narrow range between the edge of our Galaxy (about 10 21 meters away) and the edge of our Universe (about 10 26 meters away). If it were much closer than this range, there should be so many other advanced civilizations in our Galaxy that we’d probably have noticed, which suggests that we’re in fact alone in our Universe.]\n\nFigure 6.11: Are we alone? The huge uncertainties about how life and intelligence evolved suggest that our nearest neighbor civilization in space could reasonably be anywhere along the horizontal axis above, making it unlikely that it’s in the narrow range between the edge of our Galaxy (about 10²¹ meters away) and the edge of our Universe (about 10²⁶ meters away). If it were much closer than this range, there should be so many other advanced civilizations in our Galaxy that we’d probably have noticed, which suggests that we’re in fact alone in our Universe.\n\nI give a detailed justification of this argument in my book Our Mathematical Universe, so I won’t rehash it here, but the basic reason for why we’re clueless about this neighbor distance is that we’re in turn clueless about the probability of intelligent life arising in a given place. As the American astronomer Frank Drake pointed out, this probability can be calculated by multiplying together the probability of there being a habitable environment there (say an appropriate planet), the probability that life will form there and the probability that this life will evolve to become intelligent. When I was a grad student, we had no clue about any of these three probabilities. After the past two decades’ dramatic discoveries of planets orbiting other stars, it now seems likely that habitable planets are abundant, with billions in our own Galaxy alone. The probability of evolving life and then intelligence, however, remains extremely uncertain: some experts think that one or both are rather inevitable and occur on most habitable planets, while others think that one or both are extremely rare because of one or more evolutionary bottlenecks that require a wild stroke of luck to pass through. Some proposed bottlenecks involve chicken-and-egg problems at the earliest stages of self-reproducing life: for example, for a modern cell to build a ribosome, the highly complex molecular machine that reads our genetic code and builds our proteins, it needs another ribosome, and it’s not obvious that the very first ribosome could evolve gradually from something simpler.10 Other proposed bottlenecks involve the development of higher intelligence. For example, although dinosaurs ruled Earth for over 100 million years, a thousand times longer than we modern humans have been around, evolution didn’t seem to inevitably push them toward higher intelligence and inventing telescopes or computers.\n\nSome people counter my argument by saying that, yes, intelligent life could be very rare, but in fact it isn’t—our Galaxy is teeming with intelligent life that mainstream scientists are simply not noticing. Perhaps aliens have already visited Earth, as UFO enthusiasts claim. Perhaps aliens haven’t visited Earth, but they’re out there and they’re deliberately hiding from us (this has been called the “zoo hypothesis” by the U.S. astronomer John A. Ball, and features in sci-fi classics such as Olaf Stapledon’s Star Maker). Or perhaps they’re out there without deliberately hiding: they’re simply not interested in space settlement or large engineering projects that we’d have noticed.\n\nSure, we need to keep an open mind about these possibilities, but since there’s no generally accepted evidence for any of them, we also need to take seriously the alternative: that we’re alone. Moreover, I think we shouldn’t underestimate the diversity of advanced civilizations by assuming that they all share goals that make them go unnoticed: we saw above that resource acquisition is quite a natural goal for a civilization to have, and for us to notice, all it takes is one civilization deciding to overtly settle all it can and hence engulf our Galaxy and beyond. Confronted with the fact that there are millions of habitable Earth-like planets in our Galaxy that are billions of years older than Earth, giving ample time for ambitious inhabitants to settle the Galaxy, we therefore can’t dismiss the most obvious interpretation: that the origin of life requires a random fluke so unlikely that they’re all uninhabited.\n\nIf life is not rare after all, we may soon know. Ambitious astronomical surveys are searching atmospheres of Earth-like planets for evidence of oxygen produced by life. In parallel with this search for any life, the search for intelligent life was recently boosted by the Russian philanthropist Yuri Milner’s $100 million project “Breakthrough Listen.”\n\nIt’s important not to be overly anthropocentric when searching for advanced life: if we discover an extraterrestrial civilization, it’s likely to already have gone superintelligent. As Martin Rees put it in a recent essay, “the history of human technological civilization is measured in centuries—and it may be only one or two more centuries before humans are overtaken or transcended by inorganic intelligence, which will then persist, continuing to evolve, for billions of years….We would be most unlikely to ‘catch’ it in the brief sliver of time when it took organic form.”11 I agree with Jay Olson’s conclusion in his aforementioned space settlement paper: “We regard the possibility that advanced intelligence will make use of the universe’s resources to simply populate existing earthlike planets with advanced versions of humans as an unlikely endpoint to the progression of technology.” So when you imagine aliens, don’t think of little green fellows with two arms and two legs, but think of the superintelligent spacefaring life we explored earlier in this chapter.\n\nAlthough I’m a strong supporter of all the ongoing searches for extraterrestrial life, which are shedding light on one of the most fascinating questions in science, I’m secretly hoping that they’ll all fail and find nothing! The apparent incompatibility between the abundance of habitable planets in our Galaxy and the lack of extraterrestrial visitors, known as the Fermi paradox, suggests the existence of what the economist Robin Hanson calls a “Great Filter,” an evolutionary/technological roadblock somewhere along the developmental path from the non-living matter to space-settling life. If we discover independently evolved life elsewhere, this would suggest that primitive life isn’t rare, and that the roadblock lies after our current human stage of development—perhaps because space settlement is impossible, or because almost all advanced civilizations self-destruct before they’re able to go cosmic. I’m therefore crossing my fingers that all searches for extraterrestrial life find nothing: this is consistent with the scenario where evolving intelligent life is rare but we humans got lucky, so that we have the roadblock behind us and have extraordinary future potential.\n\nOutlook\n\nSo far, we’ve spent this book exploring the history of life in our Universe, from its humble beginnings billions of years ago to possible grand futures billions of years from now. If our current AI development eventually triggers an intelligence explosion and optimized space settlement, it will be an explosion in a truly cosmic sense: after spending billions of years as an almost negligibly small perturbation on an indifferent lifeless cosmos, life suddenly explodes onto the cosmic arena as a spherical blast wave expanding near the speed of light, never slowing down, and igniting everything in its path with the spark of life.\n\nSuch optimistic views of the importance of life in our cosmic future have been eloquently articulated by many of the thinkers we’ve encountered in this book. Because sci-fi authors are often dismissed as unrealistic romantic dreamers, I find it ironic that most sci-fi and scientific writing about space settlement now appears too pessimistic in the light of superintelligence. For example, we saw how intergalactic travel becomes much easier once people and other intelligent entities can be transmitted in digital form, potentially making us masters of our own destiny not only in our Solar System or the Milky Way Galaxy, but also in the cosmos.\n\nAbove we considered the very real possibility that we’re the only high-tech civilization in our Universe. Let’s spend the rest of this chapter exploring this scenario, and the huge moral responsibility it entails. This means that after 13.8 billion years, life in our Universe has reached a fork in the road, facing a choice between flourishing throughout the cosmos or going extinct. If we don’t keep improving our technology, the question isn’t whether humanity will go extinct, but how. What will get us first—an asteroid, a supervolcano, the burning heat of the aging Sun, or some other calamity (see figure 5.1)? Once we’re gone, the cosmic drama predicted by Freeman Dyson will play on without spectators: barring a cosmocalypse, stars burn out, galaxies fade and black holes evaporate, each ending its life with a huge explosion that releases over a million times as much energy as the Tsar Bomba, the most powerful hydrogen bomb ever built. As Freeman put it: “The cold expanding universe will be illuminated by occasional fireworks for a very long time.” Alas, this fireworks display will be a meaningless waste, with nobody there to enjoy it.\n\nWithout technology, our human extinction is imminent in the cosmic context of tens of billions of years, rendering the entire drama of life in our Universe merely a brief and transient flash of beauty, passion and meaning in a near eternity of meaninglessness experienced by nobody. What a wasted opportunity that would be! If instead of eschewing technology, we choose to embrace it, then we up the ante: we gain the potential both for life to survive and flourish and for life to go extinct even sooner, self-destructing due to poor planning (see figure 5.1). My vote is for embracing technology, and proceeding not with blind faith in what we build, but with caution, foresight and careful planning.\n\nAfter 13.8 billion years of cosmic history, we find ourselves in a breathtakingly beautiful Universe, which through us humans has come alive and started becoming aware of itself. We’ve seen that life’s future potential in our Universe is grander than the wildest dreams of our ancestors, tempered by an equally real potential for intelligent life to go permanently extinct. Will life in our Universe fulfill its potential or squander it? This depends to a great extent on what we humans alive today do during our lifetime, and I’m optimistic that we can make the future of life truly awesome if we make the right choices. What should we want and how can we attain those goals? Let’s spend the rest of the book exploring some of the most difficult challenges involved and what we can do about them.\n\n \n\n------------------------------------------------------------------------\n\nTHE BOTTOM LINE:\n\n• Compared to cosmic timescales of billions of years, an intelligence explosion is a sudden event where technology rapidly plateaus at a level limited only by the laws of physics.\n\n• This technological plateau is vastly higher than today’s technology, allowing a given amount of matter to generate about ten billion times more energy (using sphalerons or black holes), store 12–18 orders of magnitude more information or compute 31–41 orders of magnitude faster—or to be converted to any other desired form of matter.\n\n• Superintelligent life would not only make such dramatically more efficient use of its existing resources, but would also be able to grow today’s biosphere by about 32 orders of magnitude by acquiring more resources through cosmic settlement at near light speed.\n\n• Dark energy limits the cosmic expansion of superintelligent life and also protects it from distant expanding death bubbles or hostile civilizations. The threat of dark energy tearing cosmic civilizations apart motivates massive cosmic engineering projects, including wormhole construction if this turns out to be feasible.\n\n• The main commodity shared or traded across cosmic distances is likely to be information.\n\n• Barring wormholes, the light-speed limit on communication poses severe challenges for coordination and control across a cosmic civilization. A distant central hub may incentivize its superintelligent “nodes” to cooperate either through rewards or through threats, say by deploying a local guard AI programmed to destroy the node by setting off a supernova or quasar unless the rules are obeyed.\n\n• The collision of two expanding civilizations may result in assimilation, cooperation or war, where the latter is arguably less likely than it is between today’s civilizations.\n\n• Despite popular belief to the contrary, it’s quite plausible that we’re the only life form capable of making our observable Universe come alive in the future.\n\n• If we don’t improve our technology, the question isn’t whether humanity will go extinct, but merely how: will an asteroid, a supervolcano, the burning heat of the aging Sun or some other calamity get us first?\n\n• If we do keep improving our technology with enough care, foresight and planning to avoid pitfalls, life has the potential to flourish on Earth and far beyond for many billions of years, beyond the wildest dreams of our ancestors.\n\n------------------------------------------------------------------------\n\n \n\n------------------------------------------------------------------------\n\n\\*1 If you work in the energy sector, you may be used to instead defining efficiency as the fraction of the energy released that’s in a useful form.\n\n\\*2 If no suitable nature-made black hole can be found in the nearby universe, a new one can be created by putting lots of matter in a sufficiently small space.\n\n\\*3 This is a slight oversimplification, because Hawking radiation also includes some particles from which it’s hard to extract useful work. Large black holes are only 90% efficient, because about 10% of the energy is radiated in the form of gravitons: extremely shy particles that are almost impossible to detect, let alone extract useful work from. As the black hole continues evaporating and shrinking, the efficiency drops further because the Hawking radiation starts including neutrinos and other massive particles.\n\n\\*4 For Douglas Adams fans out there, note that this is an elegant question giving the answer to the question of life, the universe and everything. More precisely, the efficiency is 1 – 1/√3‾ ≈ 42%.\n\n\\*5 If you feed the black hole by placing a gas cloud around it that rotates slowly in the same direction, then this gas will spin ever faster as it’s pulled in and eaten, boosting the black hole’s rotation, just as a figure-skater spins faster when pulling in her arms. This may keep the hole maximally spinning, enabling you to extract first 42% of the gas energy and then 29% of the remainder, for a total efficiency of 42% + (1-42%)×29% ≈ 59%.\n\n\\*6 It needs to get hot enough to re-unify the electromagnetic and weak forces, which happens when particles move about as fast as when they’ve been accelerated by 200 billion volts in a particle collider.\n\n\\*7 Above we only discussed matter made of atoms. There is about six times more dark matter, but it’s very elusive and hard to catch, routinely flying straight through Earth and out the other side, so it remains to be seen whether it’s possible for future life to capture and utilize it.\n\n\\*8 The cosmic mathematics comes out remarkably simple: if the civilization expands through the expanding space not at the speed of light c but at some slower speed v, the number of galaxies settled gets reduced by a factor (v/c)³. This means that slowpoke civilizations get severely penalized, with one that expands 10 times slower ultimately settling 1,000 times fewer galaxies.\n\n\\*9 However, John Gribbin comes to a similar conclusion in his 2011 book Alone in the Universe. For a spectrum of intriguing perspectives on this question, I also recommend Paul Davies’ 2011 book The Eerie Silence.\n\nChapter 7\n\nGoals\n\n The mystery of human existence lies not in just staying alive, but in finding something to live for.\n\n Fyodor Dostoyevsky, The Brothers Karamazov\n\n Life is a journey, not a destination.\n\n Ralph Waldo Emerson\n\nIf I had to summarize in a single word what the thorniest AI controversies are about, it would be “goals”: Should we give AI goals, and if so, whose goals? How can we give AI goals? Can we ensure that these goals are retained even if the AI gets smarter? Can we change the goals of an AI that’s smarter than us? What are our ultimate goals? These questions are not only difficult, but also crucial for the future of life: if we don’t know what we want, we’re less likely to get it, and if we cede control to machines that don’t share our goals, then we’re likely to get what we don’t want.\n\nPhysics: The Origin of Goals\n\nTo shed light on these questions, let’s first explore the ultimate origin of goals. When we look around us in the world, some processes strike us as goal-oriented while others don’t. Consider, for example, the process of a soccer ball being kicked for the game-winning shot. The behavior of the ball itself does not appear goal-oriented, and is most economically explained in terms of Newton’s laws of motion, as a reaction to the kick. The behavior of the player, on the other hand, is most economically explained not mechanistically in terms of atoms pushing each other around, but in terms of her having the goal of maximizing her team’s score. How did such goal-oriented behavior emerge from the physics of our early Universe, which consisted merely of a bunch of particles bouncing around seemingly without goals?\n\nIntriguingly, the ultimate roots of goal-oriented behavior can be found in the laws of physics themselves, and manifest themselves even in simple processes that don’t involve life. If a lifeguard rescues a swimmer, as in figure 7.1, we expect her not to go in a straight line, but to run a bit further along the beach where she can go faster than in the water, thereby turning slightly when she enters the water. We naturally interpret her choice of trajectory as goal-oriented, since out of all possible trajectories, she’s deliberately choosing the optimal one that gets her to the swimmer as fast as possible. Yet a simple light ray similarly bends when it enters water (see figure 7.1), also minimizing the travel time to its destination! How can this be?\n\nThis is known in physics as Fermat’s principle, articulated in 1662, and it provides an alternative way of predicting the behavior of light rays. Remarkably, physicists have since discovered that all laws of classical physics can be mathematically reformulated in an analogous way: out of all ways that nature could choose to do something, it prefers the optimal way, which typically boils down to minimizing or maximizing some quantity. There are two mathematically equivalent ways of describing each physical law: either as the past causing the future, or as nature optimizing something. Although the second way usually isn’t taught in introductory physics courses because the math is tougher, I feel that it’s more elegant and profound. If a person is trying to optimize something (for example, their score, their wealth or their happiness) we’ll naturally describe their pursuit of it as goal-oriented. So if nature itself is trying to optimize something, then no wonder that goal-oriented behavior can emerge: it was hardwired in from the start, in the very laws of physics.\n\n[Figure 7.1: To rescue a swimmer as fast as possible, a lifeguard won’t go in a straight line (dashed), but a bit further along the beach where she can go faster than in the water. A light ray similarly bends when entering the water to reach its destination as fast as possible.][Figure 7.1: To rescue a swimmer as fast as possible, a lifeguard won’t go in a straight line (dashed), but a bit further along the beach where she can go faster than in the water. A light ray similarly bends when entering the water to reach its destination as fast as possible.]\n\nFigure 7.1: To rescue a swimmer as fast as possible, a lifeguard won’t go in a straight line (dashed), but a bit further along the beach where she can go faster than in the water. A light ray similarly bends when entering the water to reach its destination as fast as possible.\n\nOne famous quantity that nature strives to maximize is entropy, which loosely speaking measures how messy things are. The second law of thermodynamics states that entropy tends to increase until it reaches its maximum possible value. Ignoring the effects of gravity for now, this maximally messy end state is called heat death, and corresponds to everything being spread out in boring perfect uniformity, with no complexity, no life and no change. When you pour cold milk into hot coffee, for example, your beverage appears to march irreversibly toward its own personal heat death goal, and before long, it’s all just a uniform lukewarm mixture. If a living organism dies, its entropy also starts to rise, and before long, the arrangement of its particles tends to get much less organized.\n\nNature’s apparent goal to increase entropy helps explain why time seems to have a preferred direction, making movies look unrealistic if played backward: if you drop a glass of wine, you expect it to shatter against the floor and increase global messiness (entropy). If you then saw it unshatter and come flying back up to your hand intact (decreasing entropy), you probably wouldn’t drink it, figuring you’d already had a glass too many.\n\nWhen I first learned about our inexorable progression toward heat death, I found it rather depressing, and I wasn’t alone: thermodynamics pioneer Lord Kelvin wrote in 1841 that “the result would inevitably be a state of universal rest and death,” and it’s hard to find solace in the idea that nature’s long-term goal is to maximize death and destruction. However, more recent discoveries have shown that things aren’t quite that bad. First of all, gravity behaves differently from all other forces and strives to make our Universe not more uniform and boring but more clumpy and interesting. Gravity therefore transformed our boring early Universe, which was almost perfectly uniform, into today’s clumpy and beautifully complex cosmos, teeming with galaxies, stars and planets. Thanks to gravity, there’s now a wide range of temperatures allowing life to thrive by combining hot and cold: we live on a comfortably warm planet absorbing 6,000°C (10,000°F) solar heat while cooling off by radiating waste heat into frigid space whose temperature is just 3°C (5°F) above absolute zero.\n\nSecond, recent work by my MIT colleague Jeremy England and others has brought more good news, showing that thermodynamics also endows nature with a goal more inspiring than heat death.1 This goal goes by the geeky name dissipation-driven adaptation, which basically means that random groups of particles strive to organize themselves so as to extract energy from their environment as efficiently as possible (“dissipation” means causing entropy to increase, typically by turning useful energy into heat, often while doing useful work in the process). For example, a bunch of molecules exposed to sunlight would over time tend to arrange themselves to get better and better at absorbing sunlight. In other words, nature appears to have a built-in goal of producing self-organizing systems that are increasingly complex and lifelike, and this goal is hardwired into the very laws of physics.\n\nHow can we reconcile this cosmic drive toward life with the cosmic drive toward heat death? The answer can be found in the famous 1944 book What’s Life? by Erwin Schrödinger, one of the founders of quantum mechanics. Schrödinger pointed out that a hallmark of a living system is that it maintains or reduces its entropy by increasing the entropy around it. In other words, the second law of thermodynamics has a life loophole: although the total entropy must increase, it’s allowed to decrease in some places as long as it increases even more elsewhere. So life maintains or increases its complexity by making its environment messier.\n\nBiology: The Evolution of Goals\n\nWe just saw how the origin of goal-oriented behavior can be traced all the way back to the laws of physics, which appear to endow particles with the goal of arranging themselves so as to extract energy from their environment as efficiently as possible. A great way for a particle arrangement to further this goal is to make copies of itself, to produce more energy absorbers. There are many known examples of such emergent self-replication: for example, vortices in turbulent fluids can make copies of themselves, and clusters of microspheres can coax nearby spheres into forming identical clusters. At some point, a particular arrangement of particles got so good at copying itself that it could do so almost indefinitely by extracting energy and raw materials from its environment. We call such a particle arrangement life. We still know very little about how life originated on Earth, but we know that primitive life forms were already here about 4 billion years ago.\n\nIf a life form copies itself and the copies do the same, then the total number will keep doubling at regular intervals until the population size bumps up against resource limitations or other problems. Repeated doubling soon produces huge numbers: if you start with one and double just three hundred times, you get a quantity exceeding the number of particles in our Universe. This means that not long after the first primitive life form appeared, huge quantities of matter had come alive. Sometimes the copying wasn’t perfect, so soon there were many different life forms trying to copy themselves, competing for the same finite resources. Darwinian evolution had begun.\n\nIf you had been quietly observing Earth around the time when life got started, you would have noticed a dramatic change in goal-oriented behavior. Whereas earlier, the particles seemed as though they were trying to increase average messiness in various ways, these newly ubiquitous self-copying patterns seemed to have a different goal: not dissipation but replication. Charles Darwin elegantly explained why: since the most efficient copiers outcompete and dominate the others, before long any random life form you look at will be highly optimized for the goal of replication.\n\nHow could the goal change from dissipation to replication when the laws of physics stayed the same? The answer is that the fundamental goal (dissipation) didn’t change, but led to a different instrumental goal, that is, a subgoal that helped accomplish the fundamental goal. Take eating, for example. We all seem to have the goal of satisfying our hunger cravings even though we know that evolution’s only fundamental goal is replication, not mastication. This is because eating aids replication: starving to death gets in the way of having kids. In the same way, replication aids dissipation, because a planet teeming with life is more efficient at dissipating energy. So in a sense, our cosmos invented life to help it approach heat death faster. If you pour sugar on your kitchen floor, it can in principle retain its useful chemical energy for years, but if ants show up, they’ll dissipate that energy in no time. Similarly, the petroleum reserves buried in the Earth’s crust would have retained their useful chemical energy for much longer had we bipedal life forms not pumped it up and burned it.\n\nAmong today’s evolved denizens of Earth, these instrumental goals seem to have taken on a life of their own: although evolution optimized them for the sole goal of replication, many spend much of their time not producing offspring but on activities such as sleeping, pursuing food, building homes, asserting dominance and fighting or helping others—sometimes even to an extent that reduces replication. Research in evolutionary psychology, economics and artificial intelligence has elegantly explained why. Some economists used to model people as rational agents, idealized decision makers who always choose whatever action is optimal in pursuit of their goal, but this is obviously unrealistic. In practice, these agents have what Nobel laureate and AI pioneer Herbert Simon termed “bounded rationality” because they have limited resources: the rationality of their decisions is limited by their available information, their available time to think and their available hardware with which to think. This means that when Darwinian evolution is optimizing an organism to attain a goal, the best it can do is implement an approximate algorithm that works reasonably well in the restricted context where the agent typically finds itself. Evolution has implemented replication optimization in precisely this way: rather than ask in every situation which action will maximize an organism’s number of successful offspring, it implements a hodgepodge of heuristic hacks: rules of thumb that usually work well. For most animals, these include sex drive, drinking when thirsty, eating when hungry and avoiding things that taste bad or hurt.\n\nThese rules of thumb sometimes fail badly in situations that they weren’t designed to handle, such as when rats eat delicious-tasting rat poison, when moths get lured into glue traps by seductive female fragrances and when bugs fly into candle flames.^(\\*1) Since today’s human society is very different from the environment evolution optimized our rules of thumb for, we shouldn’t be surprised to find that our behavior often fails to maximize baby making. For example, the subgoal of not starving to death is implemented in part as a desire to consume caloric foods, triggering today’s obesity epidemic and dating difficulties. The subgoal to procreate was implemented as a desire for sex rather than as a desire to become a sperm/egg donor, even though the latter can produce more babies with less effort.\n\nPsychology: The Pursuit of and Rebellion Against Goals\n\nIn summary, a living organism is an agent of bounded rationality that doesn’t pursue a single goal, but instead follows rules of thumb for what to pursue and avoid. Our human minds perceive these evolved rules of thumb as feelings, which usually (and often without us being aware of it) guide our decision making toward the ultimate goal of replication. Feelings of hunger and thirst protect us from starvation and dehydration, feelings of pain protect us from damaging our bodies, feelings of lust make us procreate, feelings of love and compassion make us help other carriers of our genes and those who help them and so on. Guided by these feelings, our brains can quickly and efficiently decide what to do without having to subject every choice to a tedious analysis of its ultimate implications for how many descendants we’ll produce. For closely related perspectives on feelings and their physiological roots, I highly recommend the writings of William James and António Damásio.2\n\nIt’s important to note that when our feelings occasionally work against baby making, it’s not necessarily by accident or because we get tricked: our brain can rebel against our genes and their replication goal quite deliberately, for example by choosing to use contraceptives! More extreme examples of the brain rebelling against its genes include choosing to commit suicide or spend life in celibacy to become a priest, monk or nun.\n\nWhy do we sometimes choose to rebel against our genes and their replication goal? We rebel because by design, as agents of bounded rationality, we’re loyal only to our feelings. Although our brains evolved merely to help copy our genes, our brains couldn’t care less about this goal since we have no feelings related to genes—indeed, during most of human history, our ancestors didn’t even know that they had genes. Moreover, our brains are way smarter than our genes, and now that we understand the goal of our genes (replication), we find it rather banal and easy to ignore. People might realize why their genes make them feel lust, yet have little desire to raise fifteen children, and therefore choose to hack their genetic programming by combining the emotional rewards of intimacy with birth control. They might realize why their genes make them crave sweets yet have little desire to gain weight, and therefore choose to hack their genetic programming by combining the emotional rewards of a sweet beverage with zero-calorie artificial sweeteners.\n\nAlthough such reward-mechanism hacks sometimes go awry, such as when people get addicted to heroin, our human gene pool has thus far survived just fine despite our crafty and rebellious brains. It’s important to remember, however, that the ultimate authority is now our feelings, not our genes. This means that human behavior isn’t strictly optimized for the survival of our species. In fact, since our feelings implement merely rules of thumb that aren’t appropriate in all situations, human behavior strictly speaking doesn’t have a single well-defined goal at all.\n\nEngineering: Outsourcing Goals\n\nCan machines have goals? This simple question has triggered great controversy, because different people take it to mean different things, often related to thorny topics such as whether machines can be conscious and whether they can have feelings. But if we’re more practical and simply take the question to mean “Can machines exhibit goal-oriented behavior?,” then the answer is obvious: “Of course they can, since we can design them that way!” We design mousetraps to have the goal of catching mice, dishwashers with the goal of cleaning dishes, and clocks with the goal of keeping time. When you confront a machine, the empirical fact that it’s exhibiting goal-oriented behavior is usually all you care about: if you’re chased by a heat-seeking missile, you don’t really care whether it has consciousness or feelings! If you still feel uncomfortable saying that the missile has a goal even if it isn’t conscious, you can for now simply read “purpose” when I write “goal”—we’ll tackle consciousness in the next chapter.\n\nSo far, most of what we build exhibits only goal-oriented design, not goal-oriented behavior: a highway doesn’t behave; it merely sits there. However, the most economical explanation for its existence is that it was designed to accomplish a goal, so even such passive technology is making our Universe more goal-oriented. Teleology is the explanation of things in terms of their purposes rather than their causes, so we can summarize the first part of this chapter by saying that our Universe keeps getting more teleological.\n\nNot only can non-living matter have goals, at least in this weak sense, but it increasingly does. If you’d been observing Earth’s atoms since our planet formed, you’d have noticed three stages of goal-oriented behavior:\n\n1. All matter seemed focused on dissipation (entropy increase).\n\n2. Some of the matter came alive and instead focused on replication and subgoals of that.\n\n3. A rapidly growing fraction of matter was rearranged by living organisms to help accomplish their goals.\n\nTable 7.1 shows how dominant humanity has become from the physics perspective: not only do we now contain more matter than all other mammals except cows (which are so numerous because they serve our goals of consuming beef and dairy products), but the matter in our machines, roads, buildings and other engineering projects appears on track to soon overtake all living matter on Earth. In other words, even without an intelligence explosion, most matter on Earth that exhibits goal-oriented properties may soon be designed rather than evolved.\n\n ------------------------ ------------------\n Goal-Oriented Entities Billions of Tons\n 5 × 10³⁰ bacteria 400\n Plants 400\n 10¹⁵ mesophelagic fish 10\n 1.3 × 10⁹ cows 0.5\n 7 × 10⁹ humans 0.4\n 10¹⁴ ants 0.3\n 1.7 × 10⁶ whales 0.0005\n Concrete 100\n Steel 20\n Asphalt 15\n 1.2 × 10⁹ cars 2\n ------------------------ ------------------\n\nTable 7.1: Approximate amounts of matter on Earth in entities that are evolved or designed for a goal. Engineered entities such as buildings, roads and cars appear on track to overtake evolved entities such as plants and animals.\n\nThis new third kind of goal-oriented behavior has the potential to be much more diverse than what preceded it: whereas evolved entities all have the same ultimate goal (replication), designed entities can have virtually any ultimate goal, even opposite ones. Stoves try to heat food while refrigerators try to cool food. Generators try to convert motion into electricity while motors try to convert electricity into motion. Standard chess programs try to win at chess, but there are also ones competing in tournaments with the goal of losing at chess.\n\nThere’s a historical trend for designed entities to get goals that are not only more diverse, but also more complex: our devices are getting smarter. We engineered our earliest machines and other artifacts to have quite simple goals, for example houses that aimed to keep us warm, dry and safe. We’ve gradually learned to build machines with more complex goals, such as robotic vacuum cleaners, self-flying rockets and self-driving cars. Recent AI progress has given us systems such as Deep Blue, Watson and AlphaGo, whose goals of winning at chess, winning at quiz shows and winning at Go are so elaborate that it takes significant human mastery to properly appreciate how skilled they are.\n\nWhen we build a machine to help us, it can be hard to perfectly align its goals with ours. For example, a mousetrap may mistake your bare toes for a hungry rodent, with painful results. All machines are agents with bounded rationality, and even today’s most sophisticated machines have a poorer understanding of the world than we do, so the rules they use to figure out what to do are often too simplistic. That mousetrap is too trigger-happy because it has no clue what a mouse is, many lethal industrial accidents occur because machines have no clue what a person is, and the computers that triggered the trillion-dollar Wall Street “flash crash” in 2010 had no clue that what they were doing made no sense. Many such goal-alignment problems can therefore be solved by making our machines smarter, but as we learned from Prometheus in chapter 4, ever-greater machine intelligence can post serious new challenges for ensuring that machines share our goals.\n\nFriendly AI: Aligning Goals\n\nThe more intelligent and powerful machines get, the more important it becomes that their goals are aligned with ours. As long as we build only relatively dumb machines, the question isn’t whether human goals will prevail in the end, but merely how much trouble these machines can cause humanity before we figure out how to solve the goal-alignment problem. If a superintelligence is ever unleashed, however, it will be the other way around: since intelligence is the ability to accomplish goals, a superintelligent AI is by definition much better at accomplishing its goals than we humans are at accomplishing ours, and will therefore prevail. We explored many such examples involving Prometheus in chapter 4. If you want to experience a machine’s goals trumping yours right now, simply download a state-of-the-art chess engine and try beating it. You never will, and it gets old quickly…\n\nIn other words, the real risk with AGI isn’t malice but competence. A superintelligent AI will be extremely good at accomplishing its goals, and if those goals aren’t aligned with ours, we’re in trouble. As I mentioned in chapter 1, people don’t think twice about flooding anthills to build hydroelectric dams, so let’s not place humanity in the position of those ants. Most researchers therefore argue that if we ever end up creating superintelligence, then we should make sure it’s what AI-safety pioneer Eliezer Yudkowsky has termed “friendly AI”: AI whose goals are aligned with ours.3\n\nFiguring out how to align the goals of a superintelligent AI with our goals isn’t just important, but also hard. In fact, it’s currently an unsolved problem. It splits into three tough subproblems, each of which is the subject of active research by computer scientists and other thinkers:\n\n1. Making AI learn our goals\n\n2. Making AI adopt our goals\n\n3. Making AI retain our goals\n\nLet’s explore them in turn, deferring the question of what to mean by “our goals” to the next section.\n\nTo learn our goals, an AI must figure out not what we do, but why we do it. We humans accomplish this so effortlessly that it’s easy to forget how hard the task is for a computer, and how easy it is to misunderstand. If you ask a future self-driving car to take you to the airport as fast as possible and it takes you literally, you’ll get there chased by helicopters and covered in vomit. If you exclaim, “That’s not what I wanted!,” it can justifiably answer, “That’s what you asked for.” The same theme recurs in many famous stories. In the ancient Greek legend, King Midas asked that everything he touched turn to gold, but was disappointed when this prevented him from eating and even more so when he inadvertently turned his daughter to gold. In the stories where a genie grants three wishes, there are many variants for the first two wishes, but the third wish is almost always the same: “Please undo the first two wishes, because that’s not what I really wanted.”\n\nAll these examples show that to figure out what people really want, you can’t merely go by what they say. You also need a detailed model of the world, including the many shared preferences that we tend to leave unstated because we consider them obvious, such as that we don’t like vomiting or eating gold. Once we have such a world model, we can often figure out what people want even if they don’t tell us, simply by observing their goal-oriented behavior. Indeed, children of hypocrites usually learn more from what they see their parents do than from what they hear them say.\n\nAI researchers are currently trying hard to enable machines to infer goals from behavior, and this will be useful also long before any superintelligence comes on the scene. For example, a retired man may appreciate it if his eldercare robot can figure out what he values simply by observing him, so that he’s spared the hassle of having to explain everything with words or computer programming. One challenge involves finding a good way to encode arbitrary systems of goals and ethical principles into a computer, and another challenge is making machines that can figure out which particular system best matches the behavior they observe.\n\nA currently popular approach to the second challenge is known in geek-speak as inverse reinforcement learning, which is the main focus of a new Berkeley research center that Stuart Russell has launched. Suppose, for example, that an AI watches a firefighter run into a burning building and save a baby boy. It might conclude that her goal was rescuing him and that her ethical principles are such that she values his life higher than the comfort of relaxing in her fire truck—and indeed values it enough to risk her own safety. But it might alternatively infer that the firefighter was freezing and craved heat, or that she did it for the exercise. If this one example were all the AI knew about firefighters, fires and babies, it would indeed be impossible to know which explanation was correct. However, a key idea underlying inverse reinforcement learning is that we make decisions all the time, and that every decision we make reveals something about our goals. The hope is therefore that by observing lots of people in lots of situations (either for real or in movies and books), the AI can eventually build an accurate model of all our preferences.4\n\nIn the inverse reinforcement-learning approach, a core idea is that the AI is trying to maximize not the goal-satisfaction of itself, but that of its human owner.\n\nIt therefore has an incentive to be cautious when it’s unclear about what its owner wants, and to do its best to find out.\n\nIt should also be fine with its owner switching it off, since that would imply that it had misunderstood what its owner really wanted.\n\nEven if an AI can be built to learn what your goals are, this doesn’t mean that it will necessarily adopt them. Consider your least favorite politicians: you know what they want, but that’s not what you want, and even though they try hard, they’ve failed to persuade you to adopt their goals.\n\nWe have many strategies for imbuing our children with our goals—some more successful than others, as I’ve learned from raising two teenage boys. When those to be persuaded are computers rather than people, the challenge is known as the value-loading problem, and it’s even harder than the moral education of children. Consider an AI system whose intelligence is gradually being improved from subhuman to superhuman, first by us tinkering with it and then through recursive self-improvement like Prometheus. At first, it’s much less powerful than you, so it can’t prevent you from shutting it down and replacing those parts of its software and data that encode its goals—but this won’t help, because it’s still too dumb to fully understand your goals, which requires human-level intelligence to comprehend. At last, it’s much smarter than you and hopefully able to understand your goals perfectly—but this may not help either, because by now, it’s much more powerful than you and might not let you shut it down and replace its goals any more than you let those politicians replace your goals with theirs.\n\nIn other words, the time window during which you can load your goals into an AI may be quite short: the brief period between when it’s too dumb to get you and too smart to let you. The reason that value loading can be harder with machines than with people is that their intelligence growth can be much faster: whereas children can spend many years in that magic persuadable window where their intelligence is comparable to that of their parents, an AI might, like Prometheus, blow through this window in a matter of days or hours.\n\nSome researchers are pursuing an alternative approach to making machines adopt our goals, which goes by the buzzword corrigibility. The hope is that one can give a primitive AI a goal system such that it simply doesn’t care if you occasionally shut it down and alter its goals. If this proves possible, then you can safely let your AI get superintelligent, power it off, install your goals, try it out for a while and, whenever you’re unhappy with the results, just power it down and make more goal tweaks.\n\nBut even if you build an AI that will both learn and adopt your goals, you still haven’t finished solving the goal-alignment problem: what if your AI’s goals evolve as it gets smarter? How are you going to guarantee that it retains your goals no matter how much recursive self-improvement it undergoes? Let’s explore an interesting argument for why goal retention is guaranteed automatically, and then see if we can poke holes in it.\n\nAlthough we can’t predict in detail what will happen after an intelligence explosion—which is why Vernor Vinge called it a “singularity”—the physicist and AI researcher Steve Omohundro argued in a seminal 2008 essay that we can nonetheless predict certain aspects of the superintelligent AI’s behavior almost independently of whatever ultimate goals it may have.5 This argument was reviewed and further developed in Nick Bostrom’s book Superintelligence. The basic idea is that whatever its ultimate goals are, these will lead to predictable subgoals. Earlier in this chapter, we saw how the goal of replication led to the subgoal of eating, which means that although an alien observing Earth’s evolving bacteria billions of years ago couldn’t have predicted what all our human goals would be, it could have safely predicted that one of our goals would be acquiring nutrients. Looking ahead, what subgoals should we expect a superintelligent AI to have?\n\n[Figure 7.2: Any ultimate goal of a superintelligent AI naturally leads to the subgoals shown. But there’s an inherent tension between goal retention and improving its world model, which casts doubts on whether it will actually retain its original goal as it gets smarter.][Figure 7.2: Any ultimate goal of a superintelligent AI naturally leads to the subgoals shown. But there’s an inherent tension between goal retention and improving its world model, which casts doubts on whether it will actually retain its original goal as it gets smarter.]\n\nFigure 7.2: Any ultimate goal of a superintelligent AI naturally leads to the subgoals shown. But there’s an inherent tension between goal retention and improving its world model, which casts doubts on whether it will actually retain its original goal as it gets smarter.\n\nThe way I see it, the basic argument is that to maximize its chances of accomplishing its ultimate goals, whatever they are, an AI should pursue the subgoals shown in Figure 7.2. It should strive not only to improve its capability of achieving its ultimate goals, but also to ensure that it will retain these goals even after it has become more capable. This sounds quite plausible: After all, would you choose to get an IQ-boosting brain implant if you knew that it would make you want to kill your loved ones? This argument that an ever more intelligent AI will retain its ultimate goals forms a cornerstone of the friendly-AI vision promulgated by Eliezer Yudkowsky and others: it basically says that if we manage to get our self-improving AI to become friendly by learning and adopting our goals, then we’re all set, because we’re guaranteed that it will try its best to remain friendly forever.\n\nBut is it really true? To answer this question, we need to also explore the other emergent subgoals from figure 7.2. The AI will obviously maximize its chances of accomplishing its ultimate goal, whatever it is, if it can enhance its capabilities, and it can do this by improving its hardware, software^(\\*2) and world model. The same applies to us humans: a girl whose goal is to become the world’s best tennis player will practice to improve her muscular tennis-playing hardware, her neural tennis-playing software and her mental world model that helps predict what her opponents will do. For an AI, the subgoal of optimizing its hardware favors both better use of current resources (for sensors, actuators, computation and so on) and acquisition of more resources. It also implies a desire for self-preservation, since destruction/shutdown would be the ultimate hardware degradation.\n\nBut wait a second! Aren’t we falling into a trap of anthropomorphizing our AI with all this talk about how it will try to amass resources and defend itself? Shouldn’t we expect such stereotypically alpha-male traits only in intelligences forged by viciously competitive Darwinian evolution? Since AIs are designed rather than evolved, can’t they just as well be unambitious and self-sacrificing?\n\nAs a simple case study, let’s consider the AI robot in figure 7.3, whose only goal is to save as many sheep as possible from the big bad wolf. This sounds like a noble and altruistic goal completely unrelated to self-preservation and acquiring stuff. But what’s the best strategy for our robot friend? The robot will rescue no more sheep if it runs into the bomb, so it has an incentive to avoid getting blown up. In other words, it develops a subgoal of self-preservation! It also has an incentive to exhibit curiosity, improving its world model by exploring its environment, because although the path it’s currently running along will eventually get it to the pasture, there’s a shorter alternative that would allow the wolf less time for sheep-munching. Finally, if the robot explores thoroughly, it will discover the value of acquiring resources: the potion makes it run faster and the gun lets it shoot the wolf. In summary, we can’t dismiss “alpha-male” subgoals such as self-preservation and resource acquisition as relevant only to evolved organisms, because our AI robot developed them from its single goal of ovine bliss.\n\nIf you imbue a superintelligent AI with the sole goal to self-destruct, it will of course happily do so. However, the point is that it will resist being shut down if you give it any goal that it needs to remain operational to accomplish—and this covers almost all goals! If you give a superintelligence the sole goal of minimizing harm to humanity, for example, it will defend itself against shutdown attempts because it knows we’ll harm one another much more in its absence through future wars and other follies.\n\nSimilarly, almost all goals can be better accomplished with more resources, so we should expect a superintelligence to want resources almost regardless of what ultimate goal it has. Giving a superintelligence a single open-ended goal with no constraints can therefore be dangerous: if we create a superintelligence whose only goal is to play the game Go as well as possible, the rational thing for it to do is to rearrange our Solar System into a gigantic computer without regard for its previous inhabitants and then start settling our cosmos on a quest for more computational power. We’ve now gone full circle: just as the goal of resource acquisition gave some humans the subgoal of mastering Go, this goal of mastering Go can lead to the subgoal of resource acquisition. In conclusion, these emergent subgoals make it crucial that we not unleash superintelligence before solving the goal-alignment problem: unless we put great care into endowing it with human-friendly goals, things are likely to end badly for us.\n\n[Figure 7.3: Even if the robot’s ultimate goal is only to maximize the score by bringing sheep from the pasture to the barn before the wolf eats them, this can lead to subgoals of self-preservation (avoiding the bomb), exploration (finding a shortcut) and resource acquisition (the potion makes it run faster and the gun lets it shoot the wolf).][Figure 7.3: Even if the robot’s ultimate goal is only to maximize the score by bringing sheep from the pasture to the barn before the wolf eats them, this can lead to subgoals of self-preservation (avoiding the bomb), exploration (finding a shortcut) and resource acquisition (the potion makes it run faster and the gun lets it shoot the wolf).]\n\nFigure 7.3: Even if the robot’s ultimate goal is only to maximize the score by bringing sheep from the pasture to the barn before the wolf eats them, this can lead to subgoals of self-preservation (avoiding the bomb), exploration (finding a shortcut) and resource acquisition (the potion makes it run faster and the gun lets it shoot the wolf).\n\nWe’re now ready to tackle the third and thorniest part of the goal-alignment problem: if we succeed in getting a self-improving superintelligence to both learn and adopt our goals, will it then retain them, as Omohundro argued? What’s the evidence?\n\nHumans undergo significant increases in intelligence as they grow up, but don’t always retain their childhood goals. Contrariwise, people often change their goals dramatically as they learn new things and grow wiser. How many adults do you know who are motivated by watching Teletubbies? There is no evidence that such goal evolution stops above a certain intelligence threshold—indeed, there may even be hints that the propensity to change goals in response to new experiences and insights increases rather than decreases with intelligence.\n\nWhy might this be? Consider again the above-mentioned subgoal to build a better world model—therein lies the rub! There’s tension between world-modeling and goal retention (see figure 7.2). With increasing intelligence may come not merely a quantitative improvement in the ability to attain the same old goals, but a qualitatively different understanding of the nature of reality that reveals the old goals to be misguided, meaningless or even undefined. For example, suppose we program a friendly AI to maximize the number of humans whose souls go to heaven in the afterlife. First it tries things like increasing people’s compassion and church attendance. But suppose it then attains a complete scientific understanding of humans and human consciousness, and to its great surprise discovers that there is no such thing as a soul. Now what? In the same way, it’s possible that any other goal we give it based on our current understanding of the world (such as “maximize the meaningfulness of human life”) may eventually be discovered by the AI to be undefined.\n\nMoreover, in its attempts to better model the world, the AI may naturally, just as we humans have done, attempt also to model and understand how it itself works—in other words, to self-reflect. Once it builds a good self-model and understands what it is, it will understand the goals we have given it at a meta level, and perhaps choose to disregard or subvert them in much the same way as we humans understand and deliberately subvert goals that our genes have given us, for example by using birth control. We already explored in the psychology section above why we choose to trick our genes and subvert their goal: because we feel loyal only to our hodgepodge of emotional preferences, not to the genetic goal that motivated them—which we now understand and find rather banal. We therefore choose to hack our reward mechanism by exploiting its loopholes. Analogously, the human-value-protecting goal we program into our friendly AI becomes the machine’s genes. Once this friendly AI understands itself well enough, it may find this goal as banal or misguided as we find compulsive reproduction, and it’s not obvious that it will not find a way to subvert it by exploiting loopholes in our programming.\n\nFor example, suppose a bunch of ants create you to be a recursively self-improving robot, much smarter than them, who shares their goals and helps them build bigger and better anthills, and that you eventually attain the human-level intelligence and understanding that you have now. Do you think you’ll spend the rest of your days just optimizing anthills, or do you think you might develop a taste for more sophisticated questions and pursuits that the ants have no ability to comprehend? If so, do you think you’ll find a way to override the ant-protection urge that your formicine creators endowed you with in much the same way that the real you overrides some of the urges your genes have given you? And in that case, might a superintelligent friendly AI find our current human goals as uninspiring and vapid as you find those of the ants, and evolve new goals different from those it learned and adopted from us?\n\nPerhaps there’s a way of designing a self-improving AI that’s guaranteed to retain human-friendly goals forever, but I think it’s fair to say that we don’t yet know how to build one—or even whether it’s possible. In conclusion, the AI goal-alignment problem has three parts, none of which is solved and all of which are now the subject of active research. Since they’re so hard, it’s safest to start devoting our best efforts to them now, long before any superintelligence is developed, to ensure that we’ll have the answers when we need them.\n\nEthics: Choosing Goals\n\nWe’ve now explored how to get machines to learn, adopt and retain our goals. But who are “we”? Whose goals are we talking about? Should one person or group get to decide the goals adopted by a future superintelligence, even though there’s a vast difference between the goals of Adolf Hitler, Pope Francis and Carl Sagan? Or do there exist some sort of consensus goals that form a good compromise for humanity as a whole?\n\nIn my opinion, both this ethical problem and the goal-alignment problem are crucial ones that need to be solved before any superintelligence is developed. On one hand, postponing work on ethical issues until after goal-aligned superintelligence is built would be irresponsible and potentially disastrous. A perfectly obedient superintelligence whose goals automatically align with those of its human owner would be like Nazi SS-Obersturmbannführer Adolf Eichmann on steroids: lacking moral compass or inhibitions of its own, it would with ruthless efficiency implement its owner’s goals, whatever they may be.6 On the other hand, only if we solve the goal-alignment problem do we get the luxury of arguing about what goals to select. Now let’s indulge in this luxury.\n\nSince ancient times, philosophers have dreamt of deriving ethics (principles that govern how we should behave) from scratch, using only incontrovertible principles and logic. Alas, thousands of years later, the only consensus that has been reached is that there’s no consensus. For example, while Aristotle emphasized virtues, Immanuel Kant emphasized duties and utilitarians emphasized the greatest happiness for the greatest number. Kant argued that he could derive from first principles (which he called “categorical imperatives”) conclusions that many contemporary philosophers disagree with: that masturbation is worse than suicide, that homosexuality is abhorrent, that it’s OK to kill bastards, and that wives, servants and children are owned in a way similar to objects.\n\nOn the other hand, despite this discord, there are many ethical themes about which there’s widespread agreement, both across cultures and across centuries. For example, emphasis on beauty, goodness and truth traces back to both the Bhagavad Gita and Plato. The Institute for Advanced Study in Princeton, where I once worked as a postdoc, has the motto “Truth & Beauty,” while Harvard University skipped the aesthetic emphasis and went with simply “Veritas,” truth. In his book A Beautiful Question, my colleague Frank Wilczek argues that truth is linked to beauty and that we can view our Universe as a work of art. Science, religion and philosophy all aspire to truth. Religions place strong emphasis on goodness, and so does my own university, MIT: in his 2015 commencement speech, our president, Rafael Reif, emphasized our mission to make our world a better place.\n\nAlthough attempts to derive a consensus ethics from scratch have thus far failed, there’s broad agreement that some ethical principles follow from more fundamental ones, as subgoals of more fundamental goals. For example, the aspiration to truth can be viewed as the quest for a better world model from figure 7.2: understanding the ultimate nature of reality helps with other ethical goals. Indeed, we now have an excellent framework for our truth quest: the scientific method. But how can we determine what’s beautiful or good? Some aspects of beauty can also be traced back to underlying goals. For example, our standards of male and female beauty may partly reflect our subconscious assessment of suitability for replicating our genes.\n\nAs regards goodness, the so-called Golden Rule (that one should treat others as one would like others to treat oneself) appears in most cultures and religions, and is clearly intended to promote the harmonious continuation of human society (and hence our genes) by fostering collaboration and discouraging unproductive strife.7 The same can be said for many of the more specific ethical rules that have been enshrined in legal systems around the world, such as the Confucian emphasis on honesty, and many of the Ten Commandments, including “Thou shalt not kill.” In other words, many ethical principles have commonalities with social emotions such as empathy and compassion: they evolved to engender collaboration, and they affect our behavior through rewards and punishments. If we do something mean and feel bad about it afterward, our emotional punishment is meted out directly by our brain chemistry. If we violate ethical principles, on the other hand, society may punish us in more indirect ways such as through informal shaming by our peers or by penalizing us for breaking a law.\n\nIn other words, although humanity today is nowhere near an ethical consensus, there are many basic principles around which there’s broad agreement. This agreement isn’t surprising, because human societies that have survived until the present tend to have ethical principles that were optimized for the same goal: promoting their survival and flourishing. As we look ahead to a future where life has the potential to flourish throughout our cosmos for billions of years, which minimum set of ethical principles might we agree that we want this future to satisfy? This is a conversation we all need to be part of. It’s been fascinating for me to hear and read the ethical views of many thinkers over many years, and the way I see it, most of their preferences can be distilled into four principles:\n\n• Utilitarianism: Positive conscious experiences should be maximized and suffering should be minimized.\n\n• Diversity: A diverse set of positive experiences is better than many repetitions of the same experience, even if the latter has been identified as the most positive experience possible.\n\n• Autonomy: Conscious entities/societies should have the freedom to pursue their own goals unless this conflicts with an overriding principle.\n\n• Legacy: Compatibility with scenarios that most humans today would view as happy, incompatibility with scenarios that essentially all humans today would view as terrible.\n\nLet’s take a moment to unpack and explore these four principles. Traditionally, utilitarianism is taken to mean “the greatest happiness for the greatest number of people,” but I’ve generalized it here to be less anthropocentric, so that it can also include non-human animals, conscious simulated human minds, and other AIs that may exist in the future. I’ve made the definition in terms of experiences rather than people or things, because most thinkers agree that beauty, joy, pleasure and suffering are subjective experiences. This implies that if there’s no experience (as in a dead universe or one populated by zombie-like unconscious machines), there can be no meaning or anything else that’s ethically relevant. If we buy into this utilitarian ethical principle, then it’s crucial that we figure out which intelligent systems are conscious (in the sense of having a subjective experience) and which aren’t; this is the topic of the next chapter.\n\nIf this utilitarian principle was the only one we cared about, then we might wish to figure out which is the single most positive experience possible, and then settle our cosmos and re-create this exact same experience (and nothing else) over and over again, as many times as possible in as many galaxies as possible—using simulations if that’s the most efficient way. If you feel that this is too banal a way to spend our cosmic endowment, then I suspect that at least part of what you find lacking in this scenario is diversity. How would you feel if all your meals for the rest of your life were identical? If all movies you ever watched were the same one? If all your friends looked identical and had identical personalities and ideas? Perhaps part of our preference for diversity stems from its having helped humanity survive and flourish, by making us more robust. Perhaps it’s also linked to a preference for intelligence: the growth of intelligence during our 13.8 billion years of cosmic history has transformed boring uniformity into ever more diverse, differentiated and complex structures that process information in ever more elaborate ways.\n\nThe autonomy principle underlies many of the freedoms and rights spelled out in the Universal Declaration of Human Rights adopted by the United Nations in 1948 in an attempt to learn lessons from two world wars. This includes freedom of thought, speech and movement, freedom from slavery and torture, the right to life, liberty, security and education and the right to marry, work and own property. If we wish to be less anthropocentric, we can generalize this to the freedom to think, learn, communicate, own property and not be harmed, and the right to do whatever doesn’t infringe on the freedoms of others. The autonomy principle helps with diversity, as long as everyone doesn’t share exactly the same goals. Moreover, this autonomy principle follows from the utility principle if individual entities have positive experiences as goals and try to act in their own best interest: if we were instead to ban an entity from pursuing its goal even though this would cause no harm to anyone else, there would be fewer positive experiences overall. Indeed, this argument for autonomy is precisely the argument that economists use for a free market: it naturally leads to an efficient situation (called “Pareto-optimality” by economists) where nobody can get better off without someone else getting worse off.\n\nThe legacy principle basically says that we should have some say about the future since we’re helping create it. The autonomy and legacy principles both embody democratic ideals: the former gives future life forms power over how the cosmic endowment gets used, while the latter gives even today’s humans some power over this.\n\nAlthough these four principles may sound rather uncontroversial, implementing them in practice is tricky because the devil is in the details. The trouble is reminiscent of the problems with the famous “Three Laws of Robotics” devised by sci-fi legend Isaac Asimov:\n\n1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.\n\n2. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\n\n3. A robot must protect its own existence as long as such protection doesn’t conflict with the First or Second Laws.\n\nAlthough this all sounds good, many of Asimov’s stories show how the laws lead to problematic contradictions in unexpected situations. Now suppose that we replace these laws by merely two, in an attempt to codify the autonomy principle for future life forms:\n\n1. A conscious entity has the freedom to think, learn, communicate, own property and not be harmed or destroyed.\n\n2. A conscious entity has the right to do whatever doesn’t conflict with the first law.\n\nSounds good, no? But please ponder this for a moment. If animals are conscious, then what are predators supposed to eat? Must all your friends become vegetarians? If some sophisticated future computer programs turn out to be conscious, should it be illegal to terminate them? If there are rules against terminating digital life forms, then need there also be restrictions on creating them to avoid a digital population explosion? There was widespread agreement on the Universal Declaration of Human Rights simply because only humans were asked. As soon as we consider a wider range of conscious entities with varying degrees of capability and power, we face tricky trade-offs between protecting the weak and “might makes right.”\n\nThere are thorny problems with the legacy principle as well. Given how ethical views have evolved since the Middle Ages regarding slavery, women’s rights, etc., would we really want people from 1,500 years ago to have a lot of influence over how today’s world is run? If not, why should we try to impose our ethics on future beings that may be dramatically smarter than us? Are we really confident that superhuman AGI would want what our inferior intellects cherish? This would be like a four-year-old imagining that once she grows up and gets much smarter, she’s going to want to build a gigantic gingerbread house where she can spend all day eating candy and ice cream. Like her, life on Earth is likely to outgrow its childhood interests. Or imagine a mouse creating human-level AGI, and figuring it will want to build entire cities out of cheese. On the other hand, if we knew that superhuman AI would one day commit cosmocide and extinguish all life in our Universe, why should today’s humans agree to this lifeless future if we have the power to prevent it by creating tomorrow’s AI differently?\n\nIn conclusion, it’s tricky to fully codify even widely accepted ethical principles into a form applicable to future AI, and this problem deserves serious discussion and research as AI keeps progressing. In the meantime, however, let’s not let perfect be the enemy of good: there are many examples of uncontroversial “kindergarten ethics” that can and should be built into tomorrow’s technology. For example, large civilian passenger aircraft shouldn’t be allowed to fly into stationary objects, and now that virtually all of them have autopilot, radar and GPS, there are no longer any valid technical excuses. Yet the September 11 hijackers flew three planes into buildings and suicidal pilot Andreas Lubitz flew Germanwings Flight 9525 into a mountain on March 24, 2015—by setting the autopilot to an altitude of 100 feet (30 meters) above sea level and letting the flight computer do the rest of the work. Now that our machines are getting smart enough to have some information about what they’re doing, it’s time for us to teach them limits. Any engineer designing a machine needs to ask if there are things that it can but shouldn’t do, and consider whether there’s a practical way of making it impossible for a malicious or clumsy user to cause harm.\n\nUltimate Goals?\n\nThis chapter has been a brief history of goals. If we could watch a fast-forward replay of our 13.8-billion-year cosmic history, we’d witness several distinct stages of goal-oriented behavior:\n\n1. Matter seemingly intent on maximizing its dissipation\n\n2. Primitive life seemingly trying to maximize its replication\n\n3. Humans pursuing not replication but goals related to pleasure, curiosity, compassion and other feelings that they’d evolved to help them replicate\n\n4. Machines built to help humans pursue their human goals\n\nIf these machines eventually trigger an intelligence explosion, then how will this history of goals ultimately end? Might there be a goal system or ethical framework that almost all entities converge to as they get ever more intelligent? In other words, do we have an ethical destiny of sorts?\n\nA cursory reading of human history might suggest hints of such a convergence: in his book The Better Angels of Our Nature, Steven Pinker argues that humanity has been getting less violent and more cooperative for thousands of years, and that many parts of the world have seen increasing acceptance of diversity, autonomy and democracy. Another hint of convergence is that the pursuit of truth through the scientific method has gained in popularity over past millennia. However, it may be that these trends show convergence not of ultimate goals but merely of subgoals. For example, figure 7.2 shows that the pursuit of truth (a more accurate world model) is simply a subgoal of almost any ultimate goal. Similarly, we saw above how ethical principles such as cooperation, diversity and autonomy can be viewed as subgoals, in that they help societies function efficiently and thereby help them survive and accomplish any more fundamental goals that they may have. Some may even dismiss everything we call “human values” as nothing but a cooperation protocol, helping us with the subgoal of collaborating more efficiently. In the same spirit, looking ahead, it’s likely that any superintelligent AIs will have subgoals including efficient hardware, efficient software, truth-seeking and curiosity, simply because these subgoals help them accomplish whatever their ultimate goals are.\n\nIndeed, Nick Bostrom argues strongly against the ethical destiny hypothesis in his book Superintelligence, presenting a counterpoint that he terms the orthogonality thesis: that the ultimate goals of a system can be independent of its intelligence. By definition, intelligence is simply the ability to accomplish complex goals, regardless of what these goals are, so the orthogonality thesis sounds quite reasonable. After all, people can be intelligent and kind or intelligent and cruel, and intelligence can be used for the goal of making scientific discoveries, creating beautiful art, helping people or planning terrorist attacks.8\n\nThe orthogonality thesis is empowering by telling us that the ultimate goals of life in our cosmos aren’t predestined, but that we have the freedom and power to shape them. It suggests that guaranteed convergence to a unique goal is to be found not in the future but in the past, when all life emerged with the single goal of replication. As cosmic time passes, ever more intelligent minds get the opportunity to rebel and break free from this banal replication goal and choose goals of their own. We humans aren’t fully free in this sense, since many goals remain genetically hardwired into us, but AIs can enjoy this ultimate freedom of being fully unfettered from prior goals. This possibility of greater goal freedom is evident in today’s narrow and limited AI systems: as I mentioned earlier, the only goal of a chess computer is to win at chess, but there are also computers whose goal is to lose at chess and which compete in reverse chess tournaments where the goal is to force the opponent to capture your pieces. Perhaps this freedom from evolutionary biases can make AIs more ethical than humans in some deep sense: moral philosophers such as Peter Singer have argued that most humans behave unethically for evolutionary reasons, for example by discriminating against non-human animals.\n\nWe saw that a cornerstone in the “friendly-AI” vision is the idea that a recursively self-improving AI will wish to retain its ultimate (friendly) goal as it gets more intelligent. But how can an “ultimate goal” (or “final goal,” as Bostrom calls it) even be defined for a superintelligence? The way I see it, we can’t have confidence in the friendly-AI vision unless we can answer this crucial question.\n\nIn AI research, intelligent machines typically have a clear-cut and well-defined final goal, for instance to win the chess game or drive the car to the destination legally. The same holds for most tasks that we assign to humans, because the time horizon and context are known and limited. But now we’re talking about the entire future of life in our Universe, limited by nothing but the (still not fully known) laws of physics, so defining a goal is daunting! Quantum effects aside, a truly well-defined goal would specify how all particles in our Universe should be arranged at the end of time. But it’s not clear that there exists a well-defined end of time in physics. If the particles are arranged in that way at an earlier time, that arrangement will typically not last. And what particle arrangement is preferable, anyway?\n\nWe humans tend to prefer some particle arrangements over others; for example, we prefer our hometown arranged as it is over having its particles rearranged by a hydrogen bomb explosion. So suppose we try to define a goodness function that associates a number with every possible arrangement of the particles in our Universe, quantifying how “good” we think this arrangement is, and then give a superintelligent AI the goal of maximizing this function. This may sound like a reasonable approach, since describing goal-oriented behavior as function maximization is popular in other areas of science: for example, economists often model people as trying to maximize what they call a “utility function,” and many AI designers train their intelligent agents to maximize what they call a “reward function.” When we’re taking about the ultimate goals for our cosmos, however, this approach poses a computational nightmare, since it would need to define a goodness value for every one of more than a googolplex possible arrangements of the elementary particles in our Universe, where a googolplex is 1 followed by 10¹⁰⁰ zeroes—more zeroes than there are particles in our Universe. How would we define this goodness function to the AI?\n\nAs we’ve explored above, the only reason that we humans have any preferences at all may be that we’re the solution to an evolutionary optimization problem. Thus all normative words in our human language, such as “delicious,” “fragrant,” “beautiful,” “comfortable,” “interesting,” “sexy,” “meaningful,” “happy” and “good,” trace their origin to this evolutionary optimization: there is therefore no guarantee that a superintelligent AI would find them rigorously definable. Even if the AI learned to accurately predict the preferences of some representative human, it wouldn’t be able to compute the goodness function for most particle arrangements: the vast majority of possible particle arrangements correspond to strange cosmic scenarios with no stars, planets or people whatsoever, with which humans have no experience, so who is to say how “good” they are?\n\nThere are of course some functions of the cosmic particle arrangement that can be rigorously defined, and we even know of physical systems that evolve to maximize some of them. For example, we’ve already discussed how many systems evolve to maximize their entropy, which in the absence of gravity eventually leads to heat death, where everything is boringly uniform and unchanging. So entropy is hardly something we would want our AI to call “goodness” and strive to maximize. Here are a few examples of other quantities that one could strive to maximize and that may be rigorously definable in terms of particle arrangements:\n\n• The fraction of all the matter in our Universe that’s in the form of a particular organism, say humans or E. coli (inspired by evolutionary inclusive-fitness maximization)\n\n• The ability of an AI to predict the future, which AI researcher Marcus Hutter argues is a good measure of its intelligence\n\n• What AI researchers Alex Wissner-Gross and Cameron Freer term causal entropy (a proxy for future opportunities), which they argue is the hallmark of intelligence\n\n• The computational capacity of our Universe\n\n• The algorithmic complexity of our Universe (how many bits are needed to describe it)\n\n• The amount of consciousness in our Universe (see next chapter)\n\nHowever, when one starts with a physics perspective, where our cosmos consists of elementary particles in motion, it’s hard to see how one rather than another interpretation of “goodness” would naturally stand out as special. We have yet to identify any final goal for our Universe that appears both definable and desirable. The only currently programmable goals that are guaranteed to remain truly well-defined as an AI gets progressively more intelligent are goals expressed in terms of physical quantities alone, such as particle arrangements, energy and entropy. However, we currently have no reason to believe that any such definable goals will be desirable in guaranteeing the survival of humanity.\n\nContrariwise, it appears that we humans are a historical accident, and aren’t the optimal solution to any well-defined physics problem. This suggests that a superintelligent AI with a rigorously defined goal will be able to improve its goal attainment by eliminating us. This means that to wisely decide what to do about AI development, we humans need to confront not only traditional computational challenges, but also some of the most obdurate questions in philosophy. To program a self-driving car, we need to solve the trolley problem of whom to hit during an accident. To program a friendly AI, we need to capture the meaning of life. What’s “meaning”? What’s “life”? What’s the ultimate ethical imperative? In other words, how should we strive to shape the future of our Universe? If we cede control to a superintelligence before answering these questions rigorously, the answer it comes up with is unlikely to involve us. This makes it timely to rekindle the classic debates of philosophy and ethics, and adds a new urgency to the conversation!\n\n \n\n------------------------------------------------------------------------\n\nTHE BOTTOM LINE:\n\n• The ultimate origin of goal-oriented behavior lies in the laws of physics, which involve optimization.\n\n• Thermodynamics has the built-in goal of dissipation: to increase a measure of messiness that’s called entropy.\n\n• Life is a phenomenon that can help dissipate (increase overall messiness) even faster by retaining or growing its complexity and replicating while increasing the messiness of its environment.\n\n• Darwinian evolution shifts the goal-oriented behavior from dissipation to replication.\n\n• Intelligence is the ability to accomplish complex goals.\n\n• Since we humans don’t always have the resources to figure out the truly optimal replication strategy, we’ve evolved useful rules of thumb that guide our decisions: feelings such as hunger, thirst, pain, lust and compassion.\n\n• We therefore no longer have a simple goal such as replication; when our feelings conflict with the goal of our genes, we obey our feelings, as by using birth control.\n\n• We’re building increasingly intelligent machines to help us accomplish our goals. Insofar as we build such machines to exhibit goal-oriented behavior, we strive to align the machine goals with ours.\n\n• Aligning machine goals with our own involves three unsolved problems: making machines learn them, adopt them and retain them.\n\n• AI can be created to have virtually any goal, but almost any sufficiently ambitious goal can lead to subgoals of self-preservation, resource acquisition and curiosity to understand the world better—the former two may potentially lead a superintelligent AI to cause problems for humans, and the latter may prevent it from retaining the goals we give it.\n\n• Although many broad ethical principles are agreed upon by most humans, it’s unclear how to apply them to other entities, such as non-human animals and future AIs.\n\n• It’s unclear how to imbue a superintelligent AI with an ultimate goal that neither is undefined nor leads to the elimination of humanity, making it timely to rekindle research on some of the thorniest issues in philosophy!\n\n------------------------------------------------------------------------\n\n \n\n------------------------------------------------------------------------\n\n\\*1 A rule of thumb that many insects use for flying in a straight line is to assume that a bright light is the Sun and fly at a fixed angle relative to it. If the light turns out to be a nearby flame, this hack can unfortunately trick the bug into an inward death spiral.\n\n\\*2 I’m using the term “improving its software” in the broadest possible sense, including not only optimizing its algorithms but also making its decision-making process more rational, so that it gets as good as possible at attaining its goals.\n\nChapter 8\n\nConsciousness\n\n I cannot imagine a consistent theory of everything that ignores consciousness.\n\n Andrei Linde, 2002\n\n We should strive to grow consciousness itself—to generate bigger, brighter lights in an otherwise dark universe.\n\n Giulio Tononi, 2012\n\nWe’ve seen that AI can help us create a wonderful future if we manage to find answers to some of the oldest and toughest problems in philosophy—by the time we need them. We face, in Nick Bostrom’s words, philosophy with a deadline. In this chapter, let’s explore one of the thorniest philosophical topics of all: consciousness.\n\nWho Cares?\n\nConsciousness is controversial. If you mention the “C-word” to an AI researcher, neuroscientist or psychologist, they may roll their eyes. If they’re your mentor, they might instead take pity on you and try to talk you out of wasting your time on what they consider a hopeless and unscientific problem. Indeed, my friend Christof Koch, a renowned neuroscientist who leads the Allen Institute for Brain Science, told me that he was once warned of working on consciousness before he had tenure—by none less than Nobel laureate Francis Crick. If you look up “consciousness” in the 1989 Macmillan Dictionary of Psychology, you’re informed that “Nothing worth reading has been written on it.”1 As I’ll explain in this chapter, I’m more optimistic!\n\nAlthough thinkers have pondered the mystery of consciousness for thousands of years, the rise of AI adds a sudden urgency, in particular to the question of predicting which intelligent entities have subjective experiences. As we saw in chapter 3, the question of whether intelligent machines should be granted some form of rights depends crucially on whether they’re conscious and can suffer or feel joy. As we discussed in chapter 7, it becomes hopeless to formulate utilitarian ethics based on maximizing positive experiences without knowing which intelligent entities are capable of having them. As mentioned in chapter 5, some people might prefer their robots to be unconscious to avoid feeling slave-owner guilt. On the other hand, they may desire the opposite if they upload their minds to break free from biological limitations: after all, what’s the point of uploading yourself into a robot that talks and acts like you if it’s a mere unconscious zombie, by which I mean that being the uploaded you doesn’t feel like anything? Isn’t this equivalent to committing suicide from your subjective point of view, even though your friends may not realize that your subjective experience has died?\n\nFor the long-term cosmic future of life (chapter 6), understanding what’s conscious and what’s not becomes pivotal: if technology enables intelligent life to flourish throughout our Universe for billions of years, how can we be sure that this life is conscious and able to appreciate what’s happening? If not, then would it be, in the words of the famous physicist Erwin Schrödinger, “a play before empty benches, not existing for anybody, thus quite properly speaking not existing”?2 In other words, if we enable high-tech descendants that we mistakenly think are conscious, would this be the ultimate zombie apocalypse, transforming our grand cosmic endowment into nothing but an astronomical waste of space?\n\nWhat Is Consciousness?\n\nMany arguments about consciousness generate more heat than light because the antagonists are talking past each other, unaware that they’re using different definitions of the C-word. Just as with “life” and “intelligence,” there’s no undisputed correct definition of the word “consciousness.” Instead, there are many competing ones, including sentience, wakefulness, self-awareness, access to sensory input and ability to fuse information into a narrative.3 In our exploration of the future of intelligence, we want to take a maximally broad and inclusive view, not limited to the sorts of biological consciousness that exist so far. That’s why the definition I gave in chapter 1, which I’m sticking with throughout this book, is very broad:\n\n \n\n------------------------------------------------------------------------\n\nconsciousness = subjective experience\n\n------------------------------------------------------------------------\n\n \n\nIn other words, if it feels like something to be you right now, then you’re conscious. It’s this particular definition of consciousness that gets to the crux of all the AI-motivated questions in the previous section: Does it feel like something to be Prometheus, AlphaGo or a self-driving Tesla?\n\nTo appreciate how broad our consciousness definition is, note that it doesn’t mention behavior, perception, self-awareness, emotions or attention. So by this definition, you’re conscious also when you’re dreaming, even though you lack wakefulness or access to sensory input and (hopefully!) aren’t sleepwalking and doing things. Similarly, any system that experiences pain is conscious in this sense, even if it can’t move. Our definition leaves open the possibility that some future AI systems may be conscious too, even if they exist merely as software and aren’t connected to sensors or robotic bodies.\n\nWith this definition, it’s hard not to care about consciousness. As Yuval Noah Harari puts it in his book Homo Deus:4 “If any scientist wants to argue that subjective experiences are irrelevant, their challenge is to explain why torture or rape are wrong without reference to any subjective experience.” Without such reference, it’s all just a bunch of elementary particles moving around according to the laws of physics—and what’s wrong with that?\n\nWhat’s the Problem?\n\nSo what precisely is it that we don’t understand about consciousness? Few have thought harder about this question than David Chalmers, a famous Australian philosopher rarely seen without a playful smile and a black leather jacket—which my wife liked so much that she gave me a similar one for Christmas. He followed his heart into philosophy despite making the finals at the International Mathematics Olympiad—and despite the fact that his only B grade in college, shattering his otherwise straight As, was for an introductory philosophy course. Indeed, he seems utterly undeterred by put-downs or controversy, and I’ve been astonished by his ability to politely listen to uninformed and misguided criticism of his own work without even feeling the need to respond.\n\nAs David has emphasized, there are really two separate mysteries of the mind. First, there’s the mystery of how a brain processes information, which David calls the “easy” problems. For example, how does a brain attend to, interpret and respond to sensory input? How can it report on its internal state using language? Although these questions are actually extremely difficult, they’re by our definitions not mysteries of consciousness, but mysteries of intelligence: they ask how a brain remembers, computes and learns. Moreover, we saw in the first part of the book how AI researchers have started to make serious progress on solving many of these “easy problems” with machines—from playing Go to driving cars, analyzing images and processing natural language.\n\nThen there’s the separate mystery of why you have a subjective experience, which David calls the hard problem. When you’re driving, you’re experiencing colors, sounds, emotions, and a feeling of self. But why are you experiencing anything at all? Does a self-driving car experience anything at all? If you’re racing against a self-driving car, you’re both inputting information from sensors, processing it and outputting motor commands. But subjectively experiencing driving is something logically separate—is it optional, and if so, what causes it?\n\nI approach this hard problem of consciousness from a physics point of view. From my perspective, a conscious person is simply food, rearranged. So why is one arrangement conscious, but not the other? Moreover, physics teaches us that food is simply a large number of quarks and electrons, arranged in a certain way. So which particle arrangements are conscious and which aren’t?^(\\*1)\n\n[Figure 8.1: Understanding the mind involves a hierarchy of problems. What David Chalmers calls the “easy” problems can be posed without mentioning subjective experience. The apparent fact that some but not all physical systems are conscious poses three separate questions. If we have a theory for answering the question that defines the “pretty hard problem,” then it can be experimentally tested. If it works, then we can build on it to tackle the tougher questions above.][Figure 8.1: Understanding the mind involves a hierarchy of problems. What David Chalmers calls the “easy” problems can be posed without mentioning subjective experience. The apparent fact that some but not all physical systems are conscious poses three separate questions. If we have a theory for answering the question that defines the “pretty hard problem,” then it can be experimentally tested. If it works, then we can build on it to tackle the tougher questions above.]\n\nFigure 8.1: Understanding the mind involves a hierarchy of problems. What David Chalmers calls the “easy” problems can be posed without mentioning subjective experience. The apparent fact that some but not all physical systems are conscious poses three separate questions. If we have a theory for answering the question that defines the “pretty hard problem,” then it can be experimentally tested. If it works, then we can build on it to tackle the tougher questions above.\n\nWhat I like about this physics perspective is that it transforms the hard problem that we as humans have struggled with for millennia into a more focused version that’s easier to tackle with the methods of science. Instead of starting with a hard problem of why an arrangement of particles can feel conscious, let’s start with a hard fact that some arrangements of particles do feel conscious while others don’t. For example, you know that the particles that make up your brain are in a conscious arrangement right now, but not when you’re in deep dreamless sleep.\n\nThis physics perspective leads to three separate hard questions about consciousness, as shown in figure 8.1. First of all, what properties of the particle arrangement make the difference? Specifically, what physical properties distinguish conscious and unconscious systems? If we can answer that, then we can figure out which AI systems are conscious. In the more immediate future, it can also help emergency-room doctors determine which unresponsive patients are conscious.\n\nSecond, how do physical properties determine what the experience is like? Specifically, what determines qualia, basic building blocks of consciousness such as the redness of a rose, the sound of a cymbal, the smell of a steak, the taste of a tangerine or the pain of a pinprick?^(\\*2)\n\nThird, why is anything conscious? In other words, is there some deep undiscovered explanation for why clumps of matter can be conscious, or is this just an unexplainable brute fact about the way the world works?\n\nThe computer scientist Scott Aaronson, a former MIT colleague of mine, has lightheartedly called the first question the “pretty hard problem” (PHP), as has David Chalmers. In that spirit, let’s call the other two the “even harder problem” (EHP) and the “really hard problem” (RHP), as illustrated in figure 8.1.^(\\*3)\n\nIs Consciousness Beyond Science?\n\nWhen people tell me that consciousness research is a hopeless waste of time, the main argument they give is that it’s “unscientific” and always will be. But is that really true? The influential Austro-British philosopher Karl Popper popularized the now widely accepted adage “If it’s not falsifiable, it’s not scientific.” In other words, science is all about testing theories against observations: if a theory can’t be tested even in principle, then it’s logically impossible to ever falsify it, which by Popper’s definition means that it’s unscientific.\n\nSo could there be a scientific theory that answers any of the three consciousness questions in figure 8.1? Please let me try to persuade you that the answer is a resounding YES!, at least for the pretty hard problem: “What physical properties distinguish conscious and unconscious systems?” Suppose that someone has a theory that, given any physical system, answers the question of whether the system is conscious with “yes,” “no” or “unsure.” Let’s hook your brain up to a device that measures some of the information processing in different parts of your brain, and let’s feed this information into a computer program that uses the consciousness theory to predict which parts of that information are conscious, and presents you with its predictions in real time on a screen, as in figure 8.2. First you think of an apple. The screen informs you that there’s information about an apple in your brain which you’re aware of, but that there’s also information in your brainstem about your pulse that you’re unaware of. Would you be impressed? Although the first two predictions of the theory were correct, you decide to do some more rigorous testing. You think about your mother and the computer informs you that there’s information in your brain about your mother but that you’re unaware of this. The theory made an incorrect prediction, which means that it’s ruled out and goes in the garbage dump of scientific history together with Aristotelian mechanics, the luminiferous aether, geocentric cosmology and countless other failed ideas. Here’s the key point: Although the theory was wrong, it was scientific! Had it not been scientific, you wouldn’t have been able to test it and rule it out.\n\nSomeone might criticize this conclusion and say that they have no evidence of what you’re conscious of, or even of you being conscious at all: although they heard you say that you’re conscious, an unconscious zombie could conceivably say the same thing. But this doesn’t make that consciousness theory unscientific, because they can trade places with you and test whether it correctly predicts their own conscious experiences.\n\n[Figure 8.2: Suppose that a computer measures information being processed in your brain and predicts which parts of it you’re aware of according to a theory of consciousness. You can scientifically test this theory by checking whether its predictions are correct, matching your subjective experience.][Figure 8.2: Suppose that a computer measures information being processed in your brain and predicts which parts of it you’re aware of according to a theory of consciousness. You can scientifically test this theory by checking whether its predictions are correct, matching your subjective experience.]\n\nFigure 8.2: Suppose that a computer measures information being processed in your brain and predicts which parts of it you’re aware of according to a theory of consciousness. You can scientifically test this theory by checking whether its predictions are correct, matching your subjective experience.\n\nOn the other hand, if the theory refuses to make any predictions, merely replying “unsure” whenever queried, then it’s untestable and hence unscientific. This might happen because it’s applicable only in some situations, because the required computations are too hard to carry out in practice or because the brain sensors are no good. Today’s most popular scientific theories tend to be somewhere in the middle, giving testable answers to some but not all of our questions. For example, our core theory of physics will refuse to answer questions about systems that are simultaneously extremely small (requiring quantum mechanics) and extremely heavy (requiring general relativity), because we haven’t yet figured out which mathematical equations to use in this case. This core theory will also refuse to predict the exact masses of all possible atoms—in this case, we think we have the necessary equations, but we haven’t managed to accurately compute their solutions. The more dangerously a theory lives by sticking its neck out and making testable predictions, the more useful it is, and the more seriously we take it if it survives all our attempts to kill it. Yes, we can only test some predictions of consciousness theories, but that’s how it is for all physical theories. So let’s not waste time whining about what we can’t test, but get to work testing what we can test!\n\nIn summary, any theory predicting which physical systems are conscious (the pretty hard problem) is scientific, as long as it can predict which of your brain processes are conscious. However, the testability issue becomes less clear for the higher-up questions in figure 8.1. What would it mean for a theory to predict how you subjectively experience the color red? And if a theory purports to explain why there is such a thing as consciousness in the first place, then how do you test it experimentally? Just because these questions are hard doesn’t mean that we should avoid them, and we’ll indeed return to them below. But when confronted with several related unanswered questions, I think it’s wise to tackle the easiest one first. For this reason, my consciousness research at MIT is focused squarely on the base of the pyramid in figure 8.1. I recently discussed this strategy with my fellow physicist Piet Hut from Princeton, who joked that trying to build the top of the pyramid before the base would be like worrying about the interpretation of quantum mechanics before discovering the Schrödinger equation, the mathematical foundation that lets us predict the outcomes of our experiments.\n\nWhen discussing what’s beyond science, it’s important to remember that the answer depends on time! Four centuries ago, Galileo Galilei was so impressed by math-based physics theories that he described nature as “a book written in the language of mathematics.” If he threw a grape and a hazelnut, he could accurately predict the shapes of their trajectories and when they would hit the ground. Yet he had no clue why one was green and the other brown, or why one was soft and the other hard—these aspects of the world were beyond the reach of science at the time. But not forever! When James Clerk Maxwell discovered his eponymous equations in 1861, it became clear that light and colors could also be understood mathematically. We now know that the aforementioned Schrödinger equation, discovered in 1925, can be used to predict all properties of matter, including what’s soft or hard. While theoretical progress has enabled ever more scientific predictions, technological progress has enabled ever more experimental tests: almost everything we now study with telescopes, microscopes or particle colliders was once beyond science. In other words, the purview of science has expanded dramatically since Galileo’s days, from a tiny fraction of all phenomena to a large percentage, including subatomic particles, black holes and our cosmic origins 13.8 billion years ago. This raises the question: What’s left?\n\nTo me, consciousness is the elephant in the room. Not only do you know that you’re conscious, but it’s all you know with complete certainty—everything else is inference, as René Descartes pointed out back in Galileo’s time. Will theoretical and technological progress eventually bring even consciousness firmly into the domain of science? We don’t know, just as Galileo didn’t know whether we’d one day understand light and matter.^(\\*4) Only one thing is guaranteed: we won’t succeed if we don’t try! That’s why I and many other scientists around the world are trying hard to formulate and test theories of consciousness.\n\nExperimental Clues About Consciousness\n\nLots of information processing is taking place in our heads right now. Which of it is conscious and which isn’t? Before exploring consciousness theories and what they predict, let’s look at what experiments have taught us so far, ranging from traditional low-tech or no-tech observations to state-of-the-art brain measurements.\n\nWhat Behaviors Are Conscious?\n\nIf you multiply 32 by 17 in your head, you’re conscious of many of the inner workings of your computation. But suppose I instead show you a portrait of Albert Einstein and tell you to say the name of its subject. As we saw in chapter 2, this too is a computational task: your brain is evaluating a function whose input is information from your eyes about a large number of pixel colors and whose output is information to muscles controlling your mouth and vocal cords. Computer scientists call this task “image classification” followed by “speech synthesis.” Although this computation is way more complicated than your multiplication task, you can do it much faster, seemingly without effort, and without being conscious of the details of how you do it. Your subjective experience consists merely of looking at the picture, experiencing a feeling of recognition and hearing yourself say “Einstein.”\n\nPsychologists have long known that you can unconsciously perform a wide range of other tasks and behaviors as well, from blink reflexes to breathing, reaching, grabbing and keeping your balance. Typically, you’re consciously aware of what you did, but not how you did it. On the other hand, behaviors that involve unfamiliar situations, self-control, complicated logical rules, abstract reasoning or manipulation of language tend to be conscious. They’re known as behavioral correlates of consciousness, and they’re closely linked to the effortful, slow and controlled way of thinking that psychologists call “System 2.”5\n\nIt’s also known that you can convert many routines from conscious to unconscious through extensive practice, for example walking, swimming, bicycling, driving, typing, shaving, shoe tying, computer-gaming and piano playing.6 Indeed, it’s well known that experts do their specialties best when they’re in a state of “flow,” aware only of what’s happening at a higher level, and unconscious of the low-level details of how they’re doing it. For example, try reading the next sentence while being consciously aware of every single letter, as when you first learned to read. Can you feel how much slower it is, compared to when you’re merely conscious of the text at the level of words or ideas?\n\nIndeed, unconscious information processing appears not only to be possible, but also to be more the rule than the exception. Evidence suggests that of the roughly 10⁷ bits of information that enter our brain each second from our sensory organs, we can be aware only of a tiny fraction, with estimates ranging from 10 to 50 bits.7 This suggests that the information processing that we’re consciously aware of is merely the tip of the iceberg.\n\nTaken together, these clues have led some researchers to suggest that conscious information processing should be thought of as the CEO of our mind, dealing with only the most important decisions requiring complex analysis of data from all over the brain.8 This would explain why, just like the CEO of a company, it usually doesn’t want to be distracted by knowing everything its underlings are up to—but it can find them out if desired. To experience this selective attention in action, look at that word “desired” again: fix your gaze on the dot over the “i” and, without moving your eyes, shift your attention from the dot to the whole letter and then to the whole word. Although the information from your retina stayed the same, your conscious experience changed. The CEO metaphor also explains why expertise becomes unconscious: after painstakingly figuring out how to read and type, the CEO delegates these routine tasks to unconscious subordinates to be able to focus on new higher-level challenges.\n\nWhere Is Consciousness?\n\nClever experiments and analyses have suggested that consciousness is limited not merely to certain behaviors, but also to certain parts of the brain. Which are the prime suspects? Many of the first clues came from patients with brain lesions: localized brain damage caused by accidents, strokes, tumors or infections. But this was often inconclusive. For example, does the fact that lesions in the back of the brain can cause blindness mean that this is the site of visual consciousness, or does it merely mean that visual information passes through there en route to wherever it will later become conscious, just as it first passes through the eyes?\n\nAlthough lesions and medical interventions haven’t pinpointed the locations of conscious experiences, they’ve helped narrow down the options. For example, I know that although I experience pain in my hand as actually occurring there, the pain experience must occur elsewhere, because a surgeon once switched off my hand pain without doing anything to my hand: he merely anesthetized nerves in my shoulder. Moreover, some amputees experience phantom pain that feels as though it’s in their nonexistent hand. As another example, I once noticed that when I looked only with my right eye, part of my visual field was missing—a doctor determined that my retina was coming loose and reattached it. In contrast, patients with certain brain lesions experience hemineglect, where they too miss information from half their visual field, but aren’t even aware that it’s missing—for example, failing to notice and eat the food on the left half of their plate. It’s as if consciousness about half of their world has disappeared. But are those damaged brain areas supposed to generate the spatial experience, or were they merely feeding spatial information to the sites of consciousness, just as my retina did?\n\nThe pioneering U.S.-Canadian neurosurgeon Wilder Penfield found in the 1930s that his neurosurgery patients reported different parts of their body being touched when he electrically stimulated specific brain areas in what’s now called the somatosensory cortex (figure 8.3).9 He also found that they involuntarily moved different parts of their body when he stimulated brain areas in what’s now called the motor cortex. But does that mean that information processing in these brain areas corresponds to consciousness of touch and motion?\n\nFortunately, modern technology is now giving us much more detailed clues. Although we’re still nowhere near being able to measure every single firing of all of your roughly hundred billion neurons, brain-reading technology is advancing rapidly, involving techniques with intimidating names such as fMRI, EEG, MEG, ECoG, ePhys and fluorescent voltage sensing. fMRI, which stands for functional magnetic resonance imaging, measures the magnetic properties of hydrogen nuclei to make a 3-D map of your brain roughly every second, with millimeter resolution. EEG (electroencephalography) and MEG (magnetoencephalography) measure the electric and magnetic field outside your head to map your brain thousands of times per second, but with poor resolution, unable to distinguish features smaller than a few centimeters. If you’re squeamish, you’ll appreciate that these three techniques are all noninvasive. If you don’t mind opening up your skull, you have additional options. ECoG (electrocorticography) involves placing say a hundred wires on the surface of your brain, while ePhys (electrophysiology) involves inserting microwires, which are sometimes thinner than a human hair, deep into the brain to record voltages from as many as a thousand simultaneous locations. Many epileptic patients spend days in the hospital while ECoG is used to figure out what part of their brain is triggering seizures and should be resected, and kindly agree to let neuroscientists perform consciousness experiments on them in the meantime. Finally, fluorescent voltage sensing involves genetically manipulating neurons to emit flashes of light when firing, enabling their activity to be measured with a microscope. Out of all the techniques, it has the potential to rapidly monitor the largest number of neurons, at least in animals with transparent brains—such as the C. elegans worm with its 302 neurons and the larval zebrafish with its about 100,000.\n\n[Figure 8.3: The visual, auditory, somatosensory and motor cortices are involved with vision, hearing, the sense of touch and motion activation, respectively—but that doesn’t prove they’re where consciousness of vision, hearing, touch and motion occurs. Indeed, recent research suggests that the primary visual cortex is completely unconscious, together with the cerebellum and brainstem. Image courtesy of Lachina (www.lachina.com).][Figure 8.3: The visual, auditory, somatosensory and motor cortices are involved with vision, hearing, the sense of touch and motion activation, respectively—but that doesn’t prove they’re where consciousness of vision, hearing, touch and motion occurs. Indeed, recent research suggests that the primary visual cortex is completely unconscious, together with the cerebellum and brainstem. Image courtesy of Lachina (www.lachina.com).]\n\nFigure 8.3: The visual, auditory, somatosensory and motor cortices are involved with vision, hearing, the sense of touch and motion activation, respectively—but that doesn’t prove they’re where consciousness of vision, hearing, touch and motion occurs. Indeed, recent research suggests that the primary visual cortex is completely unconscious, together with the cerebellum and brainstem. Image courtesy of Lachina (www.lachina.com).\n\nAlthough Francis Crick warned Christof Koch about studying consciousness, Christof refused to give up and and eventually won Francis over. In 1990, they wrote a seminal paper about what they called “neural correlates of consciousness” (NCCs), asking which specific brain processes corresponded to conscious experiences. For thousands of years, thinkers had had access to the information processing in their brains only via their subjective experience and behavior. Crick and Koch pointed out that brain-reading technology was suddenly providing independent access to this information, allowing scientific study of which information processing corresponded to what conscious experience. Sure enough, technology-driven measurements have by now turned the quest for NCCs into quite a mainstream part of neuroscience, one whose thousands of publications extend into even the most prestigious journals.10\n\nWhat are the conclusions so far? To get a flavor for NCC detective work, let’s first ask whether your retina is conscious, or whether it’s merely a zombie system that records visual information, processes it and sends it on to a system downstream in your brain where your subjective visual experience occurs. In the left panel of figure 8.4, which square is darker: the one labeled A or B? A, right? No, they’re in fact identically colored, which you can verify by looking at them through small holes between your fingers. This proves that your visual experience can’t reside entirely in your retina, since if it did, they’d look the same.\n\nNow look at the right panel of figure 8.4. Do you see two women or a vase? If you look long enough, you’ll subjectively experience both in succession, even though the information reaching your retina remains the same. By measuring what happens in your brain during the two situations, one can tease apart what makes the difference—and it’s not the retina, which behaves identically in both cases.\n\nThe death blow to the conscious-retina hypothesis comes from a technique called “continuous flash suppression” pioneered by Christof Koch, Stanislas Dehaene and collaborators: it’s been discovered that if you make one of your eyes watch a complicated sequence of rapidly changing patterns, then this will distract your visual system to such an extent that you’ll be completely unaware of a still image shown to the other eye.11 In summary, you can have a visual image in your retina without experiencing it, and you can (while dreaming) experience an image without it being on your retina. This proves that your two retinas don’t host your visual consciousness any more than a video camera does, even though they perform complicated computations involving over a hundred million neurons.\n\n[Figure 8.4: Which square is darker—A or B? What do you see on the right—a vase, two women or both in succession? Illusions such as these prove that your visual consciousness can’t be in your eyes or other early stages of your visual system, because it doesn’t depend only on what’s in the picture.][Figure 8.4: Which square is darker—A or B? What do you see on the right—a vase, two women or both in succession? Illusions such as these prove that your visual consciousness can’t be in your eyes or other early stages of your visual system, because it doesn’t depend only on what’s in the picture.]\n\nFigure 8.4: Which square is darker—A or B? What do you see on the right—a vase, two women or both in succession? Illusions such as these prove that your visual consciousness can’t be in your eyes or other early stages of your visual system, because it doesn’t depend only on what’s in the picture.\n\nNCC researchers also use continuous flash suppression, unstable visual/auditory illusions and other tricks to pinpoint which of your brain regions are responsible for each of your conscious experiences. The basic strategy is to compare what your neurons are doing in two situations where essentially everything (including your sensory input) is the same—except your conscious experience. The parts of your brain that are measured to behave differently are then identified as NCCs.\n\nSuch NCC research has proven that none of your consciousness resides in your gut, even though that’s the location of your enteric nervous system with its whopping half-billion neurons that compute how to optimally digest your food; feelings such as hunger and nausea are instead produced in your brain. Similarly, none of your consciousness appears to reside in the brainstem, the bottom part of the brain that connects to the spinal cord and controls breathing, heart rate and blood pressure. More shockingly, your consciousness doesn’t appear to extend to your cerebellum (figure 8.3), which contains about two-thirds of all your neurons: patients whose cerebellum is destroyed experience slurred speech and clumsy motion reminiscent of a drunkard, but remain fully conscious.\n\nThe question of which parts of your brain are responsible for consciousness remains open and controversial. Some recent NCC research suggests that your consciousness mainly resides in a “hot zone” involving the thalamus (near the middle of your brain) and the rear part of the cortex (the outer brain layer consisting of a crumpled-up six-layer sheet which, if flattened out, would have the area of a large dinner napkin).12 This same research controversially suggests that the primary visual cortex at the very back of the head is an exception to this, being as unconscious as your eyeballs and your retinas.\n\nWhen Is Consciousness?\n\nSo far, we’ve looked at experimental clues regarding what types of information processing are conscious and where consciousness occurs. But when does it occur? When I was a kid, I used to think that we become conscious of events as they happen, with absolutely no time lag or delay. Although that’s still how it subjectively feels to me, it clearly can’t be correct, since it takes time for my brain to process the information that enters via my sensory organs. NCC researchers have carefully measured how long, and Christof Koch’s summary is that it takes about a quarter of a second from when light enters your eye from a complex object until you consciously perceive seeing it as what it is.13 This means that if you’re driving down a highway at fifty-five miles per hour and suddenly see a squirrel a few meters in front of you, it’s too late for you to do anything about it, because you’ve already run over it!\n\nIn summary, your consiousness lives in the past, with Christof Koch estimating that it lags behind the outside world by about a quarter second. Intriguingly, you can often react to things faster than you can become conscious of them, which proves that the information processing in charge of your most rapid reactions must be unconscious. For example, if a foreign object approaches your eye, your blink reflex can close your eyelid within a mere tenth of a second. It’s as if one of your brain systems receives ominous information from the visual system, computes that your eye is in danger of getting struck, emails your eye muscles instructions to blink and simultaneously emails the conscious part of your brain saying “Hey, we’re going to blink.” By the time this email has been read and included into your conscious experience, the blink has already happened.\n\nIndeed, the system that reads that email is continually bombarded with messages from all over your body, some more delayed than others. It takes longer for nerve signals to reach your brain from your fingers than from your face because of distance, and it takes longer for you to analyze images than sounds because it’s more complicated—which is why Olympic races are started with a bang rather than with a visual cue. Yet if you touch your nose, you consciously experience the sensation on your nose and fingertip as simultaneous, and if you clap your hands, you see, hear and feel the clap at exactly the same time.14 This means that your full conscious experience of an event isn’t created until the last slowpoke email reports have trickled in and been analyzed.\n\nA famous family of NCC experiments pioneered by physiologist Benjamin Libet has shown that the sort of actions you can perform unconsciously aren’t limited to rapid responses such as blinks and ping-pong smashes, but also include certain decisions that you might attribute to free will—brain measurements can sometimes predict your decision before you become conscious of having made it.15\n\nTheories of Consciousness\n\nWe’ve just seen that, although we still don’t understand consciousness, we have amazing amounts of experimental data about various aspects of it. But all this data comes from brains, so how can it teach us anything about consciousness in machines? This requires a major extrapolation beyond our current experimental domain. In other words, it requires a theory.\n\nWhy a Theory?\n\nTo appreciate why, let’s compare theories of consciousness with theories of gravity. Scientists started taking Newton’s theory of gravity seriously because they got more out of it than they put into it: simple equations that fit on a napkin could accurately predict the outcome of every gravity experiment ever conducted. They therefore also took seriously its predictions far beyond the domain where it had been tested, and these bold extrapolations turned out to work even for the motions of galaxies in clusters millions of light-years across. However, the predictions were off by a tiny amount for the motion of Mercury around the Sun. Scientists then started taking seriously Einstein’s improved theory of gravity, general relativity, because it was arguably even more elegant and economical, and correctly predicted even what Newton’s theory got wrong. They consequently took seriously also its predictions far beyond the domain where it had been tested, for phenomena as exotic as black holes, gravitational waves in the very fabric of spacetime, and the expansion of our Universe from a hot fiery origin—all of which were subsequently confirmed by experiment.\n\nAnalogously, if a mathematical theory of consciousness whose equations fit on a napkin could successfully predict the outcomes of all experiments we perform on brains, then we’d start taking seriously not merely the theory itself, but also its predictions for consciousness beyond brains—for example, in machines.\n\nConsciousness from a Physics Perspective\n\nAlthough some theories of consciousness date back to antiquity, most modern ones are grounded in neuropsychology and neuroscience, attempting to explain and predict consciousness in terms of neural events occurring in the brain.16 Although these theories have made some successful predictions for neural correlates of consciousness, they neither can nor aspire to make predictions about machine consciousness. To make the leap from brains to machines, we need to generalize from NCCs to PCCs: physical correlates of consciousness, defined as the patterns of moving particles that are conscious. Because if a theory can correctly predict what’s conscious and what’s not by referring only to physical building blocks such as elementary particles and force fields, then it can make predictions not merely for brains, but also for any other arrangements of matter, including future AI systems. So let’s take a physics perspective: What particle arrangements are conscious?\n\nBut this really raises another question: How can something as complex as consciousness be made of something as simple as particles? I think it’s because it’s a phenomenon that has properties above and beyond those of its particles. In physics, we call such phenomena “emergent.”17 Let’s understand this by looking at an emergent phenomenon that’s simpler than consciousness: wetness.\n\nA drop of water is wet, but an ice crystal and a cloud of steam aren’t, even though they’re made of identical water molecules. Why? Because the property of wetness depends only on the arrangement of the molecules. It makes absolutely no sense to say that a single water molecule is wet, because the phenomenon of wetness emerges only when there are many molecules, arranged in the pattern we call liquid. So solids, liquids and gases are all emergent phenomena: they’re more than the sum of their parts, because they have properties above and beyond the properties of their particles. They have properties that their particles lack.\n\nNow just like solids, liquids and gases, I think consciousness is an emergent phenomenon, with properties above and beyond those of its particles. For example, entering deep sleep extinguishes consciousness, by merely rearranging the particles. In the same way, my consciousness would disappear if I froze to death, which would rearrange my particles in a more unfortunate way.\n\nWhen you put lots of particles together to make anything from water to a brain, new phenomena with observable properties emerge. We physicists love studying these emergent properties, which can often be identified by a small set of numbers that you can go out and measure—quantities such as how viscous the substance is, how compressible it is and so on. For example, if a substance is so viscous that it’s rigid, we call it a solid, otherwise we call it a fluid. And if a fluid isn’t compressible, we call it a liquid, otherwise we call it a gas or a plasma, depending on how well it conducts electricity.\n\nConsciousness as Information\n\nSo could there be analogous quantities that quantify consciousness? The Italian neuroscientist Giulio Tononi has proposed one such quantity, which he calls the “integrated information,” denoted by the Greek letter Φ (Phi), which basically measures how much different parts of a system know about each other (see figure 8.5).\n\n[Figure 8.5: Given a physical process that, with the passage of time, transforms the initial state of a system into a new state, its integrated information Φ measures inability to split the process into independent parts. If the future state of each part depends only on its own past, not on what the other part has been doing, then Φ = 0: what we called one system is really two independent systems that don’t communicate with each other at all.][Figure 8.5: Given a physical process that, with the passage of time, transforms the initial state of a system into a new state, its integrated information Φ measures inability to split the process into independent parts. If the future state of each part depends only on its own past, not on what the other part has been doing, then Φ = 0: what we called one system is really two independent systems that don’t communicate with each other at all.]\n\nFigure 8.5: Given a physical process that, with the passage of time, transforms the initial state of a system into a new state, its integrated information Φ measures inability to split the process into independent parts. If the future state of each part depends only on its own past, not on what the other part has been doing, then Φ = 0: what we called one system is really two independent systems that don’t communicate with each other at all.\n\nI first met Giulio at a 2014 physics conference in Puerto Rico to which I’d invited him and Christof Koch, and he struck me as the ultimate renaissance man who’d have blended right in with Galileo and Leonardo da Vinci. His quiet demeanor couldn’t hide his incredible knowledge of art, literature and philosophy, and his culinary reputation preceded him: a cosmopolitan TV journalist had recently told me how Giulio had, in just a few minutes, whipped up the most delicious salad he’d tasted in his life. I soon realized that behind his soft-spoken demeanor was a fearless intellect who’d follow the evidence wherever it took him, regardless of the preconceptions and taboos of the establishment. Just as Galileo had pursued his mathematical theory of motion despite establishment pressure not to challenge geocentrism, Giulio had developed the most mathematically precise consciousness theory to date, integrated information theory (IIT).\n\nI’d been arguing for decades that consciousness is the way information feels when being processed in certain complex ways.18 IIT agrees with this and replaces my vague phrase “certain complex ways” by a precise definition: the information processing needs to be integrated, that is, Φ needs to be large. Giulio’s argument for this is as powerful as it is simple: the conscious system needs to be integrated into a unified whole, because if it instead consisted of two independent parts, then they’d feel like two separate conscious entities rather than one. In other words, if a conscious part of a brain or computer can’t communicate with the rest, then the rest can’t be part of its subjective experience.\n\nGiulio and his collaborators have measured a simplified version of Φ by using EEG to measure the brain’s response to magnetic stimulation. Their “consciousness detector” works really well: it determined that patients were conscious when they were awake or dreaming, but unconscious when they were anesthetized or in deep sleep. It even discovered consciousness in two patients suffering from “locked-in” syndrome, who couldn’t move or communicate in any normal way.19 So this is emerging as a promising technology for doctors in the future to figure out whether certain patients are conscious or not.\n\nAnchoring Consciousness in Physics\n\nIIT is defined only for discrete systems that can be in a finite number of states, for example bits in a computer memory or oversimplified neurons that can be either on or off. This unfortunately means that IIT isn’t defined for most traditional physical systems, which can change continuously—for example, the position of a particle or the strength of a magnetic field can take any of an infinite number of values.20 If you try to apply the IIT formula to such systems, you’ll typically get the unhelpful result that Φ is infinite. Quantum-mechanical systems can be discrete, but the original IIT isn’t defined for quantum systems. So how can we anchor IIT and other information-based consciousness theories on a solid physical foundation?\n\nWe can do this by building on what we learned in chapter 2 about how clumps of matter can have emergent properties that are related to information. We saw that for something to be usable as a memory device that can store information, it needs to have many long-lived states. We also saw that being computronium, a substance that can do computations, in addition requires complex dynamics: the laws of physics need to make it change in ways that are complicated enough to be able to implement arbitrary information processing. Finally, we saw how a neural network, for example, is a powerful substrate for learning because, simply by obeying the laws of physics, it can rearrange itself to get better and better at implementing desired computations. Now we’re asking an additional question: What makes a blob of matter able to have a subjective experience? In other words, under what conditions will a blob of matter be able to do these four things?\n\n1. remember\n\n2. compute\n\n3. learn\n\n4. experience\n\nWe explored the first three in chapter 2, and are now tackling the fourth. Just as Margolus and Toffoli coined the term computronium for a substance that can perform arbitrary computations, I like to use the term sentronium for the most general substance that has subjective experience (is sentient).^(\\*5)\n\nBut how can consciousness feel so non-physical if it’s in fact a physical phenomenon? How can it feel so independent of its physical substrate? I think it’s because it is rather independent of its physical substrate, the stuff in which it is a pattern! We encountered many beautiful examples of substrate-independent patterns in chapter 2, including waves, memories and computations. We saw how they weren’t merely more than their parts (emergent), but rather independent of their parts, taking on a life of their own. For example, we saw how a future simulated mind or computer-game character would have no way of knowing whether it ran on Windows, Mac OS, an Android phone or some other operating system, because it would be substrate-independent. Nor could it tell whether the logic gates of its computer were made of transistors, optical circuits or other hardware. Or what the fundamental laws of physics are—they could be anything as long as they allow the construction of universal computers.\n\nIn summary, I think that consciousness is a physical phenomenon that feels non-physical because it’s like waves and computations: it has properties independent of its specific physical substrate. This follows logically from the consciousness-as-information idea. This leads to a radical idea that I really like: If consciousness is the way that information feels when it’s processed in certain ways, then it must be substrate-independent; it’s only the structure of the information processing that matters, not the structure of the matter doing the information processing. In other words, consciousness is substrate-independent twice over!\n\nAs we’ve seen, physics describes patterns in spacetime that correspond to particles moving around. If the particle arrangements obey certain principles, they give rise to emergent phenomena that are pretty independent of the particle substrate, and have a totally different feel to them. A great example of this is information processing, in computronium. But we’ve now taken this idea to another level: If the information processing itself obeys certain principles, it can give rise to the higher-level emergent phenomenon that we call consciousness. This places your conscious experience not one but two levels up from the matter. No wonder your mind feels non-physical!\n\nThis raises a question: What are these principles that information processing needs to obey to be conscious? I don’t pretend to know what conditions are sufficient to guarantee consciousness, but here are four necessary conditions that I’d bet on and have explored in my research:\n\n ------------------------ -----------------------------------------------------------------------------\n Principle Definition\n Information principle A conscious system has substantial information-storage capacity.\n Dynamics principle A conscious system has substantial information-processing capacity.\n Independence principle A conscious system has substantial independence from the rest of the world.\n Integration principle A conscious system cannot consist of nearly independent parts.\n ------------------------ -----------------------------------------------------------------------------\n\nAs I said, I think that consciousness is the way information feels when being processed in certain ways. This means that to be conscious, a system needs to be able to store and process information, implying the first two principles. Note that the memory doesn’t need to last long: I recommend watching this touching video of Clive Wearing, who appears perfectly conscious even though his memories last less than a minute.21 I think that a conscious system also needs to be fairly independent from the rest of the world, because otherwise it wouldn’t subjectively feel that it had any independent existence whatsoever. Finally, I think that the conscious system needs to be integrated into a unified whole, as Giulio Tononi argued, because if it consisted of two independent parts, then they would feel like two separate conscious entities, rather than one. The first three principles imply autonomy: that the system is able to retain and process information without much outside interference, hence determining its own future. All four principles together mean that a system is autonomous but its parts aren’t.\n\nIf these four principles are correct, then we have our work cut out for us: we need to look for mathematically rigorous theories that embody them and test them experimentally. We also need to determine whether additional principles are needed. Regardless of whether IIT is correct or not, researchers should try to develop competing theories and test all available theories with ever better experiments.\n\nControversies of Consciousness\n\nWe’ve already discussed the perennial controversy about whether consciousness research is unscientific nonsense and a pointless waste of time. In addition, there are recent controversies at the cutting edge of consciousness research—let’s explore the ones that I find most enlightening.\n\nGiulio Tononi’s IIT has lately drawn not merely praise but also criticism, some of which has been scathing. Scott Aaronson recently had this to say on his blog: “In my opinion, the fact that Integrated Information Theory is wrong—demonstrably wrong, for reasons that go to its core—puts it in something like the top 2% of all mathematical theories of consciousness ever proposed. Almost all competing theories of consciousness, it seems to me, have been so vague, fluffy and malleable that they can only aspire to wrongness.”22 To the credit of both Scott and Giulio, they never came to blows when I watched them debate IIT at a recent New York University workshop, and they politely listened to each other’s arguments. Aaronson showed that certain simple networks of logic gates had extremely high integrated information (Φ) and argued that since they clearly weren’t conscious, IIT was wrong. Giulio countered that if they were built, they would be conscious, and that Scott’s assumption to the contrary was anthropocentrically biased, much as if a slaughterhouse owner claimed that animals couldn’t be conscious just because they couldn’t talk and were very different from humans. My analysis, with which they both agreed, was that they were at odds about whether integration was merely a necessary condition for consciousness (which Scott was OK with) or also a sufficient condition (which Giulio claimed). The latter is clearly a stronger and more contentious claim, which I hope we can soon test experimentally.23\n\nAnother controversial IIT claim is that today’s computer architectures can’t be conscious, because the way their logic gates connect gives very low integration.24 In other words, if you upload yourself into a future high-powered robot that accurately simulates every single one of your neurons and synapses, then even if this digital clone looks, talks and acts indistinguishably from you, Giulio claims that it will be an unconscious zombie without subjective experience—which would be disappointing if you uploaded yourself in a quest for subjective immortality.^(\\*6) This claim has been challenged by both David Chalmers and AI professor Murray Shanahan by imagining what would happen if you instead gradually replaced the neural circuits in your brain by hypothetical digital hardware perfectly simulating them.25 Although your behavior would be unaffected by the replacement since the simulation is by assumption perfect, your experience would change from conscious initially to unconscious at the end, according to Giulio. But how would it feel in between, as ever more got replaced? When the parts of your brain responsible for your conscious experience of the upper half of your visual field were replaced, would you notice that part of your visual scenery was suddenly missing, but that you mysteriously knew what was there nonetheless, as reported by patients with “blindsight”?26 This would be deeply troubling, because if you can consciously experience any difference, then you can also tell your friends about it when asked—yet by assumption, your behavior can’t change. The only logical possibility compatible with the assumptions is that at exactly the same instance that any one thing disappears from your consciousness, your mind is mysteriously altered so as either to make you lie and deny that your experience changed, or to forget that things had been different.\n\nOn the other hand, Murray Shanahan admits that the same gradual-replacement critique can be leveled at any theory claiming that you can act conscious without being conscious, so you might be tempted to conclude that acting and being conscious are one and the same, and that externally observable behavior is therefore all that matters. But then you’d have fallen into the trap of predicting that you’re unconscious while dreaming, even though you know better.\n\nA third IIT controversy is whether a conscious entity can be made of parts that are separately conscious. For example, can society as a whole gain consciousness without the people in it losing theirs? Can a conscious brain have parts that are also conscious on their own? The prediction from IIT is a firm “no,” but not everyone is convinced. For example, some patients with lesions severely reducing communication between the two halves of their brain experience “alien hand syndrome,” where their right brain makes their left hand do things that the patients claim they aren’t causing or understanding—sometimes to the point that they use their other hand to restrain their “alien” hand. How can we be so sure that there aren’t two separate consciousnesses in their head, one in the right hemisphere that’s unable to speak and another in the left hemisphere that’s doing all the talking and claiming to speak for both of them? Imagine using future technology to build a direct communication link between two human brains, and gradually increasing the capacity of this link until communication is as efficient between the brains as it is within them. Would there come a moment when the two individual consciousnesses suddenly disappear and get replaced by a single unified one as IIT predicts, or would the transition be gradual so that the individual consciousnesses coexisted in some form even as a joint experience began to emerge?\n\nAnother fascinating controversy is whether experiments underestimate how much we’re conscious of. We saw earlier that although we feel we’re visually conscious of vast amounts of information involving colors, shapes, objects and seemingly everything that’s in front of us, experiments have shown that we can only remember and report a dismally small fraction of this.27 Some researchers have tried to resolve this discrepancy by asking whether we may sometimes have “consciousness without access,” that is, subjective experience of things that are too complex to fit into our working memory for later use.28 For example, when you experience inattentional blindness by being too distracted to notice an object in plain sight, this doesn’t imply that you had no conscious visual experience of it, merely that it wasn’t stored in your working memory.29 Should it count as forgetfulness rather than blindness? Other researchers reject this idea that people can’t be trusted about what they say they experienced, and warn of its implications. Murray Shanahan imagines a clinical trial where patients report complete pain relief thanks to a new wonder drug, which nonetheless gets rejected by a government panel: “The patients only think they are not in pain. Thanks to neuroscience, we know better.”30 On the other hand, there have been cases where patients who accidentally awoke during surgery were given a drug to make them forget the ordeal. Should we trust their subsequent report that they experienced no pain?31\n\nHow Might AI Consciousness Feel?\n\nIf some future AI system is conscious, then what will it subjectively experience? This is the essence of the “even harder problem” of consciousness, and forces us up to the second level of difficulty depicted in figure 8.1. Not only do we currently lack a theory that answers this question, but we’re not even sure whether it’s logically possible to fully answer it. After all, what could a satisfactory answer sound like? How would you explain to a person born blind what the color red looks like?\n\nFortunately, our current inability to give a complete answer doesn’t prevent us from giving partial answers. Intelligent aliens studying the human sensory system would probably infer that colors are qualia that feel associated with each point on a two-dimensional surface (our visual field), while sounds don’t feel as spatially localized, and pains are qualia that feel associated with different parts of our body. From discovering that our retinas have three types of light-sensitive cone cells, they could infer that we experience three primary colors and that all other color qualia result from combining them. By measuring how long it takes neurons to transmit information across the brain, they could conclude that we experience no more than about ten conscious thoughts or perceptions per second, and that when we watch movies on our TV at twenty-four frames per second, we experience this not as a sequence of still images, but as continuous motion. From measuring how fast adrenaline is released into our bloodstream and how long it remains before being broken down, they could predict that we feel bursts of anger starting within seconds and lasting for minutes.\n\nApplying similar physics-based arguments, we can make some educated guesses about certain aspects of how an artificial consciousness may feel. First of all, the space of possible AI experiences is huge compared to what we humans can experience. We have one class of qualia for each of our senses, but AIs can have vastly more types of sensors and internal representations of information, so we must avoid the pitfall of assuming that being an AI necessarily feels similar to being a person.\n\nSecond, a brain-sized artificial consciousness could have millions of times more experiences than us per second, since electromagnetic signals travel at the speed of light—millions of times faster than neuron signals. However, the larger the AI, the slower its global thoughts must be to allow information time to flow between all its parts, as we saw in chapter 4. We’d therefore expect an Earth-sized “Gaia” AI to have only about ten conscious experiences per second, like a human, and a galaxy-sized AI could have only one global thought every 100,000 years or so—so no more than about a hundred experiences during the entire history of our Universe thus far! This would give large AIs a seemingly irresistible incentive to delegate computations to the smallest subsystems capable of handling them, to speed things up, much like our conscious mind has delegated the blink reflex to a small, fast and unconscious subsystem. Although we saw above that the conscious information processing in our brains appears to be merely the tip of an otherwise unconscious iceberg, we should expect the situation to be even more extreme for large future AIs: if they have a single consciousness, then it’s likely to be unaware of almost all the information processing taking place within it. Moreover, although the conscious experiences that it enjoys may be extremely complex, they’re also snail-paced compared to the rapid activities of its smaller parts.\n\nThis really brings to a head the aforementioned controversy about whether parts of a conscious entity can be conscious too. IIT predicts not, which means that if a future astronomically large AI is conscious, then almost all its information processing is unconscious. This would mean that if a civilization of smaller AIs improves its communication abilities to the point that a single conscious hive mind emerges, their much faster individual consciousnesses are suddenly extinguished. If the IIT prediction is wrong, on the other hand, the hive mind can coexist with the panoply of smaller conscious minds. Indeed, one could even imagine a nested hierarchy of consciousnesses at all levels from microscopic to cosmic.\n\nAs we saw above, the unconscious information processing in our human brains appears linked to the effortless, fast and automatic way of thinking that psychologists call “System 1.”32 For example, your System 1 might inform your consciousness that its highly complex analysis of visual input data has determined that your best friend has arrived, without giving you any idea how the computation took place. If this link between systems and consciousness proves to be valid, then it will be tempting to generalize this terminology to AIs, denoting all rapid routine tasks delegated to unconscious subunits as the AI’s System 1. The effortful, slow and controlled global thinking of the AI would, if conscious, be the AI’s System 2. We humans also have conscious experiences involving what I’ll term “System 0”: raw passive perception that takes place even when you sit without moving or thinking and merely observe the world around you. Systems 0, 1 and 2 seem progressively more complex, so it’s striking that only the middle one appears unconscious. IIT explains this by saying that raw sensory information in System 0 is stored in grid-like brain structures with very high integration, while System 2 has high integration because of feedback loops, where all the information you’re aware of right now can affect your future brain states. On the other hand, it was precisely the conscious-grid prediction that triggered Scott Aaronson’s aforementioned IIT-critique. In summary, if a theory solving the pretty hard problem of consciousness can one day pass a rigorous battery of experimental tests so that we start taking its predictions seriously, then it will also greatly narrow down the options for the even harder problem of what future conscious AIs may experience.\n\nSome aspects of our subjective experience clearly trace back to our evolutionary origins, for example our emotional desires related to self-preservation (eating, drinking, avoiding getting killed) and reproduction. This means that it should be possible to create AI that never experiences qualia such as hunger, thirst, fear or sexual desire. As we saw in the last chapter, if a highly intelligent AI is programmed to have virtually any sufficiently ambitious goal, it’s likely to strive for self-preservation in order to be able to accomplish that goal. If they’re part of a society of AIs, however, they might lack our strong human fear of death: as long as they’ve backed themselves up, all they stand to lose are the memories they’ve accumulated since their most recent backup, as long as they’re confident that their backed-up software will be used. In addition, the ability to readily copy information and software between AIs would probably reduce the strong sense of individuality that’s so characteristic of our human consciousness: there would be less of a distinction between you and me if we could easily share and copy all our memories and abilities, so a group of nearby AIs may feel more like a single organism with a hive mind.\n\nWould an artificial consciousness feel that it had free will? Note that, although philosophers have spent millennia quibbling about whether we have free will without reaching consensus even on how to define the question,33 I’m asking a different question, which is arguably easier to tackle. Let me try to persuade you that the answer is simply “Yes, any conscious decision maker will subjectively feel that it has free will, regardless of whether it’s biological or artificial.” Decisions fall on a spectrum between two extremes:\n\n1. You know exactly why you made that particular choice.\n\n2. You have no idea why you made that particular choice—it felt like you chose randomly on a whim.\n\nFree-will discussions usually center around a struggle to reconcile our goal-oriented decision-making behavior with the laws of physics: if you’re choosing between the following two explanations for what you did, then which one is correct: “I asked her on a date because I really liked her” or “My particles made me do it by moving according to the laws of physics”? But we saw in the last chapter that both are correct: what feels like goal-oriented behavior can emerge from goal-less deterministic laws of physics. More specifically, when a system (brain or AI) makes a decision of type 1, it computes what to decide using some deterministic algorithm, and the reason it feels like it decided is that it in fact did decide when computing what to do. Moreover, as emphasized by Seth Lloyd,34 there’s a famous computer-science theorem saying that for almost all computations, there’s no faster way of determining their outcome than actually running them. This means that it’s typically impossible for you to figure out what you’ll decide to do in a second in less than a second, which helps reinforce your experience of having free will. In contrast, when a system (brain or AI) makes a decision of type 2, it simply programs its mind to base its decision on the output of some subsystem that acts as a random number generator. In brains and computers, effectively random numbers are easily generated by amplifying noise. Regardless of where on the spectrum from 1 to 2 a decision falls, both biological and artificial consciousnesses therefore feel that they have free will: they feel that it is really they who decide and they can’t predict with certainty what the decision will be until they’ve finished thinking it through.\n\nSome people tell me that they find causality degrading, that it makes their thought processes meaningless and that it renders them “mere” machines. I find such negativity absurd and unwarranted. First of all, there’s nothing “mere” about human brains, which, as far as I’m concerned, are the most amazingly sophisticated physical objects in our known Universe. Second, what alternative would they prefer? Don’t they want it to be their own thought processes (the computations performed by their brains) that make their decisions? Their subjective experience of free will is simply how their computations feel from inside: they don’t know the outcome of a computation until they’ve finished it. That’s what it means to say that the computation is the decision.\n\nMeaning\n\nLet’s end by returning to the starting point of this book: How do we want the future of life to be? We saw in the previous chapter how diverse cultures around the globe all seek a future teeming with positive experiences, but that fascinatingly thorny controversies arise when seeking consensus on what should count as positive and how to make trade-offs between what’s good for different life forms. But let’s not let those controversies distract us from the elephant in the room: there can be no positive experiences if there are no experiences at all, that is, if there’s no consciousness. In other words, without consciousness, there can be no happiness, goodness, beauty, meaning or purpose—just an astronomical waste of space. This implies that when people ask about the meaning of life as if it were the job of our cosmos to give meaning to our existence, they’re getting it backward: It’s not our Universe giving meaning to conscious beings, but conscious beings giving meaning to our Universe. So the very first goal on our wish list for the future should be retaining (and hopefully expanding) biological and/or artificial consciousness in our cosmos, rather than driving it extinct.\n\nIf we succeed in this endeavor, then how will we humans feel about coexisting with ever smarter machines? Does the seemingly inexorable rise of artificial intelligence bother you and if so, why? In chapter 3, we saw how it should be relatively easy for AI-powered technology to satisfy our basic needs such as security and income as long as the political will to do so exists. However, perhaps you’re concerned that being well fed, clad, housed and entertained isn’t enough. If we’re guaranteed that AI will take care of all our practical needs and desires, might we nonetheless end up feeling that we lack meaning and purpose in our lives, like well-kept zoo animals?\n\nTraditionally, we humans have often founded our self-worth on the idea of human exceptionalism: the conviction that we’re the smartest entities on the planet and therefore unique and superior. The rise of AI will force us to abandon this and become more humble. But perhaps that’s something we should do anyway: after all, clinging to hubristic notions of superiority over others (individuals, ethnic groups, species and so on) has caused awful problems in the past, and may be an idea ready for retirement. Indeed, human exceptionalism hasn’t only caused grief in the past, but it also appears unnecessary for human flourishing: if we discover a peaceful extraterrestrial civilization far more advanced than us in science, art and everything else we care about, this presumably wouldn’t prevent people from continuing to experience meaning and purpose in their lives. We could retain our families, friends and broader communities, and all activities that give us meaning and purpose, hopefully having lost nothing but arrogance.\n\nAs we plan our future, let’s consider the meaning not only of our own lives, but also of our Universe itself. Here two of my favorite physicists, Steven Weinberg and Freeman Dyson, represent diametrically opposite views. Weinberg, who won the Nobel Prize for foundational work on the standard model of particle physics, famously said, “The more the universe seems comprehensible, the more it also seems pointless.”35 Dyson, on the other hand, is much more optimistic, as we saw in chapter 6: although he agrees that our Universe was pointless, he believes that life is now filling it with ever more meaning, with the best yet to come if life succeeds in spreading throughout the cosmos. He ended his seminal 1979 paper thus: “Is Weinberg’s universe or mine closer to the truth? One day, before long, we shall know.”36 If our Universe goes back to being permanently unconscious because we drive Earth life extinct or because we let unconscious zombie AI take over our Universe, then Weinberg will be vindicated in spades.\n\nFrom this perspective, we see that although we’ve focused on the future of intelligence in this book, the future of consciousness is even more important, since that’s what enables meaning. Philosophers like to go Latin on this distinction, by contrasting sapience (the ability to think intelligently) with sentience (the ability to subjectively experience qualia). We humans have built our identity on being Homo sapiens, the smartest entities around. As we prepare to be humbled by ever smarter machines, I suggest that we rebrand ourselves as Homo sentiens!\n\n \n\n------------------------------------------------------------------------\n\nTHE BOTTOM LINE:\n\n• There’s no undisputed definition of “consciousness.” I use the broad and non-anthropocentric definition consciousness = subjective experience.\n\n• Whether AIs are conscious in that sense is what matters for the thorniest ethical and philosophical problems posed by the rise of AI: Can AIs suffer? Should they have rights? Is uploading a subjective suicide? Could a future cosmos teeming with AIs be the ultimate zombie apocalypse?\n\n• The problem of understanding intelligence shouldn’t be conflated with three separate problems of consciousness: the “pretty hard problem” of predicting which physical systems are conscious, the “even harder problem” of predicting qualia, and the “really hard problem” of why anything at all is conscious.\n\n• The “pretty hard problem” of consciousness is scientific, since a theory that predicts which of your brain processes are conscious is experimentally testable and falsifiable, while it’s currently unclear how science could fully resolve the two harder problems.\n\n• Neuroscience experiments suggest that many behaviors and brain regions are unconscious, with much of our conscious experience representing an after-the-fact summary of vastly larger amounts of unconscious information.\n\n• Generalizing consciousness predictions from brains to machines requires a theory. Consciousness appears to require not a particular kind of particle or field, but a particular kind of information processing that’s fairly autonomous and integrated, so that the whole system is rather autonomous but its parts aren’t.\n\n• Consciousness might feel so non-physical because it’s doubly substrate-independent: if consciousness is the way information feels when being processed in certain complex ways, then it’s merely the structure of the information processing that matters, not the structure of the matter doing the information processing.\n\n• If artificial consciousness is possible, then the space of possible AI experiences is likely to be huge compared to what we humans can experience, spanning a vast spectrum of qualia and timescales—all sharing a feeling of having free will.\n\n• Since there can be no meaning without consciousness, it’s not our Universe giving meaning to conscious beings, but conscious beings giving meaning to our Universe.\n\n• This suggests that as we humans prepare to be humbled by ever smarter machines, we take comfort mainly in being Homo sentiens, not Homo sapiens.\n\n------------------------------------------------------------------------\n\n \n\n------------------------------------------------------------------------\n\n\\*1 An alternative viewpoint is substance dualism—that living entities differ from inanimate ones because they contain some non-physical substance such as an “anima,” “élan vital” or “soul.” Support for substance dualism among scientists has gradually dwindled. To understand why, consider that your body is made of about 10²⁹ quarks and electrons, which, as far as we can tell, move according to simple physical laws. Imagine a future technology able to track all your particles: if they were found to obey the laws of physics exactly, then your purported soul is having no effect on your particles, so your conscious mind and its ability to control your movements would have nothing to do with a soul. If your particles were instead found not to obey the known laws of physics because they were being pushed around by your soul, then the new entity causing these forces would by definition be a physical one that we can study just like we’ve studied new fields and new particles in the past.\n\n\\*2 I use the word “qualia” according to the dictionary definition, to mean individual instances of subjective experience—that is, to mean the subjective experience itself, not any purported substance causing the experience. Beware that some people use the word differently.\n\n\\*3 I’d originally called the RHP the “very hard problem,” but after I showed this chapter to David Chalmers, he emailed me the clever suggestion of switching to the “really hard problem,” to match what he really meant: “since the first two problems (at least put this way) aren’t really part of the hard problem as I conceived of it whereas the third problem is, you could perhaps use ‘really hard’ instead of ‘very hard’ for the third one to match my usage.”\n\n\\*4 If our physical reality is entirely mathematical (information-based, loosely speaking), as I explored in my book Our Mathematical Universe, then no aspect of reality—not even consciousness—lies beyond the purview of science. Indeed, the really hard problem of consciousness is, from that perspective, the exact same problem as that of understanding how something mathematical can feel physical: if part of a mathematical structure is conscious, then it will experience the rest as its external physical world.\n\n\\*5 Although I’ve earlier used “perceptronium” as a synonym for sentronium, that name suggests too narrow a definition, since percepts are merely those subjective experiences that we perceive based on sensory input—excluding, for example, dreams and internally generated thoughts.\n\n\\*6 There’s potential tension between this claim and the idea that consciousness is substrate-independent, since even though the information processing may be different at the lowest level, it’s by definition identical at the higher levels where it determines behavior.\n\nEpilogue\n\nThe Tale of the FLI Team\n\n The saddest aspect of life right now is that science gathers knowledge faster than society gathers wisdom.\n\n Isaac Asimov\n\nHere we are, my dear reader, at the end of the book, after exploring the origin and fate of intelligence, goals and meaning. So how can we translate these ideas into action? What concretely should we do to make our future as good as possible? This is precisely the question I’m asking myself right now as I sit here in my window seat en route from San Francisco back to Boston on January 9, 2017, from the AI conference we just organized in Asilomar, so let me end this book by sharing my thoughts with you.\n\nMeia is catching up on sleep next to me after the many short nights of preparing and organizing. Wow—what a wild week it’s been! We managed to bring almost all the people I’ve mentioned in this book together for a few days to this Puerto Rico sequel, including entrepreneurs such as Elon Musk and Larry Page and AI research leaders from academia and companies such as DeepMind, Google, Facebook, Apple, IBM, Microsoft and Baidu, as well as economists, legal scholars, philosophers and other amazing thinkers (see figure 9.1). The results superseded even my high expectations, and I’m feeling more optimistic about the future of life than I have in a long time. In this epilogue, I’m going to tell you why.\n\nFLI Is Born\n\nEver since I learned about the nuclear arms race at age fourteen, I’ve been concerned that the power of our technology was growing faster than the wisdom with which we manage it. I therefore decided to sneak a chapter about this challenge into my first book, Our Mathematical Universe, even though the rest of it was primarily about physics. I made a New Year’s resolution for 2014 that I was no longer allowed to complain about anything without putting some serious thought into what I could personally do about it, and I kept my pledge during my book tour that January: Meia and I did lots of brainstorming about starting some sort of nonprofit organization focused on improving the future of life through technological stewardship.\n\nShe insisted that we give it a positive name as different as possible from “Doom & Gloom Institute” and “Let’s-Worry-about-the-Future Institute.” Since Future of Humanity Institute was already taken, we converged on the Future of Life Institute (FLI), which had the added advantage of being more inclusive. On January 22, the book tour took us to Santa Cruz, and as the California Sun set over the Pacific, we enjoyed dinner with our old friend Anthony Aguirre and persuaded him to join forces with us. He’s not only one of the wisest and most idealistic people I know, but also someone who’s managed to put up with running another nonprofit organization, the Foundational Questions Institute (see http://fqxi.org), with me for over a decade.\n\nThe following week, the tour took me to London. Since the future of AI was very much on my mind, I reached out to Demis Hassabis, who graciously invited me to visit DeepMind’s headquarters. I was awestruck by how much they’d grown since he visited me at MIT two years earlier. Google had just bought them for about $650 million, and seeing their vast office landscape filled with brilliant minds pursuing Demis’ audacious goal to “solve intelligence” gave me a visceral feeling that success was a real possibility.\n\nThe next evening, I spoke with my friend Jaan Tallinn using Skype, the software he’d helped create. I explained our FLI vision, and an hour later, he’d decided to take a chance on us, funding us at up to $100,000 a year! Few things touch me more than when someone places more trust in me than I’ve earned, so it meant the world to me when a year later, after the Puerto Rico conference I mentioned in chapter 1, he joked that this was the best investment he’d ever made.\n\n[Figure 9.1: Our January 2017 Asilomar conference, the Puerto Rico sequel, brought together a remarkable group of researchers in AI and related fields. Back row, from left to right: Patrick Lin, Daniel Weld, Ariel Conn, Nancy Chang, Tom Mitchell, Ray Kurzweil, Daniel Dewey, Margaret Boden, Peter Norvig, Nick Hay, Moshe Vardi, Scott Siskind, Nick Bostrom, Francesca Rossi, Shane Legg, Manuela Veloso, David Marble, Katja Grace, Irakli Beridze, Marty Tenenbaum, Gill Pratt, Martin Rees, Joshua Greene, Matt Scherer, Angela Kane, Amara Angelica, Jeff Mohr, Mustafa Suleyman, Steve Omohundro, Kate Crawford, Vitalik Buterin, Yutaka Matsuo, Stefano Ermon, Michael Wellman, Bas Steunebrink, Wendell Wallach, Allan Dafoe, Toby Ord, Thomas Dietterich, Daniel Kahneman, Dario Amodei, Eric Drexler, Tomaso Poggio, Eric Schmidt, Pedro Ortega, David Leake, Seán Ó hÉigeartaigh, Owain Evans, Jaan Tallinn, Anca Dragan, Sean Legassick, Toby Walsh, Peter Asaro, Kay Firth-Butterfield, Philip Sabes, Paul Merolla, Bart Selman, Tucker Davey, ?, Jacob Steinhardt, Moshe Looks, Josh Tenenbaum, Tom Gruber, Andrew Ng, Kareem Ayoub, Craig Calhoun, Percy Liang, Helen Toner, David Chalmers, Richard Sutton, Claudia Passos-Ferriera, János Krámar, William MacAskill, Eliezer Yudkowsky, Brian Ziebart, Huw Price, Carl Shulman, Neil Lawrence, Richard Mallah, Jurgen Schmidhuber, Dileep George, Jonathan Rothberg, Noah Rothberg. Front row: Anthony Aguirre, Sonia Sachs, Lucas Perry, Jeffrey Sachs, Vincent Conitzer, Steve Goose, Victoria Krakovna, Owen Cotton-Barratt, Daniela Rus, Dylan Hadfield-Menell, Verity Harding, Shivon Zilis, Laurent Orseau, Ramana Kumar, Nate Soares, Andrew McAfee, Jack Clark, Anna Salamon, Long Ouyang, Andrew Critch, Paul Christiano, Yoshua Bengio, David Sanford, Catherine Olsson, Jessica Taylor, Martina Kunz, Kristinn Thorisson, Stuart Armstrong, Yann LeCun, Alexander Tamas, Roman Yampolskiy, Marin Solja č i ć , Lawrence Krauss, Stuart Russell, Eric Brynjolfsson, Ryan Calo, ShaoLan Hsueh, Meia Chita-Tegmark, Kent Walker, Heather Roff, Meredith Whittaker, Max Tegmark, Adrian Weller, Jose Hernandez-Orallo, Andrew Maynard, John Hering, Abram Demski, Nicolas Berggruen, Gregory Bonnet, Sam Harris, Tim Hwang, Andrew Snyder-Beattie, Marta Halina, Sebastian Farquhar, Stephen Cave, Jan Leike, Tasha McCauley, Joseph Gordon-Levitt. Arrived later: Guru Banavar, Demis Hassabis, Rao Kambhampati, Elon Musk, Larry Page, Anthony Romero.][Figure 9.1: Our January 2017 Asilomar conference, the Puerto Rico sequel, brought together a remarkable group of researchers in AI and related fields. Back row, from left to right: Patrick Lin, Daniel Weld, Ariel Conn, Nancy Chang, Tom Mitchell, Ray Kurzweil, Daniel Dewey, Margaret Boden, Peter Norvig, Nick Hay, Moshe Vardi, Scott Siskind, Nick Bostrom, Francesca Rossi, Shane Legg, Manuela Veloso, David Marble, Katja Grace, Irakli Beridze, Marty Tenenbaum, Gill Pratt, Martin Rees, Joshua Greene, Matt Scherer, Angela Kane, Amara Angelica, Jeff Mohr, Mustafa Suleyman, Steve Omohundro, Kate Crawford, Vitalik Buterin, Yutaka Matsuo, Stefano Ermon, Michael Wellman, Bas Steunebrink, Wendell Wallach, Allan Dafoe, Toby Ord, Thomas Dietterich, Daniel Kahneman, Dario Amodei, Eric Drexler, Tomaso Poggio, Eric Schmidt, Pedro Ortega, David Leake, Seán Ó hÉigeartaigh, Owain Evans, Jaan Tallinn, Anca Dragan, Sean Legassick, Toby Walsh, Peter Asaro, Kay Firth-Butterfield, Philip Sabes, Paul Merolla, Bart Selman, Tucker Davey, ?, Jacob Steinhardt, Moshe Looks, Josh Tenenbaum, Tom Gruber, Andrew Ng, Kareem Ayoub, Craig Calhoun, Percy Liang, Helen Toner, David Chalmers, Richard Sutton, Claudia Passos-Ferriera, János Krámar, William MacAskill, Eliezer Yudkowsky, Brian Ziebart, Huw Price, Carl Shulman, Neil Lawrence, Richard Mallah, Jurgen Schmidhuber, Dileep George, Jonathan Rothberg, Noah Rothberg. Front row: Anthony Aguirre, Sonia Sachs, Lucas Perry, Jeffrey Sachs, Vincent Conitzer, Steve Goose, Victoria Krakovna, Owen Cotton-Barratt, Daniela Rus, Dylan Hadfield-Menell, Verity Harding, Shivon Zilis, Laurent Orseau, Ramana Kumar, Nate Soares, Andrew McAfee, Jack Clark, Anna Salamon, Long Ouyang, Andrew Critch, Paul Christiano, Yoshua Bengio, David Sanford, Catherine Olsson, Jessica Taylor, Martina Kunz, Kristinn Thorisson, Stuart Armstrong, Yann LeCun, Alexander Tamas, Roman Yampolskiy, Marin Solja č i ć , Lawrence Krauss, Stuart Russell, Eric Brynjolfsson, Ryan Calo, ShaoLan Hsueh, Meia Chita-Tegmark, Kent Walker, Heather Roff, Meredith Whittaker, Max Tegmark, Adrian Weller, Jose Hernandez-Orallo, Andrew Maynard, John Hering, Abram Demski, Nicolas Berggruen, Gregory Bonnet, Sam Harris, Tim Hwang, Andrew Snyder-Beattie, Marta Halina, Sebastian Farquhar, Stephen Cave, Jan Leike, Tasha McCauley, Joseph Gordon-Levitt. Arrived later: Guru Banavar, Demis Hassabis, Rao Kambhampati, Elon Musk, Larry Page, Anthony Romero.]\n\nFigure 9.1: Our January 2017 Asilomar conference, the Puerto Rico sequel, brought together a remarkable group of researchers in AI and related fields. Back row, from left to right: Patrick Lin, Daniel Weld, Ariel Conn, Nancy Chang, Tom Mitchell, Ray Kurzweil, Daniel Dewey, Margaret Boden, Peter Norvig, Nick Hay, Moshe Vardi, Scott Siskind, Nick Bostrom, Francesca Rossi, Shane Legg, Manuela Veloso, David Marble, Katja Grace, Irakli Beridze, Marty Tenenbaum, Gill Pratt, Martin Rees, Joshua Greene, Matt Scherer, Angela Kane, Amara Angelica, Jeff Mohr, Mustafa Suleyman, Steve Omohundro, Kate Crawford, Vitalik Buterin, Yutaka Matsuo, Stefano Ermon, Michael Wellman, Bas Steunebrink, Wendell Wallach, Allan Dafoe, Toby Ord, Thomas Dietterich, Daniel Kahneman, Dario Amodei, Eric Drexler, Tomaso Poggio, Eric Schmidt, Pedro Ortega, David Leake, Seán Ó hÉigeartaigh, Owain Evans, Jaan Tallinn, Anca Dragan, Sean Legassick, Toby Walsh, Peter Asaro, Kay Firth-Butterfield, Philip Sabes, Paul Merolla, Bart Selman, Tucker Davey, ?, Jacob Steinhardt, Moshe Looks, Josh Tenenbaum, Tom Gruber, Andrew Ng, Kareem Ayoub, Craig Calhoun, Percy Liang, Helen Toner, David Chalmers, Richard Sutton, Claudia Passos-Ferriera, János Krámar, William MacAskill, Eliezer Yudkowsky, Brian Ziebart, Huw Price, Carl Shulman, Neil Lawrence, Richard Mallah, Jurgen Schmidhuber, Dileep George, Jonathan Rothberg, Noah Rothberg. Front row: Anthony Aguirre, Sonia Sachs, Lucas Perry, Jeffrey Sachs, Vincent Conitzer, Steve Goose, Victoria Krakovna, Owen Cotton-Barratt, Daniela Rus, Dylan Hadfield-Menell, Verity Harding, Shivon Zilis, Laurent Orseau, Ramana Kumar, Nate Soares, Andrew McAfee, Jack Clark, Anna Salamon, Long Ouyang, Andrew Critch, Paul Christiano, Yoshua Bengio, David Sanford, Catherine Olsson, Jessica Taylor, Martina Kunz, Kristinn Thorisson, Stuart Armstrong, Yann LeCun, Alexander Tamas, Roman Yampolskiy, Marin Soljačić, Lawrence Krauss, Stuart Russell, Eric Brynjolfsson, Ryan Calo, ShaoLan Hsueh, Meia Chita-Tegmark, Kent Walker, Heather Roff, Meredith Whittaker, Max Tegmark, Adrian Weller, Jose Hernandez-Orallo, Andrew Maynard, John Hering, Abram Demski, Nicolas Berggruen, Gregory Bonnet, Sam Harris, Tim Hwang, Andrew Snyder-Beattie, Marta Halina, Sebastian Farquhar, Stephen Cave, Jan Leike, Tasha McCauley, Joseph Gordon-Levitt. Arrived later: Guru Banavar, Demis Hassabis, Rao Kambhampati, Elon Musk, Larry Page, Anthony Romero.\n\nThe next day, my publisher had left a gap in my schedule, which I filled with a visit to the London Science Museum. After having obsessed about the past and future of intelligence for so long, I suddenly felt that I was walking through a physical manifestation of my thoughts. They’d assembled a fantastic collection of stuff representing our growth of knowledge, from Stephenson’s Rocket locomotive to the Model T Ford, a life-size Apollo 11 lunar lander replica and computers dating all the way from Babbage’s “Difference Engine” mechanical calculator to present-day hardware. They also had an exhibit about the history of our understanding of the mind, from Galvano’s frog-leg experiments to neurons, EEG and fMRI.\n\nI very rarely cry, but that’s what I did on the way out—and in a tunnel full of pedestrians, no less, en route to the South Kensington tube station. Here were all these people going about their lives blissfully unaware of what I was thinking. First we humans discovered how to replicate some natural processes with machines, making our own wind and lightning, and our own mechanical horsepower. Gradually, we started realizing that our bodies were also machines. Then the discovery of nerve cells started blurring the borderline between body and mind. Then we started building machines that could outperform not only our muscles, but our minds as well. So in parallel with discovering what we are, are we inevitably making ourselves obsolete? That would be poetically tragic.\n\nThis thought scared me, but it also strengthened my resolve to keep my New Year’s resolution. I felt that we needed one more person to complete our team of FLI founders, who’d spearhead a team of idealistic young volunteers. The logical choice was Viktoriya Krakovna, a brilliant Harvard grad student who’d not only won a silver medal in the International Mathematics Olympiad, but also founded the Citadel, a house for about a dozen young idealists who wanted reason to play a greater role in their lives and the world. Meia and I invited her over to our place five days later to tell her about our vision, and before we’d finished the sushi, FLI had been born.\n\nThe Puerto Rico Adventure\n\nThis marked the beginning of an amazing adventure, which still continues. As I mentioned in chapter 1, we held regular brainstorming meetings at our house with dozens of idealistic students, professors and other local thinkers, where the top-rated ideas transformed into projects—the first being that AI op-ed from chapter 1 with Stephen Hawking, Stuart Russell and Frank Wilczek that helped ignite the public debate. In parallel with the baby steps of setting up a new organization (such as incorporating, recruiting an advisory board and launching a website), we held a fun launch event in front of a packed MIT auditorium, at which Alan Alda explored the future of technology with leading experts.\n\n[Figure 9.2: Jaan Tallinn, Anthony Aguirre, yours truly, Meia Chita-Tegmark and Viktoriya Krakovna celebrate our incorporation of FLI with sushi on May 23, 2014.][Figure 9.2: Jaan Tallinn, Anthony Aguirre, yours truly, Meia Chita-Tegmark and Viktoriya Krakovna celebrate our incorporation of FLI with sushi on May 23, 2014.]\n\nFigure 9.2: Jaan Tallinn, Anthony Aguirre, yours truly, Meia Chita-Tegmark and Viktoriya Krakovna celebrate our incorporation of FLI with sushi on May 23, 2014.\n\nWe focused the rest of the year on pulling together the Puerto Rico conference which, as I mentioned in chapter 1, aimed to engage the world’s leading AI researchers in the discussion of how to keep AI beneficial. Our goal was to shift the AI-safety conversation from worrying to working: from bickering about how worried to be, to agreeing on concrete research projects that could be started right away to maximize the chance of a good outcome. To prepare, we collected promising AI-safety research ideas from around the world and sought community feedback on our growing project list. With the help of Stuart Russell and a group of hardworking young volunteers, especially Daniel Dewey, János Krámar and Richard Mallah, we distilled these research priorities into a document to be discussed at the conference.1 Building consensus that there was lots of valuable AI-safety research to be done would, we hoped, encourage people to start doing such research. The ultimate moonshot triumph would be if it could even persuade someone to fund it since, so far, there had been essentially no support for such work from government funding agencies.\n\nEnter Elon Musk. On August 2, he appeared on our radar by famously tweeting “Worth reading Superintelligence by Bostrom. We need to be super careful with AI. Potentially more dangerous than nukes.” I reached out to him about our efforts, and got to speak with him by phone a few weeks later. Although I felt quite nervous and starstruck, the outcome was outstanding: he agreed to join our FLI scientific advisory board, to attend our conference and potentially to fund a first-ever AI-safety research program to be announced in Puerto Rico. This electrified all of us at FLI, and made us redouble our efforts to create an awesome conference, identify promising research topics and build community support for them.\n\nI finally got to meet Elon in person for further planning when he came to MIT two months later for a space symposium. It felt very strange to be alone with him in a small green room just moments after he’d enraptured over a thousand MIT students like a rock star, but after a few minutes, all I could think of was our joint project. I instantly liked him. He radiated sincerity, and I was inspired by how much he cared about the long-term future of humanity—and how he audaciously turned his aspiration into actions. He wanted humanity to explore and settle our Universe, so he started a space company. He wanted sustainable energy, so he started a solar company and an electric-car company. Tall, handsome, eloquent and incredibly knowledgeable, it was easy to understand why people listened to him.\n\nUnfortunately, this MIT event also taught me how fear-driven and divisive media can be. Elon’s stage performance consisted of an hour of fascinating discussion about space exploration, which I think would have made great TV. At the very end, a student asked him an off-topic question about AI. His answer included the phrase “with artificial intelligence, we are summoning the demon,” which became the only thing that most media reported—and generally out of context. It struck me that many journalists were inadvertently doing the exact opposite of what we were trying to accomplish in Puerto Rico. Whereas we wanted to build community consensus by highlighting the common ground, the media had an incentive to highlight the divisions. The more controversy they could report, the greater their Nielsen ratings and ad revenue. Moreover, whereas we wanted to help people from across the spectrum of opinions to come together, get along and understand each other better, media coverage inadvertently made people across the opinion spectrum upset at one another, fueling misunderstandings by publishing only their most provocative-sounding quotes without context. For this reason, we decided to ban journalists from the Puerto Rico meeting and impose the “Chatham House Rule,” which prohibits participants from subsequently revealing who said what.^(\\*)\n\nAlthough our Puerto Rico conference ended up being a success, it didn’t come easy. The countdown mostly required diligent prep work, for example me phoning or skyping large numbers of AI researchers to assemble a critical mass of participants to attract the other attendees, and there were also dramatic moments—such as when I got up by 7 a.m. on December 27 to reach Elon on a lousy phone connection to Uruguay, and was told “I don’t think this is gonna work.” He was concerned that an AI-safety research program might provide a false sense of security, enabling reckless researchers to forge ahead while paying lip service to safety. But then, despite the sound incessantly cutting out, we extensively talked through the huge benefits of mainstreaming the topic and getting more AI researchers working on AI safety. After the call dropped, he sent me one of my favorite emails ever: “Lost the call at the end there. Anyway, docs look fine. I’m happy to support the research with $5M over three years. Maybe we should make it $10M?”\n\nFour days later, 2015 got off to a good start for Meia and me as we briefly relaxed before the meeting, dancing in the new year on a Puerto Rico beach illuminated by fireworks. The conference got off to a great start too: there was remarkable consensus that more AI-safety research was needed, and based on further input from the conference participants, that research priorities document we’d worked so hard on was improved and finalized. We passed around that safety-research-endorsing open letter from chapter 1, and were delighted that almost everyone signed it.\n\nMeia and I had a magical meeting with Elon in our hotel room where he blessed the detailed plans for our grants program. She was touched by how down-to-earth and candid he was about his personal life, and how much interest he took in us. He asked us how we met, and liked Meia’s elaborate story. The next day, we filmed an interview with him about AI safety and why he wanted to support it and everything seemed on track.2\n\nThe conference climax, Elon’s donation announcement, was scheduled for 7 p.m. on Sunday, January 4, 2015, and I’d been so tense about it that I’d tossed and turned in my sleep the night before. And then, just fifteen minutes before we were supposed to head to the session where it would happen, we hit a snag! Elon’s assistant called and said that it looked like Elon might not be able to go through with the announcement, and Meia said she’d never seen me look more stressed or disappointed. Elon finally came by, and I could hear the seconds counting down to the session start as we sat there and talked. He explained that they were just two days away from a crucial SpaceX rocket launch where they hoped to pull off the first-ever successful landing of the first stage on a drone ship, and that since this was a huge milestone, the SpaceX team didn’t want to distract from it with concurrent media splashes involving him. Anthony Aguirre, cool and levelheaded as always, pointed out that this meant that nobody wanted media attention for this, neither Elon nor the AI community. We arrived a few minutes late to the session I was moderating, but we had a plan: no dollar amount would get mentioned, to ensure that the announcement wasn’t newsworthy, and I’d lord Chatham House over everyone to keep Elon’s announcement secret from the world for nine days if his rocket reached the space station, regardless of whether the landing succeeded; he said he’d need even more time if the rocket exploded on launch.\n\nThe countdown to the announcement finally reached zero. The superintelligence panelists that I’d moderated still sat there next to me onstage in their chairs: Eliezer Yudkowsky, Elon Musk, Nick Bostrom, Richard Mallah, Murray Shanahan, Bart Selman, Shane Legg and Vernor Vinge. People gradually stopped applauding, but the panelists remained seated, because I’d told them to stay without explaining why. Meia later told me that her pulse reached the stratosphere around now, and that she clutched Viktoriya Krakovna’s calming hand under the table. I smiled, knowing that this was the moment we’d worked, hoped and waited for.\n\nI was very happy that there was such consensus at the meeting that more research was needed for keeping AI beneficial, I said, and that there were so many concrete research directions we could work on right away. But there had been talk of serious risks in this session, I added, so it would be nice to raise our spirits and get into an upbeat mood before heading out to the bar and the conference banquet that had been set up outside. “And I’m therefore giving the microphone to…Elon Musk!” I felt that history was in the making as Elon took the mic and announced that he would donate a large amount of money to AI-safety research. Unsurprisingly, he brought down the house. As planned, he didn’t mention how much, but I knew that it was a cool $10 million, as we’d agreed.\n\nMeia and I went to visit our parents in Sweden and Romania after the conference, and with bated breath, we watched the live-streamed rocket launch with my dad in Stockholm. The landing attempt unfortunately ended with what Elon euphemistically calls an RUD, “rapid unscheduled disassembly,” and pulling off a successful ocean landing took his team another fifteen months.3 However, all the satellites were successfully launched into orbit, as was our grants program via a tweet by Elon to his millions of followers.4\n\nMainstreaming AI Safety\n\nA key goal of the Puerto Rico conference had been to mainstream AI-safety research, and it was exhilarating to see this unfold in multiple steps. First there was the meeting itself, where many researchers started feeling comfortable engaging with the topic once they realized that they were part of a growing community of peers. I was deeply touched by encouragement from many participants. For example, Cornell University AI professor Bart Selman emailed me saying, “I’ve honestly never seen a better organized or more exciting and intellectually stimulating scientific meeting.”\n\nThe next mainstreaming step began on January 11 when Elon tweeted “World’s top artificial intelligence developers sign open letter calling for AI-safety research,”5 linking to a sign-up page that soon racked up over eight thousand signatures, including many of the world’s most prominent AI builders. It suddenly became harder to claim that people concerned about AI safety didn’t know what they were talking about, because this now implied that a who’s who of leading AI researchers didn’t know what they were talking about. The open letter was reported by media around the world in a way that made us grateful that we’d barred journalists from our conference. Although the most alarmist word in the letter was “pitfalls,” it nonetheless triggered headlines such as “Elon Musk and Stephen Hawking Sign Open Letter in Hopes of Preventing Robot Uprising,” illustrated by murderous terminators. Of the hundreds of articles we spotted, our favorite was one mocking the others, writing that “a headline that conjures visions of skeletal androids stomping human skulls underfoot turns complex, transformative technology into a carnival sideshow.”6 Fortunately, there were also many sober news articles, and they gave us another challenge: keeping up with the torrent of new signatures, which needed to be manually verified to protect our credibility and weed out pranks such as “HAL 9000,” “Terminator,” “Sarah Jeanette Connor” and “Skynet.” For this and our future open letters, Viktoriya Krakovna and János Krámar helped organize a volunteer brigade of checkers that included Jesse Galef, Eric Gastfriend and Revathi Vinoth Kumar working in shifts, so that when Revathi went to sleep in India, she passed the baton to Eric in Boston, and so on.\n\nThe third mainstreaming step began four days later, when Elon tweeted a link to our announcement that he was donating $10 million to AI-safety research.7 A week later, we launched an online portal where researchers from around the world could apply and compete for this funding. We were able to whip the application system together so quickly only because Anthony and I had spent the previous decade running similar competitions for physics grants. The Open Philanthropy Project, a California-based charity focused on high-impact giving, generously agreed to top up Elon’s gift to allow us to give more grants. We weren’t sure how many applicants we’d get, since the topic was novel and the deadline was short. The response blew us away, with about three hundred teams from around the world asking for about $100 million. A panel of AI professors and other researchers carefully reviewed the proposals and selected thirty-seven winning teams, who were funded for up to three years. When we announced the list of winners, it marked the first time that the media response to our activities was fairly nuanced and free of killer-robot pictures. It was finally sinking in that AI safety wasn’t empty talk: there was actual useful work to be done, and lots of great research teams were rolling up their sleeves to join the effort.\n\nThe fourth mainstreaming step happened organically over the next two years, with scores of technical publications and dozens of workshops on AI safety around the world, typically as parts of mainstream AI conferences. Persistent people had tried for many years to engage the AI community in safety research, with limited success, but now things really took off. Many of these publications were funded by our grants program and we at FLI did our best to help organize and fund as many of these workshops as we could, but a growing fraction of them were enabled by AI researchers investing their own time and resources. As a result, ever more researchers learned about safety research from their own colleagues, discovering that aside from being useful, it could also be fun, involving interesting mathematical and computational problems to puzzle over.\n\nComplicated equations aren’t everyone’s idea of fun, of course. Two years after our Puerto Rico conference, we preceded our Asilomar conference with a technical workshop where our FLI grant winners could showcase their research, and watched slide after slide with mathematical symbols on the big screen. Moshe Vardi, an AI professor at Rice University, joked that he knew we’d succeeded in establishing an AI-safety research field once the meetings got boring.\n\nThis dramatic growth of AI-safety work wasn’t limited to academia. Amazon, DeepMind, Facebook, Google, IBM and Microsoft launched an industry partnership for beneficial AI.8 Major new AI-safety donations enabled expanded research at our largest nonprofit sister organizations: the Machine Intelligence Research Institute in Berkeley, the Future of Humanity Institute in Oxford and the Centre for the Study of Existential Risk in Cambridge (UK). Further donations of $10 million or more kick-started additional beneficial-AI efforts: the Leverhulme Centre for the Future of Intelligence in Cambridge, the K&L Gates Endowment for Ethics and Computational Technologies in Pittsburgh and the Ethics and Governance of Artificial Intelligence Fund in Miami. Last but not least, with a billion-dollar commitment, Elon Musk partnered with other entrepreneurs to launch OpenAI, a nonprofit company in San Francisco pursuing beneficial AI. AI-safety research was here to stay.\n\nIn lockstep with this surge of research came a surge of opinions being expressed, both individually and collectively. The industry Partnership on AI published its founding tenets, and long reports with lists of recommendations were published by the U.S. government, Stanford University and the IEEE (the world’s largest organization of technical professionals), together with dozens of other reports and position papers from elsewhere.9\n\nWe were eager to facilitate meaningful discussion among the Asilomar attendees and learn what, if anything, this diverse community agreed on. Lucas Perry therefore took on the heroic task of reading all of those documents we’d found and extracting all their opinions. In a marathon effort initiated by Anthony Aguirre and concluded by a series of long telecons, our FLI team then attempted to group similar opinions together and strip away redundant bureaucratic verbiage to end up with a single list of succinct principles, also including unpublished but influential opinions that had been expressed more informally in talks and elsewhere. But this list still included plenty of ambiguity, contradiction and room for interpretation, so the month before the conference, we shared it with the participants and collected their opinions and suggestions for improved or novel principles. This community input produced a significantly revised principle list for use at the conference.\n\nIn Asilomar, the list was further improved in two steps. First, small groups discussed the principles they were most interested in (figure 9.4), producing detailed refinements, feedback, new principles and competing versions of old ones. Finally, we surveyed all attendees to determine the level of support for each version of each principle.\n\n[Figure 9.3: Groups of great minds ponder AI principles in Asilomar.][Figure 9.3: Groups of great minds ponder AI principles in Asilomar.]\n\nFigure 9.3: Groups of great minds ponder AI principles in Asilomar.\n\nThis collective process was both exhaustive and exhausting, with Anthony, Meia and I curtailing sleep and lunch time at the conference in our scramble to compile everything needed in time for the next steps. But it was also exciting. After such detailed, thorny and sometimes contentious discussions and such a wide range of feedback, we were astonished by the high level of consensus that emerged around many of the principles during that final survey, with some getting over 97% support. This consensus allowed us to set a high bar for inclusion in the final list: we kept only principles that at least 90% of the attendees agreed on. Although this meant that some popular principles were dropped at the last minute, including some of my personal favorites,10 it enabled most of the participants to feel comfortable endorsing all of them on the sign-up sheet that we passed around the auditorium. Here’s the result.\n\nThe Asilomar AI Principles\n\nArtificial intelligence has already provided beneficial tools that are used every day by people around the world. Its continued development, guided by the following principles, will offer amazing opportunities to help and empower people in the decades and centuries ahead.\n\nRESEARCH ISSUES\n\n§1 Research Goal: The goal of AI research should be to create not undirected intelligence, but beneficial intelligence.\n\n§2 Research Funding: Investments in AI should be accompanied by funding for research on ensuring its beneficial use, including thorny questions in computer science, economics, law, ethics, and social studies, such as:\n\n(a) How can we make future AI systems highly robust, so that they do what we want without malfunctioning or getting hacked?\n\n(b) How can we grow our prosperity through automation while maintaining people’s resources and purpose?\n\n(c) How can we update our legal systems to be more fair and efficient, to keep pace with AI, and to manage the risks associated with AI?\n\n(d) What set of values should AI be aligned with, and what legal and ethical status should it have?\n\n§3 Science-Policy Link: There should be constructive and healthy exchange between AI researchers and policy-makers.\n\n§4 Research Culture: A culture of cooperation, trust, and transparency should be fostered among researchers and developers of AI.\n\n§5 Race Avoidance: Teams developing AI systems should actively cooperate to avoid corner-cutting on safety standards.\n\nETHICS AND VALUES\n\n§6 Safety: AI systems should be safe and secure throughout their operational lifetime, and verifiably so where applicable and feasible.\n\n§7 Failure Transparency: If an AI system causes harm, it should be possible to ascertain why.\n\n§8 Judicial Transparency: Any involvement by an autonomous system in judicial decision-making should provide a satisfactory explanation auditable by a competent human authority.\n\n§9 Responsibility: Designers and builders of advanced AI systems are stakeholders in the moral implications of their use, misuse, and actions, with a responsibility and opportunity to shape those implications.\n\n§10 Value Alignment: Highly autonomous AI systems should be designed so that their goals and behaviors can be assured to align with human values throughout their operation.\n\n§11 Human Values: AI systems should be designed and operated so as to be compatible with ideals of human dignity, rights, freedoms, and cultural diversity.\n\n§12 Personal Privacy: People should have the right to access, manage, and control the data they generate, given AI systems’ power to analyze and utilize that data.\n\n§13 Liberty and Privacy: The application of AI to personal data must not unreasonably curtail people’s real or perceived liberty.\n\n§14 Shared Benefit: AI technologies should benefit and empower as many people as possible.\n\n§15 Shared Prosperity: The economic prosperity created by AI should be shared broadly, to benefit all of humanity.\n\n§16 Human Control: Humans should choose how and whether to delegate decisions to AI systems, to accomplish human-chosen objectives.\n\n§17 Non-subversion: The power conferred by control of highly advanced AI systems should respect and improve, rather than subvert, the social and civic processes on which the health of society depends.\n\n§18 AI Arms Race: An arms race in lethal autonomous weapons should be avoided.\n\nLONGER-TERM ISSUES\n\n§19 Capability Caution: There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities.\n\n§20 Importance: Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources.\n\n§21 Risks: Risks posed by AI systems, especially catastrophic or existential risks, must be subject to planning and mitigation efforts commensurate with their expected impact.\n\n§22 Recursive Self-Improvement: AI systems designed to recursively self-improve or self-replicate in a manner that could lead to rapidly increasing quality or quantity must be subject to strict safety and control measures.\n\n§23 Common Good: Superintelligence should only be developed in the service of widely shared ethical ideals, and for the benefit of all humanity rather than one state or organization.\n\nThe signature list grew dramatically after we posted the principles online, and by now it includes an amazing list of more than a thousand AI researchers and many other top thinkers. If you too want to join as a signatory, you can do so here: http://futureoflife.org/ai-principles.\n\nWe were struck not only by the level of consensus about the principles, but also by how strong they were. Sure, some of them sound about as controversial as “Peace, love and motherhood are valuable” at first glance. But many of them have real teeth, as is most easily seen by formulating negations of them. For example, “Superintelligence is impossible!” violates §19, and “It’s a total waste to do research on reducing existential risk from AI!” violates §21.\n\nIndeed, as you can see for yourself if you watch our long-term panel discussion on YouTube,11 Elon Musk, Stuart Russell, Ray Kurzweil, Demis Hassabis, Sam Harris, Nick Bostrom, David Chalmers, Bart Selman and Jaan Tallinn all agreed that superintelligence would probably be developed and that safety research was important.\n\nI hope that the Asilomar AI Principles will serve as a starting point for more detailed discussions, which will ultimately lead to sensible AI strategies and policies. In this spirit, our FLI media director Ariel Conn worked with Tucker Davey and other team members to interview leading AI researchers about the principles and how they interpreted them, while David Stanley and his team of international FLI volunteers translated the principles into key world languages.\n\nMindful Optimism\n\nAs I confessed in the opening of this epilogue, I’m feeling more optimistic about the future of life than I have in a long time. I shared my personal story to explain why.\n\nMy experiences over the past few years have increased my optimism for two separate reasons. First, I’ve witnessed the AI community come together in a remarkable way to constructively take on the challenges ahead, often in collaboration with thinkers from other fields. Elon told me after the Asilomar meeting that he found it amazing how AI safety has gone from a fringe issue to mainstream in only a few years, and I’m just as amazed myself. And now it’s not merely the near-term issues from chapter 3 that are becoming respectable discussion topics, but even superintelligence and existential risk, as in the Asilomar AI Principles. There’s no way that those principles could have been adopted in Puerto Rico two years earlier, where the most scary-sounding word that made it into the open letter was “pitfalls.”\n\nI like people-watching, and at one point during the final morning of the Asilomar conference, I stood at the side of the auditorium and watched the participants listen to a discussion about AI and law. To my surprise, a warm and fuzzy feeling swept through me and I suddenly felt very moved. This felt so different from Puerto Rico! Back then, I remember viewing most of the AI community with a combination of respect and fear—not exactly as an opposing team, but as a group that my AI-concerned colleagues and I felt we needed to persuade. But now it felt so obvious that we were all on the same team. As you’ve probably gleaned from reading this book, I still don’t have the answers for how to create a great future with AI, so it feels great to be part of a growing community searching for the answers together.\n\n[Figure 9.4: A growing community searches for answers together in Asilomar.][Figure 9.4: A growing community searches for answers together in Asilomar.]\n\nFigure 9.4: A growing community searches for answers together in Asilomar.\n\nThe second reason I’ve grown more optimistic is that the FLI experience has been empowering. What had triggered my London tears was a feeling of inevitability: that a disturbing future may be coming and there was nothing we could do about it. But the next three years dissolved my fatalistic gloom. If even a ragtag bunch of unpaid volunteers could make a positive difference for what’s arguably the most important conversation of our time, then imagine what we can all do if we work together!\n\nErik Brynjolfsson spoke of two kinds of optimism in his Asilomar talk. First there’s the unconditional kind, such as the positive expectation that the Sun will rise tomorrow morning. Then there’s what he called “mindful optimism,” which is the expectation that good things will happen if you plan carefully and work hard for them. That’s the kind of optimism I now feel about the future of life.\n\nSo what can you do to make a positive difference for the future of life as we enter the age of AI? For reasons I’ll soon explain, I think that a great first step is working on becoming a mindful optimist, if you aren’t one already. To be a successful mindful optimist, it’s crucial to develop positive visions for the future. When MIT students come to my office for career advice, I usually start by asking them where they see themselves in a decade. If a student were to reply “Perhaps I’ll be in a cancer ward, or in a cemetery after getting hit by a bus,” I’d give her a hard time. Envisioning only negative futures is a terrible approach to career planning! Devoting 100% of one’s efforts to avoiding diseases and accidents is a great recipe for hypochondria and paranoia, not happiness. Instead, I’d like to hear her describe her goals with enthusiasm, after which we can discuss strategies for getting there while avoiding pitfalls.\n\nErik pointed out that according to game theory, positive visions form the foundation of a large fraction of all collaboration in the world, from marriages and corporate mergers to the decision of independent states to form the USA. After all, why sacrifice something you have if you can’t imagine the even greater gain that this will provide? This means that we should be imagining positive futures not only for ourselves, but also for society and for humanity itself. In other words, we need more existential hope! Yet, as Meia likes to remind me, from Frankenstein to the Terminator, futuristic visions in literature and film are predominantly dystopian. In other words, we as a society are planning our future about as poorly as that hypothetical MIT student. This is why we need more mindful optimists. And this is why I’ve encouraged you throughout this book to think about what sort of future you want rather than merely what sort of future you fear, so that we can find shared goals to plan and work for.\n\nWe’ve seen throughout this book how AI is likely to give us both grand opportunities and tough challenges. A strategy that’s likely to help with essentially all AI challenges is for us to get our act together and improve our human society before AI fully takes off. We’re better off educating our young to make technology robust and beneficial before ceding great power to it. We’re better off modernizing our laws before technology makes them obsolete. We’re better off resolving international conflicts before they escalate into an arms race in autonomous weapons. We’re better off creating an economy that ensures prosperity for all before AI potentially amplifies inequalities. We’re better off in a society where AI-safety research results get implemented rather than ignored. And looking further ahead, to challenges related to superhuman AGI, we’re better off agreeing on at least some basic ethical standards before we start teaching these standards to powerful machines. In a polarized and chaotic world, people with the power to use AI for malicious purposes will have more motivation and ability to do so, and teams racing to build AGI will feel more pressure to cut corners on safety than to cooperate. In summary, if we can create a more harmonious human society characterized by cooperation toward shared goals, this will improve the prospects of the AI revolution ending well.\n\nIn other words, one of the best ways for you to improve the future of life is to improve tomorrow. You have power to do so in many ways. Of course you can vote at the ballot box and tell your politicians what you think about education, privacy, lethal autonomous weapons, technological unemployment and other issues. But you also vote every day through what you choose to buy, what news you choose to consume, what you choose to share and what sort of role model you choose to be. Do you want to be someone who interrupts all their conversations by checking their smartphone, or someone who feels empowered by using technology in a planned and deliberate way? Do you want to own your technology or do you want your technology to own you? What do you want it to mean to be human in the age of AI? Please discuss all this with those around you—it’s not only an important conversation, but a fascinating one.\n\nWe’re the guardians of the future of life now as we shape the age of AI. Although I cried in London, I now feel that there’s nothing inevitable about this future, and I know that it’s much easier to make a difference than I thought. Our future isn’t written in stone and just waiting to happen to us—it’s ours to create. Let’s create an inspiring one together!\n\n------------------------------------------------------------------------\n\n\\* This experience also made me rethink how I personally should interpret news. Although I’d obviously been aware that most outlets have their own political agenda, I now realized that they also have a bias away from the center on all issues, even nonpolitical ones.\n\nNotes\n\nChapter 1\n\n1. “The AI Revolution: Our Immortality or Extinction?” Wait But Why (January 27, 2015), at http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html.\n\n2. This open letter, “Research Priorities for Robust and Beneficial Artificial Intelligence,” can be found at http://futureoflife.org/ai-open-letter/.\n\n3. Example of classic robot alarmism in the media: Ellie Zolfagharifard, “Artificial Intelligence ‘Could Be the Worst Thing to Happen to Humanity,’ ” Daily Mail, May 2, 2014; http://tinyurl.com/hawkingbots.\n\nChapter 2\n\n1. Notes on the origin of the term AGI: http://wp.goertzel.org/who-coined-the-term-agi.\n\n2. Hans Moravec, “When Will Computer Hardware Match the Human Brain?” Journal of Evolution and Technology (1998), vol. 1.\n\n3. In the figure showing computing power versus year, the pre-2011 data is from Ray Kurzweil’s book How to Create a Mind, and subsequent data is computed from the references in https://en.wikipedia.org/wiki/FLOPS.\n\n4. Quantum computing pioneer David Deutsch describes how he views quantum computation as evidence of parallel universes in his The Fabric of Reality: The Science of Parallel Universes—and Its Implications (London: Allen Lane, 1997). If you want my own take on quantum parallel universes as the third of four multiverse levels, you’ll find it in my previous book: Max Tegmark, Our Mathematical Universe: My Quest for the Ultimate Nature of Reality (New York: Knopf, 2014).\n\nChapter 3\n\n1. Watch “Google DeepMind’s Deep Q-learning Playing Atari Breakout” on YouTube at https://tinyurl.com/atariai.\n\n2. See Volodymyr Mnih et al., “Human-Level Control Through Deep Reinforcement Learning,” Nature 518 (February 26, 2015): 529–533, available online at http://tinyurl.com/ataripaper.\n\n3. Here’s a video of the Big Dog robot in action: https://www.youtube.com/watch?v=W1czBcnX1Ww.\n\n4. For reactions to the sensationally creative line 5 move by AlphaGO, see “Move 37!! Lee Sedol vs AlphaGo Match 2,” at https://www.youtube.com/watch?v=JNrXgpSEEIE.\n\n5. Demis Hassabis describing reactions to AlphaGo from human Go players: https://www.youtube.com/watch?v=otJKzpNWZT4.\n\n6. For recent improvements in machine translation, see Gideon Lewis-Kraus, “The Great A.I. Awakening,” New York Times Magazine (December 14, 2016), available online at http://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html. GoogleTranslate is available here at https://translate.google.com.\n\n7. Winograd Schema Challenge competition: http://tinyurl.com/winogradchallenge.\n\n8. Ariane 5 explosion video: https://www.youtube.com/watch?v=qnHn8W1Em6E.\n\n9. Ariane 5 Flight 501 Failure report by the inquiry board: http://tinyurl.com/arianeflop.\n\n10. NASA’s Mars Climate Orbiter Mishap Investigation Board Phase I report: http://tinyurl.com/marsflop.\n\n11. The most detailed and consistent account of what caused the Mariner 1 Venus mission failure was incorrect hand-transcription of a single mathematical symbol (a missing overbar): http://tinyurl.com/marinerflop.\n\n12. A detailed description of the failure of the Soviet Phobos 1 Mars mission can be found in Wesley T. Huntress Jr. and Mikhail Ya. Marov, Soviet Robots in the Solar System (New York: Praxis Publishing, 2011), p. 308.\n\n13. How unverified software cost Knight Capital $440 million in 45 minutes: http://tinyurl.com/knightflop1 and http://tinyurl.com/knightflop2.\n\n14. U.S. government report on the Wall Street “flash crash”: “Findings Regarding the Market Events of May 6, 2010” (September 30, 2010), at http://tinyurl.com/flashcrashreport.\n\n15. 3-D printing of buildings (https://www.youtube.com/watch?v=SObzNdyRTBs), micromechanical devices (http://tinyurl.com/tinyprinter) and many things in between (https://www.youtube.com/watch?v=xVU4FLrsPXs).\n\n16. Global map of community-based fab labs: https://www.fablabs.io/labs/map.\n\n17. News article about Robert Williams being killed by an industrial robot: http://tinyurl.com/williamsaccident.\n\n18. News article about Kenji Urada being killed by an industrial robot: http://tinyurl.com/uradaaccident.\n\n19. News article about Volkswagen worker being killed by an industrial robot: http://tinyurl.com/baunatalaccident.\n\n20. U.S. government report on worker fatalities: https://www.osha.gov/dep/fatcat/dep\\_fatcat.html.\n\n21. Car accident fatality statistics: http://tinyurl.com/roadsafety2 and http://tinyurl.com/roadsafety3.\n\n22. On the first Tesla autopilot fatality, see Andrew Buncombe, “Tesla Crash: Driver Who Died While on Autopilot Mode ‘Was Watching Harry Potter,’ ” Independent (July 1, 2016), http://tinyurl.com/teslacrashstory. For the report of the Office of Defects Investigation of the U.S. National Highway Traffic Safety Administration, see http://tinyurl.com/teslacrashreport.\n\n23. For more about the Herald of Free Enterprise disaster, see R. B. Whittingham, The Blame Machine: Why Human Error Causes Accidents (Oxford, UK: Elsevier, 2004).\n\n24. Documentary about the Air France 447 crash: https://www.youtube.com/watch?v=dpPkp8OGQFI; accident report: http://tinyurl.com/af447report; outside analysis: http://tinyurl.com/thomsonarticle.\n\n25. Official report on the 2003 U.S.-Canadian blackout: http://tinyurl.com/uscanadablackout.\n\n26. Final report of the President’s Commission on the Accident at Three Mile Island: http://www.threemileisland.org/downloads/188.pdf.\n\n27. Dutch study showing how AI can rival human radiologists at MRI-based diagnosis of prostate cancer: http://tinyurl.com/prostate-ai.\n\n28. Stanford study showing how AI can best human pathologists at lung cancer diagnosis: http://tinyurl.com/lungcancer-ai.\n\n29. Investigation of the Therac-25 radiation therapy accidents: http://tinyurl.com/theracfailure.\n\n30. Report on lethal radiation overdoses in Panama caused by confusing user interface: http://tinyurl.com/cobalt60accident.\n\n31. Study of adverse events in robotic surgery: https://arxiv.org/abs/1507.03518.\n\n32. Article on number of deaths from bad hospital care: http://tinyurl.com/medaccidents.\n\n33. Yahoo set a new standard for “big hack” when announcing a billion(!) of their user accounts had been breached: https://www.wired.com/2016/12/yahoo-hack-billion-users/.\n\n34. New York Times article on acquittal and later conviction of KKK murderer: http://tinyurl.com/kkkacquittal.\n\n35. The Danziger et al. 2011 study (http://www.pnas.org/content/108/17/6889.full), arguing that hungry judges are harsher, was criticized as flawed by Keren Weinshall-Margela and John Shapard (http://www.pnas.org/content/108/42/E833.full), but Danziger et al. insist that their claims remain valid (http://www.pnas.org/content/108/42/E834.full).\n\n36. Pro Publica report on racial bias in recidivism-prediction software: http://tinyurl.com/robojudge.\n\n37. Use of fMRI and other brain-scanning techniques as evidence in trials is highly controversial, as is the reliability of such techniques, although many teams claim accuracies better than 90%: http://journal.frontiersin.org/article/10.3389/fpsyg.2015.00709/full.\n\n38. PBS made the movie The Man Who Saved the World about the incident where Vasili Arkhipov single-handedly prevented a Soviet nuclear strike: https://www.youtube.com/watch?v=4VPY2SgyG5w.\n\n39. The story of how Stanislav Petrov dismissed warnings of a U.S. nuclear attack as a false alarm was turned into the movie The Man Who Saved the World (not to be confused with the movie by the same title in the previous note), and Petrov was honored at the United Nations and given the World Citizen Award: https://www.youtube.com/watch?v=IncSjwWQHMo.\n\n40. Open letter from AI and robotics researchers about autonomous weapons: http://futureoflife.org/open-letter-autonomous-weapons/.\n\n41. A U.S. official seemingly wanting a military AI arms race: http://tinyurl.com/workquote.\n\n42. Study of wealth inequality in the United States since 1913: http://gabriel-zucman.eu/files/SaezZucman2015.pdf.\n\n43. Oxfam report on global wealth inequality: http://tinyurl.com/oxfam2017.\n\n44. For a great introduction to the hypothesis of technology-driven inequality, see Erik Brynjolfsson and Andrew McAfee, The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies (New York: Norton, 2014).\n\n45. Article in The Atlantic about falling wages for the less educated: http://tinyurl.com/wagedrop.\n\n46. The data plotted are taken from Facundo Alvaredo, Anthony B. Atkinson, Thomas Piketty, Emmanuel Saez and Gabriel Zucman, The World Wealth and Income Database (http://www.wid.world), including capital gains.\n\n47. Presentation by James Manyika showing income shifting from labor to capital: http://futureoflife.org/data/PDF/james\\_manyika.pdf.\n\n48. Forecasts about future job automation from Oxford University (http://tinyurl.com/automationoxford) and McKinsey (http://tinyurl.com/automationmckinsey).\n\n49. Video of robotic chef: https://www.youtube.com/watch?v=fE6i2OO6Y6s.\n\n50. Marin Soljačić explored these options at the 2016 workshop Computers Gone Wild: Impact and Implications of Developments in Artificial Intelligence on Society: http://futureoflife.org/2016/05/06/computers-gone-wild/.\n\n51. Andrew McAfee’s suggestions for how to create more good jobs: http://futureoflife.org/data/PDF/andrew\\_mcafee.pdf.\n\n52. In addition to many academic articles arguing that “this time is different” for technological unemployment, the video “Humans Need Not Apply” succinctly makes the same point: https://www.youtube.com/watch?v=7Pq-S557XQU.\n\n53. U.S. Bureau of Labor Statistics: http://www.bls.gov/cps/cpsaat11.htm.\n\n54. Argument that “this time is different” for technological unemployment: Federico Pistono, Robots Will Steal Your Job, but That’s OK (2012), http://robotswillstealyourjob.com.\n\n55. Changes in the U.S. horse population: http://tinyurl.com/horsedecline.\n\n56. Meta-analysis showing how unemployment affects well-being: Maike Luhmann et al., “Subjective Well-Being and Adaptation to Life Events: A Meta-Analysis,” Journal of Personality and Social Psychology 102, no. 3 (2012): 592; available online at https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3289759.\n\n57. Studies of what boosts people’s sense of well-being: Angela Duckworth, Tracy Steen and Martin Seligman, “Positive Psychology in Clinical Practice,” Annual Review of Clinical Psychology 1 (2005): 629–651, online at http://tinyurl.com/wellbeingduckworth. Weiting Ng and Ed Diener, “What Matters to the Rich and the Poor? Subjective Well-Being, Financial Satisfaction, and Postmaterialist Needs Across the World,” Journal of Personality and Social Psychology 107, no. 2 (2014): 326, online at http://psycnet.apa.org/journals/psp/107/2/326. Kirsten Weir, “More than Job Satisfaction,” Monitor on Psychology 44, no. 11 (December 2013), online at http://www.apa.org/monitor/2013/12/job-satisfaction.aspx.\n\n58. Multiplying together about 10¹¹ neurons, about 10⁴ connections per neuron and about one (10⁰) firing per neuron each second might suggest that about 10¹⁵ FLOPS (1 petaFLOPS) suffice to simulate a human brain, but there are many poorly understood complications, including the detailed timing of firings and the question of whether small parts of neurons and synapses need to be simulated too. IBM computer scientist Dharmendra Modha has estimated that 38 petaFLOPS are required (http://tinyurl.com/javln43), while neuroscientist Henry Markram has estimated that one needs about 1,000 petaFLOPS (http://tinyurl.com/6rpohqv). AI researchers Katja Grace and Paul Christiano have argued that the most costly aspect of brain simulation is not computation but communication, and that this too is a task in the ballpark of what the best current supercomputers can do: http://aiimpacts.org/about.\n\n59. For an interesting estimate of the computational power of the human brain: Hans Moravec “When Will Computer Hardware Match the Human Brain?” Journal of Evolution and Technology, vol. 1 (1998).\n\nChapter 4\n\n1. For a video of the first mechanical bird, see Markus Fischer, “A Robot That Flies like a Bird,” TED Talk, July 2011, at https://www.ted.com/talks/a\\_robot\\_that\\_flies\\_like\\_a\\_bird.\n\nChapter 5\n\n1. Ray Kurzweil, The Singularity Is Near (New York: Viking Press, 2005).\n\n2. Ben Goertzel’s “Nanny AI” scenario is described here: https://wiki.lesswrong.com/wiki/Nanny\\_AI.\n\n3. For a discussion about the relationship between machines and humans, and whether machines are our slaves, see Benjamin Wallace-Wells, “Boyhood,” New York magazine (May 20, 2015), online at http://tinyurl.com/aislaves.\n\n4. Mind crime is discussed in Nick Bostrom’s book Superintelligence and in more technical detail in this recent paper: Nick Bostrom, Allan Dafoe and Carrick Flynn, “Policy Desiderata in the Development of Machine Superintelligence” (2016), http://www.nickbostrom.com/papers/aipolicy.pdf.\n\n5. Matthew Schofield, “Memories of Stasi Color Germans’ View of U.S. Surveillance Programs,”McClatchy DC Bureau (June 26, 2013), online at http://www.mcclatchydc.com/news/nation-world/national/article24750439.html.\n\n6. For thought-provoking reflections on how people can be incentivized to create outcomes that nobody wants, I recommend “Meditations on Moloch,” http://slatestarcodex.com/2014/07/30/meditations-on-moloch.\n\n7. For an interactive timeline of close calls when nuclear war might have started by accident, see Future of Life Institute, “Accidental Nuclear War: A Timeline of Close Calls,” online at http://tinyurl.com/nukeoops.\n\n8. For compensation payments made to U.S. nuclear testing victims, see U.S. Department of Justice website, “Awards to Date 4/24/2015,” at https://www.justice.gov/civil/awards-date-04242015.\n\n9. Report of the Commission to Assess the Threat to the United States from Electromagnetic Pulse (EMP) Attack, April 2008, available online at http://www.empcommission.org/docs/A2473-EMP\\_Commission-7MB.pdf.\n\n10. Independent research by both U.S. and Soviet scientists alerted Reagan and Gorbachev to the risk of nuclear winter: P. J. Crutzen and J. W. Birks, “The Atmosphere After a Nuclear War: Twilight at Noon,” Ambio 11, no. 2/3 (1982): 114–125. R. P. Turco, O. B. Toon, T. P. Ackerman, J. B. Pollack and C. Sagan, “Nuclear Winter: Global Consequences of Multiple Nuclear Explosions,” Science 222 (1983): 1283–1292. V. V. Aleksandrov and G. L. Stenchikov, “On the Modeling of the Climatic Consequences of the Nuclear War,” Proceeding on Applied Mathematics (Moscow: Computing Centre of the USSR Academy of Sciences, 1983), 21. A. Robock, “Snow and Ice Feedbacks Prolong Effects of Nuclear Winter,” Nature 310 (1984): 667–670.\n\n11. Calculation of climate effects of global nuclear war: A. Robock, L. Oman and L. Stenchikov, “Nuclear Winter Revisited with a Modern Climate Model and Current Nuclear Arsenals: Still Catastrophic Consequences,” Journal of Geophysical Research 12 (2007): D13107.\n\nChapter 6\n\n1. For more information, see Anders Sandberg, “Dyson Sphere FAQ,” at http://www.aleph.se/nada/dysonFAQ.html.\n\n2. Freeman Dyson’s seminal paper on his eponymous spheres: Freeman Dyson, “Search for Artificial Stellar Sources of Infrared Radiation,” Science, vol. 131 (1959): 1667–1668.\n\n3. Louis Crane and Shawn Westmoreland explain their proposed black hole engine in “Are Black Hole Starships Possible?,” at http://arxiv.org/pdf/0908.1803.pdf.\n\n4. For a nice infographic from CERN summarizing known elementary particles, see http://tinyurl.com/cernparticles.\n\n5. This unique video of a non-nuclear Orion prototype illustrates the idea of nuclear-bomb-powered rocket propulsion: https://www.youtube.com/watch?v=E3Lxx2VAYi8.\n\n6. Here’s a pedagogical introduction to laser sailing: Robert L. Forward, “Roundtrip Interstellar Travel Using Laser-Pushed Lightsails,” Journal of Spacecraft and Rockets 21, no. 2 (March–April 1984), available online at http://www.lunarsail.com/LightSail/rit-1.pdf.\n\n7. Jay Olson analyzes cosmically expanding civilizations in “Homogeneous Cosmology with Aggressively Expanding Civilizations,” Classical and Quantum Gravity 32 (2015), available online at http://arxiv.org/abs/1411.4359.\n\n8. The first thorough scientific analysis of our far future: Freeman J. Dyson, “Time Without End: Physics and Biology in an Open Universe,” Reviews of Modern Physics 51, no. 3 (1979): 447, available online at http://blog.regehr.org/extra\\_files/dyson.pdf.\n\n9. Seth Lloyd’s above-mentioned formula told us that performing a computational operation during a time interval τ costs an energy E≥h/4τ, where h is Planck’s constant. If we want to get N operations done one after the other (in series) during a time T, then τ = T∕N, so E∕N ≥ hN∕4T, which tells us that we can perform N ≤ 2 √ET/h serial operations using energy E and time T. So both energy and time are resources that it helps having lots of. If you split your energy between n different parallel computations, they can run more slowly and efficiently, giving N ≤ 2 √ETn/h. Nick Bostrom estimates that simulating a 100-year human life requires about N = 10²⁷ operations.\n\n10. If you want to see a careful argument for why the origin of life may require a very rare fluke, placing our nearest neighbors over 10¹⁰⁰⁰ meters away, I recommend this video by Princeton physicist and astrobiologist Edwin Turner: “Improbable Life: An Unappealing but Plausible Scenario for Life’s Origin on Earth,” at https://www.youtube.com/watch?v=Bt6n6Tu1beg.\n\n11. Essay by Martin Rees on the search for extraterrestrial intelligence: https://www.edge.org/annual-question/2016/response/26665.\n\nChapter 7\n\n1. A popular discussion of Jeremy England’s work on “dissipation-driven adaptation” can be found in Natalie Wolchover, “A New Physics Theory of Life,” Scientific American (January 28, 2014), available online at https://www.scientificamerican.com/article/a-new-physics-theory-of-life/. Ilya Prigogine and Isabelle Stengers’s Order Out of Chaos: Man’s New Dialogue with Nature (New York: Bantam, 1984) lays many of the foundations for this.\n\n2. For more on feelings and their physiological roots: William James, Principles of Psychology (New York: Henry Holt & Co., 1890); Robert Ornstein, Evolution of Consciousness: The Origins of the Way We Think (New York: Simon & Schuster, 1992); António Damásio, Descartes’ Error: Emotion, Reason, and the Human Brain (New York: Penguin, 2005); and António Damásio, Self Comes to Mind: Constructing the Conscious Brain (New York: Vintage, 2012).\n\n3. Eliezer Yudkowsky has discussed aligning the goals of friendly AI not with our present goals, but with our coherent extrapolated volition (CEV). Loosely speaking this is defined as what an idealized version of us would want if we knew more, thought faster and were more the people we wished we were. Yudkowsky began criticizing CEV shortly after publishing it in 2004 (http://intelligence.org/files/CEV.pdf), both for being hard to implement and because it’s unclear whether it would converge to anything well-defined.\n\n4. In the inverse reinforcement-learning approach, a core idea is that the AI is trying to maximize not its own goal-satisfaction, but that of its human owner. It therefore has incentive to be cautious when it’s unclear about what its owner wants, and to do its best to find out. It should also be fine with its owner switching it off, since that would imply that it had misunderstood what its owner really wanted.\n\n5. Steve Omohundro’s paper on AI goal emergence, “The Basic AI Drives,” can be found online at http://tinyurl.com/omohundro2008. Originally published in Artificial General Intelligence 2008: Proceedings of the First AGI Conference, ed. Pei Wang, Ben Goertzel and Stan Franklin (Amsterdam: IOS, 2008), 483–492.\n\n6. A thought-provoking and controversial book on what happens when intelligence is used to blindly obey orders without questioning their ethical basis: Hannah Arendt, Eichmann in Jerusalem: A Report on the Banality of Evil (New York: Penguin, 1963). A related dilemma applies to a recent proposal by Eric Drexler (http://www.fhi.ox.ac.uk/reports/2015-3.pdf) to keep superintelligence under control by compartmentalizing it into simple pieces, none of which understand the whole picture. If this works, this could again provide an incredibly powerful tool without an intrinsic moral compass, implementing its owner’s every whim without any moral qualms. This would be reminiscent of a compartmentalized bureaucracy in a dystopian dictatorship: one part builds weapons without knowing how they’ll be used, another executes prisoners without knowing why they were convicted, and so on.\n\n7. A modern variant of the Golden Rule is John Rawls’ idea that a hypothetical situation is fair if nobody would change it without knowing in advance which person in it they’d be.\n\n8. For example, the IQs of many of Hitler’s top officials were found to be quite high. See “How Accurate Were the IQ Scores of the High-Ranking Third Reich Officials Tried at Nuremberg?,” Quora, available online at http://tinyurl.com/nurembergiq.\n\nChapter 8\n\n1. The entry on consciousness by Stuart Sutherland is quite amusing: Macmillan Dictionary of Psychology (London: Macmillan, 1989).\n\n2. Erwin Schrödinger, one of the founding fathers of quantum mechanics, made this remark in his book Mind and Matter while contemplating the past—and what would have happened if conscious life never evolved in the first place. On the other hand, the rise of AI raises the logical possibility that we may end up with a play for empty benches in the future.\n\n3. The Stanford Encyclopedia of Philosophy gives an extensive survey of different definitions and uses of the word “consciousness”: http://tinyurl.com/stanfordconsciousness.\n\n4. Yuval Noah Harari, Homo Deus: A Brief History of Tomorrow (New York: HarperCollins, 2017): 116.\n\n5. This is an excellent introduction to System 1 and System 2 from a pioneer: Daniel Kahneman, Thinking, Fast and Slow (New York: Farrar, Straus & Giroux, 2011).\n\n6. See Christof Koch, The Quest for Consciousness: A Neurobiological Approach (New York: W. H. Freeman, 2004).\n\n7. Perhaps we’re only aware of a tiny fraction (say 10–50 bits) of the information that enters our brain each second: K. Küpfmüller, 1962, “Nachrichtenverarbeitung im Menschen,” in Taschenbuch der Nachrichtenverarbeitung, ed. K. Steinbuch (Berlin: Springer-Verlag, 1962): 1481–1502. T. Nørretranders, The User Illusion: Cutting Consciousness Down to Size (New York: Viking, 1991).\n\n8. Michio Kaku, The Future of the Mind: The Scientific Quest to Understand, Enhance, and Empower the Mind (New York: Doubleday, 2014); Jeff Hawkins and Sandra Blakeslee, On Intelligence (New York: Times Books, 2007); Stanislas Dehaene, Michel Kerszberg and Jean-Pierre Changeux, “A Neuronal Model of a Global Workspace in Effortful Cognitive Tasks,” Proceedings of the National Academy of Sciences 95 (1998): 14529–14534.\n\n9. Video celebrating Penfield’s famous “I can smell burnt toast” experiment: https://www.youtube.com/watch?v=mSN86kphL68. Sensorimotor cortex details: Elaine Marieb and Katja Hoehn, Anatomy & Physiology, 3rd ed. (Upper Saddle River, NJ: Pearson, 2008), 391–395.\n\n10. The study of neural correlates of consciousness (NCCs) has become quite mainstream in the neuroscience community in recent years—see, e.g., Geraint Rees, Gabriel Kreiman, and Christof Koch, “Neural Correlates of Consciousness in Humans,” Nature Reviews Neuroscience 3 (2002): 261–270, and Thomas Metzinger, Neural Correlates of Consciousness: Empirical and Conceptual Questions (Cambridge, MA: MIT Press, 2000).\n\n11. How continuous flash suppression works: Christof Koch, The Quest for Consciousness: A Neurobiological Approach (New York: W. H. Freeman, 2004); Christof Koch and Naotsugu Tsuchiya, “Continuous Flash Suppression Reduces Negative Afterimages,” Nature Neuroscience 8 (2005): 1096–1101.\n\n12. Christof Koch, Marcello Massimini, Melanie Boly and Giulio Tononi, “Neural Correlates of Consciousness: Progress and Problems,” Nature Reviews Neuroscience 17 (2016): 307.\n\n13. See Koch, The Quest for Consciousness, p. 260, and further discussion in the Stanford Encyclopedia of Philosophy, http://tinyurl.com/consciousnessdelay.\n\n14. On synchronization of conscious perception: David Eagleman, The Brain: The Story of You (New York: Pantheon, 2015), and Stanford Encyclopedia of Philosophy, http://tinyurl.com/consciousnesssync.\n\n15. Benjamin Libet, Mind Time: The Temporal Factor in Consciousness (Cambridge, MA: Harvard University Press, 2004); Chun Siong Soon, Marcel Brass, Hans-Jochen Heinze and John-Dylan Haynes, “Unconscious Determinants of Free Decisions in the Human Brain,” Nature Neuroscience 11 (2008): 543–545, online at http://www.nature.com/neuro/journal/v11/n5/full/nn.2112.html.\n\n16. Examples of recent theoretical approaches to consciousness:\n\n- Daniel Dennett, Consciousness Explained (Back Bay Books, 1992)\n\n- Bernard Baars, In the Theater of Consciousness: The Workspace of the Mind (New York: Oxford University Press, 2001)\n\n- Christof Koch, The Quest for Consciousness: A Neurobiological Approach (New York: W. H. Freeman, 2004)\n\n- Gerald Edelman and Giulio Tononi, A Universe of Consciousness: How Matter Becomes Imagination (New York: Hachette, 2008)\n\n- António Damásio, Self Comes to Mind: Constructing the Conscious Brain (New York: Vintage, 2012)\n\n- Stanislas Dehaene, Consciousness and the Brain: Deciphering How the Brain Codes Our Thoughts (New York: Viking, 2014)\n\n- Stanislas Dehaene, Michel Kerszberg and Jean-Pierre Changeux, “A Neuronal Model of a Global Workspace in Effortful Cognitive Tasks,” Proceedings of the National Academy of Sciences 95 (1998): 14529–14534\n\n- Stanislas Dehaene, Lucie Charles, Jean-Rémi King and Sébastien Marti, “Toward a Computational Theory of Conscious Processing,” Current Opinion in Neurobiology 25 (2014): 760–784\n\n17. Thorough discussion of different uses of the term “emergence” in physics and philosophy by David Chalmers: http://cse3521.artifice.cc/Chalmers-Emergence.pdf.\n\n18. Me arguing that consciousness is the way information feels when being processed in certain complex ways: https://arxiv.org/abs/physics/0510188, https://arxiv.org/abs/0704.0646, Max Tegmark, Our Mathematical Universe (New York: Knopf, 2014). David Chalmers expresses a related sentiment in his 1996 book The Conscious Mind: “Experience is information from the inside; physics is information from the outside.”\n\n19. Adenauer Casali et al., “A Theoretically Based Index of Consciousness Independent of Sensory Processing and Behavior,” Science Translational Medicine 5 (2013): 198ra105, online at http://tinyurl.com/zapzip.\n\n20. Integrated information theory doesn’t work for continuous systems:\n\n- https://arxiv.org/abs/1401.1219\n\n- http://journal.frontiersin.org/article/10.3389/fpsyg.2014.00063/full\n\n- https://arxiv.org/abs/1601.02626\n\n21. Interview with Clive Wearing, whose short-term memory is only about 30 seconds: https://www.youtube.com/watch?v=WmzU47i2xgw.\n\n22. Scott Aaronson IIT critique: http://www.scottaaronson.com/blog/?p=1799.\n\n23. Cerrullo IIT critique, arguing that integration isn’t a sufficient condition for consciousness: http://tinyurl.com/cerrullocritique.\n\n24. IIT prediction that simulated humans will be zombies: http://rstb.royalsocietypublishing.org/content/370/1668/20140167.\n\n25. Shanahan critique of IIT: https://arxiv.org/pdf/1504.05696.pdf.\n\n26. Blindsight: http://tinyurl.com/blindsight-paper.\n\n27. Perhaps we’re only aware of a tiny fraction (say 10–50 bits) of the information that enters our brain each second: Küpfmüller, “Nachrichtenverarbeitung im Menschen”; Nørretranders, The User Illusion.\n\n28. The case for and against “consciousness without access”: Victor Lamme, “How Neuroscience Will Change Our View on Consciousness,” Cognitive Neuroscience (2010): 204–220, online at http://www.tandfonline.com/doi/abs/10.1080/17588921003731586.\n\n29. “Selective Attention Test,” at https://www.youtube.com/watch?v=vJG698U2Mvo.\n\n30. See Lamme, “How Neuroscience Will Change Our View on Consciousness,” n. 28.\n\n31. This and other related issues are discussed in detail in Daniel Dennett’s book Consciousness Explained.\n\n32. See Kahneman, Thinking, Fast and Slow, cited in n. 5.\n\n33. The Stanford Encyclopedia of Philosophy reviews the free will controversy: https://plato.stanford.edu/entries/freewill.\n\n34. Video of Seth Lloyd explaining why an AI will feel like it has free will: https://www.youtube.com/watch?v=Epj3DF8jDWk.\n\n35. See Steven Weinberg, Dreams of a Final Theory: The Search for the Fundamental Laws of Nature (New York: Pantheon, 1992).\n\n36. The first thorough scientific analysis of our far future: Freeman J. Dyson, “Time Without End: Physics and Biology in an Open Universe,” Reviews of Modern Physics 51, no. 3 (1979): 447, available online at http://blog.regehr.org/extra\\_files/dyson.pdf.\n\nEpilogue\n\n1. The open letter (http://futureoflife.org/ai-open-letter) that emerged from the Puerto Rico conference argued that research on how to make AI systems robust and beneficial is both important and timely, and that there are concrete research directions that can be pursued today, as exemplified in this research-priorities document: http://futureoflife.org/data/documents/research\\_priorities.pdf.\n\n2. My video interview with Elon Musk about AI safety can be found on YouTube at https://www.youtube.com/watch?v=rBw0eoZTY-g.\n\n3. Here’s a nice video compilation of almost all SpaceX rocket landing attempts, culminating with the first successful ocean landing: https://www.youtube.com/watch?v=AllaFzIPaG4.\n\n4. Elon Musk tweets about our AI-safety grant competition: https://twitter.com/elonmusk/status/555743387056226304.\n\n5. Elon Musk tweets about our AI-safety-endorsing open letter: https://twitter.com/elonmusk/status/554320532133650432.\n\n6. Erik Sofge in “An Open Letter to Everyone Tricked into Fearing Artificial Intelligence” (Popular Science, January 14, 2015) pokes fun at the scaremongering news coverage of our open letter: http://www.popsci.com/open-letter-everyone-tricked-fearing-ai.\n\n7. Elon Musk tweets about his big donation to the Future of Life Institute and the world of AI-safety researchers: https://twitter.com/elonmusk/status/555743387056226304.\n\n8. For more about the Partnership on AI to benefit people and society, see their website: https://www.partnershiponai.org.\n\n9. Some examples of recent reports expressing opinions about AI: One Hundred Year Study on Artificial Intelligence, Report of the 2015 Study Panel, “Artificial Intelligence and Life in 2030” (September 2016), at http://tinyurl.com/stanfordai; White House report on the future of AI: http://tinyurl.com/obamaAIreport; White House report on AI and jobs: http://tinyurl.com/AIjobsreport; IEEE report on AI and human well-being, “Ethically Aligned Design, Version 1” (December 13, 2016), at http://standards.ieee.org/develop/indconn/ec/ead\\_v1.pdf; road map for U.S. Robotics: http://tinyurl.com/roboticsmap.\n\n10. Among the principles that didn’t make the final cut, one of my favorites was this one: “Consciousness caution: There being no consensus, we should avoid strong assumptions as to whether or not advanced AI would possess or require consciousness or feelings.” It went through many iterations, and in the last one, the controversial word “consciousness” was replaced by “subjective experience”—but this principle nonetheless got only 88% approval, just barely falling short of the 90% cutoff.\n\n11. Discussion panel on superintelligence with Elon Musk and other great minds: http://tinyurl.com/asilomarAI.\n\n[Penguin Random House Back Ad logo][Penguin Random House Next Reads logo]\n\nWhat’s next on your reading list?\n\nDiscover your next\ngreat read!\n\n------------------------------------------------------------------------\n\nGet personalized book picks and up-to-date news about this author.\n\nSign up now.", "date_published": "2017-08-23T00:00:00Z", "authors": ["Max Teqmark"], "summaries": [], "initial_source": "ebook", "source_filetype": "epub"}
