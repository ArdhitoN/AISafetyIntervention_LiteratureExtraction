{"id": "025ec6c77a59b3363162576fa55a4fd7", "title": "Modeling Agents with Probabilistic Programs", "url": "https://agentmodels.org/chapters/3-agents-as-programs.html", "source": "agentmodels", "source_type": "markdown", "text": "---\nlayout: chapter\ntitle: \"Agents as probabilistic programs\"\ndescription: \"One-shot decision problems, expected utility, softmax choice and Monty Hall.\" \nis_section: true\n---\n\n## Introduction\n\nOur goal is to implement agents that compute rational *policies*. Policies are *plans* for achieving good outcomes in environments where:\n\n- The agent makes a *sequence* of *distinct* choices, rather than choosing once.\n\n- The environment is *stochastic* (or \"random\").\n\n- Some features of the environment are initially *unknown* to the agent. (So the agent may choose to gain information in order to improve future decisions.)\n\nThis section begins with agents that solve the very simplest decision problems. These are trivial *one-shot* problems, where the agent selects a single action (not a sequence of actions). We use WebPPL to solve these problems in order to illustrate the core concepts that are necessary for the more complex problems in later chapters.\n\n<a id=\"planning_as\"></a>\n\n## One-shot decisions in a deterministic world\n\nIn a *one-shot decision problem* an agent makes a single choice between a set of *actions*, each of which has potentially distinct *consequences*. A rational agent chooses the action that is best in terms of his or her own preferences. Often, this depends not on the *action* itself being preferred, but only on its *consequences*. \n\nFor example, suppose Tom is choosing between restaurants and all he cares about is eating pizza. There's an Italian restaurant and a French restaurant. Tom would choose the French restaurant if it offered pizza. Since it does *not* offer pizza, Tom will choose the Italian.\n\nTom selects an action $$a \\in A$$ from the set of all actions. The actions in this case are {\"eat at Italian restaurant\", \"eat at French restaurant\"}. The consequences of an action are represented by a transition function $$T \\colon S \\times A \\to S$$ from state-action pairs to states. In our example, the relevant *state* is whether or not Tom eats pizza. Tom's preferences are represented by a real-valued utility function $$U \\colon S \\to \\mathbb{R}$$, which indicates the relative goodness of each state. \n\nTom's *decision rule* is to take action $$a$$ that maximizes utility, i.e., the action\n\n$$\n{\\arg \\max}_{a \\in A} U(T(s,a))\n$$\n\nIn WebPPL, we can implement this utility-maximizing agent as a function `maxAgent` that takes a state $$s \\in S$$ as input and returns an action. For Tom's choice between restaurants, we assume that the agent starts off in a state `\"initialState\"`, denoting whatever Tom does before going off to eat. The program directly translates the decision rule above using the higher-order function `argMax`.\n<!-- TODO fix argmax -->\n\n~~~~\n///fold: argMax\nvar argMax = function(f, ar){\n  return maxWith(f, ar)[0]\n};\n///\n// Choose to eat at the Italian or French restaurants\nvar actions = ['italian', 'french'];\n\nvar transition = function(state, action) {\n  if (action === 'italian') {\n    return 'pizza';\n  } else {\n    return 'steak frites';\n  }\n};\n\nvar utility = function(state) {\n  if (state === 'pizza') {\n    return 10;\n  } else {\n    return 0;\n  }\n};\n\nvar maxAgent = function(state) {\n  return argMax(\n    function(action) {\n      return utility(transition(state, action));\n    },\n    actions);\n};\n\nprint('Choice in initial state: ' + maxAgent('initialState'));\n~~~~\n\n>**Exercise**: Which parts of the code can you change in order to make the agent choose the French restaurant?\n\nThere is an alternative way to compute the optimal action for this problem. The idea is to treat choosing an action as an *inference* problem. The previous chapter showed how we can *infer* the probability that a coin landed Heads from the observation that two of three coins were Heads. \n\n~~~~\nvar twoHeads = Infer({ \n  model() {\n    var a = flip(0.5);\n    var b = flip(0.5);\n    var c = flip(0.5);\n    condition(a + b + c === 2);\n    return a;\n  }\n});\n\nviz(twoHeads);\n~~~~\n\n\nThe same inference machinery can compute the optimal action in Tom's decision problem. We sample random actions with `uniformDraw` and condition on the preferred outcome happening. Intuitively, we imagine observing the consequence we prefer (e.g. pizza) and then *infer* from this the action that caused this consequence. <!-- address evidential vs causal decision theory? -->\n\nThis idea is known as \"planning as inference\" refp:botvinick2012planning. It also resembles the idea of \"backwards chaining\" in logical inference and planning. The `inferenceAgent` solves the same problem as `maxAgent`, but uses planning as inference: \n\n~~~~\nvar actions = ['italian', 'french'];\n\nvar transition = function(state, action) {\n  if (action === 'italian') {\n    return 'pizza';\n  } else {\n    return 'steak frites';\n  }\n};\n\nvar inferenceAgent = function(state) {\n  return Infer({ \n    model() {\n      var action = uniformDraw(actions);\n      condition(transition(state, action) === 'pizza');\n      return action;\n    }\n  });\n}\n\nviz(inferenceAgent(\"initialState\"));\n~~~~\n\n>**Exercise**: Change the agent's goals so that they choose the French restaurant.\n\n## One-shot decisions in a stochastic world\n\nIn the previous example, the transition function from state-action pairs to states was *deterministic* and so described a deterministic world or environment. Moreover, the agent's actions were deterministic; Tom always chose the best action (\"Italian\"). In contrast, many examples in this tutorial will involve a *stochastic* world and a noisy \"soft-max\" agent. \n\nImagine that Tom is choosing between restaurants again. This time, Tom's preferences are about the overall quality of the meal. A meal can be \"bad\", \"good\" or \"spectacular\" and each restaurant has good nights and bad nights. The transition function now has type signature $$ T\\colon S \\times A \\to \\Delta S $$, where $$\\Delta S$$ represents a distribution over states. Tom's decision rule is now to take the action $$a \\in A$$ that has the highest *average* or *expected* utility, with the expectation $$\\mathbb{E}$$ taken over the probability of different successor states $$s' \\sim T(s,a)$$:\n\n$$\n\\max_{a \\in A} \\mathbb{E}( U(T(s,a)) )\n$$\n\nTo represent this in WebPPL, we extend `maxAgent` using the `expectation` function, which maps a distribution with finite support to its (real-valued) expectation:\n\n~~~~\n///fold: argMax\nvar argMax = function(f, ar){\n  return maxWith(f, ar)[0]\n};\n///\n\nvar actions = ['italian', 'french'];\n\nvar transition = function(state, action) {\n  var nextStates = ['bad', 'good', 'spectacular'];\n  var nextProbs = (action === 'italian') ? [0.2, 0.6, 0.2] : [0.05, 0.9, 0.05];\n  return categorical(nextProbs, nextStates);\n};\n\nvar utility = function(state) {\n  var table = { \n    bad: -10, \n    good: 6, \n    spectacular: 8 \n  };\n  return table[state];\n};\n\nvar maxEUAgent = function(state) {\n  var expectedUtility = function(action) {\n    return expectation(Infer({ \n      model() {\n        return utility(transition(state, action));\n      }\n    }));\n  };\n  return argMax(expectedUtility, actions);\n};\n\nmaxEUAgent('initialState');\n~~~~\n\n>**Exercise**: Adjust the transition probabilities such that the agent chooses the Italian Restaurant.\n\nThe `inferenceAgent`, which uses the planning-as-inference idiom, can also be extended using `expectation`. Previously, the agent's action was conditioned on leading to the best consequence (\"pizza\"). This time, Tom is not aiming to choose the action most likely to have the best outcome. Instead, he wants the action with better outcomes on average. This can be represented in `inferenceAgent` by switching from a `condition` statement to a `factor` statement. The `condition` statement expresses a \"hard\" constraint on actions: actions that fail the condition are completely ruled out. The `factor` statement, by contrast, expresses a \"soft\" condition. Technically, `factor(x)` adds `x` to the unnormalized log-probability of the program execution within which it occurs.\n\nTo illustrate `factor`, consider the following variant of the `twoHeads` example above. Instead of placing a hard constraint on the total number of Heads outcomes, we give each setting of `a`, `b` and `c` a *score* based on the total number of heads. The score is highest when all three coins are Heads, but even the \"all tails\" outcomes is not ruled out completely.\n\n~~~~\nvar softHeads = Infer({ \n  model() {\n    var a = flip(0.5);\n    var b = flip(0.5);\n    var c = flip(0.5);\n    factor(a + b + c);\n    return a;\n  }\n});\n\nviz(softHeads);\n~~~~\n\nAs another example, consider the following short program:\n\n~~~~\nvar dist = Infer({ \n  model() {\n    var n = uniformDraw([0, 1, 2]);\n    factor(n * n);\n    return n;\n  }\n});\n\nviz(dist);\n~~~~\n\nWithout the `factor` statement, each value of the variable `n` has equal probability. Adding the `factor` statements adds `n*n` to the log-score of each value. To get the new probabilities induced by the `factor` statement we compute the normalizing constant given these log-scores. The resulting probability $$P(y=2)$$ is:\n\n$$\nP(y=2) = \\frac {e^{2 \\cdot 2}} { (e^{0 \\cdot 0} + e^{1 \\cdot 1} + e^{2 \\cdot 2}) }\n$$\n\nReturning to our implementation as planning-as-inference for maximizing *expected* utility, we use a `factor` statement to implement soft conditioning:\n\n~~~~\nvar actions = ['italian', 'french'];\n\nvar transition = function(state, action) {\n  var nextStates = ['bad', 'good', 'spectacular'];\n  var nextProbs = (action === 'italian') ? [0.2, 0.6, 0.2] : [0.05, 0.9, 0.05];\n  return categorical(nextProbs, nextStates);\n};\n\nvar utility = function(state) {\n  var table = { \n    bad: -10, \n    good: 6, \n    spectacular: 8 \n  };\n  return table[state];\n};\n\nvar alpha = 1;\n\nvar softMaxAgent = function(state) {\n  return Infer({ \n    model() {\n\n      var action = uniformDraw(actions);\n\n      var expectedUtility = function(action) {\n        return expectation(Infer({ \n          model() {\n            return utility(transition(state, action));\n          }\n        }));\n      };\n      \n      factor(alpha * expectedUtility(action));\n      \n      return action;\n    }\n  });\n};\n\nviz(softMaxAgent('initialState'));\n~~~~\n\nThe `softMaxAgent` differs in two ways from the `maxEUAgent` above. First, it uses the planning-as-inference idiom. Second, it does not deterministically choose the action with maximal expected utility. Instead, it implements *soft* maximization, selecting actions with a probability that depends on their expected utility. Formally, let the agent's probability of choosing an action be $$C(a;s)$$ for $$a \\in A$$ when in state $$s \\in S$$. Then the *softmax* decision rule is:\n\n$$\nC(a; s) \\propto e^{\\alpha \\mathbb{E}(U(T(s,a))) }\n$$\n\nThe noise parameter $$\\alpha$$ modulates between random choice $$(\\alpha=0)$$ and the perfect maximization $$(\\alpha = \\infty)$$ of the `maxEUAgent`.\n\nSince rational agents will *always* choose the best action, why consider softmax agents? One of the goals of this tutorial is to infer the preferences of agents (e.g. human beings) from their choices. People do not always choose the normatively rational actions. The softmax agent provides a simple, analytically tractable model of sub-optimal choice[^softmax], which has been tested empirically on human action selection refp:luce2005individual. Moreover, it has been used extensively in Inverse Reinforcement Learning as a model of human errors refp:kim2014inverse, refp:zheng2014robust. For this reason, we employ the softmax model throughout this tutorial. When modeling an agent assumed to be optimal, the noise parameter $$\\alpha$$ can be set to a large value. <!-- [TODO: Alternatively, agent could output dist.MAP().val instead of dist.] -->\n\n[^softmax]: A softmax agent's choice of action is a differentiable function of their utilities. This differentiability makes possible certain techniques for inferring utilities from choices.\n\n>**Exercise**: Monty Hall. In this exercise inspired by [ProbMods](https://probmods.org/chapters/06-inference-about-inference.html), we will approach the classical statistical puzzle from the perspective of optimal decision-making. Here is a statement of the problem:\n\n> *Alice is on a game show and she’s given the choice of three doors. Behind one door is a car; behind the others, goats. She picks door 1. The host, Monty, knows what’s behind the doors and opens another door, say No. 3, revealing a goat. He then asks Alice if she wants to switch doors. Should she switch?*\n\n> Use the tools introduced above to determine the answer. Here is some code to get you started:\n\n~~~~\n// Remove each element in array ys from array xs\nvar remove = function(xs, ys) {\n  return _.without.apply(null, [xs].concat(ys));\n};\n\nvar doors = [1, 2, 3];\n\n// Monty chooses a door that is neither Alice's door\n// nor the prize door\nvar monty = function(aliceDoor, prizeDoor) {\n  return Infer({ \n    model() {\n      var door = uniformDraw(doors);\n      // ???\n      return door;\n    }});\n};\n\n\nvar actions = ['switch', 'stay'];\n\n// If Alice switches, she randomly chooses a door that is\n// neither the one Monty showed nor her previous door\nvar transition = function(state, action) {\n  if (action === 'switch') {\n    return {\n      prizeDoor: state.prizeDoor,\n      montyDoor: state.montyDoor,\n      aliceDoor: // ???\n    };\n  } else {\n    return state;\n  }\n};\n\n// Utility is high (say 10) if Alice's door matches the\n// prize door, 0 otherwise.\nvar utility = function(state) {\n  // ???\n};\n\nvar sampleState = function() {\n  var aliceDoor = uniformDraw(doors);\n  var prizeDoor = uniformDraw(doors);\n  return {\n    aliceDoor,\n    prizeDoor,\n    montyDoor: sample(monty(aliceDoor, prizeDoor))\n  }\n}\n\nvar agent = function() {  \n  var action = uniformDraw(actions);\n  var expectedUtility = function(action){    \n    return expectation(Infer({ \n      model() {\n        var state = sampleState();\n        return utility(transition(state, action));\n      }}));\n  };\n  factor(expectedUtility(action));\n  return { action };\n};\n\nviz(Infer({ model: agent }));\n~~~~\n\n### Moving to complex decision problems\n\nThis chapter has introduced some of the core concepts that we will need for this tutorial, including *expected utility*, *(stochastic) transition functions*, *soft conditioning* and *softmax decision making*. These concepts would also appear in standard treatments of rational planning and reinforcement learning refp:russell1995modern. The actual decision problems in this chapter are so trivial that our notation and programs are overkill.\n\nThe [next chapter](/chapters/3a-mdp.html) introduces *sequential* decisions problems. These problems are more complex and interesting, and will require the machinery we have introduced here. \n\n<br>\n\n### Footnotes\n", "date_published": "2018-06-21T16:25:20Z", "authors": ["Owain Evans", "Andreas Stuhlmüller", "John Salvatier", "Daniel Filan"], "summaries": [], "filename": "3-agents-as-programs.md"}
{"id": "ea441710b479ba9d9171a1b2273a2aca", "title": "Modeling Agents with Probabilistic Programs", "url": "https://agentmodels.org/chapters/5-biases-intro.html", "source": "agentmodels", "source_type": "markdown", "text": "---\nlayout: chapter\ntitle: \"Cognitive biases and bounded rationality\"\ndescription: Soft-max noise, limited memory, heuristics and biases, motivation from intractability of POMDPs.\nis_section: true\n---\n\n\n### Optimality and modeling human actions\n\nWe've mentioned two uses for models of sequential decision making:\n\nUse (1): **Solve practical decision problems** (preferably with a fast algorithm that performs optimally)\n\nUse (2): **Learn the preferences and beliefs of humans** (e.g. to predict future behavior or to provide recommendations/advice)\n\nThe table below provides more detail about these two uses[^table]. The first chapters of the book focused on Use (1) and described agent models for solving MDPs and POMDPs optimally. Chapter IV (\"[Reasoning about Agents](/chapters/4-reasoning-about-agents.html)\"), by contrast, was on Use (2), employing agent models as *generative models* of human behavior which are inverted to learn human preferences. \n\nThe present chapter discusses the limitations of using optimal agent modes as generative models for Use (2). We argue that developing models of *biased* or *bounded* decision making can address these limitations. \n\n<a href=\"/assets/img/table_chapter5_intro.png\"><img src=\"/assets/img/table_chapter5_intro.png\" alt=\"table\" style=\"width: 650px;\"/></a>\n\n>**Table 1:** Two uses for formal models of sequential decision making. The heading \"Optimality\" means \"Are optimal models of decision making used?\".\n<br>\n\n[^table]: Note that there are important interactions between Use (1) and Use (2). A challenge with Use (1) is that it's often hard to write down an appropriate utility function to optimize. The ideal utility function is one that reflects actual human preferences. So by solving (2) we can solve one of the \"key tasks\" in (1). This is exactly the approach taken in various applications of IRL. See work on Apprenticeship Learning refp:abbeel2004apprenticeship. \n\n<!-- TABLE. TODO: find nice html/markdown rendering:\n\nGoal|Key tasks|Optimality?|Sub-fields|Fields\nSolve practical decision problems|1. Define appropriate utility function and decision problem.\n \n2. Solve optimization problem|If it’s tractable|RL, Game and Decision Theory, Experimental Design|ML/Statistics, Operations Research, Economics (normative)\nLearn the preferences and beliefs of humans|1. Collect data by observation/experiment.\n\n2. Infer parameters and predict future behavior|If it fits human data|IRL, Econometrics (Structural Estimation), Inverse Planning|ML, Economics (positive),\nPsychology, Neuroscience\n-->\n\n\n\n### Random vs. Systematic Errors\nThe agent models presented in previous chapters are models of *optimal* performance on (PO)MDPs. So if humans deviate from optimality on some (PO)MDP then these models won't predict human behavior well. It's important to recognize the flexibility of the optimal models. The agent can have any utility function and any initial belief distribution. We saw in the previous chapters that apparently irrational behavior can sometimes be explained in terms of inaccurate prior beliefs.\n\nYet certain kinds of human behavior resist explanation in terms of false beliefs or unusual preferences. Consider the following:\n\n>**The Smoker** <br> Fred smokes cigarettes every day. He has tried to quit multiple times and still wants to quit. He is fully informed about the health effects of smoking and has learned from experience about the cravings that accompany attempts to quit. \n\nIt's hard to explain such persistent smoking in terms of inaccurate beliefs[^beliefs].\n\n[^beliefs]: One could argue that Fred has a temporary belief that smoking is high utility which causes him to smoke. This belief subsides after smoking a cigarette and is replaced with regret. To explain this in terms of a POMDP agent, there has to be an *observation* that triggers the belief-change via Bayesian updating. But what is this observation? Fred has *cravings*, but these cravings alter Fred's desires or wants, rather than being observational evidence about the empirical world. \n\nA common way of modeling with deviations from optimal behavior is to use softmax noise refp:kim2014inverse and refp:zheng2014robust. Yet the softmax model has limited expressiveness. It's a model of *random* deviations from optimal behavior. Models of random error might be a good fit for certain motor or perceptual tasks (e.g. throwing a ball or locating the source of a distant sound). But the smoking example suggests that humans deviate from optimality *systematically*. That is, when not behaving optimally, humans actions remain *predictable* and big deviations from optimality in one domain do not imply highly random behavior in all domains. \n\nHere are some examples of systematic deviations from optimal action:\n<br>\n\n>**Systematic deviations from optimal action**\n\n- Smoking every week (i.e. systematically) while simultaneously trying to quit (e.g. by using patches and throwing away cigarettes).\n\n- Finishing assignments just before the deadline, while always planning to finish them as early as possible. \n\n- Forgetting random strings of letters or numbers (e.g. passwords or ID numbers) -- assuming they weren't explicitly memorized[^strings].\n\n- Making mistakes on arithmetic problems[^math] (e.g. long division).\n\n[^strings]: With effort people can memorize these strings and keep them in memory for long periods. The claim is that if people do not make an attempt to memorize a random string, they will systematically forget the string within a short duration. This can't be easily explained on a POMDP model, where the agent has perfect memory.\n\n[^math]: People learn the algorithm for long division but still make mistakes -- even when stakes are relatively high (e.g. important school exams). While humans vary in their math skill, all humans have severe limitations (compared to computers) at doing arithmetic. See refp:dehaene2011number for various robust, systematic limitations in human numerical cognition. \n\nThese examples suggest that human behavior in everyday decision problems will not be easily captured by assuming softmax optimality. In the next sections, we divide these systematics deviations from optimality into *cognitive biases* and *cognitive bounds*. After explaining each category, we discuss their relevance to learning the preferences of agents. \n\n\n### Human deviations from optimal action: Cognitive Bounds\n\nHumans perform sub-optimally on some MDPs and POMDPs due to basic computational constraints. Such constraints have been investigated in work on *bounded rationality* and *bounded optimality* refp:gershman2015computational. A simple example was mentioned above: people cannot quickly memorize random strings (even if the stakes are high). Similarly, consider the real-life version of our Restaurant Choice example. If you walk around a big city for the first time, you will forget the location of most of the restaurants you see on the way. If you try a few days later to find a restaurant, you are likely to take an inefficient route. This contrasts with the optimal POMDP-solving agent who never forgets anything.\n\nLimitations in memory are hardly unique to humans. For any autonomous robot, there is some number of random bits that it cannot quickly place in permanent storage. In addition to constraints on memory, humans and machines have constraints on time. The simplest POMDPs, such as Bandit problems, are intractable: the time needed to solve them will grow exponentially (or worse) in the problem size refp:cassandra1994acting,  refp:madani1999undecidability. The issue is that optimal planning requires taking into account all possible sequences of actions and states. These explode in number as the number of states, actions, and possible sequences of observations grows[^grows].\n\n[^grows]: Dynamic programming helps but does not tame the beast. There are many POMDPs that are small enough to be easily described (i.e. they don't have a very long problem description) but which we can't solve optimally in practice.\n\nSo for any agent with limited time there will be POMDPs that they cannot solve exactly. It's plausible that humans often encounter POMDPs of this kind. For example, in lab experiments humans make systematic errors in small POMDPs that are easy to solve with computers refp:zhang2013forgetful and refp:doshi2011comparison. Real-world tasks with the structure of POMDPs, such as choosing how to invest resources or deciding on a sequence of scientific experiments, are much more complex and so presumably can't be solved by humans exactly.\n\n### Human deviations from optimal action: Cognitive Biases\n\nCognitive bounds of time and space (for memory) mean that any realistic agent will perform sub-optimally on some problems. By contrast, the term \"cognitive biases\" is usually applied to errors that are idiosyncratic to humans and would not arise in AI systems[^biases]. There is a large literature on cognitive biases in psychology and behavioral economics refp:kahneman2011thinking, refp:kahneman1984choices. One relevant example is the cluster of biases summarized by *Prospect Theory* refp:kahneman1979prospect. In one-shot choices between \"lotteries\", people are subject to framing effects (e.g. Loss Aversion) and to erroneous computation of expected utility[^prospect]. Another important bias is *time inconsistency*. This bias has been used to explain addiction, procrastination, impulsive behavior and the use of pre-commitment devices. The next chapter describes and implements time-inconsistent agents. \n\n[^biases]: We do not presuppose a well substantiated scientific distinction between cognitive bounds and biases. Many have argued that biases result from heuristics and that the heuristics are a fine-tuned shortcut for dealing with cognitive bounds. For our purposes, the main distinction is between intractable decision problems (such that any agent will fail on large enough instances of the problem) and decision problems that appear trivial for simple computational systems but hard for some proportion of humans. For example, time-inconsistent behavior appears easy to avoid for computational systems but hard to avoid for humans. \n\n[^prospect]: The problems descriptions are extremely simple. So this doesn't look like an issue of bounds on time or memory forcing people to use a heuristic approach. \n\n\n### Learning preferences from bounded and biased agents\nWe've asserted that humans have cognitive biases and bounds. These lead to systemtic deviations from optimal performance on (PO)MDP decision problems. As a result, the softmax-optimal agent models from previous chapters will not always be good generative models for human behavior. To learn human beliefs and preferences when such deviations from optimality are present, we extend and elaborate our (PO)MDP agent models to capture these deviations. The next chapter implements time-inconsistent agents via hyperbolic discounting. The subsequent chapter implements \"greedy\" or \"myopic\" planning, which is a general strategy for reducing time- and space-complexity. In the final chapter of this section, we show (a) that assuming humans are optimal can lead to mistaken inferences in some decision problems, and (b) that our extended generative models can avoid these mistakes.\n\nNext chapter: [Time inconsistency I](/chapters/5a-time-inconsistency.html)\n\n<br>\n\n### Footnotes\n", "date_published": "2017-03-19T18:46:48Z", "authors": ["Owain Evans", "Andreas Stuhlmüller", "John Salvatier", "Daniel Filan"], "summaries": [], "filename": "5-biases-intro.md"}
{"id": "de7fd0b4b054937b7a2d8856f11ee694", "title": "Modeling Agents with Probabilistic Programs", "url": "https://agentmodels.org/chapters/7-multi-agent.html", "source": "agentmodels", "source_type": "markdown", "text": "---\nlayout: chapter\ntitle: Multi-agent models\ndescription: Schelling coordination games, tic-tac-toe, and a simple natural-language example.\nis_section: true\n---\n\nIn this chapter, we will look at models that involve multiple agents reasoning about each other.\nThis chapter is based on reft:stuhlmueller2013reasoning.\n\n## Schelling coordination games\n\nWe start with a simple [Schelling coordination game](http://lesswrong.com/lw/dc7/nash_equilibria_and_schelling_points/). Alice and Bob are trying to meet up but have lost their phones and have no way to contact each other. There are two local bars: the popular bar and the unpopular one.\n\nLet's first consider how Alice would choose a bar (if she was not taking Bob into account):\n\n~~~~\nvar locationPrior = function() {\n  if (flip(.55)) {\n    return 'popular-bar';\n  } else {\n    return 'unpopular-bar';\n  }\n};\n\nvar alice = function() {\n  return Infer({ model() {\n    var myLocation = locationPrior();\n    return myLocation;\n  }});\n};\n\nviz(alice());\n~~~~\n\nBut Alice wants to be at the same bar as Bob. We extend our model of Alice to include this:\n\n~~~~\nvar locationPrior = function() {\n  if (flip(.55)) {\n    return 'popular-bar';\n  } else {\n    return 'unpopular-bar';\n  }\n};\n\nvar alice = function() {\n  return Infer({ model() {\n    var myLocation = locationPrior();\n    var bobLocation = sample(bob());\n    condition(myLocation === bobLocation);\n    return myLocation;\n  }});\n};\n\nvar bob = function() {\n  return Infer({ model() {\n    var myLocation = locationPrior();\n    return myLocation;\n  }});\n};\n\nviz(alice());\n~~~~\n\nNow Bob and Alice are thinking recursively about each other. We add caching (to avoid repeated computations) and a depth parameter (to avoid infinite recursion):\n\n~~~~\nvar locationPrior = function() {\n  if (flip(.55)) {\n    return 'popular-bar';\n  } else {\n    return 'unpopular-bar';\n  }\n}\n\nvar alice = dp.cache(function(depth) {\n  return Infer({ model() {\n    var myLocation = locationPrior();\n    var bobLocation = sample(bob(depth - 1));\n    condition(myLocation === bobLocation);\n    return myLocation;\n  }});\n});\n\nvar bob = dp.cache(function(depth) {\n  return Infer({ model() {\n    var myLocation = locationPrior();\n    if (depth === 0) {\n      return myLocation;\n    } else {\n      var aliceLocation = sample(alice(depth));\n      condition(myLocation === aliceLocation);\n      return myLocation;\n    }\n  }});\n});\n\nviz(alice(10));\n~~~~\n\n>**Exercise**: Change the example to the setting where Bob wants to avoid Alice instead of trying to meet up with her, and Alice knows this. How do the predictions change as the reasoning depth grows? How would you model the setting where Alice doesn't know that Bob wants to avoid her?\n\n>**Exercise**: Would any of the answers to the previous exercise change if recursive reasoning could terminate not just at a fixed depth, but also at random?\n\n\n## Game playing\n\nWe'll look at the two-player game tic-tac-toe:\n\n<img src=\"/assets/img/tic-tac-toe-game-1.svg\"/>\n\n>*Figure 1:* Tic-tac-toe. (Image source: [Wikipedia](https://en.wikipedia.org/wiki/Tic-tac-toe#/media/File:Tic-tac-toe-game-1.svg))\n\n\n\nLet's start with a prior on moves:\n\n~~~~\nvar isValidMove = function(state, move) {\n  return state[move.x][move.y] === '?';\n};\n\nvar movePrior = dp.cache(function(state){\n  return Infer({ model() {\n    var move = {\n      x: randomInteger(3),\n      y: randomInteger(3)\n    };\n    condition(isValidMove(state, move));\n    return move;\n  }});\n});\n\nvar startState = [\n  ['?', 'o', '?'],\n  ['?', 'x', 'x'],\n  ['?', '?', '?']\n];\n\nviz.table(movePrior(startState));\n~~~~\n\nNow let's add a deterministic transition function:\n\n~~~~\n///fold: isValidMove, movePrior\nvar isValidMove = function(state, move) {\n  return state[move.x][move.y] === '?';\n};\n\nvar movePrior = dp.cache(function(state){\n  return Infer({ model() {\n    var move = {\n      x: randomInteger(3),\n      y: randomInteger(3)\n    };\n    condition(isValidMove(state, move));\n    return move;\n  }});\n});\n///\n\nvar assign = function(obj, k, v) {\n  var newObj = _.clone(obj);\n  return Object.defineProperty(newObj, k, {value: v})\n};\n\nvar transition = function(state, move, player) {\n  var newRow = assign(state[move.x], move.y, player);\n  return assign(state, move.x, newRow);\n};\n\nvar startState = [\n  ['?', 'o', '?'],\n  ['?', 'x', 'x'],\n  ['?', '?', '?']\n];\n\ntransition(startState, {x: 1, y: 0}, 'o');\n~~~~\n\nWe need to be able to check if a player has won:\n\n~~~~\n///fold: movePrior, transition\nvar isValidMove = function(state, move) {\n  return state[move.x][move.y] === '?';\n};\n\nvar movePrior = dp.cache(function(state){\n  return Infer({ model() {\n    var move = {\n      x: randomInteger(3),\n      y: randomInteger(3)\n    };\n    condition(isValidMove(state, move));\n    return move;\n  }});\n});\n\nvar assign = function(obj, k, v) {\n  var newObj = _.clone(obj);\n  return Object.defineProperty(newObj, k, {value: v})\n};\n\nvar transition = function(state, move, player) {\n  var newRow = assign(state[move.x], move.y, player);\n  return assign(state, move.x, newRow);\n};\n///\n\nvar diag1 = function(state) {\n  return mapIndexed(function(i, x) {return x[i];}, state);\n};\n\nvar diag2 = function(state) {\n  var n = state.length;\n  return mapIndexed(function(i, x) {return x[n - (i + 1)];}, state);\n};\n\nvar hasWon = dp.cache(function(state, player) {\n  var check = function(xs){\n    return _.countBy(xs)[player] == xs.length;\n  };\n  return any(check, [\n    state[0], state[1], state[2], // rows\n    map(first, state), map(second, state), map(third, state), // cols\n    diag1(state), diag2(state) // diagonals\n  ]);\n});\n\nvar startState = [\n  ['?', 'o', '?'],\n  ['x', 'x', 'x'],\n  ['?', '?', '?']\n];\n\nhasWon(startState, 'x');\n~~~~\n\nNow let's implement an agent that chooses a single action, but can't plan ahead:\n\n~~~~\n///fold: movePrior, transition, hasWon\nvar isValidMove = function(state, move) {\n  return state[move.x][move.y] === '?';\n};\n\nvar movePrior = dp.cache(function(state){\n  return Infer({ model() {\n    var move = {\n      x: randomInteger(3),\n      y: randomInteger(3)\n    };\n    condition(isValidMove(state, move));\n    return move;\n  }});\n});\n\nvar assign = function(obj, k, v) {\n  var newObj = _.clone(obj);\n  return Object.defineProperty(newObj, k, {value: v})\n};\n\nvar transition = function(state, move, player) {\n  var newRow = assign(state[move.x], move.y, player);\n  return assign(state, move.x, newRow);\n};\n\nvar diag1 = function(state) {\n  return mapIndexed(function(i, x) {return x[i];}, state);\n};\n\nvar diag2 = function(state) {\n  var n = state.length;\n  return mapIndexed(function(i, x) {return x[n - (i + 1)];}, state);\n};\n\nvar hasWon = dp.cache(function(state, player) {\n  var check = function(xs){\n    return _.countBy(xs)[player] == xs.length;\n  };\n  return any(check, [\n    state[0], state[1], state[2], // rows\n    map(first, state), map(second, state), map(third, state), // cols\n    diag1(state), diag2(state) // diagonals\n  ]);\n});\n///\nvar isDraw = function(state) {\n  return !hasWon(state, 'x') && !hasWon(state, 'o');\n};\n\nvar utility = function(state, player) {\n  if (hasWon(state, player)) {\n    return 10;\n  } else if (isDraw(state)) {\n    return 0;\n  } else {\n    return -10;\n  }\n};\n\nvar act = dp.cache(function(state, player) {\n  return Infer({ model() {\n    var move = sample(movePrior(state));\n    var eu = expectation(Infer({ model() {\n      var outcome = transition(state, move, player);\n      return utility(outcome, player);\n    }}));\n    factor(eu);    \n    return move;\n  }});\n});\n\nvar startState = [\n  ['o', 'o', '?'],\n  ['?', 'x', 'x'],\n  ['?', '?', '?']\n];\n\nviz.table(act(startState, 'x'));\n~~~~\n\nAnd now let's include planning:\n\n~~~~\n///fold: movePrior, transition, hasWon, utility, isTerminal\nvar isValidMove = function(state, move) {\n  return state[move.x][move.y] === '?';\n};\n\nvar movePrior = dp.cache(function(state){\n  return Infer({ model() {\n    var move = {\n      x: randomInteger(3),\n      y: randomInteger(3)\n    };\n    condition(isValidMove(state, move));\n    return move;\n  }});\n});\n\nvar assign = function(obj, k, v) {\n  var newObj = _.clone(obj);\n  return Object.defineProperty(newObj, k, {value: v})\n};\n\nvar transition = function(state, move, player) {\n  var newRow = assign(state[move.x], move.y, player);\n  return assign(state, move.x, newRow);\n};\n\nvar diag1 = function(state) {\n  return mapIndexed(function(i, x) {return x[i];}, state);\n};\n\nvar diag2 = function(state) {\n  var n = state.length;\n  return mapIndexed(function(i, x) {return x[n - (i + 1)];}, state);\n};\n\nvar hasWon = dp.cache(function(state, player) {\n  var check = function(xs){\n    return _.countBy(xs)[player] == xs.length;\n  };\n  return any(check, [\n    state[0], state[1], state[2], // rows\n    map(first, state), map(second, state), map(third, state), // cols\n    diag1(state), diag2(state) // diagonals\n  ]);\n});\n\nvar isDraw = function(state) {\n  return !hasWon(state, 'x') && !hasWon(state, 'o');\n};\n\nvar utility = function(state, player) {\n  if (hasWon(state, player)) {\n    return 10;\n  } else if (isDraw(state)) {\n    return 0;\n  } else {\n    return -10;\n  }\n};\n\nvar isComplete = function(state) {\n  return all(\n    function(x){\n      return x != '?';\n    },\n    _.flatten(state));\n}\n\nvar isTerminal = function(state) {\n  return hasWon(state, 'x') || hasWon(state, 'o') || isComplete(state);  \n};\n///\n\nvar otherPlayer = function(player) {\n  return (player === 'x') ? 'o' : 'x';\n};\n\nvar act = dp.cache(function(state, player) {\n  return Infer({ model() {\n    var move = sample(movePrior(state));\n    var eu = expectation(Infer({ model() {\n      var outcome = simulate(state, move, player);\n      return utility(outcome, player);\n    }}));\n    factor(eu);    \n    return move;\n  }});\n});\n\nvar simulate = function(state, action, player) {\n  var nextState = transition(state, action, player);\n  if (isTerminal(nextState)) {\n    return nextState;\n  } else {\n    var nextPlayer = otherPlayer(player);\n    var nextAction = sample(act(nextState, nextPlayer));\n    return simulate(nextState, nextAction, nextPlayer);\n  }\n};\n\nvar startState = [\n  ['o', '?', '?'],\n  ['?', '?', 'x'],\n  ['?', '?', '?']\n];\n\nvar actDist = act(startState, 'o');\n\nviz.table(actDist);\n~~~~\n\n## Language understanding\n\n<!-- TODO text needs more elaboration or some links to papers or online content -->\n\nA model of pragmatic language interpretation: The speaker chooses a sentence conditioned on the listener inferring the intended state of the world when hearing this sentence; the listener chooses an interpretation conditioned on the speaker selecting the given utterance when intending this meaning.\n\nLiteral interpretation:\n\n~~~~\nvar statePrior = function() {\n  return uniformDraw([0, 1, 2, 3]);\n};\n\nvar literalMeanings = {\n  allSprouted: function(state) { return state === 3; },\n  someSprouted: function(state) { return state > 0; },\n  noneSprouted: function(state) { return state === 0; }\n};\n\nvar sentencePrior = function() {\n  return uniformDraw(['allSprouted', 'someSprouted', 'noneSprouted']);\n};\n\nvar literalListener = function(sentence) {\n  return Infer({ model() {\n    var state = statePrior();\n    var meaning = literalMeanings[sentence];\n    condition(meaning(state));\n    return state;\n  }});\n};\n\nviz(literalListener('someSprouted'));\n~~~~\n\nA pragmatic speaker, thinking about the literal listener:\n\n~~~~\nvar alpha = 2;\n\n///fold: statePrior, literalMeanings, sentencePrior\nvar statePrior = function() {\n  return uniformDraw([0, 1, 2, 3]);\n};\n\nvar literalMeanings = {\n  allSprouted: function(state) { return state === 3; },\n  someSprouted: function(state) { return state > 0; },\n  noneSprouted: function(state) { return state === 0; }\n};\n\nvar sentencePrior = function() {\n  return uniformDraw(['allSprouted', 'someSprouted', 'noneSprouted']);\n};\n///\n\nvar literalListener = function(sentence) {\n  return Infer({ model() {\n    var state = statePrior();\n    var meaning = literalMeanings[sentence];\n    condition(meaning(state));\n    return state;\n  }});\n};\n\nvar speaker = function(state) {\n  return Infer({ model() {\n    var sentence = sentencePrior();\n    factor(alpha * literalListener(sentence).score(state));\n    return sentence;\n  }});\n}\n\nviz(speaker(3));\n~~~~\n\nPragmatic listener, thinking about speaker:\n\n~~~~\nvar alpha = 2;\n\n///fold: statePrior, literalMeanings, sentencePrior\nvar statePrior = function() {\n  return uniformDraw([0, 1, 2, 3]);\n};\n\nvar literalMeanings = {\n  allSprouted: function(state) { return state === 3; },\n  someSprouted: function(state) { return state > 0; },\n  noneSprouted: function(state) { return state === 0; }\n};\n\nvar sentencePrior = function() {\n  return uniformDraw(['allSprouted', 'someSprouted', 'noneSprouted']);\n};\n///\n\nvar literalListener = dp.cache(function(sentence) {\n  return Infer({ model() {\n    var state = statePrior();\n    var meaning = literalMeanings[sentence];\n    condition(meaning(state));\n    return state;\n  }});\n});\n\nvar speaker = dp.cache(function(state) {\n  return Infer({ model() {\n    var sentence = sentencePrior();\n    factor(alpha * literalListener(sentence).score(state));\n    return sentence;\n  }});\n});\n\nvar listener = dp.cache(function(sentence) {\n  return Infer({ model() {\n    var state = statePrior();\n    factor(speaker(state).score(sentence));\n    return state;\n  }});\n});\n\nviz(listener('someSprouted'));\n~~~~\n\nNext chapter: [How to use the WebPPL Agent Models library](/chapters/8-guide-library.html)\n", "date_published": "2016-12-04T11:26:34Z", "authors": ["Owain Evans", "Andreas Stuhlmüller", "John Salvatier", "Daniel Filan"], "summaries": [], "filename": "7-multi-agent.md"}