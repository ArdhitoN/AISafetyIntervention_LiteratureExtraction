{"id": "d057fa7c65282c0ba9252bb7c5351332", "title": "The Majority Is Always Wrong", "url": "https://www.lesswrong.com/posts/yxFkuyPANtL6GSwiC/the-majority-is-always-wrong", "source": "lesswrong", "source_type": "GreaterWrong", "text": "Today my coworker Marcello pointed out to me an interesting anti-majoritarian effect.  There are three major interpretations of probability: the \"subjective\" view of probabilities as measuring the uncertainty of agents, the \"propensity\" view of probabilities as chances inherent within objects, and the \"frequentist\" view of probabilities as the limiting value of long-run frequencies.  I was remarking on how odd it was that frequentism, the predominant view in mainstream statistics, is the worst of the three major alternatives (in my view, you have to presume either uncertainty or propensity in order to talk about the limiting frequency of events that have not yet happened).\n\n\n\n\nAnd Marcello said something along the lines of, \"Well, of course.  If anything were worse than frequentism, it wouldn't be there.\"  I said, \"What?\"  And Marcello said, \"Like the saying that Mac users have, 'If Macs really *were* worse than Windows PCs, *no one* would use them.'\"\n\n\n\n\nAt this point the light bulb went on over my head - a fluorescent light bulb - and I understood what Marcello was saying: an alternative to frequentism that was even worse than frequentism would have dropped off the radar screens long ago.  You can survive by being popular, or by being superior, but alternatives that are neither popular nor superior quickly go extinct.\n\nI can personally testify that Dvorak seems to be much easier on the fingers than Qwerty - but this is not surprising, since if Dvorak really were inferior to Qwerty, it would soon cease to exist.  (Yes, I am familiar with the controversy in this area - bear in mind that this is a [politically charged topic](/lw/gz/policy_debates_should_not_appear_onesided/) since it has been used to make accusations of market failure.  Nonetheless, my fingers now sweat less, my hands feel less tired, my carpal tunnel syndrome went away, and none of this is surprising because I can feel my fingers traveling shorter distances.)\n\n\n\n\nIn any case where you've got (1) a popularity effect (it's easier to use something other people are using) and (2) a most dominant alternative, plus a few smaller niche alternatives, then the most dominant alternative will probably be the worst of the lot - or at least strictly superior to none of the others.\n\n\n\n\nCan anyone else think of examples from their experience where there are several major alternatives *that you've heard of*, and a popularity effect (which may be as simple as journal editors preferring well-known usages), and the most popular alternative seems to be noticeably the worst?\n\n\n**Addendum:**  Metahacker [said](http://peregrinejohn.livejournal.com/161510.html) of this hypothesis, \"It's wrong, but only sometimes.\"  Sounds about right to me.", "date_published": "2007-04-03T01:12:23Z", "authors": ["Eliezer Yudkowsky"], "summaries": [], "tags": ["Ought"], "karma": 42, "votes": 37, "words": 424, "modified_at": "2020-01-06T08:24:44.506Z", "comment_count": 54}
{"id": "787d8ff50e9e75fb44c906bcd5e78af5", "title": "Pascal's Mugging: Tiny Probabilities of Vast Utilities", "url": "https://www.lesswrong.com/posts/a5JAiTdytou3Jg749/pascal-s-mugging-tiny-probabilities-of-vast-utilities", "source": "lesswrong", "source_type": "GreaterWrong", "text": "The most common [formalizations of Occam's Razor](/lw/jp/occams_razor/), Solomonoff induction and Minimum Description Length, measure the program size of a computation used in a hypothesis, but don't measure the running time or space requirements of the computation.  What if this makes a mind vulnerable to finite forms of Pascal's Wager?  A compactly specified wager can grow in size *much* faster than it grows in complexity.  The utility of a Turing machine can grow much faster than its prior probability shrinks.\n\n\nConsider [Knuth's up-arrow notation](http://en.wikipedia.org/wiki/Knuth%27s_up-arrow_notation):\n\n\n* 3^3 = 3\\*3\\*3 = 27\n* 3^^3 = (3^(3^3)) = 3^27 = 3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3 = 7625597484987\n* 3^^^3 = (3^^(3^^3)) = 3^^7625597484987 = 3^(3^(3^(... 7625597484987 times ...)))\n\n\nIn other words:  3^^^3 describes an exponential tower of threes 7625597484987 layers tall.  Since this number can be computed by a simple Turing machine, it contains very little information and requires a very short message to describe.  This, even though writing out 3^^^3 in base 10 would require *enormously* more writing material than there are atoms in the known universe (a paltry 10^80).\n\n\nNow suppose someone comes to me and says, \"Give me five dollars, or I'll use my magic powers from outside the Matrix to run a Turing machine that simulates and kills 3^^^^3 people.\"\n\n\nCall this Pascal's Mugging.\n\n\n\n\"Magic powers from outside the Matrix\" are easier said than done - we have to suppose that our world is a computing simulation run from within an environment that can afford simulation of arbitrarily large finite Turing machines, and that the would-be wizard has been spliced into our own Turing tape and is in continuing communication with an outside operator, etc.\n\n\nThus the Kolmogorov complexity of \"magic powers from outside the Matrix\" is larger than the mere English words would indicate.  Therefore the Solomonoff-inducted probability, two to the *negative* Kolmogorov complexity, is exponentially tinier than one might naively think.\n\n\nBut, small as this probability is, it isn't anywhere *near* as small as 3^^^^3 is large.  If you take a decimal point, followed by a number of zeros equal to the length of the Bible, followed by a 1, and multiply this unimaginably tiny fraction by 3^^^^3, the result is pretty much 3^^^^3.\n\n\nMost people, I think, envision an \"infinite\" God that is nowhere near as large as 3^^^^3.  \"Infinity\" is reassuringly featureless and blank.  \"Eternal life in Heaven\" is nowhere near as intimidating as the thought of spending 3^^^^3 years on one of those fluffy clouds.  The notion that the diversity of life on Earth springs from God's infinite creativity, sounds more plausible than the notion that life on Earth was created by a superintelligence 3^^^^3 bits large.  Similarly for envisioning an \"infinite\" God interested in whether women wear men's clothing, versus a superintelligence of 3^^^^3 bits, etc.\n\n\nThe original version of Pascal's Wager is easily dealt with by the gigantic multiplicity of possible gods, an Allah for every Christ and a Zeus for every Allah, including the \"Professor God\" who places only atheists in Heaven.   And since all the expected utilities here are allegedly \"[infinite](http://www.nickbostrom.com/ethics/infinite.pdf)\", it's easy enough to argue that they cancel out.  Infinities, being featureless and blank, are all the same size.\n\n\nBut suppose I built an AI which worked by some bounded analogue of Solomonoff induction - an AI sufficiently Bayesian to insist on calculating complexities and assessing probabilities, rather than just waving them off as \"large\" or \"small\".\n\n\nIf the probabilities of various scenarios considered did not *exactly* cancel out, the AI's action in the case of Pascal's Mugging would be *overwhelmingly* dominated by whatever tiny differentials existed in the various tiny probabilities under which 3^^^^3 units of expected utility were actually at stake.\n\n\nYou or I would probably wave off the whole matter with a laugh, planning according to the dominant mainline probability:  Pascal's Mugger is just a philosopher out for a fast buck.\n\n\nBut a silicon chip does not look over the code fed to it, assess it for reasonableness, and correct it if not.  An AI is not given its code like a human servant given instructions.  An AI *is* its code.  What if a philosopher tries Pascal's Mugging on the AI for a joke, and the tiny probabilities of 3^^^^3 lives being at stake, override *everything* else in the AI's calculations?   What is the mere Earth at stake, compared to a tiny probability of 3^^^^3 lives?\n\n\nHow do *I* know to be worried by this line of reasoning?  How do *I* know to [rationalize](/lw/ju/rationalization/) reasons a Bayesian shouldn't work that way?  A mind that worked strictly by Solomonoff induction would not know to rationalize reasons that Pascal's Mugging mattered less than Earth's existence.  It would simply go by whatever answer Solomonoff induction obtained.\n\n\nIt would seem, then, that I've implicitly declared my existence as a mind that does not work by the logic of Solomonoff, at least not the way I've described it.  What am I comparing Solomonoff's answer to, to determine whether Solomonoff induction got it \"right\" or \"wrong\"?\n\n\nWhy do I think it's unreasonable to focus my entire attention on the magic-bearing possible worlds, faced with a Pascal's Mugging?  Do I have an instinct to resist exploitation by arguments \"anyone could make\"?  Am I unsatisfied by any visualization in which the dominant mainline probability leads to a loss?  Do I drop sufficiently small probabilities from consideration entirely?  Would an AI that lacks these instincts be exploitable by Pascal's Mugging?\n\n\nIs it me who's wrong?  Should I worry more about the possibility of some Unseen Magical Prankster of very tiny probability taking this post literally, than about the fate of the human species in the \"mainline\" probabilities?\n\n\nIt doesn't feel to me like 3^^^^3 lives are *really* at stake, even at very tiny probability.  I'd sooner question my grasp of \"rationality\" than give five dollars to a Pascal's Mugger because I thought it was \"rational\".\n\n\nShould we penalize computations with large space and time requirements?  This is a hack that solves the problem, but is it *true?* Are computationally costly explanations less likely?  Should I think the universe is probably a coarse-grained simulation of my mind rather than real quantum physics, because a coarse-grained human mind is *exponentially* cheaper than real quantum physics?  Should I think the galaxies are tiny lights on a painted backdrop, because that Turing machine would require less space to compute?\n\n\nGiven that, in general, a Turing machine can increase in utility vastly faster than it increases in complexity, how should an Occam-abiding mind avoid being dominated by tiny probabilities of vast utilities?\n\n\nIf I could formalize whichever internal criterion was telling me I didn't want this to happen, I might have an answer.\n\n\nI talked over a variant of this problem with Nick Hay, Peter de Blanc, and Marcello Herreshoff in summer of 2006.  I don't feel I have a satisfactory resolution as yet, so I'm throwing it open to any analytic philosophers who might happen to read Overcoming Bias.", "date_published": "2007-10-19T23:37:38Z", "authors": ["Eliezer Yudkowsky"], "summaries": [], "tags": ["Utility Functions", "Pascal's Mugging", "Paradoxes", "Solomonoff Induction", "Decision Theory", "Infinities In Ethics"], "karma": 100, "votes": 74, "words": 1136, "modified_at": "2020-12-02T01:23:18.440Z", "comment_count": 353}
{"id": "a6469dd70d5afa4000935401036fac78", "title": "Fake Optimization Criteria", "url": "https://www.lesswrong.com/posts/i6fKszWY6gLZSX2Ey/fake-optimization-criteria", "source": "lesswrong", "source_type": "GreaterWrong", "text": "[I've](https://www.lesswrong.com/lw/il/hindsight_bias/) [previously](https://www.lesswrong.com/lw/im/hindsight_devalues_science/) [dwelt](https://www.lesswrong.com/lw/if/your_strength_as_a_rationalist/) [in](https://www.lesswrong.com/lw/ia/focus_your_uncertainty/) [considerable](https://www.lesswrong.com/lw/ih/absence_of_evidence_is_evidence_of_absence/) [length](https://www.lesswrong.com/lw/ii/conservation_of_expected_evidence/) [upon](https://www.lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/) [forms](https://www.lesswrong.com/lw/i4/belief_in_belief/) [of](https://www.lesswrong.com/lw/jt/what_evidence_filtered_evidence/) [rationalization](https://www.lesswrong.com/lw/ju/rationalization/) [whereby](https://www.lesswrong.com/lw/i6/professing_and_cheering/) [our](https://www.lesswrong.com/lw/i7/belief_as_attire/) [beliefs](https://www.lesswrong.com/lw/i8/religions_claim_to_be_nondisprovable/) [appear](https://www.lesswrong.com/lw/ip/fake_explanations/) [to](https://www.lesswrong.com/lw/iq/guessing_the_teachers_password/) [match](https://www.lesswrong.com/lw/is/fake_causality/) [the](https://www.lesswrong.com/lw/it/semantic_stopsigns/) [evidence](https://www.lesswrong.com/lw/jl/what_is_evidence/) [much](https://www.lesswrong.com/lw/iu/mysterious_answers_to_mysterious_questions/) [more](https://www.lesswrong.com/lw/iv/the_futility_of_emergence/) [strongly](https://www.lesswrong.com/lw/ix/say_not_complexity/) [than](https://www.lesswrong.com/lw/iw/positive_bias_look_into_the_dark/) [they](https://www.lesswrong.com/lw/he/knowing_about_biases_can_hurt_people/) [actually](https://www.lesswrong.com/lw/js/the_bottom_line/) [do](https://www.lesswrong.com/lw/jt/what_evidence_filtered_evidence/). And I'm not overemphasizing the point, either. If we could beat this fundamental metabias and see what every hypothesis *really* predicted, we would be able to recover from almost any other error of fact.\n\nThe mirror challenge for decision theory is seeing which option a choice criterion *really* endorses. If your [stated moral principles](https://www.lesswrong.com/lw/kq/fake_justification/) call for you to provide laptops to everyone, does that *really* endorse buying a $1 million gem-studded laptop for yourself, or spending the same money on shipping 5000 OLPCs?\n\nWe seem to have evolved a knack for arguing that practically any goal implies practically any action. A phlogiston theorist explaining why magnesium gains weight when burned has nothing on an Inquisitor explaining why God's infinite love for all His children requires burning some of them at the stake.\n\nThere's no mystery about this. [Politics](https://www.lesswrong.com/lw/gw/politics_is_the_mindkiller/) was a feature of the ancestral environment. We are descended from those who argued most persuasively that the good of the tribe meant executing their hated rival Uglak. (We sure ain't descended from Uglak.) \n\nAnd yet... is it possible to *prove* that if Robert Mugabe cared *only* for the good of Zimbabwe, he would resign from its presidency? You can *argue* that the policy follows from the goal, but haven't we just seen that humans can match up any goal to any policy? How do you know that you're right and Mugabe is wrong? (There are a number of reasons this is a good guess, but bear with me here.)\n\nHuman motives are manifold and obscure, our decision processes as vastly complicated as our brains. And the world itself is vastly complicated, on every choice of real-world policy. Can we even *prove* that human beings are rationalizing—that we're systematically distorting the link from principles to policy—when we lack a single firm place on which to stand? When there's no way to find out *exactly* what even a single optimization criterion implies? (Actually, you can just observe that people *disagree* about office politics in ways that strangely correlate to their own interests, while simultaneously denying that any such interests are at work. But again, bear with me here.)\n\nWhere is the standardized, open-source, generally intelligent, consequentialist optimization process into which we can feed a complete morality as an XML file, to find out what that morality *really* recommends when applied to our world? Is there even a single real-world case where we can know *exactly* what a choice criterion recommends? Where is the *pure* moral reasoner—of known utility function, purged of all other stray desires that might distort its optimization—whose trustworthy output we can contrast to human rationalizations of the same utility function?\n\nWhy, it's our old friend the [alien god](https://www.lesswrong.com/lw/kr/an_alien_god/), of course! Natural selection is guaranteed free of all mercy, all love, all compassion, all aesthetic sensibilities, all political factionalism, all ideological allegiances, all academic ambitions, all libertarianism, all socialism, [all Blue and all Green](https://www.lesswrong.com/lw/gt/a_fable_of_science_and_politics/). Natural selection doesn't *maximize* its criterion of inclusive genetic fitness—it's [not that smart](https://www.lesswrong.com/lw/kt/evolutions_are_stupid_but_work_anyway/). But when you look at the output of natural selection, you are guaranteed to be looking at an output that was optimized *only* for inclusive genetic fitness, and not the interests of the US agricultural industry.\n\nIn the case histories of evolutionary science—in, for example, [The Tragedy of Group Selectionism](https://www.lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/)—we can directly compare human rationalizations to the result of *pure* optimization for a known criterion. What did Wynne-Edwards think would be the result of group selection for small subpopulation sizes? Voluntary individual restraint in breeding, and enough food for everyone. What was the actual laboratory result? Cannibalism.\n\nNow you might ask: Are these case histories of evolutionary science really relevant to human morality, which doesn't give two figs for inclusive genetic fitness when it gets in the way of love, compassion, aesthetics, healing, freedom, fairness, et cetera? Human societies didn't even have a concept of \"inclusive genetic fitness\" until the 20th century.\n\nBut I ask in return: If we can't see clearly the result of a single monotone optimization criterion—if we can't even train ourselves to hear a single pure note—then how will we listen to an orchestra? How will we see that \"Always be selfish\" or \"Always obey the government\" are poor guiding principles for human beings to adopt—if we think that even *optimizing genes for inclusive fitness* will yield organisms which sacrifice reproductive opportunities in the name of social resource conservation?\n\nTo train ourselves to see clearly, we need simple practice cases.", "date_published": "2007-11-10T00:10:51Z", "authors": ["Eliezer Yudkowsky"], "summaries": [], "tags": ["Motivated Reasoning", "Optimization"], "karma": 61, "votes": 52, "words": 754, "comment_count": 21}
