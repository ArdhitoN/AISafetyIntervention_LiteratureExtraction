{
  "title": "DeepMind x UCL RL Lecture Series - Introduction to Reinforcement Learning [1/13]",
  "authors": [
    "Google DeepMind"
  ],
  "date_published": "2022-03-29T12:01:55Z",
  "text": "hello and welcome to this course on\nreinforcement learning\nmy name is harvan husselt and i'm a\nresearch scientist at deepmind in london\nand every year we teach this course on\nreinforcement training at ucl\nthis year it's a little bit different\nbecause due to the pandemic situation\nwith covet 19\nwe are pre-recording the lectures so\ninstead of talking from a lecture hall\ni'm now talking to you from my home\num the topic of course as mentioned is\nreinforcement training i will explain\nwhat that means what those words mean\nreinforcement learning and\nwe'll go into\nsome depth in uh multiple lectures to\nexplain different concepts and different\nalgorithms that we can build\ni'm not teaching this course by myself\nsome of the lectures will be taught by\ndiana bursa and some will be told by\nmatteo hessel\nand today will be about introducing\nreinforcement learning\nthere's also a really good book on this\ntopic by\nrich sutton and andy bartow which i\nhighly recommend and this is also going\nto be used basically background material\nfor this course\nand if you go to the url that is shown\nhere on the slide you can access\na free\ncopy of that book\njust a little bit of admin before we get\nstarted for students taking this for\ncredit at ucl\nthere's a portal called moodle\nand we'll be using that to communicate\nwith you so please check that for\nupdates and please use the forum thereon\nfor asking questions\nif you do that then if we answer these\nquestions and other people can also\nbenefit from that interaction and\nmultiple people might have the same\nquestion\nor people might have a question but not\neven realize that they have that\nquestion so then it's very useful if you\nask it publicly on that forum\nin terms of grading we will have\nassignments\nwhich will be graded this year there\nwill not be an exam\nso now about this course specifically\nwhat are we talking about\num the main question for this first\nlecture especially is just the question\nwhat is reinforcement learning and\ni'll explain it a little bit and then\nwe'll go into a lot of depth into\ndifferent subtopics of this question\nand in order to understand what\nreinforcement is it's actually useful to\nfirst ask the question what artificial\nintelligence is and how these two are\nrelated because turns out these are\nclosely related\nand\nto understand at least what i mean when\ni say artificial intelligence i'm going\nto pop up a level\nand we're going to turn first to the\nindustrial revolution\nso\nthis is a period in time that happened a\ncouple of hundred years ago or started a\ncouple hundred years ago and one could\nargue this is all about automating\nrepeated physical solutions or manual\nsolutions if you will so think for\ninstance of a steam train or a steamboat\nand how this replaced the manual labor\nof pulling a cart by yourself or using\nfor instance animal labor horses\nto draw those cards\nnow of course some of that still happens\nwe still have a manual layer but we\nreplaced a lot of that with machines and\nthis led to the machine age where we\nstarted replacing more and more things\nwith machines and in addition to that\nalso coming up with new things that we\ncould solve with machines so even things\nthat we weren't doing before we could\nnow make machines that could do those\nthings for us\nof course this led to huge productivity\nincrease worldwide\nand it also fed into a new stage you\ncould argue comes after this\nwhich you could call the digital\nrevolution\nand one way to interpret this is to say\nthe digital revolution was all about\nautomating repeated mental solutions\nso a classic example here would be a\ncalculator\nwe know how to add two numbers together\nwe know how to multiply two numbers\ntogether and in fact we know that\nprecisely enough that we can write a\nprogram and implement that on a machine\non a computer if you will\nand then automate that process in such a\nway that it's very fast and very precise\nand therefore replace the slower mental\ncalculations that we had to do before\nnow this of course\nalso led to a lot of productivity\nincrease\nbut both of these phases they have\nsomething in common which is that we\nfirst had to come up with the solutions\nnow i'm going to argue that there's a\nnext thing that you can think of which\nis already ongoing\nand that would be\nto allow machines to find solutions\nthemselves and this you could call the\ndomain of artificial intelligence\nnow this has huge potential upside\nbecause if you are able to find machines\nthat can learn for themselves to find\nsolutions\nthen this takes away the responsibility\non us to find a solution in advance and\nthen to automate it\ninstead then all that we need to do is\nspecify a problem and a goal\nand then have the machine figure out how\nto solve this\nas we'll see later this will often\ninvolve interacting\nyou have to have some data to find the\nsolution\nand this means that there's a process of\nlearning so here we already bump into\nthis term learning which i'll get into\nmuch more in depth\nin addition this requires you to\nautonomously make decisions so i'm i'm\nputting these\nterms basically up front and center so\nthere's learning and autonomy and\ndecisions and these are all quite\ncentral to this\ngeneric problem of trying to find\nsolutions\nof course we're not the first to talk\nabout artificial intelligence\nthis has been a topic of investigation\nfor many decades now and there's this\nwonderful paper by alan turing from 1950\ncalled computing machinery and\nintelligence\nand the very first sentence of that\npaper reads i propose to consider the\nquestions can machines think\nnow\ni by the way recommend you to read this\npaper it's wonderfully written it's very\naccessible and it has lots of really\ninteresting thoughts but there's one\nparagraph that i want to highlight\nspecifically\nand i'll read that to you now\nso alan turing writes\nin the process of trying to imitate an\nadult human mind we are bound to think a\ngood deal about the process which has\nbrought it to the state that it is in\nwe may notice three components the\ninitial state of the mind say at birth\nthe education to which it has been\nsubjected and other experience not to be\ndescribed as education to which it has\nbeen subjected\ninstead of trying to produce a program\nto simulate the adult mind why not\nrather try to produce one which\nsimulates the child's\nif this were then subjected to an\nappropriate course of education one\nwould obtain the adult brain\npresumably the child brain is something\nlike a notebook as one buys it from the\nstationers\nrather little mechanism and lots of\nblank sheets mechanism and writing are\nfrom our point of view almost synonymous\nour hope is that there is so little\nmechanism in the child brain that\nsomething like it can be easily\nprogrammed\nso what is aventurine talking about here\nhe's essentially talking about learning\nhe's essentially conjecturing that\ntrying to write the program which\nconstitutes an adult mind might be quite\ncomplicated which makes sense because\nwe're subjected to a lot of experience\nthroughout our lives this means we learn\na lot of you can think of these as being\nrules or\npattern matching that we learn how to do\nskills that we acquire\nand enumerating all of that describing\nthat all of that clearly enough and\ncleanly enough that you have something\nwith the same capability as an adult\nmind\nhe's conjecturing that that might\nactually be quite tricky\nand maybe it's easier to actually write\na program that can itself learn in the\nsame way maybe that we do or maybe in a\nsimilar way or maybe in a slightly\ndifferent way\nbut it can learn by interacting with the\nworld\nby\nin in his words subjecting itself to\neducation\num\nmaybe to find similar solutions as the\nadult mind has\nand he's conjecturing that maybe this is\neasier\nnow this is a really interesting thought\nand it's really interesting to think\nabout this a little bit\nso\nmaybe this is a good time for you also\nto maybe pause the video and ponder this\na little bit whether you agree with this\nconjecture that indeed maybe it might be\neasier to write a program that can learn\nthan it is to write a program that has\nthe same capabilities as the program\nthat can learn will achieve over time\nso what is an artificial intelligence\nwell one way to define it would be\nthat the\ngoal would be to to be able to learn to\nmake decisions to achieve goals this is\nnot the only possible definition of\nartificial intelligence and other people\nhave proposed sometimes slightly\ndifferent versions or vastly different\nversions i'm not going to argue that\nthis is the best definition of\nartificial intelligence maybe there are\ndifferent types of artificial\nintelligence that we could consider but\nthis is the one that is central to us so\nwe're going to basically ask this\nquestion how could we build something\nthat is able to learn to make decisions\nto achieve goals\nand that is our central question\nand note that the learning decisions and\ngoals are all very central concepts in\nthis and we'll get into a little bit\nmore detail what i mean with all of them\nso this brings us to this question what\nis reinforcement learning\nand\nthis is related to this um\nexperience that alan turing was also\ntalking about because we know that\npeople and animals learn by interacting\nwith our environment\nand this differs from certain other\ntypes of learning and that's good to\nappreciate first of all it's active\nrather than passive and we'll get back\nto that extensively in the next lecture\nwhat this means is that you\nare subjected to some data or experience\nif you will\nbut the experience is not fully out of\nyour control the actions that you take\nmight influence the experience that you\nget\nin addition to that interactions might\nbe sequential so\nfuture interactions might depend on\nearlier ones if you do something this\nmight change the world in such a way\nthat later other things are possible or\nimpossible we are also goal directed we\ndon't just randomly meander we do things\nwith a purpose\nand this this is also this is at large\nbut also at small scale i might have a\ngoal to pick up a glass for instance\nthat is a small thing perhaps but you\ncould think of this as being a directed\naction where i pick up that glass and of\ncourse this consists of many small\nlittle micro actions of\nme sending signals to my muscles to\nactually execute that\nalso we can learn without examples of\noptimal behavior and this one's\ninteresting it's good to think about\nthat a little bit and to appreciate what\ni mean when i say with that because\nobviously we are subjected to education\nas engineering courses so we do get\nexamples of what we um\nbehavior that we want or behavior that\nother people want us to do and we try to\nfollow those examples in many cases\nbut what i mean here is something a\nlittle bit different i mean that when\nyou do pick up a cup\nthat maybe somebody showed you at some\npoint oh it's useful to pick up a cup um\nor you could think of it that way maybe\nthat's not the greatest example of that\nbut somebody taught you how to write or\ntaught you how to do math\nbut nobody actually told you exactly how\nto steer your muscles in such a way as\nto move your arm to pick up a pen to\npick up a cup things like this\nso clearly we still learn some sort of a\nbehavior there we learn to control our\nmuscles but not in a way that somebody\ntells you exactly oh this is how you\nshould have moved your muscle and now\nyou just replicate so that is what i\nmean when i say we learn without\nexamples nobody gives you exactly the\nlow level actions that are required to\nexecute\nthat thing that you want to execute\nand\nthis actually\nmaybe constitutes most of the learning\nthat we do most of the learning that we\ndo is actually of that form where maybe\nwe do interpret something that we see in\nsome sense as an example but maybe\ntypically at a much higher level of\nabstraction and in order to actually\nfill in that example in order to execute\nwhat we want to mimic this might still\nrequire us to learn skills in a much\nmore\nautonomous way without clear examples\nso one way to think about this and i'll\ngo back to this is that you can think of\nthis as optimizing some reward signal we\nwant to achieve something and by\nachieving it we feel in some sense\nsatisfaction or happiness and this is\nwhat stares our behavior we notice that\nsome things are more pleasing than other\nthings\nso this brings us to a very central uh\npicture that i'm going to show multiple\ntimes during this course which is the\ninteraction loop and one can perceives\nthis as being basically the setting that\nwe find ourselves in\nso this is something to keep in mind\nthat we are basically considering an\nagent interacting with an environment\nand here i drew them separately but you\ncould also think of the agent as\nbasically being inside that environment\nthere's a huge world out there the agent\nlives somewhere in that world now this\ncould be um quite concrete for instance\nthe agent could be a robot and the\nenvironment could be the real world it\ncould also be much more abstract for\ninstance the environment could be some\nabstract game or it could be a virtual\nenvironment\nit could be the internet and the agent\ncould be some program that tries to\ninteract with that environment instead\nso it's quite a flexible framework\nand we basically say that the agent then\nexecutes actions and observes the\nenvironment this is typically drawn in\nsuch a way as i did here where the\nactions go from the agent into the\nenvironment and the observations go from\nthe environment into the agent but of\ncourse you could also think of the\nobservation as being something that the\nagent pulls in in fact the observation\ntypically depends on the agent because\nthe agent will have some sort of sent a\nsensory\nmotor stream that is defined by its\ninterface for instance the agent could\nhave a camera and that defines which\nobservations it gets\nso\nthe main purpose of this course is then\nto go basically inside that agent and\nfigure out how we could build learning\nalgorithms that can help that agent\nlearn\nto interact better and what does better\nmean here well the\nagent is going to try to optimize some\nreward signal\nthis is how we're going to specify the\ngoal and the goal is not to optimize the\nimmediate reward so we're not just\ninterested in taking an action and then\nforgetting about everything that might\nhappen after no we're actually\ninterested in optimizing the sum of\nrewards into the future\ni'll explain it a little bit more\nclearly in a moment but it's good to\nappreciate that there must be some goal\nto this right if there's no goal\nspecified then it's unclear what we're\nactually optimizing and it's unclear\nwhat the agent will actually learn to do\nso we need some way some mechanism to\nspecify that goal\nmany cases when people show versions of\nthis interaction loop when talking about\nreinforcement learning they put these\nrewards next to the observation and\nthat's one useful way to think about\nthis that you take an action and then\nthe environment gives you an observation\nand a reward\nbut sometimes it's a bit more natural to\nthink of the reward as being internal to\nthe agent\nso you could also think of the reward\nsignal as being some sort of a\npreference function over these\nobservations or over sequences of\nobservations that the agent receives and\nthe agent just observes the world and\nfeels happier or less happy about what\nit sees and then tries to optimize its\nbehavior in such a way that it achieves\nmore of these rewards\nso this is why i didn't put the reward\nin the figure because sometimes it's\neasier to think of it as just coming\nfrom the environment through the\nexternal to the agent sometimes it's\neasier to think of it as being in some\nsense part of the agent but it should\nstill be there and it should be clearly\nspecified because otherwise it's unclear\nwhat the goal of this whole system would\nbe\nand\nthis reward function is quite central so\nit's good to stop and think about why\nthis is a good way to specify a goal and\nthis is formulated in this reward\nhypothesis that we see on the slide\nwhich states that any goal can be\nformalized as the outcome of maximizing\na cumulative reward\nnow i want to encourage you to think\nabout that and think about it critically\nand try to see if you can maybe even\nbreak it in some sense so breaking it\nwould mean coming up with a counter\nexample of a goal that you cannot\nspecify by maximizing the cumulative\nreward\nfeel free to pause the video and think\nabout this for a bit\nso i've not been able to come up with\nexamples myself\nand maybe this is somewhat even\ntrivially\ntrue in some sense\nbecause you could think of a reward\nsignal that basically just checks\nwhether you've achieved that goal that\nwe want to specify and then whenever\nyou've achieved the goal the reward\nsignal becomes one and before that it's\nzero\nthen optimizing this cumulative reward\nwould clearly correspond to maximizing\nor achieving that goal\ndoesn't mean it's easy to specify that\nsometimes it's hard to specify your goal\nprecisely or\nsometimes it's hard to specify a reward\nwhich is easy to optimize which is a\ncompletely different problem but that's\nnot under\nthis reward hypothesis this just states\nthat there must exist a reward and\nindeed sometimes there's many different\nways to specify the same goal for\ninstance instead of saying you get a\nreward of plus one whenever you achieve\nthe goal and zero before that you could\nalso say let me give you a reward of\nminus one in some sense a penalty on\nevery step before you've achieved the\ngoal and then zero rewards after you've\nachieved it\nthen you could think of the agent as\nmaximizing this cumulative reward as in\nsome sense minimizing these penalties\nwhich would also then maybe lead to the\nbehavior of achieving the goal as\nquickly as possible because minimizing\nthe number of -1 rewards number of steps\nuntil you've achieved the goal will then\nbecome relevant\nso we see that a goal could also include\nnot just\nthat it happens but also when it happens\nif we specify it in this way so it's\nquite a flexible framework\nand\nit seems to be a useful one that we can\nalso use to\ncreate\ncreate concrete algorithms that work\nrather well\nso some examples some concrete examples\nof what reinforcement learning problems\ncould then exist\nso here's a list including flying a\nhelicopter managing an investment\nportfolio controlling a power station\nmaking a robot walk or playing video or\nboard games\nall of these examples were picked\nbecause they have actually been used and\nreinforcement has been applied to them\nsuccessfully\nand for instance we could have a reward\nfunction for the helicopter that is\nrelated to air time or inverse distance\nto some goal\nor to pick for instance the video games\nyou could or board games you could think\nof a reward function that just looks at\nwhether you win or not so think of the\ngame of chess for instance you could\nhave a reward function that gives you\nplus one whenever you win minus one\nwhenever you lose\nand if the goal isn't to learn via\ninteraction these are all reinforcement\nlearning problems\nthis is irrespective of which solution\nyou use and i put that on the slide\nbecause sometimes people conflate the\ncurrent set of algorithms that we have\nin reinforcement learning to solve these\ntype of problems with the field of\nreinforcement learning but it's good to\nseparate that out and to appreciate that\nthere's a reinforcement building problem\nand then there's a set of current\nsolutions that people have considered to\nsolve these problems\nand\nthat set of solutions might be under a\nlot of development they might change\nover time\nbut it's first good to think about and\nappreciate whether we agree with the\nthe goal with the problem statement\nand if we agree with the problem\nstatement then we can think flexibly\nabout the solutions we don't have to be\ndogmatic about that and we can think\nabout new solutions that achieve the\nsame goal\nso it's good to separate that out and i\nwould argue that if you're doing any of\nthese problems where there is a reward\nfunction errors or sequential\ninteraction then you're doing\nreinforcement learning whether or not\nyou call your algorithm three and four\nenforcement learning algorithms\nso\nin each of these\nproblems that i specified these\nreinforcement training problems there\nmight actually be two distinct reasons\nto learn\nso the first one maybe obviously is to\nfind solutions\nso going back to the example of the\nhelicopter for instance you might want\nto find a\npolicy of behavior for this for this\nhelicopter so that it flies to a goal as\nquickly as possible\nbut maybe in order to optimize its\ncumulative reward it also sometimes has\nto do some more complicated things such\nas\nfirst go somewhere else to refuel\nbecause otherwise it won't even reach\nthe goal\nbut in the end you might have some\nlearning process and you might find a\nsolution two examples here to make that\nconcrete you could think of a program\nthat plays chess really well that might\nbe something you desire\nor you might want a manufacturing robot\nwith a specific purpose\nand then reinforcement could potentially\nbe used to solve these problems and then\nto deploy that solution\nnow a subtly different but importantly\ndifferent thing that you might want is a\nsystem that can adapt online\nand the purpose for this would be to\ndeal with unforeseen circumstances\nso to take the same two examples and to\ncontrast what we\nhow this is different in the chess\nprogram for instance you might not want\na chess program that displays maybe the\nmost optimal form of tests that you can\nfind\nbut instead you might want to find a\nprogram that learns to adapt to you now\nwhy would you do that well for instance\nyou might want a program that doesn't\nwin too often because then maybe your\nenjoyment is less so instead of\noptimizing the number of times it wins\nmaybe it actually wants to optimize so\nthat the number of times it wins is\nmaybe like roughly half of the time or\nsomething like that or maybe it wants to\noptimize how often or how long you play\nit because maybe that's a good proxy for\nhow much you enjoy playing it\nsimilarly you can think of a robot that\ncan learn to navigate unknown terrains\nmaybe you can pre-train this\nmanufacturing robot from the first\nexample because you have a very good\nsimulator for the setting that it might\nbe in\nbut in other cases maybe you don't have\na very good simulator or maybe you do\nhave good simulators for different types\nof the terrains that the robot might\nencounter but you do not know yet\nexactly what the terrain will look at\nwhere it will be deployed and there\nmight be unknown unknowns there might be\nthings that you haven't foreseen\nin those cases obviously it's quite\nuseful if you can continue to adapt if\nyou can continue to learn and we do that\nas well we continue to learn throughout\nour lifetimes\nso that's a different purpose but\nfortunately reinforcement learning can\nprovide algorithms for both those cases\nit's still good to keep in mind that\nthese are actually different goals and\nsometimes that becomes important\nnote that the second point about\nadaptive\nalgorithms to be able to adapt online is\nnot just about generalizing it's not\nabout finding a solution similar to in\nthe first category but one solution that\nis very general in some sense now it's\nreally about unknown unknowns it's\nreally about what if the environment\nchanges what if for instance you have a\nrobot and it gets deployed and at some\npoint there's wear and tear and you\nhaven't foreseen this there was no way\nto know exactly what would happen\nand\nall of a sudden the robot has to deal\nwith this somehow then if it can't adapt\nonline then it's really hard to find a\nsolution that is generic enough general\nenough that can deal with that\nand in indeed there are other reasons\nwhy it might be useful to learn online\nbecause it might be easier to have a\nsmaller program that continues to\ncontinue to track the world around it\nthan it is to try to find this one\nhumongous solution that can deal with\nall of the unforeseen circumstances that\nyou could possibly come up with\nso these are really different\ndifferent settings\nokay so now we finally are ready\nbasically to answer this question what\nis reinforcement learning and i'm going\nto say that basically reinforcement\nlearning is the science and framework of\nlearning to make decisions from\ninteraction\nso reinforcement learning is not a set\nof algorithms also not a set of problem\nproblems sometimes in shorthand we say\nreinforcement planning when referring to\nthe algorithms but maybe it's better to\nsay reinforcement learning problems or\nreinforcement learning algorithms\nespecially if we want to specify those\ntwo different parts of it and then\nreinforcement learning itself could just\nbe the the science and the framework\naround all of that\nthis has some interesting properties it\nrequires us to think about time and\nconsequences of actions and this is a\nlittle bit different from many other\ntypes of learning from for instance\nother types of machine learning where\noftentimes you are given a data set and\nfor instance you want to find a\nclassifier or something of the form\nand then\nmaybe there are no long-term\nconsequences you basically just\nspecify that your goal is to minimize\nthe errors that the system makes\nnow in reinforcement learning we would\nargue that maybe\nyou want to consider the whole of the\nsystem so maybe you don't just want to\nconsider the classifier but you also\nwant to consider the consequences of\nclassifying something wrong and that\nmight be taken into account if you\nconsider the whole framework\nthis makes it more challenging\nand it also means we have to actively\ngather experience because these actions\nwill change the data that we see\nwe might want to predict the future so\nnot just on one step thing so unlike a\nclassifier where you just get an input\nand you're only interested in like the\ninput for that or the output for that\nspecific input we might actually want to\nconsider\nfuture steps further into the future\nwhich is an interesting and tricky\nsubject\nand in addition we this is a more\ntypical thing that happens in machine\nlearning we have to deal with\nuncertainty somehow\nnow the benefit of this is that there's\nhuge potential scope but you might have\nalso realized by that this is also very\ncomplicated\nor difficult question in general how to\nsolve this very generic problem\nbut the upside is huge if we are able to\nfind good generic algorithms that can\ndeal with this very generic setting then\nmaybe we can apply this to many\ndifferent problems successfully\nand indeed one way to think about\nreinforcement planning is that it's a\nformalization of the ai problem as i\ndefined it earlier\nso\nit's good to appreciate the the basic\nthe ambition here that reinforcement\nbuilding is quite an ambitious vicious\nendeavor that doesn't mean that of\ncourse it sits on an island and in fact\nwe will see\nuh during this course that current day\nreinforcement learning is very\nsynergetic with uh\nwith deep learning which is all about\ntraining deep neural networks\nand\nindeed\nthis is also very it seems very very uh\nsuitable component for a full ai problem\nso the reinforcement learning\ndescription is just about formalizing\nthe problem that doesn't mean that we\ndon't need solutions from all sorts of\nsubparts of machinery\nokay now i'm going to show you an\nexample\nwhat we see here is an atari game this\nis an old video game from the 1980s\ncalled beam rider\nand the agent that is playing this game\nhas learned to play it by itself\nits observations were the pixels as you\nalso see them on the screen\nhere's a different atari game with\ndifferent pixels\nand in each of these cases the actions\nthat the agents would take are the motor\ncontrols which are basically just the\njoystick inputs for the atari\ngames which means the agent could press\nup down left right or diagonally and it\nwill press a fire button\nand then the agent just had to deal with\nthat in input output stream so it just\ngets these observations these pixels\nfrom the screen and it outputs these uh\njoystick controls and we see that they\ndid relatively well learning to play\neach of these different games even\nthough they're quite different so here's\na racing game enduro\nand it's good to appreciate that the\nagent is not even told what it's\ncontrolling right it just gets these\npixels so it's not told oh there's this\nthing at the bottom here which is kind\nof meant to be a racing car and your\ngoal is to pass these other cars now\ninstead you just get these pixels you\nget your motor controls and you get a\nreward signal\nnow in these games the reward signal was\ndefined as the difference in score on\nevery time step\non a lot of time steps this difference\nin score is zero that's fine but on\nother time steps it will be positive and\nthe agent tries to maximize the\nsummation of that over time so it wants\nto take actions that will lead it to\ngood rewards later on\nnow the most important thing to take\naway from this is that we have used a\nlearning system to find these solutions\nbut we didn't need to know anything\nabout these games ourselves like there\nwas nothing put into the agent in terms\nof strategy or even in terms of prior\nknowledge on what you're controlling on\nthe screen\nso the agent when it started playing\nspace invaders did not know that it was\ngoing to control this thing at the\nbottom which is shooting\nor that it was controlling one of these\nboxes in this example\nand that is the benefit of having a\ngeneric learning algorithm in this case\nthis algorithm is called dq n and we'll\ndiscuss it later in the course as well\nokay so now i'll go back to the\nslides\nso now i've given you a couple of\nexamples i've shown you these atari\ngames\nand now is a good time to start\nformalizing things a little bit more\ncompletely so that we know a little bit\nmore what's happening\nand in future lectures of course we will\nmake this much more\nclear and rigorous and for now we're\ngoing to give you kind of like a high\nlevel overview of what happens here what\nis this reinforcement learning problem\nwhat's inside that agent how could this\nwork\nso we're going to go back to this\ninteraction loop\nand\nwe're going to introduce a little bit of\nnotation where we basically say that\nevery time step t we receive some\nobservation ot and some reward rt\nas i mentioned the reward could also be\nthought of as being inside the agent\nmaybe it's some function of the\nobservations or you could think of this\nas coming with the observations from the\nenvironment and then the agent executes\nsome action so the action can be based\non this observation ot in terms of our\nsequence of interactions\nand then the environment receives that\naction and emits a new observation or we\ncould think of the agent as pulling in a\nnew observation and a next reward\nnote that we increment the time step\nafter taking the action\nso we say that the action is emitted at\ntime step t and then the next\nobservation is\nreceived at time set t plus one that's\njust convention this is where we\nincrement the time index\nyou can actually extend reinforcement\nlearning to continuous time as well\nrather than having these discrete time\nsteps\nbut we won't cover that in this course\nthe extensions are in some sense\nnot too difficult\nso it's good to have that in mind\nbut there are some subtleties that one\nwould have to consider\nso the reward here is a scalar feedback\nsignal it's just a number it can be\npositive it can be negative a negative\nreward you could call a penalty but we\njust call that a negative reward just to\nhave this one word that refers to the\nfeedback signal\nand just to recall i put the reward\nhypothesis on the slide again where we\nstate that any goal can be formalized as\nthe outcome of maximizing a cumulative\nreward\nthis\nthis instantaneous reward that indicates\nhow well the agent is doing at that time\nstep t\nand this helps define the goal of the\nagent\nand the cumulative reward is and the\naccumulation or the sum of these rewards\nover time\nit's useful to\nalso devote a letter to that which we'll\ncall g\nso roughly speaking you can think of g\nas kind of specifying the goal but we'll\nuse the term return to refer to this\nso the return\nis just shorthand for the cumulative\nreward or the sum of rewards into the\nfuture\nnote that the return is only about the\nfuture right so this is at some time set\nt so this is useful to determine which\naction to take because your actions\ncannot influence the past they can only\ninfluence the future so when we define\nthe return the return is defined as all\nof the future rewards summoned together\nbut the past rewards are in the past and\nwe can't change them anymore\nthen we can't maybe always hope to\noptimize the return itself so instead\nwe're going to define the expected\nreturn which we'll call a value\nso the value at time s\nwould simply be the expectation of the\nreturn\nso that's the sum of the rewards going\ninto the future conditioned on the fact\nthat you're in that state s\ni haven't defined what a state is yet\nbut for simplicity you could now think\nof this as just being your observation\nbut i'll talk more about that in a\nmoment\nso this value does depend on the actions\nthe agent takes and i will also make\nthat a little bit more clear in the\nnotation later on so it's good to know\nthat the expectation depends on the\ndynamics of the world but also the\npolicy that the agent is following and\nthen the goal is to maximize the values\nwe want to pick actions such that this\nvalue becomes large\nso one way to think about that is that\nrewards and values together define the\nutility of states and actions\nand there's no supervised feedback so\nwe're not saying this action is correct\nthat action is wrong instead we're\nsaying this sequence of actions has this\nvalue that sequence of actions has that\nvalue\nand then maybe pick the one that has the\nhighest value\nconveniently this and this is used in\nmany algorithms the rewards sorry the\nreturns and the values can be defined\nrecursively so the return at time sub t\ncan be thought of as simply the first\nreward plus the return from that time\nstep t plus one\nsimilarly the value can be defined\nrecursively so the value at some time\nsorry the value at some state s\nis the expected first reward you get\nafter being in that state and then the\nvalue of the state you expect to be in\nafter being in that state\nso the goal is maximizing value by\ntaking actions\nand actions might have long-term\nconsequences so this is captured in this\nvalue function because the value is\ndefined as the expected return where the\nreturn sums the rewards into the future\nand one way to think about this is that\nactual rewards associated with certain\nactions can be delayed\nwhat i mean with that is you might pick\nan action that might have consequences\nlater on\nthat are important to keep in mind but\nthat do not show up immediately in the\nreward that you get immediately after\ntaking that action\nthis also means that sometimes it's\nbetter to sacrifice immediate reward to\ngain more long-term reward and i'll talk\nmore about that in the next lecture\nso some examples of this might be one\nthat i mentioned before today\nrefuting a helicopter might be an\nimportant action to take even if it\ntakes you slightly farther away from\nwhere you want to go so this could be\nformalized in such a way that the\nrewards for that are low or even\nnegative for the act of refuting but the\nsum of rewards over time might be higher\nbecause eventually you get closer to\nyour goal than if you wouldn't refuel or\nto pick the last example learning a new\nskill might be something that is costly\nand time consuming at first might not be\nhugely enjoyable but maybe in the long\nterm it will yield you more benefits and\ntherefore you learn this new skill to\nmaximize your value rather than the\ninstantaneous reward\nfor instance maybe that's why you're\nfollowing this course\njust in terms of terminology we call a\nmapping from states to actions a policy\nthis is just shorthand in some sense for\nan action selection policy\nit's also possible to define\nvalues on not just states but on actions\nso these are typically denoted with the\nletter q for historical reasons so we\nhave the letter v to denote the value\nfunction of states and we have the\nletter q to denote the value function of\nstates and actions and this is simply\ndefined as the expected return condition\non being in that state and then taking\nthat action a\nso instead of considering some sort of a\npolicy which immediately could pick a\ndifferent action in state s we're saying\nno no we're in state s and we're\nconsidering taking this first action a\nnow this total expectation will then of\ncourse still depend on the future\nactions that you take so this still\ndepends on some policy that we have to\ndefine for the future actions but we're\njust pinning down the first action and\nconditioning the expectation on that\nwe'll talk much more in depth about this\nin\nlectures three four five and six\nso now we can basically summarize the\ncourse concepts before we continue\nso we said that the reinforcement\nplanning formalism includes an\nenvironment which basically defines the\ndynamics of the problem\nit includes a reward signal which\nspecifies the goal\nand sometimes this is taken to be part\nof the environment but it's good to\nbasically list it separately\nand then it contains an agent\nnow this agent might contain different\nparts and most of this course will\nessentially be about what's in the agent\nwhat should be in the agent how could we\nbuild learning algorithms that work well\nand some of the parts are listed here so\nthe agent will contain some agent state\nthis is just the internal state of the\nagent it will contain some policy\nand it could contain a value function\nestimate so a prediction of the value or\na model which might be a prediction of\nthe dynamics of the world i put\nquestions mark marks there because these\nare in some sense more optional than the\nfirst two the agent must have some\ninternal state this could be a very\nsimplistic state it could be a null\nstate or your agency could simply be the\nimmediate observation that you've\nreceived right now but it could be a\nmore complicated state and it must have\nsome policy it must select actions in\nsome way\nagain this policy could be particularly\nsimple it could be a random policy that\njust selects actions\ncompletely uniformly at random but there\nmust be some policy\nthe value function and the model are\nmore optional in the sense that they're\nnot essential parts but they are very\ncommon parts and i will discuss them a\nlittle bit in the remainder of this\nlecture\nso now it's time to go into the agent\nand we'll start with the alien state\nso this is one way to depict the\ninternals of the agent so now we're\ninside the agent and in this schematic\nhere on the right hand side time\nincrements as we go to the right\nand so we see inside the agent from the\nview inside the agent on every time set\nthere's an observation that comes in\nand then there's some internal state of\nthe agent and from the state of the age\nthe agent might make predictions\nand it should define some policy somehow\nand then the action gets selected by\nthis policy so i could have basically\ndrawn another arrow going from the\npolicy into the action which would then\ngo back into the environment\nbut we're focusing here on that state\ncomponent and state here basically\nrefers to everything that the agent\ntakes along with it from one time to the\nnext\nso if there are things that are not\ntaking along for instance the policy at\nthe instantaneous policy at the time set\nmight not be\ntaken along the predictions might not be\ntaken along or they could be in that\ncase they could just be part of the\nstate\nbut there might be other things in the\nstate as well there might be some memory\nin the state there might be a learned\ncomponents in the states everything that\nyou take along with you from one time to\nthe next we could call the agent state\nwe can also talk about the environment\nstates which is the uh\nthe other side of that coin\num in many cases the environment will\nhave some really complicated internal\nstates for instance in uh the example\nwhere the agent is a robot and the\nenvironment is the real world then the\nstate of the environment is basically\njust the state of all of the physical\nquantities of the world all of the atoms\nall of the quantum mechanics of the\nworld that's the environment state\nof course in many smaller examples if\nit's a virtual environment it could be\nmuch smaller but it could still be quite\ncomplicated this also means it's usually\ninvisible to the agent it's very really\nreally large and it's not part of the\nobservation stream per se\neven if it would be visible it might\ncontain lots of irrelevant information\nand it might just be simply too large to\nprocess but actually the first one is\nmore interesting it's usually just\ninvisible to the agent we can only see a\nsub-slice of it we can see a small part\nof it via our observation stream\nan important concept to keep in mind\nthen is that we can also formulate the\nwhole interaction sequence into\nsomething that we could call the history\nof the agent\nand this is simply everything that the\nagent could have observed so far so that\nincludes the observation from the\nenvironment but also the actions that\nthe agent took and the rewards that it\nreceived so this is really just taking\nthat interface and storing everything\nthat happens on the interface level and\nwe could call that the history of the\nagent\nso for instance it could be the full\nsensory motor stream of the robo\nnow we can say that the history is the\nonly thing that can be used to construct\nthe aging stage in some sense apart from\nwhatever prior knowledge you put in all\nthe way at the beginning but let's just\nset that aside for a moment and\neverything else must be a function of\nyour history there's nothing else\nessentially the agent has no additional\ninformation\napart from its sensory motor stream so\nthat's that's what you should be using\nto construct your agency\nthen a special case\nis when the\nagent can see the full environment state\nso that the observation is the full\nenvironment state and this is called\nfull observability i mentioned before\nalready this is a very special case this\nis not the common case at all but it's a\nuseful one and sometimes it's used for\ninstance in theoretical statements just\nbecause it's easier to reason about in\nsome cases\nand in that case the agent state can\njust be the observation right we don't\nneed to worry about this whole\ninteraction stream we can just observe\nwhatever the environment state is\nand then this should be sufficient in\norder to basically tell where you are\nyou don't need additional memory you\ndon't need anything else you just need\nthe environment state as your state\nnow in addition to that\nthere could be the learnable parts of\nthe agent right the agent might have\nsome parameters that it's learning and\nyou could also consider that to be part\nof the agent state\nin this case i'm actually not\nconsidering that to be part of the\nagency that's something that we also\nhave that's also part of the asians but\nlet's just set that aside and call that\nthe\nthe agent's mind is essentially separate\nfrom its state in this sense\nso in the fully observable case you can\njust look at your observation you could\nsay oh that tells me everything i need\nto know about the environment so i don't\nneed to log any of the previous\ninteractions\nand this leads us to an important\nconcept in reinforcement training which\nis the markov property\nand this has been used to formulate\nessentially the reinforcement problem\nand also precursors to this\nand importantly a markov decision\nprocess is essentially a very useful\nmathematical framework that allows us to\nreason about algorithms that can be used\nto solve these decision problems\nthe mark property itself states\nthat a process is markovian or a state\nis markovian for this process if the\nprobability of a reward and a subsequent\nstate\ndoesn't change if we add more history\nthat's what the equation on the slide\nmeans\nso we can see the probability of a\nreward and a state\nyou could you should interpret this as\nthe probability of those occurring on\ntimes of t plus one\nand we say that the the probability of\nthis happening condition on state st is\nequal to conditions on the full history\nup to the time set\nthat means if this is true that the\nstate contains\nexcuse me that the state contains all\nthe means you need to know\nso we don't need to store anything else\nfrom the history\ndoesn't mean that the state contains\neverything\nit just means that adding more history\ndoesn't help for instance if your\nobservations are particularly\nuninformative\nthen adding more uninformative\nobservations might not help so that\nmight lead to a markovian state but it\ndoesn't mean that you can observe the\nfull environment state\nhowever if you can observe the full\nenvironment states then you're also\nmarkovian\nso once the state is known the history\nmight be thrown away if you have this\nmarket property and of course this\nsounds uh very useful because the state\nitself might be a lot smaller than the\nfull history\nso as an example the full agent and\nenvironment state\nis markov but it might be really really\nlarge because as i mentioned the\nenvironment state might be humongous it\nmight be the real world\nand also the full history is markov\nwhich you can kind of clearly read from\nthis equation because if you put\nht on the left hand side where it says\nst then obviously this is true\nbut the problem with that is that that\nstate keeps growing so if we want to use\nthe full history as our agent states\nthen the\namount of memory that we're using inside\nthe agent's head keeps growing linearly\nover time and sometimes that also\nbecomes too too large or actually\noftentimes it also becomes too large\nso typically the agent said is some\ncompression of the history\nwhether it instead of the markov\nproperty is actually\nmaybe not even the most important\nquestion but it's an interesting thing\nto keep in mind\nso\nnote here that we use st deno to denote\nthe agent states not the environment\nstates\nand we'll use that convention basically\nthroughout where sometimes as a special\ncase\nthese are these will be the same because\nthe environment state might be fully\nobservable\nbut in general we will not assume that\nand then whenever you say states this is\nbasically the state and part on the side\nof the agent and that's specified\ndifferently\nnow i said that full observable cases\nare very rare so that we should talk\nabout the uh the complement of that\nwhich is the partial observable case so\nin this case the observations are not\nassumed to be markovian and i'll give\nyou an example or a couple of examples\nso for instance a robot with a camera\nwhich is not tallest absolute location\nwould not have markovian observations\nbecause at some point it might be\nstaring at a wall\nand it might not be able to tell where\nit is it might not be able to tell\nwhat's behind it or behind the wall\nit can maybe just see the wall and then\nthis observation will not be markovian\nbecause the probability of something\nhappening might depend on things that it\nhas seen before\nbut it doesn't see right now it may have\njust turned around and there might be\ninformation about what's behind it which\nshould influence the probability of what\nhappens next\nbut it can't see this from its\nobservations per se\nsimilarly\na poker playing agent only observes\npublic cards and its own cards it\ndoesn't observe the cards from the other\nplayers but obviously these are\nimportant for its future rewards so part\nof the environment state is then hidden\nto the agent\nso now using the observation essay would\nnot be markovian that doesn't mean it's\nnecessarily a bad idea but it means it\ncan be a bad idea because you're\nignoring some information that might be\ncontained in your past observations\nthis is then called a partial observable\nmarkov decision process or pomdp for\nshort\nand it's basically just an extension of\nthe market decision processes which\nwe'll define more rigorously in future\nlectures\nand it's good to keep in mind that this\nis basically the common case\nnote that the environment state itself\ncould still be mark off it's just that\nthe agent can't see it and therefore\ncan't know it\nin addition we might still be able to\nconstruct a markov agent state\nthe example i gave in the previous slide\nis you could always take your full\nhistory and that would be markovian the\nproblem with that is just it's too large\nbut maybe there are smaller asian states\nwe can construct which still hold enough\ninformation to be markovian\nso the agent states\nis\nan important concept and\nit must depend on this information that\nyou've seen before right this must\ndepend on this interaction stream\nand the agent actions then depend on the\nstate\nand it's some function of history so the\nexamples that i gave were like the\nstates could be the observation could be\nyour full history but more generally you\ncan also write this on recursively\nwhere the state at your next time set t\nplus one\nis some function of your previous state\nthe action that you take that you've\ntaken the reward you've seen and the\nobservation that you see so we're taking\none step in this interaction loop and\nwe're basically saying we're going to\nupdate the state\nto\n[Music]\nbe aware of this new time step\nclearly if we're concatenating the\naction reward and observation then st 1\ncould just be your full history if st is\nyour full history up to time step t\nso\nthe full history is contained within\nthis formulation also quite clearly the\nspecial case of just looking at the\nobservations contains formulation and\nthis is a more flexible way to think\nabout it\nand then u is the state update function\nnow as i mentioned it's often useful um\nto consider the agent's say to be much\nmuch smaller than the environment set\nand in addition you also typically\nwanted to be much smaller than the full\nhistory so we want this agent update\nfunction\nto give us some compression of the full\nhistory\nmaybe recursively and maybe the state\nactually stays the same size right so st\ncould be of a certain size we see new\naction reward and observation and we\ncondense all of the information together\ninto something that is the same size as\nst\nand here's an example just to make that\na little bit more concrete all of that\nso let's consider a maze and let's say\nthat the full state of the environment\nin a maze is this layout\nand in addition it's where you are in\nthe maze\nand that would define the full\nenvironment state\nbut let's say that the agent can't\nobserve all of that it can't observe its\nlocation in the maze and instead maybe\nyou can only see this little three by\nthree around itself so in this case the\nagent would be in the center of this\nthree by three uh block\nand what it can see is exactly the\npixels around it the the cells around it\nso it can see for instance that above it\nit's empty\nto the left and the right there's a wall\nin black below it it's empty so it could\nwalk up it could walk down and also it\ncan look slightly around the corner\nwhere i can see that if it goes up and\nthen right there's also an empty spot\nbut if it goes up and left it would bump\ninto a wall\nand that's all that it can see\nnow this observation is\nnot markovian because if we look at a\ndifferent location\nthese observations are actually\nindistinguishable\nso if we would just use the observation\nin this case then the agent won't be\nable to tell where it is\nand we can also talk about why that\nmight be problematic\nso let's say that the agent starts in\nthe top right corner\nand let's say that the goal for the\nagent is to go to the top left corner\nthen if you consider the shortest path\nin the state that we see the observation\nthat we see in the top right here the\noptimal action would be the step down\nbecause that's in the direction of the\ngoal because we have to go via the\nbottom of this maze in order to reach\nthe top left corner\nhowever if you then look at the left\nobservation in that observation the\noptimal action would be to go up\nbut if the agent can't distinguish\nbetween these two if it would be using\nthe observation as its full agent state\nand its action selection policy must\ndepend on only that observation\nthen it's unclear what it should be\ndoing\nin the top right it should be going down\nin the left should be going up but\nthere's no single policy single function\nof this observation that will do the\nright thing in both cases\nthis is why it can be problematic to not\nhave a markovian state observation\nso now\ni actually actually want you to think\nabout for a second so feel free to pause\nthe video here\nand think about how you might be able to\nconstruct a markov in asian state for\nthis specific problem and maybe for any\nreward sequence so not just for the one\nthat goes from the top right to the top\nleft but maybe that one that works for\nany reward signal\nso feel free to pause the video and then\ni'll talk about this a little bit more\nso one thing that you may have come up\nwith is well maybe we can use that thing\nthat you said where you can use the full\nhistory yes the full history would be\nmarkovian would be would be rather large\nso i think many of you will have kind of\ndiscounted that as being not the most\npleasant or feasible solution\nso maybe we could do something that's a\nlittle bit\nin that direction but not quite the same\nso let's say we consider storing not\njust the observation that we see right\nnow but also the previous observation\nwould that work\nwell it kind of depends actually it\ndepends on the policy\nand it depends whether the state\ntransitions here in the real world are\nare completely deterministic so if you\ngo down you really go down or whether\nthere are some noise in there where\nsometimes when you press down you\nactually go up\nbecause note that if you look at both of\nthese observations that are highlighted\nright now if you step down one step the\nobservation is still the same\nso if you would come from this situation\nbelow where we currently are and you\nwould just concatenate these two\nobservations that would not be\nsufficient to be able to tell where you\nare so just concatenating two\nobservations is not necessarily\nmarkovian in this environment\nhowever\nit can be sufficient if your policy\nnever does this so if we stepped on the\nright hand side in the right top corner\nright first we step left and then we\nstep down this brothers to where we\ncurrently are\nif we would have stored the fact that we\njust stepped down and then we see this\nthen we know that we are where we are\nbecause then the previous observation is\nsufficient and then we step down again\nand if the policy never does that same\naction in the left state\nthen the ordering of the observations is\nenough to distinguish the left from the\nfrom the top right\nbut in general for any policy for\ninstance for a uniformly random policy\njust concatenating two observations is\nnot sufficient in order to get a\nmarkovian state in this case\nokay\nso in general what i'm doing there is\nbasically trying to construct a suitable\nstate representation to deal with the\npartial observability in the maze\nand as examples i mentioned using just\nthe observation might be enough using\nthe full history might be too large but\ngeneric you can think of some update\nfunction and then the question is how do\nwe pick that update function and that's\nactually what we were doing just now\nlike we were trying to hand pick a\nfunction u\nthat updates\nthe state in such a way to take into\naccount the stream of observations the\nexample that i gave where i just\nconcatenate two observations\nwould be where you just keep track of\nthis buffer and whenever you see a new\nobservation it basically replaces the\noldest observation with a new one\nwith a newer one and then adds the\nnewest one on top so you have\nlike a two observation buffer in that\ncase\nexcuse me\nso this is a generic update you can you\ncan do other\nother things there as well of course\nbut it's good to to note that\nconstructing a full markov unit you say\nit might not be feasible like your\nobservation might be really complicated\nand it might be really hard to construct\na full markovian agency and so instead\ninstead of trying to always shoot for\ncomplete markovianness maybe that's not\nnecessary maybe it's more important that\nwe allow good policies and good value\npredictions and sometimes that's easier\nsometimes going for optimal is really\nreally hard but going for very good is\nsubstantially easier and that's\nsomething more generally that we'll keep\nin mind when we want to deal with messy\nbig real world problems where optimality\nmight be out of reach\nokay\nnow we're going to continue our journey\ninside the agent and we're going to go\nto the next bits which are the policy\nthe value function in the model starting\nwith the policy\nso we covered the agencies now we're\ngoing into policy and then immediately\ninto the value function and the model\nand the policy is simply something that\ndefines the agent behavior it's not a\nvery complicated construct it's a\nmapping from agent say to actions and\nfor instance we can write this like this\nfor a deterministic policy it could be\nconsidered simply a function that takes\na state as input and outputs an action\nnow actually it will be more common and\nuh often more useful to think of\nstochastic policies where instead\npi means the probability of an action\ngiven a state\npi is just conventional notation for\npolicies we often use pi to denote a\npolicy and the stochastic policies in\nsome some more general case so typically\nwe consider this a probability\ndistribution of actions\nand that's basically it in terms of\npolicies of course we're going to say a\nlot more about how to optimize these\npolicies how to represent them how to\noptimize them and so on but in terms of\ndefinitions all that you need to\nremember is that pi denotes the\nprobability of an action given a state\nand then we can move on to value\nfunctions and value estimates\nand i'm going to\nwhat i have here on the slide is a\nversion of the value function as i\ndefined it earlier and i want to mention\na couple of things about this first of\nall it's good to appreciate that this is\nthe definition of the value later we'll\ntalk about how to approximate that this\nis just defining it\nand i've extended it in two different\nways from the previous definition that i\nhad\nfirst i made it very explicit now that\nthe value function depends on the policy\nand the way to reason about this if i\nhave this conditioning on pi\nmeans that i\nuh i could write this long form to say\nthat every action at subsequent time\nsteps is selected according to this\npolicy pi\nso note that we're not conditioning on a\nsequence of actions\nnowhere conditioning on a function that\nis allowed to look at the states that we\nencounter and then pick an action which\nis slightly different\nthe other thing that we've done now on\nthis slide introduce a discount factor\nthis is a somewhat orthogonal thing but\ni thought i should include it here so\nthat we have the generic form of a value\nfunction which conditions on the policy\nand includes potentially this discount\nfactor which is a very common construct\nin reinforcement learning\nand one way to think about that is that\nthe discount factor helps determine the\ngoal in addition to the reward function\nfor instance if you consider a reward\nfunction to be plus one on every time\nstep\num\nthen it could be\ninfinitely large\nalternatively if you think of a maze\nwhere the reward is zero on every time\nset until you reach the goal\nthen the value function for a uniformly\nrandom policy would be\nsorry if it's zero every time's at one\nwhen you reach the goal then any policy\nthat eventually reaches goals gets a\nvalue of one so then we can't\ndistinguish between getting there\nquickly\nso sometimes discount factors are used\nto define goals in the sense oh maybe\nit's better to look at the near-term\nrewards a little bit more unless it's a\nlong-term reward so this allows us to\ntrade off the importance of immediate\nversus long-term rewards\nso to look at the extremes to make it a\nbit more concrete you can consider a\ndiscount factor of zero\nif you plug that into the definition of\nthe value as it's\nwritten on the slide there you see that\nthen the value function just becomes the\nimmediate reward all of the other\nrewards are cancelled out because\nthey're multiplied with the zero\ndiscount\nso that means if your discount factor is\nsmall or in a special case if it's zero\nthen you only care about the near term\nfuture\nif you don't want to optimize your\npolicy then the policy would also be a\nmyopic policy a short-sighted policy\nwhich only cares about immediate reward\nconversely the other extreme would be\nwhen the discount factor is one this is\nsometimes called the undiscounted case\nbecause then the discounts basically\ndisappear from the value definition we\nget the definition that we had before\nwhere all rewards are equally important\nnot just the first one but the second\none also is equally important the first\none and that also means that you no\nlonger care in which order you receive\nthese rewards\nand sometimes it's useful to have a\ndiscount factor that is in between these\ntwo extremes in order to define the\nproblem that you actually want to be\nsolving\nnow as i mentioned the value depends on\nthe policy and then ultimately we want\nto optimize these so we want to be able\nto reason about how can we pick\ndifferent policies\nand we can now do that because the value\nfunction cannot be used to evaluate the\ndesirability of states and also we can\ncompare different policies on the same\nstate we can say one value might have a\ndifferent sorry one policy might have a\nhigher value than a different policy and\nthen we can maybe talk about the\ndesirability of different policies\nand ultimately we can also then use this\nto select between actions so we could do\nso no here we've defined the value\nfunction as a function of a policy\nbut then if we have a value function or\nestimated value function we can then\nmaybe use that to determine a new policy\nso this will be talked about in a lot\nmore depth in future lectures but you\ncan think of this as kind of being\nan incremental learning system where you\nfirst estimate the value of a policy and\nthen you improve your policy by picking\nbetter policies according to these\nvalues and that's indeed a relevant\nalgorithm idea that we'll get back back\nto later\nas i mentioned before the value\nfunctions and returns have recursive\nforms so the return now has its discount\nfactor in the more general case\nand the value function is also recursive\nwhere again as i mentioned before the\nvalue of a state can be defined as the\nexpected value of the immediate reward\nplus now the discounted value at the\nfuture state for that same policy\nand here the notation a\ntilde pi just means that a is sampled\naccording to the probability\ndistribution pi\nand we'll just use that same notation\neven if the probability distribution is\njust deterministic for simplicity\nthis is called a bellman equation it was\nfirst described by richard bellman in\nthe 1950s and it's useful because you\ncan turn it into algorithms\nso these equations are heavily exploited\nand a similar equation can be written\ndown\nfor the optimal value which is really\ninteresting\nso note that the equation above this is\nconditioned on some policies so we have\nsome policy and we can then determine\nits value turns out we can also write\ndown an equation for the optimal\nvalue that you can have so there is no\nhigher value that you can get in this\nsetting\nand this turned out to adhere to this\nrecursion that is written on the slide\nwhere v star the optimal value of state\ns\nis equal to the maximization over\nactions of the expected reward plus\ndiscounted next value conditioned on\nthat state in action\nimportantly this does not depend on any\npolicy it just depends on the state\nand this recursion is useful\nit defines recursively the um the\noptimal value because know that the v\nstar is on the left hand side and the\nright hand side but we can use this to\nconstruct algorithms that can then learn\nto approximate v-star\nin the future\nin future lectures we will heavily\nexploit these equations and we'll use\nthem to create concrete algorithms\nand in particular of course we often\nneed to approximately so the previous\nslide just defines the value of a\ncertain policy and it defines the\noptimal value doesn't tell you how to\nget them and in practice you can't\nactually get them exactly and we'll have\nto approximate them somehow and we will\ndiscuss several algorithms to learn easy\nefficiently\nand the goal of this would be that if\nyou have an accurate value function then\nwe can behave optimally i mean if we\nhave a fully accurate value function\nbecause then you can just look at the\nvalue function we could define a similar\nequation that we had on the previous\nslide for state action values rather\nthan just for state values\nand then the optimal policy could just\nbe picking the optimal action according\nto those values so if we have a fully\naccurate value function we can use that\nto construct an optimal policy this is\nwhy these value functions are important\nbut if we have a suitable approximation\nwhich might not be optimal might be we\nmight not be perfect it might still be\npossible to behave very well even in\ninteractively large domains\nand this is kind of the promise for\nthese approximations that we don't need\nto find the precise optimal value in\nmany cases it might be good enough to\nget close and then the resulting\npolicies might also be performed very\nwell\nokay so the final component inside the\nagent will be a potential model this is\nan optional component similar to how the\nvalue functions are optional although\nthey are very common\nand a model here refers to a dynamics\nmodel of the environment the term is\nsometimes used more generally for other\nthings as well in uh artificial\nintelligence or machine learning but in\nreinforcement we typically when we say\nwe have a model we typically mean a\nmodel of the world in some sense so that\nmeans the model predicts what the\nenvironment will do next for instance we\ncould have a model\np which predicts the next state\nwhere maybe if you give it inputs as\ninputs a state an action and a next\nstate the output of this thing is an\napproximation to the actual probability\nof seeing that next state after\nobserving this previous state and action\nso again for simplicity might be good to\nkeep in mind a specific agent states\nwhere for instance these agencies could\nbe your observation\nthen this would be the probability of\nthe next observation given the previous\nobservation and the previous action\nand we could try to model that we could\ntry to approximate this\nand then in addition we could also\napproximate the reward function which\ncould be for instance conditioned on\nstate and action where this would just\nbe the expected reward given that you\nare in that state and taking that action\na model doesn't immediately give us a\ngood policy like for value functions we\ncan actually just kind of read off a\npolicy if we have state action value\nfunctions we can pick actions according\nto these values for a model we don't\nimmediately have that we would still\nneed to conduct some sort of a planning\nmechanism we'll talk about specific\nalgorithms that can be used in addition\nto like uh sorry on top of a model in\norder to extract a policy but it's good\nto keep that in mind in general that the\nmodel would still require additional\ncomputation in order to extract a good\npolicy\nin addition to the expectation above for\ninstance for the reward we consider the\nexpected reward\nwe could also consider a stochastic\nmodel or\nan expectation model for the state so\nthe state's\nmodel here in particular\nthis would be an example of a\ndistribution model where we try to\nactually\ngrasp the full distribution of the next\nstate given the current state in action\nyou could also instead try to\napproximate the expected next state or\nyou could try to find a model that just\noutputs a plausible next state\nor maybe randomly gives you one of the\nstates that could happen these are all\nchoices design choices and it's not 100\nclear in general yet what the best\nchoices are\nand now i'll go through an example to\ntalk about all of these agent components\na little bit\nit's just a very simple example we'll\nsee much more extensive examples in data\nlectures\nand in particular we're going to\nconsider this maze\nso we'll start at the left and the goal\nis at the right and we define a certain\nreward function which gives you a minus\none per time step\nthat means that the optimal thing to do\nis to go to the goal as quickly as\npossible because then you'll have the\nlowest number of minus ones\nthen the actions will be up down left\nand right or north east south and west\nif you prefer\nand the agent location is the state\nlet's say that this is fully observable\nso you can basically just tell where you\nare maybe you could think of this as x y\ncoordinates which are easily\neasily shown to be markovian in this\nsetting\nso here's an example which shows a\npolicy\nand in fact it shows the optimal policy\nso in every state we see an arrow this\narrow depicts which action to take so\nfor instance in the left most states the\narrow points right\nso we say that in the leftmost state the\npolicy now will take the action right\nthis policy is a deterministic policy\nthat indeed gives us the shortest\npath to the goal and it will be an\noptimal policy you could also consider a\nstochastic policy which might select\nmultiple actions with non-zero\nprobability\nhere is the value of that policy on the\nprevious slide which happens to also be\nthe optimal value function which as you\ncan see increments every time you step\naway from the goal and this is because\nthe value function is defined as the\nexpected\nsum of rewards until uh\ninto in into the indefinite future but\nif the episode ends at the goal then the\nrewards stop there\nso if you're one step away from the goal\nthe value will just be minus one for\nthat optimal policy if your two steps\naway it will be -2 and so on\nthis is a model and specifically this is\nan inaccurate model because note that\nall of a sudden a part of the maze went\nmissing\nso in this case the numbers inside the\nsquares are the rewards so these are\nmolds as just oh we've learned the\nreward is basically minus one everywhere\nmaybe this is very quick and easy to\nlearn and the dynamics model was learned\nby simply interacting with the\nenvironments but turns out maybe we\nhaven't actually gone to that it's\nthat portion there in the left corner\nleft bottom corner and therefore the\nmodel is inaccurate and wrong there if\nyou then would use this model's plan it\nwould still come up with the optimal\nsolution for the other states that this\ncan see but it might not have any\nsolutions for the states it hasn't seen\nit's just an example of course it's\nunrealistic to have an accurate value\nfunction but an inaccurate model in uh\nin this way specifically\nbut it's just an example to say oh yeah\nyour model doesn't have to be perfect if\nyou learn it it could be imperfect the\nsame of course holds for the policy and\nvalue function these could also be\nimperfect\nokay now finally before we reach the end\nof this lecture i'm going to talk about\na different\nsome different agent categories\nand in particular this is basically a\ncategorization it's good to have this\nterminology in mind which refers to\nwhich part of the agent are used or not\nused\nand a value-based agent is a very common\nuh\nversion of an agent and in this agent\nwe'll learn a value function but there's\nnot explicitly a policy separately\ninstead the policy is based on the value\nfunction\nthis agent that i showed earlier that\nwas playing atari games is actually of\nthis form where this agent learns state\naction value functions and then picks\nthe picks the highest rated action in\nevery state with a high probability\nconversely you can think of a\npolicy-based agent which has an explicit\nnotion of a policy but doesn't have a\nvalue function i haven't yet told you\nany algorithms how you could learn such\na policy if you're not learning values\nbut we'll actually see an example of\nthat in the next lecture\nand then there's the terminology actor\ncritic the term actor critic refers to\nan agent which both has an explicit\nrepresentation of a policy\nand an explicit representation of a\nvalue function these are called actor\ncritics because the actor refers to the\npolicy part there's some part of the\nagent that acts and the value function\nis then typically used to update that\npolicy in some way so this is an\ninterpreted as a critic that critiques\nthe actions that the policies takes and\nhelps it select better policies over\ntime\nnow all of these agents could be\nmodel-free which means they could have a\npolicy and or a value function but they\ndon't have an explicit model of the\nenvironment\nso note in particular that a value\nfunction can of course also be\nconsidered some model of some part of\nthe environment it's a model of the\ncumulative expected rewards\nbut we're not calling them a model in\nreinforcement learning parlance\ntypically so instead if you just have a\nvalue function we tend to call this\nmodel free\ni'm saying that not because it's a great\ndefinition or a great division between\nagents but because it's a very common\none so if you read papers and they say\nsomething about model-free reinforcement\nplanning this is what they mean there's\nno explicit dynamics model\nso conversely a model-based agent could\nstill ultimately oh sorry could still\noptionally have an explicit policy and\nor a value function but it does in any\ncase have a model some model based\nagents only have a model and then have\nto plan in order to extract their policy\nother model based agents have a model\nbut in addition to that have an explicit\npolicy and for instance use the model to\nsometimes just incrementally improve\nvalue function or our policy\nso now finally we're going to talk about\nsome sub problems of the rl problem\nprediction\nis about evaluating the future\nso for instance learning a value\nfunction you could call a prediction\nproblem and this is indeed often the\nterminology that is used typically when\nwe say prediction we mean for a given\npolicy so you could think about\npredicting the value of the uniformly\nrandom policy for instance or of a\npolicy that always goes left or\nsomething of the form\nconversely control is about optimizing\nthe future finding the best policy\nso it's good to note that this\nterminology is used quite\nfrequently in papers so it's good to\nhave that in mind and often of course\nthese are quite related because if we\nhave good predictions then we can use\nthat to pick new policies\nin fact the definition of the optimal\npolicy\npi star is the arc max of policies over\nthese value functions by definition the\nvalue function defines which policies\nare uh\nthe ranking on policies essentially your\npreference on policies that doesn't mean\nthat you need to have these value\nfunctions per se in order to learn\npolicies but it just shows shows how\nstrongly related to the problems of\nprediction and control are\nin addition there's an interesting\nquestion that i encourage you to ponder\na little bit which is that\nthis is something that rich saturn often\nsays\nthat in one way or the other prediction\nis maybe very good\nform of knowledge\nand in particular\nif we could predict everything\nit's unclear that we need additional\ntypes of knowledge\nand i want you to ponder that and think\nabout whether you agree with this or not\nso if you could predict everything\nis there anything else that we need\nso feel free to pause the video and\nthink about that for a second\ni'm going to give you one suggestion so\nindeed if you can't predict everything\nabout the world this gives you a lot of\nknowledge\nit might not immediately tell you how to\ndo things so maybe it's sometimes useful\nsimilar to these policies and value\nfunctions sometimes it can be useful\nespecially if we're approximating so we\ncan't predict everything perfectly it\ncan be useful to separately store\npredictions and separately scores\nstore policies or you could think of\nthese as them being skills in some sense\nbut indeed predictions are very\nrich form of knowledge and many things\ncan be phrased as a predictive problem\neven if they're not immediately clearly\na predictive problem uh\nif you first think about them\nand as i've referred to when i was\ntalking about models there's two\ndifferent parts to the reinforcement\nvoting uh\nproblem one is about learning\nand this is the common setting which we\nassume where the environment is\ninitially unknown and the agent\ninteracts with the environment and\nsomeone has to learn\nwhether it's learning a value function a\npolicy or a model all of that could be\nput under the header of learning\nand then separately we could talk about\nplanning so planning is a common term in\nartificial intelligence research\nand planning is typically about when you\nhave a model so let's say the model of\nthe environment is just given to you\nand then the agent somehow figures out\nhow best to optimize that problem\nthat would be planning so that means\nyou're using some compute to infer from\nthe statement of the problem from the\nmodel is given what the best thing to be\nto be done is\nnow\nimportantly the model doesn't have to be\ngiven but could also be learned but then\nit's good to keep in mind that the model\nmight be slightly inaccurate so if you\nplan exhaustively in a learn model you\nmight find a certain policy but it's\nunclear that this policy is actually\noptimal in the true world because the\nmodel might not be completely accurate\nand indeed the planning might latch on\nto certain inaccuracies in the model and\nhence might find solutions that are\nactually not that suitable for the real\nworld because for instance the model\nmight have a holy wall somewhere that is\nnot actually there and then the shortest\npath might take the agent through that\nhole which isn't actually there and the\npolicy might not be great that you get\nfrom there\nbut we can think of planning more\ngenerally as some sort of an internal\ncomputation process so then learning\nrefers to\nabsorbing new experiences from this\ninteraction loop and planning is\nsomething that sits internally inside\nthe agent's head it's a purely\ncomputational process and indeed i\npersonally like to define planning as\nany computational process that helps you\nimprove your policies or predictions or\nother things inside the agent without\nlooking at new experience\nlearning is the part that looks at a new\nexperience that takes in your experience\nand somehow condenses that and planning\nis the part that does the additional\ncompute\nthat maybe turns in a model that you've\nlearned into a new policy\nit's important also to know that all of\nthese components that we've talked about\nso far can be represented as functions\nwe could have policies that map states\nto actions or to probabilities over\nactions value functions that map states\nto to expected rewards or indeed also\ntwo probabilities of these we have\nmodels that map states to states or\nstate actions to states and we could\nhave rewards that map states to rewards\nagain or distributions over these and we\nhave a state of that function that takes\na state and an observation and\npotentially an action and a reward and\nmaps it to a subsequent state\nall of these are functions and that's\nimportant because we have very good\ntools to learn functions specifically\nthese days neural networks are very\npopular very successful and the field of\nresearching how to train neural networks\nis called deep learning\nand indeed in reinforcement we can use\nthese deep learning techniques to learn\neach of these functions and this has\nbeen done with great success\nit is good to take a little bit of care\nwhen we do so because we do often\nviolate assumptions from say supervised\nlearning for instance the data coming at\nus might be correlated\nbecause for instance think of a robot\noperating in a room it might spend some\nsubstantial time in that room so if you\nlook at the data coming into the agent\nit might be correlated over time and\nthen sometime later might go somewhere\nelse and this might be less correlated\nbut there might be in the near term\nquite some strong correlations in the\ndata which are sometimes assumed not to\nbe there when you do supervised learning\nin addition the problem is often assumed\nto be stationary in supervised learning\nin many supervised learning problems not\nin all of course but in reinforcement\nwe're often interested in non-stationary\nthings think for instance of a value\nfunction as i mentioned the value\nfunction is a is typically\nconditioned on a policy but if we're\ndoing control if we're trying to\noptimize our policy the policy keeps on\nchanging that means that the relevant\nvalue functions maybe also keep on\nchanging over time because maybe we want\nto keep track of the value of the\ncurrent policy but if the policy keeps\non changing that means that the value\nfunction also needs to change\nso this is what i mean when i say we\noften violate assumptions from\nsupervised learning that's not\nnecessarily a huge problem but it does\nmean that whenever we want to use some\nsort of a deep learning technique\nsometimes they don't work out of the box\nso deep learning is an important tool\nfor us when we want to apply\nreinforcement learning to big problems\nbut deep reinforcement learning which is\nbasically a research field of the merger\nof deep learning and reinforcement\nburning or how to use deep learning in\nreinforcement birding is a very rich and\nactive research field you can't just\nplug in deep learning and then hope that\neverything will immediately work that\nworks up to a point but there's lots of\nreasons to be done exactly at that\nintersection of deep learning and deep\nreinforcement learning we'll talk much\nmore about that later in this course\nokay\nnow that brings us to the final examples\nso i talked about atari let's make it a\nlittle bit more specific now what was\nhappening in the atari game that i\nshowed you so you can think of the\nobservations as the pixels as i\nmentioned at that time point in time as\nwell\nthe output is the action which is the\njoystick controls and the input is the\nreward here on the slide it actually\nshows the score but the actual reward\nwas the difference in score on every\ntime step\nnote that the rules of the game are\nunknown and you learn directly from\ninteractive gameplay so you pick actions\non the joystick you see pixels and\nscores and this\nis a well-defined resource and printing\nproblem and we have algorithms that can\nlearn to deal well with this\nas a different example here's a\nschematic example a little bit more of\nan illustrative example\nand this is easy to easier to reason\nthrough this is why we sometimes use\nthese much smaller examples and\noftentimes the conclusions still\ntransfer so the entire example is an\nexample of a rich messy\nhard problem in some sense right and\nthis would be an example of a very small\nscale illusive problem\nand we do this because we can often\nlearn something from these smaller\nproblems that we can apply to these much\nharder to understand difficult big\nproblems\nso in this specific example which is\nfrom the susan lombardo book\nit's basically a great world without any\nwalls although there might be walls at\nthe at about at the edges essentially\nbut not any walls inside\nthe 5x5 grid\nand there's a reward function which is\ndefined as -1 when bumping into a wall\nzero on most steps\nbut in if you take any action from state\na the state that is labeled with a\nyou get a plus 10 reward and you\ntransition to a prime\nso even if you press say up from set a\nyou still find yourself in a prime and\nyou get plus 10. similarly from state b\nyou would transition to state b prime\nand you get plus five\nnow we can ask several different\nquestions about this setting and there\nmight be reasons why we might be\ninterested in these different\nquestions so a first question could be a\nprediction question which is for\ninstance what is the value of the\nuniform random policy that selects all\nof the actions uniformly at random\nand that's depicted here on the right\nhand side in figure b\nand what we see here is that this is\nquite a complicated construct right i\nwouldn't have been able to tell you just\nimmediately just by looking at the\nproblem what the value function is for\nthe optimal for sorry for the uniformly\nrandom policy but we can use\nreinforcement building algorithms which\nwe'll talk about in future lectures to\ninfer this\nand to figure out what that value is\nand it turns out just to look at this a\nlittle bit more in detail that of course\nthe value of state a is quite high\nbecause from this state you often get a\nhigh you always get a high reward\nbut it's lower than 10 because the\nrewards after this first reward of 10\nare negative you see that the value of\nstate a prime is actually minus 1.3\nsorry i didn't say but there's a\ndiscount factor here as well 0.9\nwhich means that the\nthis is why the the value of state a is\n8.8\nand the value of state a prime is 1.3\nand the difference between them is not\nquite 10 right um\nbut from from state b\nyou often get a minus one because you\noften find yourself bumping to the to\nthe ball wall to bottom\nor you don't get a minus one but then\nyou might get a minus one on the next\ntime so because you might have walked\nleft to the corner\nand it's quite a complicated thing\nbecause of the discount factor because\nof the dynamics of the world but we can\nsee that state a is desirable state b is\nsomewhat desirable\nstates in the bottom left are quite\nundesirable\nbut you might actually be more\ninterested in okay but what's the\noptimal thing to be doing and that to me\nis not immediately obvious right should\nyou be going to state a and then loop to\na prime\nand get this plus 10 every time you\ncould but it takes you a couple of steps\nin between each two times you do that\ntransition\nyou could also go to state b and go to b\nprime and then you can get these\ntransitions more often\nnow it turns out we could also figure\nout what the optimal value function is\nfor this problem and what the optimal\npolicy is\nif you look at the optimal value they're\nall positive now because you never have\nto bump into a wall anymore because the\noptimal policy doesn't bump into walls\nso even the top sorry the bottom left\ncorner now has positive values\nand in fact the lowest positive values\nare in the bottom right corner now\nbecause from there it takes you a long\ntime to get to the best possible state\nyou can and it turns out the best state\nyou can be in is state a\nlooping with these plus 10 rewards is\napparently more beneficial than looping\nwith these plus 5 rewards even though\nthe difference in\nuh\ndistance\non these plus fives is smaller\nso you can get more plus fives in a row\nvery quickly by going from b to b prime\nafter each like every time again but\ngoing from a to a prime is apparently\nmore profitable in the long term and we\ncan see this in figure c here as well\nwhere the optimal policy is depicted\nwe see that if you're in almost any\nstate what you should be doing is you\nshould be moving to state a this will\ntransition you all the way to the bottom\nto a prime and from there you'll just\nmove straight up again\nup to state a and repeat\nconversely if you're in state b prime\nif you just look at where b prime is in\nthis you would either go up or left it\ndoesn't actually matter which one\nthey're equally good\nbut if you go up you would then move\nleft so you wouldn't move into state b\ninstead you\nwould move left and then you move up or\nleft again in order to get state a\nthere's only one state that would move\ninto b\nwhich is the top right corner\nbecause from the top right going around\nstate b and then going all the way to a\nwould take so long that it's actually\nmore beneficial to jump into state b\nwhich will transition you to b prime and\nthen from there we'll go to state a and\nthen loop indefinitely\nso this is quite subtle i wouldn't have\nbeen able to tell you just from looking\nat the problem that this would be the\noptimal policy but fortunately we have\nlearning and planning algorithms that\ncan sort that out for us and they can\nfind this optimal solution without us\nhaving to find it\nso popping up in this course we will\ndiscuss how to learn by interaction we\ndidn't really discuss it in this lecture\nin this lecture we just talked about the\nconcepts and the terminology and things\nlike that but we haven't really given\nyou algorithms yet we will do that in\nthe subsequent lectures\nand the focus will be on understanding\nthe core principles and learning\nalgorithms so it's less about what the\ncurrent state of the artist will touch\nupon that a little bit for sure but it's\nless about specific algorithms that\npeople happen to use right now and then\ngo all the way to the depth of those\nwe will do that for some algorithms but\nit's much more important to understand\nthe core principles and learning\nalgorithms because the algorithms that\nare currently safer they will change\nnext year there will be new algorithms\nand if you understand the core\nprinciples\nthen you can understand these new\nalgorithms and maybe you could even\ninvent your own algorithms\ntopics include exploration in the next\nlecture\nin something called bandits which is\nbasically one step\nmark of decision processes\nwe will talk about more about what\nmarketing decision processes actually\nare like how are they mathematically\ndefined and what can we say about them\nand we will talk about how to plan in\nthose with dynamic programming this will\nbe the lectures after the next lecture\nby this user will be given by diana\nand then we will use that to\nthen go into model-free prediction and\ncontrol algorithms you may have heard of\nan algorithm called q-learning\nor i mentioned earlier in this lecture\nan algorithm called dqn dqm is short for\ndeep q network\nq as i mentioned is often used to refer\nto state action values\nq learning is an algorithm that can\nlearn state action values and then the\ndqn algorithm is an algorithm that uses\nq-learning in combination with deep\nneural networks to learn these entire\ngames\nthis falls on the model 3 prediction and\ncontrol because no explicit model of the\nenvironment is learned in that algorithm\nwe will also talk about policy gradient\nmethods we in fact already touch upon\nthem in the next lecture but we'll talk\nabout them more later and these are\nmethods that can use be used to learn\npolicies directly without necessarily\nusing a value function but we also\ndiscuss actor critic algorithms in which\nyou have both an explicit policy network\nor function and you have an explicit\nvalue function\nand this brings us also to deep\nreinforcement training because as i\nmentioned these functions are often\nrepresented these days with deep neural\nnetworks that's not the only choice they\ncould also be linear or it could be\nsomething else\nbut\nit's a popular\nchoice for a reason and it works really\nwell and we'll discuss that at some\nlength later in this course\nand also we will talk about how to\nintegrate learning and planning i talked\na little bit about planning being an\ninternal computation process and then\nlearning meaning the process that takes\na new experience and learns from that\nand of course we could have both of\nthose happening at the same time in an\nagent and then we want them to play\nnicely together\nand there will be much more there will\nbe other topics that we'll touch upon\nwhen we go through all of this\nokay\nnow finally i want to show you one final\nexample of a reinforcement writing\nproblem\nagain what we'll see here is a little\nbit more of a complicated example\nso what we'll see is a system which was\nlearned to\ncontrol the body so you can see the body\nalready here on the still\ni'll press play in a moment and what\nwill happen is is that there's an\nalgorithm that controls the uh basically\nthe forces of these uh body parts so\nthis agent specifically it can run right\nand it had to learn itself how to move\nits limbs in such a way as to produce\nforward motion the reward was a very\nsimple one\nthe reward was just go in that one\ndirection\nand you'll get plus one or you get\npositive reward\nbasically proportional to how fast you\ngo in that direction so it really wants\nto go really fast in one direction it\nwas not told how to do that so it\ndoesn't at first when it starts swelling\nit doesn't know how to control its limbs\nit just knows that it\nthat it's\nit perceives the world in some sense by\nby sensors which i won't go into that\nmuch depth it's too not too important\nhere but the point is it doesn't know\nhow to move its limbs it has to figure\nthat out by itself and it just notices\nthat when it moves in certain ways it\ngets more rewarding in other ways\ndoing that\nyou get the following behavior\nwith simplified vision as it says on the\nslide and proprioception which means it\ncan it can feel essentially where its\nown limbs are in some sense\nand then it can traverse through this\nvery complicated uh domain and it can\nlearn how to jump over things and how to\nmaybe even climb in some sense\njust because it wants to go to the right\nnot everything is easy but it does\nmanage to get there\nnow interestingly\nby using this setting by just having a\nsimple reward you can traverse different\ntypes of domains you can learn to\nterrain sorry you can learn to traverse\ndifferent types of terrains and do this\nin very non-trivial ways\nit would be very hard to specify a\npolicy by hand that does this and in\nfact because we have a learning system\nit's not just that we don't have to\nspecify a thing by hand but we can also\napply the exact same learning system to\ndifferent body types\nso this was learned with the exact same\nsystem\nthat was used for this other thing and\nyou can use this in two dimensions or\nyou can use it in three dimensions\nand in each of these cases the agent can\nlearn\nby interaction how to actually scale\nthese obstacles\nso the reward is particularly simple we\ndidn't have to think about how to move\nthe limbs in order to do that we can\njust have the learning system come up\nwith that and that's the important point\nhere\nand you can apply some more difficult uh\nterrains you can apply this to different\nbody types you can get quite non-trivial\nbehavior in doing so\nokay so that brings us to the end of\nthis lecture\nthank you for paying attention\nand\nwe'll see you in the next lecture which\nwill be on the topic of expiration",
  "abstract": null,
  "url": "https://www.youtube.com/watch?v=TCCjZe0y4Qc"
}